\documentclass{report}

\usepackage{commath}

%\usepackage[french]{babel}
%\usepackage{palatino, eulervm}  % commenter pour rétablir la police usuelle
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fullpage}
\usepackage{hyperref}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}

\usepackage[parfill]{parskip}

\DeclareMathOperator{\Vect}{Vect}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\supp}{supp}

\newcommand{\C}{{\mathbb C}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\scpr}[2]{\left\langle#1, #2\right\rangle}
\newcommand{\tq}{\text{ t.q. }}
\newcommand{\st}{\tq}
\newcommand{\pinfty}{{+\infty}}
\newcommand{\minfty}{{-\infty}}
\newcommand{\intint}[2]{{[[#1, #2]]}}
\newcommand{\cste}{\text{c}^{\text{ste}}}
\newcommand{\dx}{\dif x}
\newcommand{\Id}{\mathrm {Id}}

\newcommand{\restr}[2]{\left.#1\vphantom{\big|}\right|_{#2}}

\newcommand{\TODO}{TODO}
\newcommand{\unic}{{\underline {\textbf{unicité}~:}} }
\newcommand{\exis}{{\underline {\textbf{existence}~:}} }

\newtheorem{thm}{Théorème}[chapter]
\newtheorem{prp}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollaire}
\newtheorem{lem}[thm]{Lemme}
%\addto\captionsfrench{\renewcommand\proofname{\underline{Démonstration}}}
\theoremstyle{definition}
\newtheorem{déf}[thm]{Définition}
\theoremstyle{remark}
\newtheorem*{rmq}{Remarque}
\newtheorem{ex}{Exemple}[chapter]

\title{Espaces fonctionnels et séries de Fourier --- Notes de cours de Pr. P. Godin}
\author{Robin Petit}
\date{Année académique 2017-2018}

\begin{document}

\pagenumbering{Roman}
\maketitle
\tableofcontents
\setcounter{page}{1}
\pagenumbering{arabic}

\chapter{Transformation de Fourier}

\section{Définitions}

On considère $\R^n$ à $n$ fixé en tant qu'espace de mesure $(\R^n, \mathcal M, \lambda)$ avec $\mathcal M$ la famille des ensembles Lebesgue-mesurables et $\lambda$
la mesure de Lebesgue sur $\R^n$.

\begin{déf} Pour $u \in L^1(\R^n)$, on définit sa \textit{transformée de Fourier} par~:
\begin{equation}
	\hat u : \R^n \to \R : \xi \mapsto \int e^{-i\scpr x\xi}u(x)\dif x.
\end{equation}
\end{déf}

Cette fonction est bien définie car $x \mapsto e^{-i\scpr x\xi}$ est continue (et donc $L^\infty$), et $u$ est intégrable, donc $x \mapsto e^{-i\scpr x\xi}u(x)$ est intégrable
par Hölder.

\begin{prp} Pour $u \in L^1$, $\hat u$ est continue.
\end{prp}

\begin{proof} Soient $\xi_0 \in \R^n$ et $(h_k)_{k \in \N} \in {\R^n}^\N \tq h_k \xrightarrow[k \to \pinfty]{} 0$.
\[\hat f(\xi_0 + h_k) = \int e^{-i\scpr x{\xi_0}}e^{-i\scpr x{h_k}}f(x)\dif x.\]

Puisque $\abs {e^{-i\scpr x{\xi_0}}e^{-i\scpr x{h_k}}f(x)} = \abs {f(x)}$ et $x \mapsto e^{-i\scpr x{\xi_0}}e^{-i\scpr x{h_k}}f(x)$ converge partout
(en particulier presque partout), par le théorème de la convergence dominée~:
\[\hat f(\xi_0 + h_k) \xrightarrow[k \to \pinfty]{} \int e^{-i\scpr x{\xi_0}}f(x)\dif x = \hat f(\xi_0).\]
\end{proof}

\begin{prp} Soit $u \in L^1$. Si $\forall j \in \intint 1n : x_jf \in L^1$, alors~: $\hat u \in C^1$ et~:
\begin{equation}\label{eq:pd transformée Fourier}
	\forall j \in \intint 1n : \pd {\hat u}{\xi_j}\sVert[3]_\xi = \int e^{-i\scpr x\xi}(-ix_j)u(x)\dif x.
\end{equation}
\end{prp}

\begin{proof} Soit $(h_k)_{k \in \N} \in \R^\N \tq h_k \xrightarrow[k \to \pinfty]{} 0$, et prenons $\{e_j\}_{j=1}^n$ la base canonique de $\R^n$.
\[\frac {\hat u(\xi + h_ke_j) - \hat u(\xi)}{h_k} = \int e^{-i\scpr x\xi}\underbrace {\frac {e^{-ih_kx_j}-1}{h_k}}_{\xrightarrow[k \to \pinfty]{} -ix_j}u(x)\dif x.\]

En module~:
\[\abs {e^{-i\scpr x\xi}\frac {e^{-ih_kx_j}-1}{h_k}f(x)} = \abs {e^{-i\scpr x\xi}}\abs {\frac {e^{-ih_kx_j}-1}{h_k}}\abs {f(x)} \leq C\abs {x_j}\abs {f(x)},\]
qui est intégrable par hypothèse.

En effet, si $x_j = 0$, alors tout est nul et l'inégalité devient une égalité~; et si $x_j \neq 0$, alors $\frac {\abs {e^{-ih_kx_j}-1}}{\abs {x_jh_k}}$ est borné.

Dès lors, par le théorème de convergence dominée, la limite passe sous l'intégrale et on a~\eqref{eq:pd transformée Fourier}.
\end{proof}

\begin{cor} Pour $m \in \N^*$, si $(1+\abs x)^mu \in L^1$, alors $u \in C^m$ et on peut dériver $m$ fois sous le signe:
\[\partial^\alpha\hat u(\xi) = \int e^{-i\scpr x\xi}(-i)^{\abs \alpha}x^\alpha u(x)\dif x\]
\end{cor}

\begin{proof} Exercice (récurrence sur $m$).
\end{proof}

\begin{déf} On définit l'ensemble de Schwartz~:
\begin{equation}\begin{aligned}
	\mathcal S(\R^n) \coloneqq &\left\{u \in C^\infty(\R^n) \tq \forall \alpha, \beta \in \N^n : x^\alpha\partial^\beta u \text{ est borné dans } \R^n\right\} \\
		= &\left\{u \in C^\infty(\R^n) \tq \forall \alpha, \beta \in \N^n : x^\alpha\partial^\beta u \in L^\infty(\R^n)\right\}.
\end{aligned}\end{equation}
\end{déf}

Dans l'idée, $\mathcal S$ est l'ensemble dont toutes les dérivées décroissent plus vite vers 0 que tout polynôme.

\begin{prp} $\mathcal S(\R^n)$ est un $\R$-espace vectoriel.
\end{prp}

\begin{proof} Immédiat par le fait que $C^\infty(\R^n)$ et $L^\infty(\R^n)$ sont des $\R$-evs.
\end{proof}

\begin{prp} Si $C^\infty_0(\R^n) = C^\infty_c(\R^n)$ désigne l'ensemble des fonctions $C^\infty$ à support compact, alors~:
\[C^\infty_0(\R^n) \subseteq \mathcal S(\R^n) \subseteq \bigcap_{1 \leq p \leq \pinfty} L^p(\R^n).\]
\end{prp}

\begin{proof}~
\begin{itemize}
	\item[$(i)$] Pour $u \in \mathcal S$ et $\pinfty > p \geq 1$~:
	\[\int \abs u^p\dif x = \int \Big(\underbrace{\abs u(1+\abs x)^N}_{\text{borné pour tout $N$}}\Big)^p(1+\abs x)^{-Np}\dif x
		\leq \int (C_N)^p\underbrace {(1+\abs x)^{-Np}}_{\text{intégrable pour $Np > n$}}\dif x.\]

	Dès lors, pour $N$ suffisamment grand ($Np>n$), on a $\int \abs u\dif x \leq \cste$

	Le cas $p = \pinfty$ vient uniquement du fait que pour $u \in \mathcal S$, pour $\alpha = \beta = (0, \ldots, 0) \in \N^n$:
	\[u = x^\alpha\partial^\beta u \in L^\infty.\]

	\item[$(ii)$] Soit $u \in C^\infty_0(\R^n)$. Pour $\alpha, \beta \in \N^n$~: $x^\alpha\partial^\beta u \neq 0 \subseteq \supp u$ compact. Par Heine-Cantor,
	$u$ est uniformément continue sur $\supp u$.
\end{itemize}
\end{proof}

\begin{prp} Pour $u \in \mathcal S$ et $\alpha, \beta \in \N^n$, alors~: $x^\alpha\partial^\beta u \in \mathcal S$.
\end{prp}

\begin{proof} Soient $\lambda, \mu \in \N^n$. Par Leibniz~:
\[\partial^\mu(fg) = \sum_{\sigma \leq \mu}\binom \mu\sigma\partial^\sigma f \partial^{\mu-\sigma} g.\]

Donc~:
\[x^\lambda\partial^\mu(x^\alpha\partial^\beta u) = \sum_{\sigma \leq \mu}\binom \mu\sigma x^\lambda\partial^\sigma(x^\alpha)\partial^{\mu-\sigma}u,\]
où $x^\lambda\partial^\sigma(x^\alpha) \leq \cste x^\gamma$. On en déduit que $x^\lambda\partial^\mu(x^\alpha\partial^\beta u)$ est une somme finie de termes
bornés et est donc bornée.
\end{proof}

À défaut de définir une topologie sur $\mathcal S(\R^n)$, on définit uniquement une notion de convergence.

\begin{déf} Soit $(u_k)_{k \in \N} \in \mathcal S^\N$, $u \in \mathcal S$, on dit que $u_k$ converge vers $u$ lorsque $k \to \pinfty$
(noté $u_k \xrightarrow[k \to \pinfty]{\mathcal S} u$) lorsque~:
\begin{equation}
	\forall \alpha, \beta \in \N^n : \sup_{x \in \R^n}\abs {x^\alpha\partial^\beta(u-u_k)} \xrightarrow[k \to \pinfty]{} 0.
\end{equation}
\end{déf}

\begin{thm} Soit $u \in \mathcal S$. Alors~:
\begin{enumerate}
	\item $\hat u \in \mathcal S$. De plus si $u_k \xrightarrow[k \to \pinfty]{\mathcal S} u$, alors $\hat {u_k} \xrightarrow[k \to \pinfty]{\mathcal S} \hat u$.
	\item $\widehat {D_ju}(\xi) = \xi_j\hat u(\xi)\quad$ (de plus $\widehat {x_ju} = D_j \hat u$).
\end{enumerate}
\end{thm}

\begin{proof} Pour le premier point, on calcule~:
\[D^\alpha_\xi\hat u(\xi) = \int D^\alpha_\xi(e^{-i\scpr x\xi})u(x)\dif x = \int e^{-i\scpr x\xi}(-x)^\alpha u(x)\dif x.\]

Donc~:
\[\xi^\beta D^\alpha_\xi\hat u(\xi) = \int \xi^\beta e^{-i\scpr x\xi}(-x)^\alpha u(x)\dif x = \int (-D_x)^\beta(e^{-i\scpr x\xi})(-x)^\alpha u(x)\dif x
	= \int e^{-i\scpr x\xi} D^\beta_x\left((-x)^\alpha u(x)\right)\dif x.\]

Pour montrer cette dernière égalité, intégrons par partie. D'abord observons pour $\phi \in \mathcal S$ et $j \in \intint 1n$~:
\[\int \partial_j\phi\dx = \int\ldots\int\left(\int \partial_j\phi\dx_j\right)\dx_1\ldots\dx_{j-1}\dx_{j+1}\ldots\dx_n.\]
Or~:
\[\int \partial_j\phi\dx_j = \lim_{N \to \pinfty}\int_{-N}^N\partial_j\phi(x)\dx_j
	= \lim_{N \to \pinfty}\left(\underbrace {\phi(x_1, \ldots, x_{j-1}, N, x_{j+1}, \ldots, x_n)}_{\xrightarrow[N \to \pinfty]{} 0} - \underbrace {\phi(x_1, \ldots, x_{j-1}, N, x_{j+1}, \ldots, x_n)}_{\xrightarrow[N \to \pinfty]{} 0}\right) = 0,\]
puisque $\phi \in \mathcal S$.

On en déduit donc que $\int\partial_j\phi\dx = 0$.

Dès lors, puisque $e^{-i\scpr x\xi}(-x)^\alpha u(x) \in \mathcal S$ et par récurrence~:
\[\int (-D_x)^\beta(e^{-i\scpr x\xi})(-x)^\alpha u(x)\dif x	= \int e^{-i\scpr x\xi} D^\beta_x\left((-x)^\alpha u(x)\right)\dif x.\]

Montrons alors que $\forall N \in \N : \exists C_N \geq 0 \tq \abs {D_x^\beta((-x)^\alpha u(x))} \leq C_N(1 + \abs x)^{-N}$. Par Leibniz~:
\[(1 + \abs x)^N\partial^\beta (x^\alpha u(x)) = (1 + \abs x)^N\sum_{\gamma \leq \beta} \binom \beta\gamma\partial^\gamma x^\alpha\partial^{\beta-\gamma}u(x)\]
est borné car $u \in \mathcal S$. Dès lors~:
\[\abs {\xi^\beta D_\xi^\alpha \hat u(x)} \leq C_N\int(1 + \abs x)^{-N}\dx.\]

Pour $N$ suffisamment grand ($N > n$), on a $\abs {\xi^\beta D_\xi^\alpha \hat u(x)} \leq \cste$.

Dès lors, on trouve~:
\[\abs {\xi^\beta D_\xi^\alpha\hat u(x)} \leq \sup_{x \in \R^n}(1 + \abs x)^{-N}\abs {D_x^\beta((-x)^\alpha(u - u_k))} \xrightarrow[k \to \pinfty]{} 0.\]
On en déduit donc $\hat {u_k} \xrightarrow[k \to \pinfty]{\mathcal S} \hat u$.

Pour le second point, la seconde formule découle directement du premier pour $\alpha = e_j$~:
\[D_j\hat u(\xi) = \int e^{-i\scpr x\xi}(-x_j)u(x)\dx = \widehat {x_ju}(\xi).\]

La première égalité se démontre par~:
\[\widehat {D_ju}(\xi) = \int e^{-i\scpr x\xi} D_ju(x)\dx = -\int D_{x,j}\left(e^{-i\scpr x\xi}\right)u(x)\dx = \xi_j\int e^{-i\scpr x\xi}u(x)\dx = \xi_j\hat u(\xi).\]
\end{proof}

\section{Formule d'inversion}

\begin{thm}\label{thm:inversion Fourier} Soit $u \in \mathcal S$. Alors~:
\begin{equation}\label{eq:inversion Fourier}
	u(x) = (2\pi)^{-n}\int e^{i\scpr x\xi}\hat u(\xi)\dif\xi.
\end{equation}
\end{thm}

La fonction $(y, \xi) \mapsto e^{i\scpr x\xi}e^{-i\scpr y\xi}u(y)$ n'est pas intégrable pour $(y, \xi)$. On ne va donc pas pouvoir appliquer Fubini.

\begin{proof} Pour $\chi \in \mathcal S$, $(y, \xi) \mapsto e^{-i\scpr y\xi}e^{i\scpr x\xi}\chi(\xi)u(y)$ est intégrable. Donc par Fubini~:
\[\int e^{i\scpr x\xi}\chi(\xi)\hat u(\xi)\dif\xi = \int e^{i\scpr x\xi}\chi(\xi)\int e^{-i\scpr y\xi}u(y)\dif y\dif\xi
	= \int u(y)\int e^{-i\scpr {y-x}\xi}\chi(\xi)\dif\xi \dif y = \int u(y)\hat \chi(y-x)\dif y.\]

Pour $\psi \in \mathcal S$, $\delta > 0$ tels que $\chi(\xi) = \psi(\delta\xi)$~:
\[\hat \chi(\xi) = \int e^{-i\scpr y\xi}\psi(\delta\xi)\dif\xi = \delta^{-n}\hat \psi(\xi/\delta).\]

Alors~:
\[\int e^{i\scpr x\xi}\psi(\delta\xi)\hat u(\xi)\dif\xi = \int u(x+y)\delta^{-n}\psi(y/\delta)\dif y = \int u(x+\delta y)\hat \psi(y)\dif y.\]

Par le théorème de convergence dominée~:
\[\int u(x+\delta y)\hat \psi(y)\dif y \xrightarrow[\delta \to \pinfty]{} u(x)\int \hat \psi(y)\dif y,\]
or~:
\[\int e^{i\scpr x\xi}\psi(\delta \xi)\hat u(\xi)\dif\xi \xrightarrow[\delta \to \pinfty]{} \psi(0)\int e^{i\scpr x\xi}\hat u(\xi)\dif\xi.\]

Par unicité de la limite, si $\int \hat \psi \dif y\neq 0$~:
\[u(x) = \frac {\psi(0)}{\int\hat \psi(y)\dif y}\int e^{i\scpr x\xi}\hat u(\xi)\dif\xi\]

Dans le cas $n=1$, on prend $\psi_1 : x \mapsto e^{-x^2/2}$. En intégrant $z \mapsto e^{-z^2/2}$ sur un chemin rectangulaire $[a,b,c,d] \subset \C$, on trouve~:
\[\int_a^b e^{-x^2/2}\dx + \int_b^c e^{-z^2/2}\dif z + \int_c^d e^{-z^2/2}\dif z + \int_d^a e^{-z^2/2}\dif z = 0\]
par Cauchy. Pour $(a, b) \to (\minfty, \pinfty)$, on trouve que $\int_b^c e^{-z^2/2}$ et $\int_d^a e^{-z^2/2}$ tendent vers $0$. Donc à la limite~:
\[\int_a^b e^{-x^2/2}\dx = \int_{\Im z = t}e^{-z^2/2}\dif z = \int e^{(x^2-t^2)/2}e^{-itx}\dx.\]

Donc $\hat \psi(t) = \psi(t)\int\psi\dx$. On en déduit~:
\[\int \hat \psi(t)\dif t = \left(\int \psi(x)\dx\right)^2 = \left(\int e^{-x^2/2}\right)^2 = 2\pi.\]

Dès lors $\psi(0) = 1$ et $\int\hat\psi\dx = 2\pi$, qui donne bien la formule.

Dans le cas général $n > 1$, on prend $\psi(x) = e^{-\abs x^2/2} = \prod_{j=1}^n\psi_1(x_j)$. Donc~:
\[\hat\psi(\xi) = \int e^{-i\scpr x\xi}\psi(x)\dx = \int e^{-i\scpr x\xi}\prod_{j=1}^ne^{-x_j^2/2}\dx = \int\prod_{j=1}^ne^{-ix_j\xi_j}\prod_{j=1}^ne^{-x_j^2/2}\dx
	= \int\prod_{j=1}^n\left(e^{-ix_j\xi_j}e^{-x_k^2/2}\right)\dx.\]

Par Fubini~:
\[\hat\psi(\xi) = \prod_{j=1}^n\int e^{-ix_j\xi_j}e^{-x_j^2/2}\dx = \prod_{j=1}^n\hat \psi_1(\xi_j).\]

On trouve alors~:
\[\int \hat \psi(\xi)\dif\xi = \int \prod_{j=1}^n\hat\psi_j(\xi_j)\dif\xi = \prod_{j=1}^n\int\hat \psi_1(\xi_j)\dif\xi_j = (2\pi)^{-n},\]
où l'avant dernière égalité s'obtient en appliquant Fubini.

Puisque $\hat \psi(0) = 1$, on a bien~\eqref{eq:inversion Fourier}.
\end{proof}

On définit une application \textit{transformée de Fourier} $\mathcal F : \mathcal S \to \mathcal S : u \mapsto \mathcal Fu \coloneqq \hat u$.

\begin{prp} $\mathcal F$ est une bijection linéaire.
\end{prp}

\begin{proof} Par la formule d'inversion, $\mathcal F$ est injective~: si $\mathcal Fu = 0$, alors $u = 0$.

De plus, $\mathcal F$ est surjective. Pour $f \in \mathcal S$, montrons qu'il existe $u \in \mathcal S \tq \mathcal Fu = f$.
Prenons $u(x) = (2\pi)^{-n}\int e^{i\scpr x\xi}f(\xi)\dif\xi$. Alors~:
\[\mathcal Ff(x) = \int e^{-i\scpr x\xi}f(\xi)\dif\xi = (2\pi)^nu(-x).\]

De plus~:
\[\hat {\hat u}(\xi) = \int e^{-i\scpr x\xi}\hat u(x)\dx = (2\pi)^n(2\pi)^{-n}\int e^{-i\scpr x\xi}\hat u(x)\dx = (2\pi)^nu(-\xi),\]
donc $\hat f = \hat {\hat u}$ pour tout $x$, et puisque $\mathcal F$ est injective, $f = \hat u$. Donc $\mathcal F$ est surjective, et donc surjective.

La linéarité est triviale~:
\[\mathcal F(f+\lambda g)(\xi) = \int e^{-i\scpr x\xi}(f+\lambda g)(x)\dx = \int e^{-i\scpr x\xi}f(x)\dx + \lambda\int e^{-i\scpr x\xi}g(x)\dx
	= \left(\hat f + \lambda \hat g\right)(\xi).\]
\end{proof}

En posant $\tilde {\mathcal F} : \mathcal S \to \mathcal S : u \mapsto \tilde {\mathcal F}u$ où $\tilde {\mathcal F}u(x) = (2\pi)^{-n}\int e^{i\scpr x\xi}u(\xi)\dif\xi$.
Par un raisonnement similaire à la Proposition précédente, on trouve $\tilde {\mathcal F}$ est une bijection linéaire. De plus $\mathcal F^{-1} = \tilde {\mathcal F}$.

De plus, puisque $\mathcal F$ transforme des suites convergentes en suites convergentes sur $\mathcal S$, $\tilde {\mathcal F}$ fait de même.

Cela veut dire que $\mathcal F$ est une homéomorphisme linéaire de $\mathcal S$ dans $\mathcal S$ pour la topologie non définie ici.

\begin{prp} Pour $u, v \in \mathcal S$~:
\begin{enumerate}
	\item $\int u\hat v = \int\hat uv$~;
	\item $\int u\overline v = (2\pi)^{-n}\int \hat u\hat {\overline v}$. Cette égalité est appelée \textit{identité de Parseval}.
\end{enumerate}
\end{prp}

\begin{proof} Le premier point se montre par la formule de la preuve du Théorème~\ref{thm:inversion Fourier} pour $u,\chi \in \mathcal S$~:
\[\int e^{i\scpr x\xi}\chi(\xi)\hat u(\xi)\dif\xi = \int u(x+y)\hat \chi(y)\dif y\]
en $x=0$.

Le second point, prenons $u, w \in \mathcal S$ et posons $v \coloneqq (2\pi)^{-n}\overline {\hat w}$. Par le premier point~:
\[\int \hat uv = \int u\hat v = \int u(2\pi)^{-n}\hat {\overline {\hat w}}.\]

On peut voir que~:
\[(2\pi)^{-n}\hat {\overline {\hat w}}(\xi) = (2\pi)^{-n}\int e^{-i\scpr x\xi}\overline {\hat w}(x)\dx =
	(2\pi)^{-n}\overline {\int e^{i\scpr x\xi}\hat w(x)\dx} = \overline {w(\xi)}.\]

Dès lors~:
\[\int \hat u(2\pi)^{-n}\overline {\hat w} = \int u\overline w.\]
\end{proof}

\begin{cor}[Formule de Plancherel] Pour $u \in \mathcal S$, on a~:
\begin{equation}\label{eq:Plancherel}
	\int \abs u^2\dx = (2\pi)^{-n}\int\abs {\hat u}^2\dif\xi
\end{equation}
\end{cor}

\section{Discussion sur la définition de la transformée}

On peut définir la transformée de Fourier de plusieurs manières, paramétrisé par $a, b \in \R$~:
\[\mathcal F_{a,b}u(\xi) = a\int e^{-ib\scpr x\xi}u(x)\dx.\]

La théorie reste la même à homothétie près puisque~:
\[\mathcal Fu(\xi) = \frac 1a\mathcal F_{a,b}u(\xi/b).\]

\[(2\pi)^{-n}\int \hat u(\xi)\overline {\hat v}(\xi)\dif\xi = \frac {(2\pi)^{-n}b^n}{a^n}\int\mathcal F_{a,b}u(\eta)\overline {\mathcal F_{a,b}v(\eta)}\dif\eta.\]
Donc on peut choisir $a=1$ et $b=2\pi$ ou encore $a=(2\pi)^{n/2}$ et $b=1$ afin de simplifier la formule de Parseval qui devient~:
\[\int u\overline v = \int \mathcal Fu\overline {\mathcal Fv}.\]

Cependant le choix $a=b=1$ permet de ne pas avoir de terme $b^k$ lors des dérivations sous le signe intégral.

\section{Extension de la transformée à $L^1 \cap L^2$}

\begin{prp} Il existe une unique application linéaire continue $\mathbb F : L^2 \to L^2$ tel que $\mathbb F\sVert[2]_{\mathcal S} = \mathcal F$ et~:
\[\int u\overline v = (2\pi)^{-n}\int \mathbb Fu\overline {\mathbb Fv}\dif\xi,\]
i.e. $\mathbb F$ préserve l'identité de Parseval.
\end{prp}

\begin{proof} Admettons que $C_0^\infty(\R^n)$ est dense dans $L^2(\R^n)$. Puisque $C_0^\infty(\R^n) \subseteq \mathcal S(\R^n)$, on a $\mathcal S(\R^n)$ dense dans $L^2(\R^n)$.
Par cette densité, pour $u \in \mathcal S$, il existe $(u_k)_{k \in \N} \in \mathcal S^\N$ tel que $u_k \xrightarrow[k \to \pinfty]{L^2} u$, et donc $(u_k)$ est de Cauchy
pour cette norme. $(\hat {u_k})_{k \in \N}$ est également de Cauchy car~:
\[\norm {\hat {u_k} - \hat {u_m}} = (2\pi)^{n/2}\norm {u_k - u_m}.\]

Par cette complétude, il existe $z \in \mathcal S \tq \hat {u_k} \xrightarrow[k \to \pinfty]{L^2} z$. On pose alors $\mathbb Fu \coloneqq z$. Montrons que $z$ ne dépend pas
de la suite $(\hat {u_k})$ choisie pour montrer que $\mathbb F$ est bien définie.

Soit $(v_k)_{k \in \N} \tq v_k \xrightarrow[k \to \pinfty]{L^2} z$. Alors $v_k-u_k \xrightarrow[k \to \pinfty]{L^2} 0$. Par Plancherel,
$\hat {v_k}-\hat {u_k} \xrightarrow[k \to \pinfty]{L^2} 0$. Dès lors $\hat {v_k} \xrightarrow[k \to \pinfty]{L^2} z$.

Montrons alors que $\mathbb F$ est linéaire.

Soient $u, v \in L^2$. Soient $(u_k)_{k \in \N}, (v_k)_{k \in \N} \in \mathcal S^\N$ telles que $u_k \xrightarrow[k \to \pinfty]{L^2} u$ et $v_k \xrightarrow[k \to \pinfty]{L^2} v$.
Alors $u_k+v_k \xrightarrow[k \to \pinfty]{L^2} u+v$. Par linéarité de $\mathcal F$, $\hat {u_k} + \hat {v_k} = \widehat {u_k + v_k} \xrightarrow[k \to \pinfty]{L^2} \mathbb F(u+v)$.

Donc $\hat {u_k} + \hat {v_k} \xrightarrow[k \to \pinfty]{L^2} \mathbb Fu + \mathbb Fv$ et $\hat {u_k} + \hat {v_k} \xrightarrow[k \to \pinfty]{L^2} \mathbb F(u+v)$.
On en déduit $\mathbb F u + \mathbb F v = \mathbb F(u+v)$. Il est également trivial que pour $\lambda \in \R : \mathbb F(\lambda u) = \lambda \mathbb Fu$.

Pour montrer que $\mathbb F\sVert[2]_{\mathcal S} = \mathcal F$, prenons $u \in \mathcal S$, et la suite $(u_k)_{k \in \N}$ constante $u_k = u$. Par définition de $\mathbb F$,
on a $\mathbb Fu = \hat u$ car $\forall k \in \intint 1n : \hat {u_k} = \hat u$, donc $\hat {u_k} \xrightarrow[k \to \pinfty]{} \hat u$.

Montrons finalement que $\mathbb F$  vérifie Parseval.

Premier cas~: $u \in L^2$ et $v \in \mathcal S$. Il existe $\mathcal S^\N \ni (u_k)_{k \in \N} \xrightarrow[k \to \pinfty]{L^2} u$. Donc~:
\[\int u\overline v = \int u_k\overline v + \int (u-u_k)\overline v.\]

Puisque~:
\[\abs {\int (u-u_k)\overline v} \leq \underbrace {\norm {u-u_k}_{L^2}}_{\xrightarrow[k \to \pinfty]{} 0} \norm v \xrightarrow[k \to \pinfty]{} 0,\]
on sait~:
\[\int u_k\overline v \xrightarrow[k \to \pinfty]{} \int u\overline v.\]

Or $u_k, v \in \mathcal S$. Donc pour $k \to \pinfty$, par Cauchy-Schwarz et par Parseval pour $\mathcal F$~:
\[\int u\overline v = \int u_k\overline v \xrightarrow[k \to \pinfty]{L^2} (2\pi)^{-n}\int\mathbb Fu\overline {\hat v}.\]

Dans le cas général $u, v \in L^2$, par le premier point pour $(v_k)_{k \in \N} \tq v_k \xrightarrow[k \to \pinfty]{} v$~:
\[\int u\overline {v_k} = (2\pi)^{-n}\int \mathbb Fu\overline {\hat {v_k}}.\]

Or $v_k \xrightarrow[k \to \pinfty]{L^2} \mathbb Fv$. Par Cauchy-Schwarz, on a~:
\begin{enumerate}
	\item $\int u\overline {v_k} \xrightarrow[k \to \pinfty]{L^2} \int u\overline v$~;
	\item et $\int \mathbb Fu\overline {\hat {v_k}} \xrightarrow[k \to \pinfty]{L^2} \int \mathbb Fu\overline {\mathbb Fv}$.
\end{enumerate}

L'identité de Parseval est donc bien vérifiée pour $\mathbb F$. Il reste à vérifier que $\mathbb F$ est continue et qu'elle est unique.

La continuité découle de Parseval~:
\[\norm u_{L^2} = (2\pi)^{-n/2}\norm {\mathbb Fu}_{L^2},\]
donc pour $\varepsilon > 0$, pour $\delta = (2\pi)^{-n/2}\varepsilon$, on a que si $\norm {u-v}_{L^2} < \delta$, alors $\norm {\mathbb Fu - \mathbb Fv}_{L^2} < \varepsilon$.

Si il existe $\mathbb F_1 : L^2 \to L^2$ continue et linéaire telle que $\mathbb F_1\sVert[2]_{\mathcal S} = \mathcal F$, alors par densité, pour $u \in L^2$, il existe
$(u_k)_{k \in \N} \in \mathcal S^\N \tq u_k \xrightarrow[k \to \pinfty]{} u$, et donc, par continuité~:
\[\underbrace {\mathbb F(u_k)}_{\xrightarrow[k \to \pinfty]{} \mathbb F(u)} = \underbrace {\mathbb F_1(u_k)}_{\xrightarrow[k \to \pinfty]{} \mathbb F_1(u)}.\]

Donc puisque deux application continues qui coïncident sur une sous-ensemble dense coïncident partout, on a bien que $\mathbb F = \mathbb F_1$.
\end{proof}

De la même manière, $\tilde {\mathcal F}$ se prolonge sur $L^2$ en $\tilde {\mathbb F}$

\begin{prp} $\mathbb F \circ \tilde {\mathbb F} = \Id_{L^2} = \tilde {\mathbb F} \circ \mathbb F$.
\end{prp}

\begin{proof} Ceci vient directement de la même propriété sur $\mathcal F$ et $\tilde {\mathbb F}$. Soit $u \in L^2$ et soit
$\mathcal S^\N \ni (u_k)_{k \in \N} \xrightarrow[k \to \pinfty]{L^2} u$. On sait~:
\[\mathcal S \ni \tilde {\mathcal F}(u_k) = \tilde {\mathbb F}(u_k) \xrightarrow[k \to \pinfty]{L^2} \tilde {\mathbb F}(u)\]
par continuité de $\tilde {\mathbb F}$. Par continuité de $\mathbb F$, on a~:
\[\mathcal F \circ \tilde {\mathcal F}(u_k) = \mathbb F \circ \tilde {\mathbb F}(u_k) \xrightarrow[k \to \pinfty]{L^2} \mathbb F \circ \tilde {\mathbb F}(u).\]

Or $\mathcal F \circ \tilde {\mathcal F}(u_k) = u_k \xrightarrow[k \to \pinfty]{L^2} u$. Par unicité de la limite, on a $\mathbb F \circ \tilde {\mathbb F}(u)$.
L'autre égalité se démontre de la même manière.
\end{proof}

À ce stade, il est légitime de se demander si les définitions que l'on a sur $L^1$ (la formule intégrale définie depuis $\mathcal S$) et sur $L^2$ (la définition de $\mathbb F$)
sont compatibles, i.e. si pour $u \in L^1 \cap L^2$ on a bien $\hat u = \mathbb Fu$. Cette égalité tient bien (démonstration à venir).

\section{Exemple d'application de la théorie de Fourier}

Pour $\Delta = \sum_{j=1}^n\partial_j^2$ le Laplacien sur $\R^n$ et $f \in \mathcal S$, soit la PDE suivante~:
\begin{equation}
	(1+\sum_{j=1}^nD_j^2)u = u-\Delta u = f,
\end{equation}
ou plus généralement, pour des $a_\alpha \in \C$~:
\begin{equation}\label{eq:PDE générale Fourier}
	\underbrace {\sum_{\abs \alpha \leq m}a_\alpha D^\alpha}_{P(D) \text{ polynôme}} u = f,
\end{equation}
dans le cas du Laplacien, ce polynôme est $P(\xi) = 1+\abs \xi^2$.

Sous l'hypothèse $\inf_{\xi \in \R^n}\abs {P(\xi)} \gneqq 0$, trouvons $u \tq P(D)u = f$.

Formellement~:
\begin{align*}
	\widehat {P(D)u}(\xi) &= \hat f(\xi) \\
	P(\xi)\hat u(\xi) &= \hat f(\xi) \\
	\hat u(\xi) &= \frac {\hat f(\xi)}{P(\xi)} \\
	u(x) &= (2\pi)^{-n}\int e^{i\scpr x\xi}\frac {\hat f(\xi)}{P(\xi)}\dif\xi.
\end{align*}

Plus rigoureusement, puisque $f \in \mathcal S$, on sait $\hat f \in \mathcal S$. De plus, $P$ est borné par dessous. Donc $\abs {\hat f/P} \leq C_N(1+\abs \xi)^{-N}$,
et du coup la fonction sous l'intégrale ($\xi  \mapsto e^{i\scpr x\xi}\frac {\hat f(\xi)}{P(\xi)}$) est $L^1$, et cette intégrale est bien définie pour $N > n$.

De plus, puisque la dérivation selon $x$ sur $u$ fait juste descendre du $\xi$ de l'exponentielle, par récurrence avec le théorème de convergence dominée et par la borne supérieure
ci-dessus, on trouve que $u \in C^\infty(\R^n)$. On peut alors vérifier que la fonction $u$ ainsi trouvée est bien une solution de~\eqref{eq:PDE générale Fourier}~:
\[\sum_{\abs \alpha \leq m}a_\alpha D^\alpha u = (2\pi)^{-n}\int e^{i\scpr x\xi}\underbrace {\sum_{\abs \alpha \leq m}a_\alpha \xi^\alpha}_{=P(\xi)} \frac {\hat f(\xi)}{P(\xi)}\dif\xi
	= (2\pi)^{-n} \int e^{i\scpr x\xi}\hat f(\xi)\dif\xi = f(x).\]

\chapter{Espaces de Hilbert}

\begin{déf} Soit $H$ un $\C$-espace vectoriel. Un produit scalaire (forme hermitienne définie positive) sur $H$ est une application $\scpr \cdot\cdot : H \times H \to \C \tq$~:
\begin{itemize}
	\item[$(i)$]   à $y \in \C$ fixé~: $x \mapsto \scpr xy$ est une application linéaire de $H$ dans $\C$~;
	\item[$(ii)$]  pour $x, y \in \C$~: $\scpr xy = \overline {\scpr yx}$~;
	\item[$(iii)$] pour $x \in \C$~: $\scpr xx \geq 0$ où $\scpr xx = 0 \iff x = 0$.
\end{itemize}

Sur un produit scalaire, on peut définir une norme $\norm x \coloneqq \scpr xx^{1/2}$.
\end{déf}

\begin{rmq} Une forme hermitienne définie positive est donc anti-linéaire pour le 2e paramètre : $\scpr x{\lambda y} = \overline \lambda \scpr xy$.
\end{rmq}

\begin{prp} $\norm \cdot : H \to \R^+$ est une norme.
\end{prp}

\begin{prp} $\norm \cdot$ vérifie Cauchy-Schwarz, i.e.~:
\[\forall x, y \in H : \abs {\scpr xy} \leq \norm x\norm y.\]
\end{prp}

\begin{proof} Soit $\alpha \in \C \tq \abs \alpha = 1$ et $\alpha\scpr yx \in \R^+$ (i.e. $\alpha\scpr yx = \abs {\scpr xy}$). Soit $r \in \R$.

\begin{align*}
	0 &\leq \scpr {x-r\alpha y}{x-r\alpha y} = \scpr xx - r\alpha\scpr yx - r\overline \alpha\scpr xy + r^2\scpr yy \\
		&= \scpr xx - r\underbrace {\alpha \scpr yx}_{= \abs {\scpr yx}} - r\underbrace {\overline \alpha \scpr xy}_{= \abs {\scpr yx}} + r^2\underbrace {\alpha\overline\alpha}_{= \abs \alpha = 1} \scpr yy = A - 2Br + Cr^2,
\end{align*}
pour $A = \scpr xx \in \R^+$, $B = \alpha\scpr xy = \abs {\scpr xy} \in \R^+$, $C = \scpr yy \in \R^+$.

Si $C = 0$, alors $B = 0$, et donc $\scpr yx = 0$ et Cauchy-Schwarz est vérifié.

Si $C \gneqq 0$, alors pour $r = B/C$~: $0 \leq A - 2Br + Cr^2 = \frac {AC-B^2}C$, donc $B^2 \leq AC$, donc Cauchy-Schwarz est vérifié.
\end{proof}

\begin{prp} $\norm \cdot$ vérifie l'inégalité triangulaire, i.e.~:
\[\norm {x+y}^2 \leq \norm x^2 + \norm y^2.\]
\end{prp}

\begin{proof} $\norm {x+y}^2 = \scpr xx + \scpr xy + \scpr yx + \scpr yy \leq \norm x^2 + 2\norm x\norm y + \norm y^2 = (\norm x + \norm y)^2$.
\end{proof}

On a donc $(H, \norm \cdot)$ un e.v. normé, depuis lequel on peut alors définir une distance~: $d(x, y) \coloneqq \norm {x-y}$.

\begin{déf} Si $H$ est complet pour $d$, on dit que $H$ est un espace de Hilbert.
\end{déf}

Quelques exemples d'espaces de Hilbert~:
\begin{itemize}
	\item[(0)] $\C^n$ pour $\scpr xy \coloneqq \sum_{j=1}^nx_j\overline {y_j}$~;
	\item[(1)] Pour $(\Omega, \mathcal A, \mu)$ un espace de mesure, $L^2(\Omega, \mathcal A, \mu)$ muni du produit scalaire $\scpr fg \coloneqq \int f\overline g\dif\mu$~;
	\item[(2)] Pour $(\N, \mathcal P(\N), \#)$ comme espace de mesure, on a l'équivalent dénombrable de l'exemple (0)~:
	\[\scpr fg_{\ell^2} \coloneqq \int f\overline g\dif\# = \sum_{k \geq 1}f_k\overline {g_k}.\]
	On note $\ell^2(\N) \coloneqq L^2(\N, \mathcal P(\N), \#)$.
\end{itemize}

Un dernière exemple bien moins trivial~: les espaces de Sobolev.

\begin{déf} Soit $s \geq 0$ un paramètre, on définit l'espace de Sobolev d'ordre $s$ sur $\R^n$ par~:
\begin{equation}
	H^s(\R^n) \coloneqq \left\{u \in L^2(\R^n) \tq (2\pi)^{-n}\int \abs {\mathbb Fu(\xi)}^2(1+\abs \xi)^s\dif\xi \lneqq \pinfty\right\}.
\end{equation}

On y définit le produit scalaire suivante pour $u, v \in H^s(\R^n)$~:
\begin{equation}
	\scpr uv_s \coloneqq (2\pi)^{-n}\int\mathbb Fu\overline {\mathbb Fv}(1+\abs\xi)^s\dif\xi.
\end{equation}
\end{déf}

\begin{rmq} Remarquons que $u \in H^s \iff \xi \mapsto (1+\abs \xi^2)^{s/2}\mathbb Fu(\xi)$ est dans $L^2$.
\end{rmq}

% Montrer que cette intégrale est bien définie

\begin{prp} $(u, v) \mapsto \scpr uv_s$ est un produit scalaire.
\end{prp}

\begin{proof} À $v$ fixé, $u \mapsto \scpr uv_s$ est linéaire par linéarité de $\mathbb F$ et par linéarité de l'intégrale.

Soient $u, v \in H^s$.
\[\scpr uv_s = (2\pi)^{-n}\int \mathbb Fu\overline {\mathbb Fv}\underbrace {(1+\abs \xi)^s}_{\in \R^+}\dif\xi
	= (2\pi)^{-n}\overline {\int \mathbb Fv\overline {\mathbb Fu}(1+\abs \xi)^s\dif\xi} = \overline {\scpr vu_s}.\]

Finalement, pour $u \in H^s$~:
\[\scpr uu_s = (2\pi)^{-n}\int\underbrace {\abs {\mathbb Fu}^2}_{\geq 0}\underbrace {(1+\abs\xi)^s}_{\geq 0}\dif\xi \geq 0,\]
et de plus, il est évident que $\scpr uu_s = 0 \iff u = 0$ puisque $\hat u = 0 \iff u = 0$.
\end{proof}

Par linéarité de Fourier, $H^s(\R^n)$ est un espace vectoriel, et de plus il est normé par le produit scalaire défini ci-dessus. Montrons alors que c'est un espace ce Hilbert.

Soit $(u_k)_{k \in \N} \in H^s$ une suite de Cauchy. $(1+\abs\xi^2)^{s/2}\mathbb Fu_k$ est de Cauchy dans $L^2$, qui est complet. Donc il en existe une limite
$V \in L^2 \tq (1+\abs\xi^2)^{s/2}\mathbb Fu_k \xrightarrow[k \to \pinfty]{L^2} V$. Il existe $u \in L^2 \tq (1+\abs\xi^2)^{s/2}\mathbb Fu = V$ car $(1+\abs\xi^2)^{-s/2}V \in L^2$,
et $\mathbb F$ est une bijection sur $L^2$. De plus, $u \in H^s$ car $V = (1+\abs\xi^2)^{s/2}\mathbb Fu \in L^2$. Puisque $u_k \xrightarrow[k \to \pinfty]{H^s} u$,
on a que $H^s$ est complet.

Pour un contre-exemple, on a $C^\infty_0(\R^n)$ muni du produit scalaire $\scpr fg \coloneqq \int f\overline g\dx$ n'est pas un Hilbert. En effet, pour $f \in L^2 \setminus C^\infty_0$,
par densité de $C^\infty_0$ dans $L^2$, $\exists (f_k)_{k \in \N} \in {C^\infty_0}^\N \tq f_k \xrightarrow[k \to \pinfty]{L^2} f$. De plus, $(f_k)$ est de Cauchy dans $C^\infty_0$.
Par l'absurde, si $\exists g \in C^\infty_0 \tq f_k \xrightarrow[k \to \pinfty]{L^2} g$, par unicité de la limite, $g=f$, or $f \not \in C^\infty_0$.

À partir d'ici, $H$ désigne un espace de Hilbert quelconque.

\begin{déf} Soit $y \in H$. On définit~:
\[\begin{cases}
	&f_1 : x \mapsto \scpr xy \\
	&f_2 : x \mapsto \scpr yx \\
	&f_3 : x \mapsto \norm x
\end{cases}\]
\end{déf}

\begin{prp} $f_i$ est continue pour $i=1,2,3$.
\end{prp}

\begin{proof}
\begin{enumerate}
	\item $\abs {f_1(x_1) - f_1(x_2)} = \abs {\scpr {x_1-x_2}y} \leq \norm {x_1-x_2}\norm y \xrightarrow[x_1 \to x_2]{} 0$. ($f_1$ est même uniformément continue et Lipschitzienne).
	\item Idem pour $f_2$, à permutation près.
	\item La continuité vient directement de $\abs {\norm x-\norm z} \leq \norm {x-z}$.
\end{enumerate}
\end{proof}

\begin{prp} Pour $F \leq H$, $\overline F \leq H$.
\end{prp}

\begin{proof} Pour $x, y \in \overline F$, il existe $(x_k), (y_k) \in F^\N \tq x_k \xrightarrow[k \to \pinfty]{} x$ et $y_k \xrightarrow[k \to \pinfty]{} y$.

Donc $F \ni x_k+y_k \xrightarrow[k \to \pinfty]{} x+y$.
De plus, pour $\lambda \in \C$, $\underbrace {\lambda x_k}_{\in F} \xrightarrow[k \to \pinfty]{} \lambda x \in \overline F$.
\end{proof}

\begin{rmq} Contrairement aux e.v. de dimension finie, en dimension infinie, il est possible d'avoir un sous-e.v. \textit{strict} dense (e.g. $C^\infty_0$ dans $L^2$).
\end{rmq}

\begin{prp} $F \coloneqq \{f \in L^2 \tq f=0 \text{ sur } x_n > 0\}$ est un e.v. fermé dans $L^2$.
\end{prp}

\begin{proof} Soit $g \in \overline F$. Il existe $(f_k)_{k \in \N} \in F^\N \tq f_k \xrightarrow[k \to \pinfty]{L^2} g$. Pour $k \in \N$~:
\[\int_{x_n > 0}\abs g^2\dx = \int_{x_n > 0}\abs {g-f_k}^2\dx \leq \abs {g-f_k}^2_{L^2} \xrightarrow[k \to \pinfty]{} 0\]
car $f_k \in F$ et $f_k \xrightarrow[k \to \pinfty]{L^2} g$. Dès lors $\int_{x_n > 0}\abs g^2\dx = 0$, i.e. $g \in F$. Donc $\overline F = F$.
\end{proof}

\section{Orthogonalité}

\begin{déf} Pour $x, y \in H$, $x$ et $y$ sont \textit{orthogonaux}, noté $x \perp y$ lorsque $\scpr xy = 0$.

Pour $x \in H$, on définit $x^\perp \coloneqq \{y \in H \tq \scpr xy = 0\}$, et pour $M \subset H$, on définit
$M^\perp \coloneqq \{y \in H \tq \forall x \in M : \scpr xy = 0\} = \bigcap_{x \in M}x^\perp$.
\end{déf}

\begin{prp} Pour $x \in H$, $x^\perp \leq H$, et $x^\perp$ est fermé.
\end{prp}

\begin{proof} À $x \in H$ fixé, on remarque que $x^\perp = f_1^{-1}(\{0\})$, or $f_2$ est continue. Donc $x^\perp$ est fermé. Vérifier que $x^\perp$ est un sous-e.v. est trivial.
\end{proof}

\begin{cor} Pour $M \leq H$, $M^\perp$ est un sous-e.v. fermé de $H$.
\end{cor}

Ce résultat découle directement du fait que $M^\perp$ est une intersection d'e.v. fermés.

\begin{déf} $E \subseteq H$ est dit \textit{convexe} lorsque $\forall x, y \in E : \forall t \in [0, 1] : (1-t)x + ty \in E$.
\end{déf}

\begin{ex}~
\begin{itemize}
	\item tout sous-e.v. de $H$ est convexe~;
	\item toute boule (ouverte ou fermée) dans $H$ est convexe~;
	\item pour $\Omega \subseteq \R^n, u \in L^2(\Omega)$, $E = \{v \in L^2 \tq u=v \text{ sur } \Omega\}$ est convexe.
\end{itemize}

Montrons également que $E$ est fermé dans $L^2$. Soit $f \in \overline E$. Il existe $(f_k)_{k \in \N} \in E^\N \tq f_k \xrightarrow[k \to \pinfty]{L^2} f$.
\[\int_\Omega \abs {u-f}^2\dx = \int_\Omega\abs {f_k-f}\dif x \leq \int_{\R^n}\abs {f_k-f}^2\dx \xrightarrow[k \to \pinfty]{} 0.\]
\end{ex}

\begin{thm} Soit $E \neq \emptyset$ convexe fermé dans $H$. Alors $\exists! x \in E \tq \norm x = \min_{z \in E}\norm z = \inf_{z \in E}\norm z \eqqcolon \delta$.
\end{thm}

\begin{proof} \unic soient $x, y \in E \tq \norm x = \norm y = \delta$. Par la formule du parallélogramme~:
\[\norm {x+y}^2 + \norm {x-y}^2 = 2(\norm x^2+\norm y^2).\]

Par convexité de $E$, $\frac 12(x+y) \in E$. Donc $\norm {\frac 12(x+y)} \geq \delta$. On trouve alors~:
\[\norm {x-y}^2 \leq 2(\underbrace {\norm x^2 + \norm y^2}_{=2\delta^2}) - 4\delta^2 = 0.\]

On en déduit $\norm {x-y} = 0$, i.e. $x=y$.

\exis Soit $(y_k)_{k \in \N} \in E^\N \tq \norm {y_k} \xrightarrow[k \to \pinfty]{} \delta$ qui existe par définition de l'infimum. Par la règle du parallélogramme~:
\[\norm {y_k - y_m}^2 \leq 2(\underbrace {\norm {y_k}^2+\norm {y_m}^2)}_{\xrightarrow[k, m \to \pinfty]{} 2\delta^2} - 4\delta^2 \xrightarrow[k, m \to \pinfty]{} 0.\]
Donc $(y_k)$ est de Cauchy. Par complétude de $H$, $\exists x_0 \in H \tq y_k \xrightarrow[k \to \pinfty]{} x_0$, et par fermeture de $E$, $x_0 \in E$.

De plus, par continuité de la norme, $\norm {y_k} \xrightarrow[k \to \pinfty]{} \norm {x_0}$, et par unicité de la limite, $\norm {x_0} = \delta$.
\end{proof}

\begin{ex} Si $\Omega \subseteq \R^n$ est un ouvert, $u \in L^2(\Omega)$, $E = \{v \in L^2 \tq u=v \text{ sur } \Omega\}$ est un convexe fermé, donc par ce théorème,
il existe un unique $u^* \in E$ qui minimise la norme~: $u^* = u$ sur $\Omega$ et $u^*=0$ sur $\R^n \setminus \Omega$.
\end{ex}

\begin{thm}[Décomposition orthogonale] Soit $M \leq H$ fermé. Alors~:
\begin{enumerate}
	\item $\forall x \in H : \exists! (y, z) \in M \times M^\perp \tq x=y+z$~;
	\item ces valeurs $y, z$ sont les points les plus proches de $x$ dans $M$ et $M^\perp$ respectivement~;
	\item Les applications $P : x \mapsto y$ et $Q : x \mapsto z$ sont linéaires~;
	\item $\norm x^2 = \norm {Px}^2 + \norm {Qx}^2$ (et donc $P,Q$ sont continues)~;
	\item $P$ et $Q$ sont les projections orthogonales de $x$ sur $M$ et $M^\perp$ respectivement.
\end{enumerate}
\end{thm}

\begin{proof}~
\begin{enumerate}
	\item \unic si $x = y_1+z_1 = y_2+z_2$, pour $y_1,y_2 \in M$ et $z_1,z_2 \in M^\perp$, on a
	$\underbrace {y_1-y_2}_{\in M} = \underbrace {z_2-z_1}_{\in M^\perp}$. Or $M \cap M^\perp = \{0\}$. Donc $y_1=y_2$ et $z_1=z_2$.

 	\exis $x+M$ est convexe (trivial par le fait que $M \leq H$). Montrons que $x+M$ est fermé. Soit $u \in \overline {x+M}$.
	Il existe $(u_k)_{k \in \N} \in {(x+M)^\N} \tq u_k \xrightarrow[k \to \pinfty]{} u$. $\forall k \in \N : x+M \ni u_k = x + y_k$. On en déduit
	$y_k \xrightarrow[k \to \pinfty]{} u-x$. Par fermeture de $M$, on a $u-x \in M$, et donc $u \in x+M$ (i.e. $x+M$ est fermé).

	Soit $z \in x+M$ l'élément qui minimise la norme. On pose $y \coloneqq x-z \in M$. Montrons alors que $z \in x+M$. Soit $w \in M$~; WLOG, supposons $\norm w = 1$.
	Puisque $z \in x+M$, $\forall \alpha \in \C : z-\alpha w \in x+M$. Donc~:
	\[\norm z^2 \leq \norm {z-\alpha w}^2 = \norm z^2 - 2\Re\alpha\scpr wz + \abs\alpha^2.\]

	$0 = 2\Re\alpha\scpr wz - \abs\alpha^2$. En particulier, pour $\alpha = \scpr zw$~: $0 = \norm {\scpr zw}^2$, donc $\scpr zw = 0$. Dès lors $z \in M^\perp$.

	\item Soit $Y \in M$. Montrons que $\norm {x-Y} \geq \norm {x-y} = \norm z$. Par Pythagore~:
	\[\norm {x-Y}^2 = \norm {y+z-Y}^2 = \norm {(y-Y) + z}^2 = \norm {y-Y}^2 + \norm z^2 \geq \norm z^2.\]

	Idem pour $Z \in M^\perp$~: $\norm {x-Z} \geq \norm {x-z} = y$.

	\item Soient $x_1, x_2 \in H, \alpha_1, \alpha_2 \in \C$. On a $\alpha_1x_1 = \alpha_1Px_1 + \alpha_1Qx_1$, et $\alpha_2x_2 = \alpha_2Px_2 + \alpha_2Qx_2$. Donc~:
	\[P(\alpha_1x_1+\alpha_2x_2) + Q(\alpha_1x_1 + \alpha_2x_2) = \alpha_1x_1 + \alpha_2x_2 = \alpha_1Px_1 + \alpha_1Qx_1 + \alpha_2Px_2 + \alpha_2Qx_2,\]
	et donc~:
	\[\underbrace {P(\alpha_1x_1+\alpha_2x_2) - \alpha_1Px_1 - \alpha_2px_2}_{\in M} = \underbrace {\alpha_1Qx_1 + \alpha_2Qx_2 - Q(\alpha_1x_1 + \alpha_2x_2)}_{\in M^\perp}.\]
	Or $M \cap M^\perp = \{0\}$, donc $P(\alpha_1x_1+\alpha_2x_2) = \alpha_1Px_1 - \alpha_2px_2$, et $\alpha_1Qx_1 + \alpha_2Qx_2 = Q(\alpha_1x_1 + \alpha_2x_2)$.

	\item Par Pythagore $\norm x^2 = \norm {Px}^2 + \norm {Qx}^2$. Donc $\norm {Px} \leq \norm x$ et $\norm {Qx} \leq \norm x$, i.e. $P$ et $Q$ sont Lipschitziennes,
	donc en particulier continues.
\end{enumerate}
\end{proof}

\begin{cor} Si $M \leq H$, avec $M \neq H$, il existe $y \in H \setminus \{0\} \tq y \perp M$.
\end{cor}

\begin{proof} Pour $x \in H \setminus M$, $x = Px+Qx$, où $Qx \neq 0$, et $Qx \perp M$.
\end{proof}

%\begin{rmq} TODO: Commenter l'hypothèse de fermeture...
%\end{rmq}

\begin{cor} Si $M \leq H$ est fermé, alors $M = {M^\perp}^\perp$.
\end{cor}

\begin{proof} La première inclusion est triviale~: si $x \in M$, alors $x \perp M^\perp$.

La seconde inclusion se démontre comme suit~: soit $x \in {M^\perp}^\perp \subseteq H$. $x = y+z$ où $y \in M$ et $z \in M^\perp$.
Or $0 = \scpr xz = \scpr {y+z}z = \scpr yz + \norm z^2$, et $\scpr yz = 0$ par définition d'orthogonalité. Donc $\abs z = 0$ et $z=0$, i.e. $x=y \in M$.
\end{proof}

\begin{lem}[Lemme de Riesz] Soit $L : H \to \C$, une forme linéaire continue. Alors $\exists! y \in H \tq L = \scpr \cdot y$.
\end{lem}

\begin{proof} \unic pour $y_1, y_2 \in H \tq \forall x \in H : \scpr x{y_1} = \scpr x{y_2}$, on a $\scpr x{y_1-y_2} = 0$, donc $y_1-y_2 \in H^\perp = \{0\}$, i.e. $y_1=y_2$.

\exis si $L \equiv 0$, alors $y=0$. Supposons alors que $L$ n'est pas identiquement nulle. $\Ker L \lneqq H$ et est fermé par continuité de $L$. Dès lors, il existe $z \in H$,
$z \neq 0 \tq z \perp \Ker L$. WLOG, supposons $\norm z = 1$. Posons $y \coloneqq (\overline {Lz})z$ et $u \coloneqq (Lx)z - (Lz)x$. Calculons~:
\[Lu = (Lx)Lz - (Lz)Lx = 0,\]
donc $u \in \Ker L$, et donc $0 = \scpr uz = (Lx)\scpr zz - (Lz)\scpr xz = Lx-(Lz)\scpr xz$. Dès lors, $Lx = (Lz)\scpr xz = \scpr xy$.
\end{proof}

\section{Systèmes orthonormaux}

\begin{déf} Pour $V$ un e.v. et $S \subseteq V$, on note $\Vect S = \Span S$ l'e.v. engendré par $S$.

$(e_\alpha)_{\alpha \in A} \subset V$ est appelé \textit{orthonormal} lorsque $\forall \alpha, \beta \in A : \scpr {e_\alpha}{e_\beta} = \delta_{\alpha,\beta}$.

Pour $x \in H$, on définit $\hat x(\alpha) \coloneqq \scpr x{e_\alpha}$.
\end{déf}

Les $\hat x(\alpha)$ sont les coefficients de Fourier relativement au système $(e_\alpha)_{\alpha \in A}$.

\begin{ex} Sur $L^2(\R^n)$, les $(e_\alpha)_{\alpha \in \Z}$ sont les $e_\alpha : [0, 2\pi) \to \C : t \mapsto \frac {e^{i\alpha t}}{\sqrt {2\pi}}$.
\end{ex}

\begin{thm} Pour $H$ un espace de Hilbert et $(e_\alpha)_{\alpha \in A}$ un système orthonormal, $F \subset A$ fini, et $M_F \coloneqq \Vect {\{e_\alpha\}}_{\alpha \in F}$, on a~:
\begin{enumerate}
	\item si $\varphi : A \to \C$ est nulle sur $A \setminus F$, pour $y = \sum_{\alpha \in F}\varphi(\alpha)e_\alpha$, alors~:
	\[\forall \alpha \in A : \varphi(\alpha) = \hat y(\alpha).\]
	De plus, $\norm y^2 = \sum_{\alpha \in F}\abs {\varphi(\alpha)}^2$.

	\item Si $x \in H$, $s_F(x) \coloneqq \sum_{\alpha \in F}\hat x(\alpha)e_\alpha \in M_F$. Si $s \in M_F \setminus \{s_F(x)\}$, alors~:
	\[\norm {x-s_F(x)} \lneqq \norm {x-s}.\]

	De plus~: $\sum_{\alpha \in F}\abs {\hat x(\alpha)}^2 \leq \norm x^2$ (inégalité de Bessel).
\end{enumerate}
\end{thm}

\begin{proof}~
\begin{enumerate}
	\item $\hat y(\alpha) = \scpr y{e_\alpha} = \sum_{\beta \in F}\varphi(\beta)\scpr {e_\beta}{e_\alpha} = \varphi(\alpha)$ et~:
	\[\norm y^2 = \norm {\sum_{\beta \in F}\varphi(\beta)e_\beta}^2 \stackrel {\text{Pythagore}}= \sum_{\beta \in F}\abs {\varphi(\beta)}^2\norm {e_\beta}^2 = \sum_{\beta \in F}\abs {\varphi(\beta)}^2.\]
	\item Soit $s \in M_F$. $\forall \alpha \in F : x-s_F(x) \perp e_\alpha$ et $x-s_F(x) \perp s_F(x)-s \in M_F$. En effet~:
	\[\scpr {x-s_F(x)}{e_\alpha} = \scpr x{e_\alpha} - \scpr {s_F(x)}{e_\alpha} = \hat x(\alpha) - \hat x(\alpha) = 0.\]

	Dès lors~:
	\[x-s = (x - s_F(x)) + (s_F(x) - s),\]
	et donc, par Pythagore~:
	\begin{align}\label{eq:x-s in norm}
		\norm {x-s}^2 = \norm {x-s_F(x)}^2 + \norm {s_F(x) - s}^2.
	\end{align}

	Cette norme est minimisée (strictement) en $s = s_F(x)$ et donc~:
	\[\norm {x-s_F(x)} \lneqq \norm {x-s}\]
	si $s \neq s_F(x)$. Ensuite~:
	\[\norm {s_F(x)}^2 \leq \norm {s_F(x)}^2 + \norm {x-s_F(x)}^2.\]
	Par l'équation~\ref{eq:x-s in norm}~: si $s = 0$~:
	\[\norm {s_F(x)}^2 \leq \norm {s_F(x)}^2 \leq \norm {s_F(x)}^2 + \norm {x-s_F(x)}^2 = \norm x^2.\]
	Or~:
	\[\norm {s_F(x)}^2 = \sum_{\alpha \in F}\abs {\hat x(\alpha)}^2.\]
	Dès lors $\norm {s_F(x)}^2 \leq \norm x^2$.
\end{enumerate}
\end{proof}

\begin{rmq} Sur $A$, on a un espace mesuré canonique~: $(A, \mathcal P(A), \#)$ pour lequel on adopte les notations~:
\[\forall B \in \mathcal P(A) : \int_B\varphi\dif\# \eqqcolon \sum_{\alpha \in B}\varphi(\alpha).\]

Remarquons également que par définition de l'intégrale, si $\varphi : A \to [0, \pinfty]$~:
\[\sum_{\alpha \in A}\varphi(\alpha) = \sup_{\stackrel {B \subset A}{\abs B \lneqq \pinfty}}\sum_{\alpha \in B}\varphi(\alpha).\]

Et si $\varphi \in \ell^1(A)$ et $\varphi \geq 0$, alors pour $A_k = \{\varphi \geq k^{-1}\}$ ($k \geq 1$), on a $\abs {A_k} \lneqq \pinfty$ puisque $\varphi \in \ell^1(A)$.
Or $\bigcup_{k \geq 1}A_k = \{\varphi \gneqq 0\}$, et donc $\{\varphi \neq 0\}$ est au plus dénombrable.

Dans le cas général, pour $\varphi : A \to \C$, alors $(\Re \varphi)^\pm$ et $(\Im \varphi)^\pm$ sont non-nulles sur un ensemble au plus dénombrable, et donc $\{\varphi \neq 0\}$
est au plus dénombrable.
\end{rmq}

\begin{lem} Pour $(\Omega, \mathcal F, \mu)$ un espace mesuré, l'ensemble des fonctions simples mesurables nulles hors d'un ensemble de mesure finie est dense dans
$L^p(\Omega, \mathcal F, \mu)$ pour $p \in [1, \pinfty)$.
\end{lem}

\begin{lem}\label{lem:isométrie dense} Soient $X$ un espace métrique complet, $Y$ un espace métrique, et $X_0 \subset X$, un sous-ensemble dense.
Si $f \in C^0(X, Y)$ telle que $\restr f{X_0}$  une isométrie et $f(X_0)$ est dense dans $Y$, alors $f$ est surjective et est une isométrie.
\end{lem}

\begin{proof} Fixons $x, y \in X$. Il existe $(x_k)_{k \geq 0}, (y_k)_{k \geq 0} \in {X_0}^\N$ telles que $x_k \xrightarrow[k \to \pinfty]{} x$ et $y_k \xrightarrow[k \to \pinfty]{} y$.
Pour $k \geq 0$~:
\[d(x_k, y_k) \xrightarrow[k \to \pinfty]{} d(x, y)\]
car la distance est continue sur un espace métrique. De plus~:
\[d(x_k, y_k) = d\left(f(x_k), f(y_k)\right) \xrightarrow[k \to \pinfty]{} d\left(f(x), f(y)\right),\]
à nouveau par continuité de la métrique, et par continuité de $f$. Donc par unicité de la limite, on a $d(x, y) = d(f(x), f(y))$, et donc $f$ est une isométrie.

Il reste à montrer que $f$ est surjective. Soit $y \in Y$. $f(X_0)$ est dense dans $Y$, et donc par continuité de $f$, on sait~: $\exists (x_k)_{k \geq 0} \in {X_0}^\N$ telle que
$f(x_k) \xrightarrow[k \to \pinfty]{} y$. $(f(x_k))_k$ est de Cauchy dans $Y$, et puisque $f$ est une isométrie, $(x_k)_k$ est de Cauchy dans $X$. Par complétude, on sait
que $\exists x \in X \tq x_k \xrightarrow[k \to \pinfty]{} x$.

Finalement, par continuité de $f$~: $f(x_k) \xrightarrow[k \to \pinfty]{} f(x)$, et par construction $f(x_k) \xrightarrow[k \to \pinfty]{} y$. Par unicité de la limite dans les
espaces métriques, on a $f(x) = y$, et donc $y$ admet une préimage par $f$.
\end{proof}

\begin{thm}\label{thm:cefficients Fourier isométrie} Soit $(e_\alpha)_{\alpha \in A}$, un système orthonormal dans $H$. Soit $P = \Span \{e_\alpha\}_{\alpha \in A}$. Alors~:
\begin{enumerate}
	\item $\forall x \in H : \sum_{\alpha \in A}\abs {\hat x(\alpha)}^2 \leq \norm x^2 \qquad$ (inégalité de Bessel généralisée)~;
	\item $f : H \to \ell^2(A) : x \mapsto \hat x$ est linéaire, continue, et surjective~;
	\item $\restr f{\overline P}$ est une isométrie surjective $\overline P \to \ell^2(A)$.
\end{enumerate}
\end{thm}

\begin{proof}~
\begin{enumerate}
	\item Pour tout $F \subset A$ fini, on a~:
	\[\sum_{\alpha \in F}\abs {\hat x(\alpha)}^2 \leq \norm x^2.\]
	Or par la remarque précédente~:
	\[\sum_{\alpha \in A}\abs {\hat x(\alpha)}^2 = \sup_{\stackrel {B \subset A}{\abs B \lneqq \pinfty}}\sum_{\alpha \in B}\abs {\hat x(\alpha)}^2 \leq \norm x^2\]
	par passage au supremum (à la limite) et par l'inégalité de Bessel finie.
	\item Soit $x \in H$. Puisque $\norm x^2 \lneqq \pinfty$, par l'inégalité de Bessel, on sait $\sum_{\alpha \in A}\abs {\hat x(\alpha)}^2 \lneqq \pinfty$, i.e. $\hat x \in \ell^2(A)$.

	$f$ est linéaire par linéarité de $x \mapsto \scpr x{e_\alpha}$ pour tout $\alpha \in A$.

	$f$ est continue car Lipschitzienne~:
	\[\norm {f(x)-f(y)}_{\ell^2(A)}^2 = \sum_{\alpha \in A}\abs {\widehat {x-y}(\alpha)}^2 \leq \norm {x-y}_H^2.\]

	La surjectivité vient du point 3~: si $\restr f{\overline P}$ est surjective, alors en particulier $f$ est surjective.
	\item Pour $X = \overline P$, $X_0 = P$, $Y = \ell^2(A)$, remarquons que~:
	\[\underbrace {\left\{\chi \in \ell^2(A) \tq \chi(\alpha) = 0 \text{ si } \alpha \not \in F \subset A \text{ fini }\right\}}_{\text{dense dans } \ell^2(A)} \subset f(P).\]

	De plus $\restr fP$ est une isométrie. En effet, pour $x \in P$, on sait qu'il existe $F \subset A$ fini et $\lambda_\alpha \in \C$ ($\alpha \in F$) tels que
	$x = \sum_{\alpha \in F}\lambda_\alpha e_\alpha$. Dès lors~:
	\[\norm {f(x)}_{\ell^2(A)}^2 = \norm x_{\ell^2(A)}^2 = \sum_{\alpha \in A}\abs {\hat x(\alpha)}^2 = \sum_{\alpha \in F}\abs {\hat x(\alpha)}^2
	= \sum_{\alpha \in A}\abs {\sum_{\beta \in F}\lambda_\beta \scpr {e_\beta}{e_\alpha}}^2 = \sum_{\alpha \in F}\abs {\lambda_\alpha}^2,\]
	et~:
	\[\norm x_H^2 = \scpr xx_H = \scpr {\sum_{\alpha \in F}\lambda_\alpha e_\alpha}{\sum_{\beta \in F}\lambda_\beta e_\beta}
	= \sum_{\alpha \in F}\sum_{\beta \in F}\lambda_\alpha\overline {\lambda_\beta}\scpr {e_\alpha}{e_\beta} = \sum_{\alpha \in F}\abs {\lambda_\alpha}^2.\]

	Finalement, puisque $\overline P$ est complet (car sous-ensemble fermé de $H$), on peut ensuite appliquer le lemme~\ref{lem:isométrie dense} qui affirme que
	$\restr f{\overline P}$ est une isométrie surjective sur $Y$.
\end{enumerate}
\end{proof}

\begin{déf} Un \textit{système orthonormal maximal} (SOM) est un système orthonormal qui est maximal au sens de l'inclusion.
\end{déf}

\begin{thm}\label{thm:caractérisation SOM} Soit $(e_\alpha)_{\alpha \in A}$ un système orthonormal dans $H$. Alors les conditions suivantes sont équivalentes~:
\begin{enumerate}
	\item $(e_\alpha)_\alpha$ est un SOM.
	\item $M \coloneqq \Span \{e_\alpha\}_\alpha$ est dense dans $H$.
	\item $\forall x \in H : \sum_{\alpha \in A}\abs {\hat x(\alpha)}^2 = \norm x^2$.
	\item $\forall x, y \in H : \sum_{\alpha \in A}\hat x(\alpha)\overline {\hat y(\alpha)} = \scpr xy \qquad$ (Identité de Parseval).
	\item $\forall x \in H : \forall \varepsilon > 0 : \exists A_0 \subset A$ fini tel que $\forall A_1 \supset A_0$ : si $A_1$ est fini, alors $\norm {x - \sum_{\alpha \in A_1}\hat x(\alpha)e_\alpha} \leq \varepsilon$.
\end{enumerate}
\end{thm}

\begin{proof}~
\begin{itemize}
	\item[$1 \Rightarrow 2$] Par l'absurde, supposons que $M \subsetneqq A$. Alors il existe $y \in A \setminus \{0\}$ tel que $y \perp M$ et donc le système n'est pas maximal
	car on peut lui ajouter $\frac y{\norm y_H}$.
	\item[$2 \Rightarrow 3$] Par le Théorème~\ref{thm:cefficients Fourier isométrie} (point 3), on sait que $\overline M \to \ell^2(A) : x \mapsto \hat x$ est une isométrie,
	ce qui revient à dire que si $x \in \overline M$, alors l'inégalité de Bessel est une égalité, i.e.~:
	\[\norm x^2 = \sum_{\alpha \in A}\abs {\hat x(\alpha)}^2.\]
	\item[$3 \Rightarrow 4$] On veut montrer que $\norm {\hat \cdot}_{\ell^2(A)} = \norm \cdot_H \Rightarrow \scpr {\hat \cdot}{\hat \cdot}_{\ell^2(A)} = \scpr \cdot\cdot_H$.
	Dans $\mathcal H$, un espace de Hilbert quelconque (e.g. $\mathcal H = H$ ou $\mathcal H = \ell^2(A)$), on a~:
	\[4\scpr xy_{\mathcal H} = \norm {x+y}^2_{\mathcal H} - \norm {x-y}^2_{\mathcal H} + i\norm {x+iy}^2_{\mathcal H} - i\norm {x-iy}^2_{\mathcal H}.\]
	Or par hypothèse, $\norm \cdot_H = \norm {\hat \cdot}_{\ell^2(A)}$. Donc~:
	\begin{align*}
		4 \scpr {\hat x}{\hat y}_{\ell^2(A)} &= \norm {\hat x+\hat y}^2_{\ell^2(A)} - \norm {\hat x-\hat y}^2_{\ell^2(A)} + i\norm {\hat x+i\hat y}^2_{\ell^2(A)} - i\norm {\hat x-i\hat y}^2_{\ell^2(A)} \\
		&= \norm {x+y}^2_H - \norm {x-y}^2_H + i\norm {x+iy}^2_H - i\norm {x-iy}^2_H = 4\scpr xy_H.
	\end{align*}
	\item[$4 \Rightarrow 1$] Par l'absurde, supposons qu'il existe $u \neq 0$ tel que $\forall \alpha \in A : u \perp e_\alpha$. Alors $\forall \alpha \in A : \hat u(\alpha) = 0$.
	Or $0 \neq \norm u^2 = \sum_{\alpha \in A}\hat u(\alpha)\overline {\hat u(\alpha)} = \sum_{\alpha \in A}\abs {\hat u(\alpha)}^2 = 0$, ce qui est une contradiction.
	\item[$5 \Rightarrow 2$] Fixons $x \in H$ et $\varepsilon = \frac 1k$. Pour tout $k$, il existe $x_k = \sum_{\alpha \in A_1}\scpr x{e_\alpha}e_\alpha \in M$ tel que
	$\norm {x-x_k} \leq \frac 1k = \varepsilon$
	\item[$3 \Rightarrow 5$]
	\[\sum_{\alpha \in A}\abs {\hat x(\alpha)}^2 = \sup_{B \subset A \text{ fini}}\sum_{\beta \in B}\abs {\hat x(\beta)}^2.\]
	Par définition du $\sup$~: $\forall \varepsilon > 0 : \exists A_0$ fini $\subset A \tq \forall A_1$ fini $\supset A_0$~:
	\[\norm {x- \sum_{\alpha \in A_1}\hat x(\alpha)e_\alpha}^2 = \norm x^2 - \sum_{\alpha \in A_1}\abs {\hat x(\alpha)}^2 \leq \varepsilon^2.\]
\end{itemize}
\end{proof}

\begin{thm} Tout espace de Hiblert possède un système orthonormal maximal.
\end{thm}

\begin{proof} Soit $A$ l'ensemble des SOMs de $H$. $(A, \subseteq)$ est ordonné. Soit $\mathcal S \subset A$ une partie totalement ordonnée. On pose~:
\[\hat S \coloneqq \bigcup_{s \in \mathcal S}s.\]
Mq $\hat S \in A$.

Soient $a_1, a_2 \in \hat S$. Il existe $s_1, s_2 \in \mathcal S$ tels que $a_1 \in s_1$ et $a_2 \in s_2$, or $\mathcal S$ est totalement ordonné. Donc soit $s_1 \subseteq s_2$,
soit $s_2 \subseteq s_1$, donc $a_1, a_2 \in s_1$ ou $a_1, a_2 \in s_2$. En particulier, ils sont orthogonaux, et de plus $\hat S$ majore tout $s \in \mathcal S$.

Par le lemme de Zorn, on a l'existence d'un élément maximal pour l'inclusion, i.e. un SOM.
\end{proof}

\begin{thm}[Gram-Schmidt] Soit $\{x_1, x_2, \ldots\} \subseteq H$ une suite finie ou dénombrable de vecteurs linéairement indépendants. Alors il existe un système orthonormal
$\{u_1, u_2, \ldots,\}$ fini et de même cardinalité que $\{x_1, \ldots\}$ si ce dernier est fini ou dénombrable si $\{x_1, x_2, \ldots\}$ est dénombrable tel que
$\Span \{u_1, \ldots, \} = \Span \{x_1, \ldots\}$.
\end{thm}

\begin{proof} Les $x_j$ sont non-nuls car $\{x_1, x_2, \ldots\}$ est linéairement indépendant. Posons $y_1 \coloneqq x_1$ et $u_1 \coloneqq \frac {y_1}{\norm {y_1}}$ et pour tout $n > 1$
posons $y_n \coloneqq x_n - \sum_{j=1}^n\scpr {x_n}{u_j}u_j$ et $u_n \coloneqq \frac {y_n}{\norm {y_n}}$.

Il faut maintenant s'assurer que pour tout $n \geq 1$~: $y_n \neq 0$ afin que les $u_n$ soient bien définis. Par l'absurde, supposons qu'il existe $n \geq 1$ tel que $y_n = 0$. Alors~:
\[x_n \in \Span\{u_1, \ldots, u_{n-1}\} \subseteq \Span\{y_1, \ldots, y_{n-1}\} \subseteq \Span\{x_1, \ldots, x_{n-1}\}.\]
Or les $x_j$ sont linéairement indépendants.

Il est évident que $u_n \in \Span\{x_1, \ldots, x_n\}$ et $x_n \in \Span\{u_1, \ldots, u_n\}$ pour tout $n \geq 1$. Il reste alors uniquement à montrer que $u_n \perp u_j$ ($j < n$),
ou de manière équivalente $y_n \perp u_j$, et cette dernière formulation est évidente par définition de $y_n$.
\end{proof}

\begin{déf} Un espace topologique $E$ est dit \textit{séparable} s'il admet une partie dense dénombrable.
\end{déf}

\begin{thm} Un espace de Hilbert est séparable ssi il possède un système orthonormal maximal.
\end{thm}

\begin{proof} \underline {$\Rightarrow$~:} Soit $\{a_1, a_2, \ldots\}$ une suite dense dans $H$. Soit $\{a'_1, a'_2, \ldots\}$ la suite partielle (possiblement finie) de
$\{a_1, \ldots\}$ consitituée des $a_i \not \in \Span\{a_1, \ldots, a_{i-1}\}$. Par définition, on a que les $a'_i$ sont indépendants et tous les $a_j$ sont combinaisons linéaires
des $a'_i$, i.e. $\{a_1, a_2, \ldots\} \subset \Span\{a'_1, a'_2, \ldots\}$.

On en déduit alors que $\Span\{a'_1, a'_2, \ldots\}$ est dense dans $H$ Par Gram-Schmidt sur $\{a'_1, a'_2, \ldots\}$, on a un système orthonormal $\{u_1, \ldots\}$ tel que
$\Span\{u_1, \ldots\}$ est dense dans $H$. Dès lors, par le Théorème~\ref{thm:caractérisation SOM}, $\{u_1, \ldots\}$ est un SOM.

\underline {$\Leftarrow$~:} Soit $(e_k)_{k \in \N}$ un SOM. Pour $N \in \N^*$, on pose~:
\[E_N \coloneqq \left\{\sum_{j=1}^Nq_je_j \tq q_j \in \Q[i]\right\}.\]

$E_N$ est dénombrable, et donc $E \coloneqq \bigcup_{N > 0}E_N$ est également dénombrable. Par le théorème~\ref{thm:caractérisation SOM}, on a
$\forall \varepsilon > 0 : \exists A_0 \subset A$ fini tel que~:
\[\forall A_1 \supset A_0 : \norm {x-\sum_{\alpha \in A_1}\hat x(\alpha)e_\alpha} \leq \varepsilon.\]

Montrons que $\forall \varepsilon > 0 : \exists y \in E \st \norm {x-y} \leq \varepsilon$.
Soit $(q_\alpha)_{\alpha \in A_1} \subset \Q[i] \st \sum_{\alpha \in A_1}\norm {q_\alpha - \hat x(\alpha)}^2 \leq \varepsilon^2$. De tels $q_\alpha$ existent bien par densité
de $\Q$ dans $\R$, et donc par densité de $\Q[i]$ dans $\C$.

\[\norm {x - \sum_{\alpha \in A_1}q_\alpha e_\alpha}^2 = \norm {x-\sum_{\alpha \in A_1}\hat x(\alpha)e_\alpha}^2 + \sum_{\alpha \in A_1}\norm {q_\alpha - \hat x(\alpha)} \leq 2\varepsilon^2.\]

Donc pour $y = \sum_{\alpha \in A_1}q_\alpha e_\alpha$, on a bien le résultat.
\end{proof}

\begin{ex}~
\begin{itemize}
	\item[(0)] $\C^N$ muni du produit scalaire usuel admet une base canonique. Cette dernière est orthonormale et maximale.
	\item[(1)] Dans $\ell^2(\N_0)$, posons le suites $e_k \; (k \in \Z)$ telles que $e_{k,j} = 1$ si $k=j$ et $0$ sinon. $(e_k)_{k \in \Z}$ est un système orthonormal maximal car~:
	\[\norm x^2 = \scpr xx = \sum_{j \geq 1}x_j\overline {x_j} = \sum_{j \geq 1}\abs {x_j}^2.\]
	Par le point 3 du Théorème~\ref{thm:caractérisation SOM}, on a que $(e_k)_{k \in \Z}$ est un SOM.
	\item[(2)] Dans $L^2[0, 2\pi)$, on définit (pour $k \in \Z$)~:
	\[e_k : [0, 2\pi) \to \C : t \mapsto \frac {e^{ikt}}{\sqrt {2\pi}}.\]
	Pour le produit scalaire usuel de $L^2$, on a~:
	\[\scpr {e_k}{e_\ell}_{L^2} = \int_0^{2\pi}\frac {e^{i(k-\ell)t}}{2\pi}\dif t = \begin{cases}1 &\text{ si } k-\ell = 0 \\0 &\text{ sinon}\end{cases}.\]

	Pour montrer que $(e_k)_{k \in \Z}$ est maximal, prenons $f \in L^2[0, 2\pi)$. $S_kf \coloneqq \sum_{m=-k}^k\scpr f{e_m}e_m.$ Montrons que $S_kf \xrightarrow[k \to \pinfty]{L^2} f$.
	Pour $\varepsilon > 0, \exists \varphi \in C_0^\infty((0, 2\pi)) : \norm {f-\varphi}_{L^2} \leq \varepsilon$ (par densité de $C_0^\infty$ dans $L^2$).

	On a $S_k\varphi \xrightarrow[k \to \pinfty]{\text{CVU}} \varphi$ (théorème de Dirichlet global), ce qui implique $S_k\varphi \xrightarrow[k \to \pinfty]{L^2} \varphi$.
	Finalement, remarquons~:
	\[\norm {f-S_kf}_{L^2} \leq \norm {f-S_k\varphi}_{L^2} \leq \norm {f-\varphi}_{L^2} + \norm {\varphi - S_k\varphi} \leq 2\varepsilon\]
	si $k$ est assez grand.

	Attention, $\{e_k\}_k$ n'est \textbf{pas} une base au sens algébrique car $\forall k \in \Z : e_k \in C^\infty$. Donc si $\{e_k\}_k$ est une base, toute fonction $f \in L^2$ est
	égale à $\sum_{j=1}^Nc_je_{k_j} \in C^\infty$. Or $L^2 \not \subseteq C^\infty$.
\end{itemize}
\end{ex}

\section{Applications linéaires entre espaces vectoriels normés}
Soient $E, F$ deux espaces de Banach. Pour $T : E \to F$ linéaire, on dit que $T$ est \textit{bornée} lorsque~:
\[\sup_{\norm x \leq 1}\norm {Tx} \lneqq \pinfty.\]

\begin{rmq} \textit{borné} doit se comprendre \textit{borné sur la boule unité} car $T$ n'est pas borné puisque linéaire.
\end{rmq}

\begin{prp} Soit $T : E \to F$ linéaire.
\[\sup_{\norm x \leq 1}\norm {Tx} = \sup_{\norm x \lneqq 1}\norm {Tx} = \sup_{\norm x = 1}\norm {Tx}.\]
\end{prp}

\begin{proof} On note $A = \{\norm {Tx}\}_{\norm x \leq 1}$, $B = \{\norm {Tx}\}_{\norm x < 1}$, et $C = \{\norm {Tx}\}_{\norm x = 1}$.
Notons également $a = \sup A$, $b = \sup B$ et $c = \sup C$.

Puisque $A \supset B \cup C$, on sait que $a \geq b$ et $a \geq c$. Maintenant, si $x \neq 0 \st \norm x \leq 1$, alors $B \ni T\frac x{\norm x} = \frac 1{\norm x}Tx \geq \norm {Tx}$.
En particulier, $c \geq a$, et donc $c=a$.

Si $a \lneqq \pinfty$, alors $\forall \delta > 0 : \exists x \st \norm x = 1$ et $\norm {Tx} \geq a-\delta$ (par définition du $\sup$ et puisque $a=c$). Pour $\varepsilon \in (0, 1)$~:
\[\norm {T((1-\varepsilon)x)} = (1-\varepsilon)\norm {Tx} \geq (1-\varepsilon)(a-\delta) \geq a-\eta,\]
pour $\eta = \varepsilon a - \varepsilon \delta + \delta$. Pour $\delta, \varepsilon \to 0$, on a $\eta \to 0$ et donc $b \geq a$.

Finalement, si $a = \pinfty$, alors $\forall M > 0 : \exists x_M \st \norm {x_M} \leq 1$ et $\norm {Tx_M} \geq M$. Or par linéarité de $T$, on a $\norm {x_{M/2}} \lneqq 1$,
et finalement~:
\[\forall M > 0 : \exists \tilde x_M (= x_{M/2}) \st \norm {\tilde x_M} \leq 1 \text{ et } \norm {T\tilde x_M} \geq M.\]
On en déduit également que $a=b$.

Dès lors, on a bien $a = b = c$.
\end{proof}

\begin{déf} L'ensemble des applications linéaires bornées de $E$ dans $F$ est noté $\mathcal L(E, F)$. On munit cet ensemble de la norme~:
\[\norm \cdot_{\mathcal L(E, F)} : \mathcal L(E, F) \to \mathbb R^+ : T \mapsto \norm T_{\mathcal L(E, F)} \coloneqq \sup_{\norm x \leq 1}\norm {Tx}.\]

On note également $\mathcal L(E) \coloneqq \mathcal L(E, E)$.
\end{déf}

\begin{rmq} Il est à noter que cette norme sur $\mathcal L(E, F)$ dépend des normes sur $E$ et sur $F$~!
\end{rmq}

\begin{prp} $\norm \cdot_{\mathcal L(E, F)}$ est une norme.
\end{prp}

\begin{proof} \TODO: Exercice
\end{proof}

\begin{prp} Soit $T \in \mathcal L(E, F)$. Pour tout $y \in E$, on a $\norm {Ty} \leq \norm T\norm y$.
\end{prp}

\begin{proof} Si $y = 0$, alors $Ty = 0$, et donc ok. Sinon, $Ty = \norm yT\frac y{\norm y}$. Par passage à la norme dans $F$~:
\[\norm {Ty} = \norm y \underbrace {\norm {T\frac y{\norm y}}}_{\leq \norm T} \leq \norm y\norm T.\]
\end{proof}

On remarque également que $\norm {(T_2 \circ T_1)(x)} \leq \norm {T_2}\norm {T_1x} \leq \norm {T_2}\norm {T_1}\norm x$, et donc $\norm {T_2T_1} \leq \norm {T_2}\norm {T_1}$. Dès lors
si $T_1 \in \mathcal L(E, F)$ et $T_2 \in \mathcal L(F, G)$, alors $T_2T_1 \in \mathcal L(E, G)$.

\begin{thm} Soit $T : E \to F$ linéaire. Alors les conditions suivantes sont équivalentes~:
\begin{itemize}
	\item[$(i)$]   $T$ est bornée~;
	\item[$(ii)$]  $T$ est continue en tous points~;
	\item[$(iii)$] $\exists x_0 \in E \st T$ est continue en $x_0$.
\end{itemize}
\end{thm}

\begin{proof}~
\begin{itemize}
	\item[$(i) \Rightarrow (ii)$]   Soit $y \in E$. $\norm {Tx - Ty} = \norm {T(x-y)} \leq \norm T\norm {x-y}$. Dès lors, $T$ est Lipschitzienne et donc continue.
	\item[$(ii) \Rightarrow (iii)$] Trivial (je cite~: \textit{Si vous avez un problème ici, je crois qu'il y a un sérieux problème dans l'enseignement}).
	\item[$(iii) \Rightarrow (i)$]  Pour $\varepsilon > 0$, il existe $\delta > 0$ tel que si $\norm {x-x_0} < \delta$, alors $\norm {T(x-x_0)} < \varepsilon$.
		En posant $y \coloneqq x-x_0$, si $\norm y < \delta$, alors $\norm {Ty} < \varepsilon$. Autrement dit, $T(B(0, \delta)) \subset B(0, \varepsilon)$.
		Pour $z \in B(0, 1)$ (i.e. $\norm z < 1$), on a~:
		\[\norm {Tz} = \frac 1\delta\norm {T(\delta z)} < \frac 1\delta \varepsilon.\]
		Dès lors $\forall z : \norm z < 1 \Rightarrow \norm {Tz} < \varepsilon/\delta$, et donc $\norm T \lneqq \pinfty$, i.e. $T$ est bornée.
\end{itemize}
\end{proof}

\begin{ex}~
\begin{itemize}
	\item[(0)] Un opérateur linéaire $T : \C^n \to \C^m$ est déterminé par une matrice $(T_{k\ell})_{k,\ell}$ dans les bases canoniques de $\C^m$ et $\C^n$.
	$T$ est continue\footnote{En dimension finie, toute application linéaire entre espaces vectoriels est continue.} donc bornée.
	\item[(1)] $T : L^1(\Omega, \mathcal A, \mu) \to \C : f \mapsto \int f\dif\mu$ est borné car~:
	\[\abs {Tf} = \abs {\int f\dif\mu} \leq \int\abs f\dif\mu = \norm f_{L^1}.\]
	Dès lors l'opérateur $T$ d'intégration est continue.
	\item[(2)] Si $1 \leq p, q \leq \pinfty$ sont conjugués, pour $g \in L^q(\Omega, \mathcal A, \mu)$ on définit~:
	\[T_g : L^p(\Omega, \mathcal A, \mu) \to \C : f \mapsto \int fg\dif\mu.\]
	$T_g$ est bien défini par l'inégalité de Hölder. De plus~: $\abs {T_gf} \leq \norm f_{L^p}\norm g_{L^q}$, et donc $\norm {T_g} \leq \norm g_{L^q}$.
	\item[(3)] $\mathbb F : L^2(\R^n) \to L^2(\R^n)$ est un opérateur linéaire. De plus, par Plancherel, on a $\norm {\mathbb Ff}_{L^2} = (2\pi)^{n/2}\norm f_{L^2}$,
	et donc $\norm {\mathbb F} = (2\pi)^{n/2}$, i.e. la transformée de Fourier est un opérateur borné (donc continu).
	\item[(4)] Pour $H$, un espace de Hilbert quelconque et $M$ un sous-espace vectoriel fermé, $P : H \to M$, la projection orthogonale sur $M$ est bornée car Lipschitzienne
	(et donc également continue).
	\item[(5)] Dans $(\Omega, \mathcal A, \mu)$ un espace mesuré, on fixe $K \in L^2(\Omega \times \Omega, \mathcal A \otimes \mathcal A, \mu \otimes \mu)$, et on pose~:
	\[T : L^2(\Omega, \mathcal A, \mu) \to L^2(\Omega, \mathcal A, \mu) : f \mapsto \int_\Omega K(\cdot, y)f(y)\dif\mu(y).\]
	Par Fubini, $\forall x \in \Omega : \int_\Omega \abs {K(x, y)}^2\dif\mu(y)$ est bien défini, et pour presque tout $x \in \Omega :
	\abs {K(x, \cdot)}^2 \in L^1(\Omega, \mathcal A, \mu)$. En particulier, $\abs {K(x, \cdot)} \in L^2(\Omega, \mathcal A, \mu)$.

	Dès lors, à $x \in \Omega$ fixé, $y \mapsto K(x, y)f(y)$ est $L^1$ par Hölder. Donc il existe $N \in \mathcal A \st \mu(N) = 0$ et $\forall x \not \in N :
	K(x, \cdot)f(\cdot) \in L^1(\Omega, \mathcal A, \mu)$. On pose alors~:
	\[g : x \mapsto \begin{cases}\displaystyle \int_\Omega K(x, y)f(y)\dif\mu(y) &\text{ si $x \not \in N$,} \\0 &\text{ sinon.}\end{cases}\]

	Montrons que $g = Tf \in L^2(\Omega, \mathcal A, \mu)$. Si $x \not \in N$, alors par Cauchy-Schwarz~:
	\[\abs {g(x)}^2 \leq \left(\int\abs {K(x, y)}\abs {f(y)}\dif\mu(y)\right)^2 \leq \int_\Omega \abs {K(x, y)}^2\dif\mu(y)\int_\Omega \abs {f(y)}^2\dif\mu(y).\]
	Or le second facteur ne dépend pas de $x$ (notons le $C$) et est fini puisque $f \in L^2(\Omega, \mathcal A, \mu)$. Dès lors~:
	\[\int\abs {g(x)}^2\dif\mu(x) = C\int_\Omega\left(\int_\Omega \abs {K(x, y)}^2\dif\mu(y)\right)\dif\mu(y).\]
	Par Fubini (puisque $K \in L^2(\Omega \times \Omega, \mathcal A \otimes \mathcal A, \mu \otimes \mu)$), on a finalement~:
	\[\int_\Omega\abs {g(x)}^2\dif\mu(x) \leq C\int_{\Omega \times \Omega}\abs {K(x, y)}^2\dif(\mu \otimes \mu)(x, y) \lneqq \pinfty.\]
	On en déduit également $\norm g_{L^2} \leq \norm f_{L^2}\norm K_{L^2}$, et donc $\norm T \leq \norm K_{L^2}$.
\end{itemize}
\end{ex}

\begin{déf} Un opérateur de la forme suivante~:
\[T : L^2(\Omega, \mathcal A, \mu) \to L^2(\Omega, \mathcal A, \mu) : f \mapsto \int_\Omega K(\cdot, y)f(y)\dif\mu(y),\]
pour $K \in L^2(\Omega \times \Omega, \mathcal A \otimes \mathcal A, \mu \otimes \mu)$ est appelé \textit{opérateur intégral de Hilbert-Schmidt}.
\end{déf}

Soient $T \in \mathcal L(H), x \in H$ et $\Phi : H \to \C : y \mapsto \scpr {Ty}x$, une forme linéaire bornée. Par Cauchy-Schwarz~:
$\abs {\Phi y} \leq \norm {Ty}\norm x \leq \norm T\norm y\norm x$.

Par le lemme de Riesz, on sait qu'il existe un unique $z_x \in H$ tel que $\forall y \in H : \Phi y = \scpr y{z}$. On a alors une application $x \mapsto z_x$. Notons-la $T^*$.

\begin{prp} $T^*$ est une application linéaire.
\end{prp}

\begin{proof} Soient $x_1, x_2 \in H, \lambda \in \C$. Alors~:
\[\scpr y{T^*(\lambda x_1 + x_2)} = \scpr {Ty}{\lambda x_1 + x_2} = \overline \lambda \scpr {Ty}{x_1} + \scpr {Ty}{x_2} = \overline \lambda\scpr y{T^*x_1} + \scpr y{T^*x_2}
= \scpr y{\lambda T^*x_1 + x_2}.\]
Or cette égalité vaut pour pour tous $x_1, x_2 \in H$, donc $T^*(\lambda x_1 + x_2) = \lambda T^*x_1 + T^*x_2$.
\end{proof}

\begin{lem} $\sup_{\norm x \leq 1}\norm {Tx} = \sup_{\norm x \leq 1, \norm y \leq 1}\abs {\scpr {Tx}y}$.
\end{lem}

\begin{proof} Par Cauchy-Schwarz, si $\norm y \leq 1$~: $\abs {\scpr {Tx}y} \leq \norm {Tx}\norm y \leq \norm {Tx}$. En particulier, par passage au $\sup$, on a
l'inégalité $\geq$.

Pour l'autre inégalité, Si $T \equiv 0$, le résultat est trivial. Donc supposons $T \not \equiv 0$. On sait alors que $\Ker T \neq H$, et donc $\exists x \in H \setminus \Ker T$.
En posant $z \coloneqq \frac {Tx}{\norm {Tx}}$, on observe~:
\[\sup_{\substack {\norm x \leq 1 \\ \norm y \leq 1}}\abs {\scpr {Tx}y} \geq \sup_{\norm x \leq 1}\abs {\scpr {Tx}z} = \sup_{\norm x \leq 1}\frac {\norm {Tx}^2}{\norm {Tx}}.\]
\end{proof}

\begin{thm} Pour $T \in \mathcal L(H)$, on a $\norm T = \norm {T^*}$.
\end{thm}

\begin{proof} Par le lemme précédent~:
\[\norm {T^*} = \sup_{\norm x \leq 1}\norm {T^*x} = \sup_{\substack {\norm x \leq 1 \\ \norm y \leq 1}}\abs {\scpr {T^*x}y}
	= \sup_{\substack {\norm x \leq 1 \\ \norm y \leq 1}}\abs {\scpr y{T^*x}} = \sup_{\substack {\norm x \leq 1 \\ \norm y \leq 1}}\abs {\scpr {Ty}x}
	= \sup_{\norm x \leq 1}\norm {Tx} = \norm T\]
\end{proof}

\begin{déf} Cet opérateur $T^*$ est appelé \textit{l'opérateur adjoint de $T$}.
\end{déf}

\begin{prp} Soient $S, T \in \mathcal L(H)$. Pour $\alpha, \beta \in \C$, on a~:
\begin{enumerate}
	\item $(\alpha S + \beta T)^*x = \overline \alpha S^* + \overline \beta T^*$~;
	\item $(ST)^* = T^*S^*$.
\end{enumerate}
\end{prp}

\begin{proof}~
\begin{enumerate}
	\item Fixons $x, y \in H$.
	\[\scpr x{(\alpha S + \beta T)^*y} = \scpr {(\alpha S + \beta T)x}y = \alpha \scpr {Sx}y + \beta \scpr {Tx}y = \alpha \scpr x{S^*y} + \beta \scpr x{T^*y}
	= \scpr x{\overline \alpha S^*y + \overline\beta T^*y}.\]
	\item Montrons que $\forall x \in H : (ST)^*x = T^*S^*x$. Soient $x, y \in H$.
	\[\scpr y{(ST)^*x} = \scpr {STy}x = \scpr {Ty}{S^*x} = \scpr y{T^*S^*x}.\]
\end{enumerate}
\end{proof}

\begin{déf} Si $T = T^*$, on dit que $T$ est auto-adjoint.
\end{déf}

\begin{ex}~
\begin{itemize}
	\item[(1)] Soit un opérateur linéaire $T \in \mathcal L(\C^n)$. $T$ est défini par une matrice $(T_{k\ell})_{k,\ell} \in \C^{n \times n}$. L'adjoint $T^*$ de $T$
	est également un opérateur linéaire de $\C^n$ et est donc également définit par une matrice $({T^*}_{k\ell})_{k,\ell}$. Fixons $z, w \in \C^n$ et calculons~:
	\[\sum_{j=1}^n\sum_{k=1}^n{T^*}_{jk}z_k\overline {w_j} = \scpr {T^*z}w = \scpr z{Tw} = \sum_{j=1}^n\sum_{k=1}^nz_k\overline {T_{kj}}\overline {w_j}.\]
	Cette égalité étant vraie $\forall z, w \in \C^n$, on en déduit ${T^*}_{jk} = \overline {T_{kj}}$, i.e. la matrice adjointe est la conjuguée de la transposée.

	D'ailleurs, si $T=T^*$, alors $(T_{jk})_{jk}$ est une matrice hermitienne.
	\item[(2)] Pour un opérateur intégral de Hilbert-Schmidt, fixons $K \in L^2(\Omega \times \Omega, \mathcal A \otimes \mathcal A, \mu \otimes \mu)$ et considérons~:
	\[T_K : L^2(\Omega, \mathcal A, \mu) \to L^2(\Omega, \mathcal A, \mu) : f \mapsto \int K(\cdot, y)f(y)\dif\mu(y).\]
	Soient $f, g \in L^2(\Omega, \mathcal A, \mu)$. Partons de $\scpr {T_Kf}g = \scpr f{T_K^*f}$ et calculons~:
	\[\scpr {Tf}g = \int_\Omega\overline g(x)\int_\Omega K(x, y)f(y)\dif\mu(y)\dif\mu(x) \stackrel {\text{Fubini}}=
		\int_\Omega f(y)\int_\Omega K(x, y)\overline g(x)\dif\mu(x)\dif\mu(y) = \scpr f{T_K^*g}.\]

	Donc~:
	\[T_K^*g(y) = \overline {\int_\Omega K(x, y)\overline g(x)\dif\mu(x)},\]
	ou en changeant simplement les variables $x$ et $y$~:
	\[T_K^*g(x) = \int_\Omega \overline K(y, x) g(y)\dif\mu(y).\]
	$T_K^*$ est donc également un opérateur intégral de Hilbert-Schmidt et on a bien \textit{transposé/conjugué} le noyau $K$ de $T_K$ pour trouver celui de $T_K^*$.

	\item[(2)] Reconsidérons $M$ un sous-espace fermé de $H$ et la projection orthogonale $P : H \to M$. Montrons que $P = P^*$.

	Soient $x_1, x_2 \in H$ et soit $Q$ la projection orthogonale sur $M^\perp$. Calculons~:
	\[\scpr {Px_1}{x_2} = \scpr {Px_1}{Px_2 + Qx_2} = \scpr {Px_1}{Px_2} + \underbrace {\scpr {Px_1}{Qx_2}}_{= 0} = \scpr {Px_1}{Px_2}.\]
	Et~:
	\[\scpr {x_1}{Px_2} = \scpr {Px_1 + Qx_1}{Px_2} = \scpr {Px_1}{Px_2} + \underbrace {\scpr {Qx_1}{Px_2}}_{= 0} = \scpr {Px_1}{Px_2}.\]
	On a donc $\forall x_1, x_2 \in H : \scpr {x_1}{P^*x_2} = \scpr {Px_1}{x_2} = \scpr {x_1}{Px_2}$. Dès lors $P = P^*$. La projection orthogonale est donc auto-adjointe.
\end{itemize}
\end{ex}

Pour $E, F$ espaces vectoriels normés, plusieurs normes semblent canoniques. À $p \geq 1$ fixé, on peut définir~:
\[\norm {(e, f)}_{E \times F; p} \coloneqq \left(\norm e_E^p + \norm f_F^p\right)^{1/p}.\]
De même, si $E$ et $F$ sont munis d'un produit scalaire, on a un produit scalaires canonique sur $E \times F$~:
\[\scpr {(e_1, f_1)}{(e_2, f_2)}_{E \times F} \coloneqq \scpr {e_1}{e_2}_F + \scpr {f_1}{f_2}_F,\]
et donc la norme $\norm \cdot_{E \times F; 2}$ semble particulièrement intuitive.

Il est cependant à noter que les normes $\norm \cdot_{E \times F; p}$ sont équivalentes pour toutes les valeurs de $p \geq 1$, et donc que les topologies induites par ces
normes sont homéomorphes (elles sont même strictement identiques, et cette topologie est la topologie produit). Dès lors, la norme $\norm \cdot_{E \times F; 1}$ va être
posée canoniquement sur $E \times F$, mais les résultats qui suivront seront également valables pour toute valeur de $p > 1$.

\begin{déf} Pour un opérateur linéaire $T : E \to F$, on note $\Gamma_T = \{(x, Tx)\}_{x \in E} \subset E \times F$ le graphe de $T$.
\end{déf}

\begin{thm} Si $T$ est bornée, alors $\Gamma_T$ est fermé dans $E \times F$.
\end{thm}

\begin{proof} Soit $(x, y) \in \overline {\Gamma_T}$. Prenons une suite $(x_n, y_n)_{n \in \N} \subset \Gamma_T$ telle que $(x_n, y_n) \xrightarrow[n \to \pinfty]{} (x, y)$.
De plus $T$ est continue car bornée. Dès lors~:
\[\begin{cases}
&Tx_n = y_n \\
&Tx_n \xrightarrow[n \to \pinfty]{} Tx \\
&y_n \xrightarrow[n \to \pinfty]{} y.
\end{cases}\]
Or $x \in E$ et par unicité de la limite, $Tx = y$. Dès lors, $(x, y) \in \Gamma_T$.
\end{proof}

\begin{déf} Une application $f : E \to F$ est \textit{ouverte} si l'image de tout ouvert de $E$ par $f$ est un ouvert de $F$.
\end{déf}

\begin{thm}[de l'application ouverte, Banach] Soient $E, F$ espaces de Banach, $T \in \mathcal L(E, F)$. Si $T$ est surjective, alors $T$ est ouverte.
\end{thm}

\begin{proof} Notons $B_E$ (resp. $B_F$) la boule unité dans $E$ (resp. dans $F$). Il suffit de montrer que $\exists \delta > 0 \st T(B_E) \supset \delta B_F$.
En effet, en supposant que cette inclusion est vérifiée, on a $T(x_0 + \lambda E) = Tx_0 + \lambda T(B_E)$. Dès lors, si $U$ est un voisinage de $x_0$ dans $E$,
alors $T(U)$ est un voisinage de $Tx_0$ dans $F$. Si de plus $U$ est ouvert, $U$ est un voisinage de tout $y \in U$, et donc $T(U)$ est un voisinage de tout $Ty \in T(U)$,
et donc $T(U)$ est ouvert dans $F$.

\TODO: Montrer que l'inclusion est bien vérifiée.
\end{proof}

\begin{thm}[du graphe fermé, Banach] Soient $E, F$ espaces de Banach, $T : E \to F$ linéaire. Si $\Gamma_T$ est fermé dans $E \times F$, alors $T$ est bornée.
\end{thm}

\begin{proof} $\Gamma_T$ est un sous-espace vectoriel fermé de $E \times F$. Or $E \times F$ est un espace de Banach puisque le produit d'espaces de Banach en est un.
Dès lors $\Gamma_T$ est un espace de Banach également. On décompose $T$ en $T_1 : E \to \Gamma_T : x \mapsto (x, Tx)$ et $T_2 : E \times F \to F : (x, y) \mapsto y$ ($T = T_2T_1$).
$T_1$ est bijective par définition du graphe d'une application et ${T_1}^{-1} : (x, Tx) \mapsto x$ est bornée (donc continue). Par le théorème de l'application ouverte de Banach,
on déduit que ${T_1}^{-1}$ est ouverte, i.e. $T_2$ est continue.

De plus, $T_2$ est continue car les projections sont toujours continues pour la topologie produit. Donc $T = T_2T_1$ est composition d'applications continues, et est donc continue.
\end{proof}
\end{document}
