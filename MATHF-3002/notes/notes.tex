\documentclass{report}

\usepackage{commath}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fullpage}
\usepackage[bottom]{footmisc}
\usepackage{hyperref}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{stmaryrd}

%% from mathabx.sty and mathabx.dcl
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
      <5> <6> <7> <8> <9> <10>
      <10.95> <12> <14.4> <17.28> <20.74> <24.88>
      mathx10
      }{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareFontSubstitution{U}{mathx}{m}{n}
\DeclareMathAccent{\widecheck}{0}{mathx}{"71}
\DeclareMathAccent{\wideparen}{0}{mathx}{"75}
%%%%%%%%%

\usepackage[parfill]{parskip}
\usepackage[framemethod=tikz]{mdframed}

\mdfdefinestyle{resultstyle}{%
	skipabove=5pt,%
	skipbelow=5pt,%
	hidealllines=true,%
	leftline=true,%
	rightline=true,%
	innerleftmargin=10pt,%
	innerrightmargin=10pt,%
	innertopmargin=10pt,%
	innerbottommargin=8pt,%
}

\surroundwithmdframed[style=resultstyle]{thm}
\surroundwithmdframed[style=resultstyle]{prp}
\surroundwithmdframed[style=resultstyle]{cor}
\surroundwithmdframed[style=resultstyle]{lem}

\DeclareMathOperator{\Vect}{Vect}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\spectreOperator}{spectre\!}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\Imapp}{Im}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\rot}{rot}
\DeclareMathOperator{\dist}{dist}

\newcommand{\C}{{\mathbb C}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\scpr}[2]{\left\langle#1, #2\right\rangle}
\newcommand{\tq}{\text{ t.q. }}
\newcommand{\st}{\tq}
\newcommand{\pinfty}{{+\infty}}
\newcommand{\minfty}{{-\infty}}
\newcommand{\intint}[2]{{\left\llbracket#1, #2\right\rrbracket}}
\newcommand{\cste}{\text{c}^{\text{ste}}}
\newcommand{\dx}{\dif x}
\newcommand{\Id}{\mathrm {Id}}
\newcommand{\spectre}[1]{{\spectreOperator\left(#1\right)}}
\newcommand{\restr}[2]{\left.#1\vphantom{\big|}\right|_{#2}}
\newcommand{\loc}{{\text{\textnormal{loc}}}}

\newcommand{\TODO}{TODO}
\newcommand{\unic}{{\underline {\textbf{unicité}~:}} }
\newcommand{\exis}{{\underline {\textbf{existence}~:}} }

\newtheorem{thm}{Théorème}[chapter]
\newtheorem{prp}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollaire}
\newtheorem{lem}[thm]{Lemme}
\newtheorem{claim}[thm]{Affirmation}
\renewcommand\proofname{\textnormal{\textbf{Démonstration}}}
\theoremstyle{definition}
\newtheorem{déf}[thm]{Définition}
\theoremstyle{remark}
\newtheorem*{rmq}{Remarque}
\newtheorem{ex}{Exemple}[chapter]

\renewcommand{\chaptername}{Chapitre}

\title{Espaces fonctionnels et séries de Fourier --- Notes de cours de Pr. P. Godin}
\author{Robin Petit}
\date{Année académique 2017-2018}

\begin{document}

\pagenumbering{Roman}
\maketitle
\tableofcontents
\setcounter{page}{1}
\pagenumbering{arabic}

\chapter{Transformation de Fourier}

\section{Définitions}

On considère $\R^n$ à $n$ fixé en tant qu'espace de mesure $(\R^n, \mathcal M, \lambda)$ avec $\mathcal M$ la famille des ensembles Lebesgue-mesurables et $\lambda$
la mesure de Lebesgue sur $\R^n$. Pour $f \in L^1(\R^n)$, on note $\int f\dif x$ l'intégrale de $f$ par rapport à la mesure de Lebesgue sur $\R^n$.

\begin{déf}\label{déf:Fourier dans L^1} Pour $u \in L^1(\R^n)$, on définit sa \textit{transformée de Fourier} par~:
\begin{equation}
	\hat u : \R^n \to \C : \xi \mapsto \int e^{-i\scpr x\xi}u(x)\dif x.
\end{equation}
\end{déf}

Cette fonction est bien définie car $x \mapsto e^{-i\scpr x\xi}$ est bornée en module (et donc $L^\infty$), et $u$ est intégrable, donc $x \mapsto e^{-i\scpr x\xi}u(x)$ est intégrable
par Hölder. Ou plus simplement, pour $x \in \R^n : \abs {e^{-\scpr x\xi}u(x)} = \abs {u(x)}$, et donc $x \mapsto e^{-i\scpr x\xi}u(x)$ est intégrable.

\begin{prp} Pour $u \in L^1(\R^n)$, $\hat u$ est continue.
\end{prp}

\begin{proof} Soient $\xi_0 \in \R^n$ et $(h_k)_{k \in \N} \subset \R^n \tq h_k \xrightarrow[k \to \pinfty]{} 0$.
\[\hat f(\xi_0 + h_k) = \int e^{-i\scpr x{\xi_0}}e^{-i\scpr x{h_k}}f(x)\dif x.\]

Puisque $\abs {e^{-i\scpr x{\xi_0}}e^{-i\scpr x{h_k}}f(x)} = \abs {f(x)}$ et
$e^{-i\scpr \cdot{\xi_0}}e^{-i\scpr \cdot{h_k}}f(\cdot) \xrightarrow[k \to \pinfty]{\lambda\text{-p.p.}}e^{-i\scpr \cdot{\xi_0}}f(\cdot)$ (la suite converge même partout),
par le théorème de la convergence dominée, on sait~:
\[\hat f(\xi_0 + h_k) \xrightarrow[k \to \pinfty]{} \int e^{-i\scpr x{\xi_0}}f(x)\dif x = \hat f(\xi_0).\]
\end{proof}
\vspace{1cm}

\begin{prp} Soit $u \in L^1(\R^n)$. Si $\forall j \in \intint 1n : x_jf \in L^1$, alors~: $\hat u \in C^1$ et~:
\begin{equation}\label{eq:pd transformée Fourier}
	\forall j \in \intint 1n : \dpd {\hat u}{\xi_j}\sVert[3]_\xi = \int e^{-i\scpr x\xi}(-ix_j)u(x)\dif x.
\end{equation}
\end{prp}

\begin{proof} Soit $(h_k)_{k \in \N} \subset \R \tq h_k \xrightarrow[k \to \pinfty]{} 0$, et prenons $\{e_j\}_{j=1}^n$ la base canonique de $\R^n$. Pour tout $\xi \in \R^n$~:
\[\frac {\hat u(\xi + h_ke_j) - \hat u(\xi)}{h_k} = \int e^{-i\scpr x\xi}\underbrace {\frac {e^{-ih_kx_j}-1}{h_k}}_{\xrightarrow[k \to \pinfty]{} -ix_j}u(x)\dif x.\]

En module~:
\[\abs {e^{-i\scpr x\xi}\frac {e^{-ih_kx_j}-1}{h_k}f(x)} = \abs {e^{-i\scpr x\xi}}\abs {\frac {e^{-ih_kx_j}-1}{h_k}}\abs {f(x)} \leq C\abs {x_j}\abs {f(x)},\]
qui est intégrable par hypothèse.

En effet, si $x_j = 0$, alors tout est nul et l'inégalité devient une égalité~; et si $x_j \neq 0$, alors $\frac {\abs {e^{-ih_kx_j}-1}}{\abs {x_jh_k}}$ est borné.

Dès lors, par le théorème de convergence dominée, la limite passe sous l'intégrale et on a~\eqref{eq:pd transformée Fourier}.
\end{proof}

\begin{cor} Pour $m \in \N^*$, si $(1+\abs x)^mu \in L^1$, alors $u \in C^m$ et on peut dériver $m$ fois sous le signe:
\[\partial^\alpha\hat u(\xi) = \int e^{-i\scpr x\xi}(-i)^{\abs \alpha}x^\alpha u(x)\dif x\]
\end{cor}

\begin{proof} Exercice (récurrence sur $m$).
\end{proof}

\begin{déf} On définit l'ensemble de Schwartz~:
\begin{equation}\begin{aligned}
	\mathcal S(\R^n) \coloneqq &\left\{u \in C^\infty(\R^n) \tq \forall \alpha, \beta \in \N^n : x^\alpha\partial^\beta u \text{ est borné dans } \R^n\right\} \\
		= &\left\{u \in C^\infty(\R^n) \tq \forall \alpha, \beta \in \N^n : x^\alpha\partial^\beta u \in L^\infty(\R^n)\right\}.
\end{aligned}\end{equation}
\end{déf}

Dans l'idée, $\mathcal S$ est l'ensemble des fonctions dont toutes les dérivées décroissent plus vite vers 0 aux infinis que tout polynôme.

\begin{prp} $\mathcal S(\R^n)$ est un $\C$-espace vectoriel.
\end{prp}

\begin{proof} Immédiat par le fait que $C^\infty(\R^n)$ et $L^\infty(\R^n)$ sont des $\C$-evs.
\end{proof}

\begin{prp}\label{prp:C^infty_0 dans Schartz dans l'intersection des L^p} Si $C^\infty_0(\R^n) = C^\infty_c(\R^n)$ désigne l'ensemble des fonctions $C^\infty$ à support compact, alors~:
\[C^\infty_0(\R^n) \subseteq \mathcal S(\R^n) \subseteq \bigcap_{1 \leq p \leq \pinfty} L^p(\R^n).\]
\end{prp}

\begin{proof}~
\begin{itemize}
	\item[$(i)$] Pour $u \in \mathcal S$ et $\pinfty > p \geq 1$~:
	\[\int \abs u^p\dif x = \int \Big(\underbrace{\abs u(1+\abs x)^N}_{\text{borné pour tout $N$}}\Big)^p(1+\abs x)^{-Np}\dif x
		\leq \int (C_N)^p\underbrace {(1+\abs x)^{-Np}}_{\text{intégrable pour $Np > n$}}\dif x.\]

	Dès lors, pour $N$ suffisamment grand ($Np>n$), on a $\int \abs u\dif x \leq \cste$

	Le cas $p = \pinfty$ vient uniquement du fait que pour $u \in \mathcal S$, pour $\alpha = \beta = (0, \ldots, 0) \in \N^n$:
	\[u = x^\alpha\partial^\beta u \in L^\infty.\]

	\item[$(ii)$] Soit $u \in C^\infty_0(\R^n)$. Pour $\alpha, \beta \in \N^n$~: $x^\alpha\partial^\beta u \neq 0 \subseteq \supp u$ compact. Par Heine-Cantor,
	$u$ est uniformément continue sur $\supp u$.
\end{itemize}
\end{proof}

\begin{prp} Pour $u \in \mathcal S$ et $\alpha, \beta \in \N^n$, alors~: $x^\alpha\partial^\beta u \in \mathcal S$.
\end{prp}

\begin{proof} Soient $\lambda, \mu \in \N^n$. Par Leibniz~:
\[\partial^\mu(fg) = \sum_{\sigma \leq \mu}\binom \mu\sigma\partial^\sigma f \partial^{\mu-\sigma} g.\]

Donc~:
\[x^\lambda\partial^\mu(x^\alpha\partial^\beta u) = \sum_{\sigma \leq \mu}\binom \mu\sigma x^\lambda\partial^\sigma(x^\alpha)\partial^{\mu-\sigma}u,\]
où $x^\lambda\partial^\sigma(x^\alpha) \leq \cste x^\gamma$. On en déduit que $x^\lambda\partial^\mu(x^\alpha\partial^\beta u)$ est une somme finie de termes
essentiellement bornés et est donc essentiellement bornée.
\end{proof}

À défaut de définir une topologie sur $\mathcal S(\R^n)$, on définit uniquement une notion de convergence.

\begin{déf}\label{déf:convergence Schwartz} Soit $(u_k)_{k \in \N} \subset \mathcal S(\R^n)$, $u \in \mathcal S(\R^n)$, on dit que \textit{$u_k$ converge vers $u$ dans
$\mathcal S(\R^n)$ lorsque $k \to \pinfty$} (noté $u_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} u$) lorsque~:
\[\forall \alpha, \beta \in \N^n : \sup_{x \in \R^n}\abs {x^\alpha\partial^\beta(u-u_k)} \xrightarrow[k \to \pinfty]{} 0,\]
i.e. lorsque~:
\[\forall \alpha, \beta \N^n : \norm {x^\alpha\partial^\beta(u_k-u)}_{L^\infty} \xrightarrow[k \to \pinfty]{} 0.\]
\end{déf}

\begin{thm}\label{thm:Fourier continu sur Schwartz} Soit $u \in \mathcal S(\R^n)$. Alors~:
\begin{enumerate}
	\item $\hat u \in \mathcal S(\R^n)$. De plus si $u_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} u$, alors $\hat {u_k} \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \hat u$.
	\item $\widehat {D_ju}(\xi) = \xi_j\hat u(\xi)\quad$ (de plus $\widehat {x_ju} = -D_j \hat u$) où $D_j = \frac 1i\partial_j$.
\end{enumerate}
\end{thm}

\begin{proof} Pour le premier point, on calcule~:
\[D^\alpha_\xi\hat u(\xi) = \int D^\alpha_\xi(e^{-i\scpr x\xi})u(x)\dif x = \int e^{-i\scpr x\xi}(-x)^\alpha u(x)\dif x.\]

Donc~:
\[\xi^\beta D^\alpha_\xi\hat u(\xi) = \int \xi^\beta e^{-i\scpr x\xi}(-x)^\alpha u(x)\dif x = \int (-D_x)^\beta(e^{-i\scpr x\xi})(-x)^\alpha u(x)\dif x
	= \int e^{-i\scpr x\xi} D^\beta_x\left((-x)^\alpha u(x)\right)\dif x.\]

Pour montrer cette dernière égalité, intégrons par partie. D'abord observons pour $\phi \in \mathcal S$ et $j \in \intint 1n$~:
\[\int \partial_j\phi\dx = \idotsint\left(\int \partial_j\phi\dx_j\right)\dx_1\ldots\dx_{j-1}\dx_{j+1}\ldots\dx_n.\]
En effet, $\partial_j\phi$ est Borélienne, donc par Fubini, on peut passer de l'intégrale sur $\R^n$ à $n$ intégrale itérées sur $\R$. Or~:
\begin{align*}
	\int \partial_j\phi\dx_j &= \lim_{N \to \pinfty}\int_{-N}^N\partial_j\phi(x)\dx_j \\
	&= \lim_{N \to \pinfty}\left(\underbrace {\phi(x_1, \ldots, x_{j-1}, N, x_{j+1}, \ldots, x_n)}_{\xrightarrow[N \to \pinfty]{} 0}
			- \underbrace {\phi(x_1, \ldots, x_{j-1}, N, x_{j+1}, \ldots, x_n)}_{\xrightarrow[N \to \pinfty]{} 0}\right) = 0,
\end{align*}
puisque $\phi \in \mathcal S(\R^n)$ implique $\phi(x) \xrightarrow[\abs x \to \pinfty]{} 0$. On en déduit donc que $\int\partial_j\phi\dx = 0$.

Dès lors, puisque $e^{-i\scpr x\xi}(-x)^\alpha u(x) \in \mathcal S(\R^n)$ et par récurrence~:
\[\int (-D_x)^\beta(e^{-i\scpr x\xi})(-x)^\alpha u(x)\dif x	= \int e^{-i\scpr x\xi} D^\beta_x\left((-x)^\alpha u(x)\right)\dif x.\]

Montrons alors que $\forall N \in \N : \exists C_N \geq 0 \tq \abs {D_x^\beta\left((-x)^\alpha u(x)\right)} \leq C_N(1 + \abs x)^{-N}$. Par Leibniz~:
\[(1 + \abs x)^N\partial^\beta (x^\alpha u(x)) = (1 + \abs x)^N\sum_{\gamma \leq \beta} \binom \beta\gamma\partial^\gamma x^\alpha\partial^{\beta-\gamma}u(x)\]
est borné car $u \in \mathcal S(\R^n)$. Dès lors~:
\[\abs {\xi^\beta D_\xi^\alpha \hat u(x)} \leq C_N\int(1 + \abs x)^{-N}\dx.\]

Pour $N$ suffisamment grand ($N > n$), on a $\abs {\xi^\beta D_\xi^\alpha \hat u(x)} \leq \cste$.

Dès lors, on trouve~:
\[\abs {\xi^\beta D_\xi^\alpha\hat u(x)} \leq \sup_{x \in \R^n}(1 + \abs x)^{-N}\abs {D_x^\beta\left((-x)^\alpha(u - u_k)\right)} \xrightarrow[k \to \pinfty]{} 0.\]
On en déduit donc $\hat {u_k} \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \hat u$.

Pour le second point, la seconde formule découle directement du premier pour $\alpha = e_j$~:
\[D_j\hat u(\xi) = \int e^{-i\scpr x\xi}(-x_j)u(x)\dx = -\widehat {x_ju}(\xi).\]

La première égalité se démontre par~:
\[\widehat {D_ju}(\xi) = \int e^{-i\scpr x\xi} D_ju(x)\dx = -\int D_{x,j}\left(e^{-i\scpr x\xi}\right)u(x)\dx = \xi_j\int e^{-i\scpr x\xi}u(x)\dx = \xi_j\hat u(\xi).\]
\end{proof}

\section{Formule d'inversion}

\begin{thm}\label{thm:inversion Fourier} Soit $u \in \mathcal S(\R^n)$. Alors~:
\begin{equation}\label{eq:inversion Fourier}
	u(x) = (2\pi)^{-n}\int e^{i\scpr x\xi}\hat u(\xi)\dif\xi.
\end{equation}
\end{thm}

La fonction $(y, \xi) \mapsto e^{i\scpr x\xi}e^{-i\scpr y\xi}u(y)$ n'est pas intégrable pour $(y, \xi)$. On ne va donc pas pouvoir appliquer Fubini naïvement.

\begin{proof} Pour $\chi \in \mathcal S(\R^n)$, $(y, \xi) \mapsto e^{-i\scpr y\xi}e^{i\scpr x\xi}\chi(\xi)u(y)$, à $x$ fixé, est intégrable. Donc par Fubini~:
\[\int e^{i\scpr x\xi}\chi(\xi)\hat u(\xi)\dif\xi = \int e^{i\scpr x\xi}\chi(\xi)\int e^{-i\scpr y\xi}u(y)\dif y\dif\xi
	= \int u(y)\int e^{-i\scpr {y-x}\xi}\chi(\xi)\dif\xi \dif y = \int u(y)\hat \chi(y-x)\dif y.\]

Pour $\psi \in \mathcal S(\R^n)$, $\delta > 0$ tels que $\chi(\xi) = \psi(\delta\xi)$~:
\[\hat \chi(\xi) = \int e^{-i\scpr y\xi}\psi(\delta\xi)\dif\xi = \delta^{-n}\hat \psi(\xi/\delta).\]

Alors~:
\[\int e^{i\scpr x\xi}\psi(\delta\xi)\hat u(\xi)\dif\xi = \int u(x+y)\delta^{-n}\psi(y/\delta)\dif y = \int u(x+\delta y)\hat \psi(y)\dif y.\]

Par le théorème de convergence dominée~:
\[\int u(x+\delta y)\hat \psi(y)\dif y \xrightarrow[\delta \to \pinfty]{} u(x)\int \hat \psi(y)\dif y,\]
or~:
\[\int e^{i\scpr x\xi}\psi(\delta \xi)\hat u(\xi)\dif\xi \xrightarrow[\delta \to \pinfty]{} \psi(0)\int e^{i\scpr x\xi}\hat u(\xi)\dif\xi.\]

Par unicité de la limite, si $\int \hat \psi \dif y\neq 0$~:
\[u(x) = \frac {\psi(0)}{\int\hat \psi(y)\dif y}\int e^{i\scpr x\xi}\hat u(\xi)\dif\xi\]

Dans le cas $n=1$, on prend $\psi_1 : x \mapsto e^{-x^2/2}$. En intégrant $z \mapsto e^{-z^2/2}$ sur un chemin rectangulaire $[a,b,c,d] \subset \C$, on trouve~:
\[\int_a^b e^{-x^2/2}\dx + \int_b^c e^{-z^2/2}\dif z + \int_c^d e^{-z^2/2}\dif z + \int_d^a e^{-z^2/2}\dif z = 0\]
par Cauchy. Pour $(a, b) \to (\minfty, \pinfty)$, on trouve que $\int_b^c e^{-z^2/2}$ et $\int_d^a e^{-z^2/2}$ tendent vers $0$. Donc à la limite~:
\[\int_a^b e^{-x^2/2}\dx = \int_{\Im z = t}e^{-z^2/2}\dif z = \int e^{(x^2-t^2)/2}e^{-itx}\dx.\]

Donc $\hat \psi(t) = \psi(t)\int\psi\dx$. On en déduit~:
\[\int \hat \psi(t)\dif t = \left(\int \psi(x)\dx\right)^2 = \left(\int e^{-x^2/2}\right)^2 = 2\pi.\]

Dès lors $\psi(0) = 1$ et $\int\hat\psi\dx = 2\pi$, qui donne bien la formule.

Dans le cas général $n > 1$, on prend $\psi(x) = e^{-\abs x^2/2} = \prod_{j=1}^n\psi_1(x_j)$. Donc~:
\[\hat\psi(\xi) = \int e^{-i\scpr x\xi}\psi(x)\dx = \int e^{-i\scpr x\xi}\prod_{j=1}^ne^{-x_j^2/2}\dx = \int\prod_{j=1}^ne^{-ix_j\xi_j}\prod_{j=1}^ne^{-x_j^2/2}\dx
	= \int\prod_{j=1}^n\left(e^{-ix_j\xi_j}e^{-x_k^2/2}\right)\dx.\]

Par Fubini~:
\[\hat\psi(\xi) = \prod_{j=1}^n\int e^{-ix_j\xi_j}e^{-x_j^2/2}\dx = \prod_{j=1}^n\hat \psi_1(\xi_j).\]

On trouve alors~:
\[\int \hat \psi(\xi)\dif\xi = \int \prod_{j=1}^n\hat\psi_j(\xi_j)\dif\xi = \prod_{j=1}^n\int\hat \psi_1(\xi_j)\dif\xi_j = (2\pi)^{-n},\]
où l'avant dernière égalité s'obtient en appliquant Fubini.

Puisque $\hat \psi(0) = 1$, on a bien~\eqref{eq:inversion Fourier}.
\end{proof}

On définit une application \textit{transformée de Fourier} $\mathcal F : \mathcal S(\R^n) \to \mathcal S(\R^n) : u \mapsto \mathcal Fu \coloneqq \hat u$.

\begin{prp} $\mathcal F$ est une bijection linéaire.
\end{prp}

\begin{proof} Par la formule d'inversion, $\mathcal F$ est injective~: si $\mathcal Fu = 0$, alors $u = 0$.

De plus, $\mathcal F$ est surjective. Pour $f \in \mathcal S(\R^n)$, montrons qu'il existe $u \in \mathcal S(\R^n) \tq \mathcal Fu = f$.
Prenons $u(x) = (2\pi)^{-n}\int e^{i\scpr x\xi}f(\xi)\dif\xi$. Alors~:
\[\mathcal Ff(x) = \int e^{-i\scpr x\xi}f(\xi)\dif\xi = (2\pi)^nu(-x).\]

De plus~:
\[\hat {\hat u}(\xi) = \int e^{-i\scpr x\xi}\hat u(x)\dx = (2\pi)^n(2\pi)^{-n}\int e^{-i\scpr x\xi}\hat u(x)\dx = (2\pi)^nu(-\xi),\]
donc $\hat f = \hat {\hat u}$ pour tout $x$, et puisque $\mathcal F$ est injective, $f = \hat u$. Donc $\mathcal F$ est surjective, et donc bijective.

La linéarité est triviale~:
\[\mathcal F(f+\lambda g)(\xi) = \int e^{-i\scpr x\xi}(f+\lambda g)(x)\dx = \int e^{-i\scpr x\xi}f(x)\dx + \lambda\int e^{-i\scpr x\xi}g(x)\dx
	= \left(\hat f + \lambda \hat g\right)(\xi).\]
\end{proof}

Posons la transformation de Fourier inverse $\tilde {\mathcal F} : \mathcal S(\R^n) \to \mathcal S(\R^n) : u \mapsto \tilde {\mathcal F}u$ où
$\tilde {\mathcal F}u(x) = (2\pi)^{-n}\int e^{i\scpr x\xi}u(\xi)\dif\xi$. Par un raisonnement similaire à la Proposition précédente,
on trouve $\tilde {\mathcal F}$ est une bijection linéaire. De plus $\mathcal F^{-1} = \tilde {\mathcal F}$.

Également, puisque $\mathcal F$ transforme des suites convergentes en suites convergentes sur $\mathcal S(\R^n)$, $\tilde {\mathcal F}$ fait de même.

\begin{rmq} Cela veut dire que $\mathcal F$ est une homéomorphisme linéaire de $\mathcal S(\R^n)$ dans $\mathcal S(\R^n)$ pour la topologie non définie ici.
\end{rmq}

\begin{prp} Pour $u, v \in \mathcal S(\R^n)$~:
\begin{enumerate}
	\item $\int u\hat v = \int\hat uv$~;
	\item $\int u\overline v = (2\pi)^{-n}\int \hat u\hat {\overline v}$. Cette égalité est appelée \textit{identité de Parseval}.
\end{enumerate}
\end{prp}

\begin{proof} Le premier point se montre par la formule de la preuve du Théorème~\ref{thm:inversion Fourier} pour $u,\chi \in \mathcal S(\R^n)$~:
\[\int e^{i\scpr x\xi}\chi(\xi)\hat u(\xi)\dif\xi = \int u(x+y)\hat \chi(y)\dif y\]
en $x=0$.

Le second point, prenons $u, w \in \mathcal S(\R^n)$ et posons $v \coloneqq (2\pi)^{-n}\overline {\hat w}$. Par le premier point~:
\[\int \hat uv = \int u\hat v = \int u(2\pi)^{-n}\hat {\overline {\hat w}}.\]

On peut voir que~:
\[(2\pi)^{-n}\hat {\overline {\hat w}}(\xi) = (2\pi)^{-n}\int e^{-i\scpr x\xi}\overline {\hat w}(x)\dx =
	(2\pi)^{-n}\overline {\int e^{i\scpr x\xi}\hat w(x)\dx} = \overline {w(\xi)}.\]

Dès lors~:
\[\int \hat u(2\pi)^{-n}\overline {\hat w} = \int u\overline w.\]
\end{proof}

\begin{cor}[Formule de Plancherel] Pour $u \in \mathcal S(\R^n)$, on a~:
\begin{equation}\label{eq:Plancherel}
	\int \abs u^2\dx = (2\pi)^{-n}\int\abs {\hat u}^2\dif\xi
\end{equation}
\end{cor}

\section{Discussion sur la définition de la transformée}

On peut définir la transformée de Fourier de plusieurs manières, paramétrisé par $a, b \in \R$~:
\[\mathcal F_{a,b}u(\xi) = a\int e^{-ib\scpr x\xi}u(x)\dx.\]

La théorie reste la même à homothétie près puisque~:
\[\mathcal Fu(\xi) = \frac 1a\mathcal F_{a,b}u(\xi/b).\]

\[(2\pi)^{-n}\int \hat u(\xi)\overline {\hat v}(\xi)\dif\xi = \frac {(2\pi)^{-n}b^n}{a^n}\int\mathcal F_{a,b}u(\eta)\overline {\mathcal F_{a,b}v(\eta)}\dif\eta.\]
Donc on peut choisir $a=1$ et $b=2\pi$ ou encore $a=(2\pi)^{n/2}$ et $b=1$ afin de simplifier la formule de Parseval qui devient~:
\[\int u\overline v = \int \mathcal Fu\overline {\mathcal Fv}.\]

Cependant le choix $a=b=1$ permet de ne pas avoir de terme $b^k$ lors des dérivations sous le signe intégral.

\section{Extension de la transformée à $L^2(\R^n)$}

\begin{prp} Il existe une unique application linéaire continue $\mathbb F : L^2(\R^n) \to L^2(\R^n)$ telle que $\mathbb F\sVert[2]_{\mathcal S} = \mathcal F$ et~:
\[\int u\overline v\dif x = (2\pi)^{-n}\int \mathbb Fu\overline {\mathbb Fv}\dif\xi,\]
i.e. $\mathbb F$ préserve l'identité de Parseval.
\end{prp}

\begin{proof} Admettons que $C_0^\infty(\R^n)$ est dense dans $L^2(\R^n)$. Puisque $C_0^\infty(\R^n) \subseteq \mathcal S(\R^n)$, on a $\mathcal S(\R^n)$ dense dans $L^2(\R^n)$.
Par cette densité, pour $u \in L^2(\R^n)$, il existe $(u_k)_{k \in \N} \subset \mathcal S(\R^n)$ telle que $u_k \xrightarrow[k \to \pinfty]{L^2} u$, et donc $(u_k)$ est de
Cauchy pour cette norme. $(\widehat {u_k})_{k \in \N}$ est également de Cauchy car~:
\[\norm {\widehat {u_k} - \widehat {u_m}}_{L^2} = (2\pi)^{n/2}\norm {u_k - u_m}_{L^2}.\]

Par complétude de $L^2$, il existe $z \in L^2(\R^n) \tq \widehat {u_k} \xrightarrow[k \to \pinfty]{L^2} z$. On pose alors $\mathbb Fu \coloneqq z$. Montrons que $z$ ne dépend pas
de la suite $(u_k)_k$ choisie pour montrer que $\mathbb F$ est bien définie.

Soit $(v_k)_{k \in \N} \subset \mathcal S(\R^n) \tq v_k \xrightarrow[k \to \pinfty]{L^2} u$. Alors $v_k-u_k \xrightarrow[k \to \pinfty]{L^2} 0$. Par Plancherel,
$\widehat {v_k}-\widehat {u_k} \xrightarrow[k \to \pinfty]{L^2} 0$. Dès lors $\widehat {v_k} \xrightarrow[k \to \pinfty]{L^2} z$.

\textbf{Montrons que $\mathbb F$ est linéaire.}

Soient $u, v \in L^2(\R^n)$. Soient $(u_k)_{k \in \N}, (v_k)_{k \in \N} \subset \mathcal S(\R^n)$ telles que $u_k \xrightarrow[k \to \pinfty]{L^2(\R^n)} u$ et
$v_k \xrightarrow[k \to \pinfty]{L^2(\R^n)} v$. Alors $u_k+v_k \xrightarrow[k \to \pinfty]{L^2(\R^n)} u+v$. Par linéarité de $\mathcal F$,
$\widehat {u_k} + \widehat {v_k} = \widehat {u_k + v_k} \xrightarrow[k \to \pinfty]{L^2(\R^n)} \mathbb F(u+v)$.

Donc $\widehat {u_k} + \widehat {v_k} \xrightarrow[k \to \pinfty]{L^2(\R^n)} \mathbb Fu + \mathbb Fv$ et
$\widehat {u_k} + \widehat {v_k} \xrightarrow[k \to \pinfty]{L^2(\R^n)} \mathbb F(u+v)$. Par unicité de la limite dans $L^2(\R^n)$, on en déduit
$\mathbb F u + \mathbb F v = \mathbb F(u+v)$. Il est également trivial que pour $\lambda \in \R : \mathbb F(\lambda u) = \lambda \mathbb Fu$.

Pour montrer que $\mathbb F\sVert[2]_{\mathcal S} = \mathcal F$, prenons $u \in \mathcal S(\R^n)$, et la suite $(u_k)_{k \in \N}$ constante $u_k = u$. Par définition de $\mathbb F$,
on a $\mathbb Fu = \hat u$ car $\forall k \in \intint 1n : \widehat {u_k} = \hat u$, donc $\widehat {u_k} \xrightarrow[k \to \pinfty]{L^2(\R^n)} \hat u$.

\textbf{Montrons alors que $\mathbb F$  vérifie Parseval.}

Premier cas~: $u \in L^2(\R^n)$ et $v \in \mathcal S(\R^n)$. Il existe $\mathcal S(\R^n) \supset (u_k)_{k \in \N} \xrightarrow[k \to \pinfty]{L^2} u$. Donc~:
\[\int u\overline v = \int u_k\overline v + \int (u-u_k)\overline v.\]

Puisque~:
\[\abs {\int (u-u_k)\overline v} \leq \underbrace {\norm {u-u_k}_{L^2}}_{\xrightarrow[k \to \pinfty]{} 0} \norm v \xrightarrow[k \to \pinfty]{} 0,\]
on sait~:
\[\int u_k\overline v \xrightarrow[k \to \pinfty]{} \int u\overline v.\]

De même, puisque~:
\[\abs {\int (\mathbb Fu - \widehat {u_k})\overline {\widehat v}\dif x} \leq \norm {\mathbb Fu - \widehat {u_k}}_{L^2}\norm {\overline {\widehat v}}_{L^2}
\xrightarrow[k \to \pinfty]{} 0,\]
on sait~:
\[\int \widehat {u_k}\overline {\widehat v}\dif\xi \xrightarrow[k \to \pinfty]{} \int\mathbb Fu\overline {\widehat v}\dif\xi.\]

Or $u_k, v \in \mathcal S(\R^n)$. Donc pour $k \to \pinfty$, par Cauchy-Schwarz et par Parseval pour $\mathcal F$~:
\[\int u\overline v\dif x = \lim_{k \to \pinfty} \int u_k\overline v\dif x = (2\pi)^{-n}\lim_{k \to \pinfty}\int \widehat {u_k}\overline {\widehat v}\dif\xi
= (2\pi)^{-n}\int\mathbb Fu\overline {\hat v}\dif \xi.\]

Dans le cas général $u, v \in L^2(\R^n)$, par le premier point pour $(v_k)_{k \in \N} \subset \mathcal S(\R^n) \tq v_k \xrightarrow[k \to \pinfty]{L^2} v$~:
\[\int u\overline {v_k}\dif x = (2\pi)^{-n}\int \mathbb Fu\overline {\hat {v_k}}\dif x.\]

Or $v_k \xrightarrow[k \to \pinfty]{L^2} \mathbb Fv$. Par Cauchy-Schwarz, on a~:
\begin{enumerate}
	\item $\int u\overline {v_k}\dif x \xrightarrow[k \to \pinfty]{L^2} \int u\overline v\dif x$~;
	\item et $\int \mathbb Fu\overline {\hat {v_k}}\dif \xi \xrightarrow[k \to \pinfty]{L^2} \int \mathbb Fu\overline {\mathbb Fv}\dif\xi$.
\end{enumerate}

L'identité de Parseval est donc bien vérifiée pour $\mathbb F$. Il reste à vérifier que $\mathbb F$ est continue et qu'elle est unique.

\textbf{Montrons que $\mathbb F$ est continue.}

La continuité découle de Parseval~:
\[\norm u_{L^2} = (2\pi)^{-n/2}\norm {\mathbb Fu}_{L^2},\]
donc pour $\varepsilon > 0$, pour $\delta = (2\pi)^{-n/2}\varepsilon$, on a que si $\norm {u-v}_{L^2} < \delta$, alors $\norm {\mathbb Fu - \mathbb Fv}_{L^2} < \varepsilon$.

\textbf{Montrons finalement que $\mathbb F$ est unique.}

Si il existe $\mathbb F_1 : L^2(\R^n) \to L^2(\R^n)$ continue et linéaire telle que $\mathbb F_1\sVert[2]_{\mathcal S} = \mathcal F$, alors par densité de $\mathcal S(\R^n)$
dans $^2(\R^n)$, pour $u \in L^2(\R^n)$, il existe $(u_k)_{k \in \N} \subset \mathcal S(\R^n) \tq u_k \xrightarrow[k \to \pinfty]{L^2} u$, et donc, par continuité~:
\[\mathbb F(u) = \lim_{k \to \pinfty} \mathbb F(u_k) = \lim_{k \to \pinfty} \mathbb F_1(u_k) = \mathbb F_1(u).\]

Donc puisque deux application continues qui coïncident sur une sous-ensemble dense coïncident partout, on a bien que $\mathbb F = \mathbb F_1$.
\end{proof}

De la même manière, $\tilde {\mathcal F}$ se prolonge sur $L^2$ en $\tilde {\mathbb F}$

\begin{prp} $\mathbb F \circ \tilde {\mathbb F} = \Id_{L^2} = \tilde {\mathbb F} \circ \mathbb F$.
\end{prp}

\begin{proof} Ceci vient directement de la même propriété sur $\mathcal F$ et $\tilde {\mathcal F}$. Soit $u \in L^2(\R^n)$ et soit
$(u_k)_{k \in \N} \subset \mathcal S(\R^n) \st u_k \xrightarrow[k \to \pinfty]{L^2} u$. On sait~:
\[\mathcal S \ni \tilde {\mathcal F}(u_k) = \tilde {\mathbb F}(u_k) \xrightarrow[k \to \pinfty]{L^2} \tilde {\mathbb F}(u)\]
par continuité de $\tilde {\mathbb F}$. Par continuité de $\mathbb F$, on a~:
\[\mathcal F \circ \tilde {\mathcal F}(u_k) = \mathbb F \circ \tilde {\mathbb F}(u_k) \xrightarrow[k \to \pinfty]{L^2} \mathbb F \circ \tilde {\mathbb F}(u).\]

Or $\mathcal F \circ \tilde {\mathcal F}(u_k) = u_k \xrightarrow[k \to \pinfty]{L^2} u$. Par unicité de la limite, on a $\mathbb F \circ \tilde {\mathbb F}(u)$.
L'autre égalité se démontre de la même manière.
\end{proof}

À ce stade, il est légitime de se demander si les définitions que l'on a sur $L^1$ (la formule intégrale définie depuis $\mathcal S$) et sur $L^2$ (la définition de $\mathbb F$)
sont compatibles, i.e. si pour $u \in L^1 \cap L^2$ on a bien $\hat u = \mathbb Fu$. Cette égalité tient bien (démonstration à venir).

\section{Exemple d'application de la théorie de Fourier}

Pour $\Delta = \sum_{j=1}^n\partial_j^2$ le Laplacien sur $\R^n$ et $f \in \mathcal S(\R^n)$, soit la PDE suivante~:
\begin{equation}
	(1+\sum_{j=1}^nD_j^2)u = u-\Delta u = f,
\end{equation}
ou plus généralement, pour des $a_\alpha \in \C$~:
\begin{equation}\label{eq:PDE générale Fourier}
	\underbrace {\sum_{\abs \alpha \leq m}a_\alpha D^\alpha}_{P(D) \text{ polynôme}} u = f,
\end{equation}
dans le cas du Laplacien, ce polynôme est $P(\xi) = 1+\abs \xi^2$.

Sous l'hypothèse $\inf_{\xi \in \R^n}\abs {P(\xi)} \gneqq 0$, trouvons $u \tq P(D)u = f$.

Formellement~:
\begin{align*}
	\widehat {P(D)u}(\xi) &= \hat f(\xi) \\
	P(\xi)\hat u(\xi) &= \hat f(\xi) \\
	\hat u(\xi) &= \frac {\hat f(\xi)}{P(\xi)} \\
	u(x) &= (2\pi)^{-n}\int e^{i\scpr x\xi}\frac {\hat f(\xi)}{P(\xi)}\dif\xi.
\end{align*}

Plus rigoureusement, puisque $f \in \mathcal S(\R^n)$, on sait $\hat f \in \mathcal S(\R^n)$. De plus, $P$ est borné par dessous. Donc $\abs {\hat f/P} \leq C_N(1+\abs \xi)^{-N}$,
et du coup la fonction sous l'intégrale ($\xi  \mapsto e^{i\scpr x\xi}\frac {\hat f(\xi)}{P(\xi)}$) est $L^1$, et cette intégrale est bien définie pour $N > n$.

De plus, puisque la dérivation selon $x$ sur $u$ fait juste descendre du $\xi$ de l'exponentielle, par récurrence avec le théorème de convergence dominée et par la borne supérieure
ci-dessus, on trouve que $u \in C^\infty(\R^n)$. On peut alors vérifier que la fonction $u$ ainsi trouvée est bien une solution de~\eqref{eq:PDE générale Fourier}~:
\[\sum_{\abs \alpha \leq m}a_\alpha D^\alpha u = (2\pi)^{-n}\int e^{i\scpr x\xi}\underbrace {\sum_{\abs \alpha \leq m}a_\alpha \xi^\alpha}_{=P(\xi)} \frac {\hat f(\xi)}{P(\xi)}\dif\xi
	= (2\pi)^{-n} \int e^{i\scpr x\xi}\hat f(\xi)\dif\xi = f(x).\]

\chapter{Espaces de Hilbert}

\begin{déf} Soit $H$ un $\C$-espace vectoriel. Un produit scalaire (forme hermitienne définie positive) sur $H$ est une application $\scpr \cdot\cdot : H \times H \to \C \tq$~:
\begin{itemize}
	\item[$(i)$]   à $y \in \C$ fixé~: $x \mapsto \scpr xy$ est une application linéaire de $H$ dans $\C$~;
	\item[$(ii)$]  pour $x, y \in \C$~: $\scpr xy = \overline {\scpr yx}$~;
	\item[$(iii)$] pour $x \in \C$~: $\scpr xx \geq 0$ où $\scpr xx = 0 \iff x = 0$.
\end{itemize}

Sur un produit scalaire, on peut définir une norme $\norm x \coloneqq \scpr xx^{1/2}$.
\end{déf}

\begin{rmq} Une forme hermitienne définie positive est donc anti-linéaire pour le 2e paramètre : $\scpr x{\lambda y} = \overline \lambda \scpr xy$.
\end{rmq}

\begin{prp} $\norm \cdot : H \to \R^+$ est une norme.
\end{prp}

\begin{prp} $\norm \cdot$ vérifie Cauchy-Schwarz, i.e.~:
\[\forall x, y \in H : \abs {\scpr xy} \leq \norm x\norm y.\]
\end{prp}

\begin{proof} Soit $\alpha \in \C \tq \abs \alpha = 1$ et $\alpha\scpr yx \in \R^+$ (i.e. $\alpha\scpr yx = \abs {\scpr xy}$). Soit $r \in \R$.

\begin{align*}
	0 &\leq \scpr {x-r\alpha y}{x-r\alpha y} = \scpr xx - r\alpha\scpr yx - r\overline \alpha\scpr xy + r^2\scpr yy \\
		&= \scpr xx - r\underbrace {\alpha \scpr yx}_{= \abs {\scpr yx}} - r\underbrace {\overline \alpha \scpr xy}_{= \abs {\scpr yx}} + r^2\underbrace {\alpha\overline\alpha}_{= \abs \alpha = 1} \scpr yy = A - 2Br + Cr^2,
\end{align*}
pour $A = \scpr xx \in \R^+$, $B = \alpha\scpr xy = \abs {\scpr xy} \in \R^+$, $C = \scpr yy \in \R^+$.

Si $C = 0$, alors $B = 0$, et donc $\scpr yx = 0$ et Cauchy-Schwarz est vérifié.

Si $C \gneqq 0$, alors pour $r = B/C$~: $0 \leq A - 2Br + Cr^2 = \frac {AC-B^2}C$, donc $B^2 \leq AC$, donc Cauchy-Schwarz est vérifié.
\end{proof}

\begin{prp} $\norm \cdot$ vérifie l'inégalité triangulaire, i.e.~:
\[\norm {x+y}^2 \leq \norm x^2 + \norm y^2.\]
\end{prp}

\begin{proof} $\norm {x+y}^2 = \scpr xx + \scpr xy + \scpr yx + \scpr yy \leq \norm x^2 + 2\norm x\norm y + \norm y^2 = (\norm x + \norm y)^2$.
\end{proof}

On a donc $(H, \norm \cdot)$ un e.v. normé, depuis lequel on peut alors définir une distance~: $d(x, y) \coloneqq \norm {x-y}$.

\begin{déf} Si $H$ est complet pour $d$, on dit que $H$ est un espace de Hilbert.
\end{déf}

Quelques exemples d'espaces de Hilbert~:
\begin{itemize}
	\item[(0)] $\C^n$ pour $\scpr xy \coloneqq \sum_{j=1}^nx_j\overline {y_j}$~;
	\item[(1)] Pour $(\Omega, \mathcal A, \mu)$ un espace de mesure, $L^2(\Omega, \mathcal A, \mu)$ muni du produit scalaire $\scpr fg \coloneqq \int f\overline g\dif\mu$~;
	\item[(2)] Pour $(\N, \mathcal P(\N), \#)$ comme espace de mesure, on a l'équivalent dénombrable de l'exemple (0)~:
	\[\scpr fg_{\ell^2} \coloneqq \int f\overline g\dif\# = \sum_{k \geq 1}f_k\overline {g_k}.\]
	On note $\ell^2(\N) \coloneqq L^2(\N, \mathcal P(\N), \#)$.
\end{itemize}

Un dernière exemple bien moins trivial~: les espaces de Sobolev.

\begin{déf} Soit $s \geq 0$ un paramètre, on définit l'espace de Sobolev d'ordre $s$ sur $\R^n$ par~:
\begin{equation}
	H^s(\R^n) \coloneqq \left\{u \in L^2(\R^n) \tq (2\pi)^{-n}\int \abs {\mathbb Fu(\xi)}^2(1+\abs \xi)^s\dif\xi \lneqq \pinfty\right\}.
\end{equation}

On y définit le produit scalaire suivante pour $u, v \in H^s(\R^n)$~:
\begin{equation}
	\scpr uv_s \coloneqq (2\pi)^{-n}\int\mathbb Fu\overline {\mathbb Fv}(1+\abs\xi)^s\dif\xi.
\end{equation}
\end{déf}

\begin{rmq} Remarquons que $u \in H^s \iff \xi \mapsto (1+\abs \xi^2)^{s/2}\mathbb Fu(\xi)$ est dans $L^2$.
\end{rmq}

\begin{prp} $(u, v) \mapsto \scpr uv_s$ est un produit scalaire.
\end{prp}

\begin{proof} À $v$ fixé, $u \mapsto \scpr uv_s$ est linéaire par linéarité de $\mathbb F$ et par linéarité de l'intégrale.

Soient $u, v \in H^s$.
\[\scpr uv_s = (2\pi)^{-n}\int \mathbb Fu\overline {\mathbb Fv}\underbrace {(1+\abs \xi)^s}_{\in \R^+}\dif\xi
	= (2\pi)^{-n}\overline {\int \mathbb Fv\overline {\mathbb Fu}(1+\abs \xi)^s\dif\xi} = \overline {\scpr vu_s}.\]

Finalement, pour $u \in H^s$~:
\[\scpr uu_s = (2\pi)^{-n}\int\underbrace {\abs {\mathbb Fu}^2}_{\geq 0}\underbrace {(1+\abs\xi)^s}_{\geq 0}\dif\xi \geq 0,\]
et de plus, il est évident que $\scpr uu_s = 0 \iff u = 0$ puisque $\hat u = 0 \iff u = 0$.
\end{proof}

Par linéarité de Fourier, $H^s(\R^n)$ est un espace vectoriel, et de plus il est normé par le produit scalaire défini ci-dessus. Montrons alors que c'est un espace ce Hilbert.

Soit $(u_k)_{k \in \N} \subset H^s$ une suite de Cauchy. $(1+\abs\xi^2)^{s/2}\mathbb Fu_k$ est de Cauchy dans $L^2$, qui est complet. Donc il en existe une limite
$V \in L^2 \tq (1+\abs\xi^2)^{s/2}\mathbb Fu_k \xrightarrow[k \to \pinfty]{L^2} V$. Il existe $u \in L^2 \tq (1+\abs\xi^2)^{s/2}\mathbb Fu = V$ car $(1+\abs\xi^2)^{-s/2}V \in L^2$,
et $\mathbb F$ est une bijection sur $L^2$. De plus, $u \in H^s$ car $V = (1+\abs\xi^2)^{s/2}\mathbb Fu \in L^2$. Puisque $u_k \xrightarrow[k \to \pinfty]{H^s} u$,
on a que $H^s$ est complet.

Pour un contre-exemple, on a $C^\infty_0(\R^n)$ muni du produit scalaire $\scpr fg \coloneqq \int f\overline g\dx$ n'est pas un Hilbert. En effet, pour
$f \in L^2(\R^n) \setminus C^\infty_0(\R^n)$, par densité de $C^\infty_0(\R^n)$ dans $L^2(\R^n)$~:
\[\exists (f_k)_{k \in \N} \subset C^\infty_0(\R^n) \tq f_k \xrightarrow[k \to \pinfty]{L^2(\R^n)} f.\]
De plus, $(f_k)$ est de Cauchy dans $C^\infty_0(\R^n)$ pour la norme $\norm \cdot_{L^2}$, mais $(f_k)_k$ ne converge pas dans $C^\infty_0(\R^n)$. En effet, par l'absurde, si
$\exists g \in C^\infty_0(\R^n) \tq f_k \xrightarrow[k \to \pinfty]{L^2(\R^n)} g$, par unicité de la limite, $g=f$, or $f \not \in C^\infty_0(\R^n)$.

À partir d'ici, $H$ désigne un espace de Hilbert quelconque muni d'un produit scalaire $\scpr \cdot\cdot$.

\begin{déf} Soit $y \in H$. On définit~:
\[\begin{cases}
	&f_1 : x \mapsto \scpr xy \\
	&f_2 : x \mapsto \scpr yx \\
	&f_3 : x \mapsto \norm x
\end{cases}\]
\end{déf}

\begin{prp} $f_i$ est continue pour $i=1,2,3$.
\end{prp}

\begin{proof}~
\begin{enumerate}
	\item $\abs {f_1(x_1) - f_1(x_2)} = \abs {\scpr {x_1-x_2}y} \leq \norm {x_1-x_2}\norm y \xrightarrow[x_1 \to x_2]{} 0$. ($f_1$ est même uniformément continue et Lipschitzienne).
	\item Idem pour $f_2$, à permutation près.
	\item La continuité vient directement de $\abs {\norm x-\norm z} \leq \norm {x-z}$, et donc $\norm \cdot$ est Lipschitzienne.
\end{enumerate}
\end{proof}

\begin{prp} Pour $F \leq H$, $\overline F \leq H$.
\end{prp}

\begin{proof} Pour $x, y \in \overline F$, il existe $(x_k)_k, (y_k)_k \subset F \tq x_k \xrightarrow[k \to \pinfty]{} x$ et $y_k \xrightarrow[k \to \pinfty]{} y$.

Donc $F \ni x_k+y_k \xrightarrow[k \to \pinfty]{} x+y$.
De plus, pour $\lambda \in \C$, $\underbrace {\lambda x_k}_{\in F} \xrightarrow[k \to \pinfty]{} \lambda x \in \overline F$.
\end{proof}

\begin{rmq} Contrairement aux e.v. de dimension finie, en dimension infinie, il est possible d'avoir un sous-e.v. \textit{strict} dense (e.g. $C^\infty_0(\R^n)$ dans $L^2(\R^n)$).
\end{rmq}

\begin{prp} $F \coloneqq \{f \in L^2(\R^n) \tq f=0 \text{ sur } x_n > 0\}$ est un e.v. fermé dans $L^2(\R^n)$.
\end{prp}

\begin{proof} Soit $g \in \overline F$. Il existe $(f_k)_{k \in \N} \subset F \tq f_k \xrightarrow[k \to \pinfty]{L^2} g$. Pour $k \in \N$~:
\[\int_{x_n > 0}\abs g^2\dx = \int_{x_n > 0}\abs {g-f_k}^2\dx \leq \norm {g-f_k}^2_{L^2} \xrightarrow[k \to \pinfty]{} 0\]
car $f_k \in F$ et $f_k \xrightarrow[k \to \pinfty]{L^2} g$. Dès lors $\int_{x_n > 0}\abs g^2\dx = 0$, i.e. $g \in F$. Donc $\overline F = F$.
\end{proof}

\section{Orthogonalité}

\begin{déf} Pour $x, y \in H$, $x$ et $y$ sont \textit{orthogonaux}, noté $x \perp y$ lorsque $\scpr xy = 0$.

Pour $x \in H$, on définit $x^\perp \coloneqq \{y \in H \tq \scpr xy = 0\}$, et pour $M \subset H$, on définit
$M^\perp \coloneqq \{y \in H \tq \forall x \in M : \scpr xy = 0\} = \bigcap_{x \in M}x^\perp$.
\end{déf}

\begin{prp} Pour $x \in H$, $x^\perp \leq H$, et $x^\perp$ est fermé.
\end{prp}

\begin{proof} À $x \in H$ fixé, on remarque que $x^\perp = f_2^{-1}(\{0\})$, or $f_2$ est continue. Donc $x^\perp$ est fermé. Vérifier que $x^\perp$ est un sous-e.v. est trivial.
\end{proof}

\begin{cor} Pour $M \leq H$, $M^\perp$ est un sous-e.v. fermé de $H$.
\end{cor}

Ce résultat découle directement du fait que $M^\perp$ est une intersection d'e.v. fermés.

\begin{déf} $E \subseteq H$ est dit \textit{convexe} lorsque~:
\[\forall x, y \in E : \forall t \in [0, 1] : (1-t)x + ty \in E.\]
\end{déf}

\begin{ex}~
\begin{itemize}
	\item tout sous-e.v. de $H$ est convexe~;
	\item toute boule (ouverte ou fermée) dans $H$ est convexe~;
	\item pour $\Omega \subseteq \R^n, u \in L^2(\Omega)$, $E = \{v \in L^2 \tq u=v \text{ sur } \Omega\}$ est convexe.
\end{itemize}

Montrons également que $E$ est fermé dans $L^2$. Soit $f \in \overline E$. Il existe $(f_k)_{k \in \N} \subseteq E \tq f_k \xrightarrow[k \to \pinfty]{L^2} f$.
\[\int_\Omega \abs {u-f}^2\dx = \int_\Omega\abs {f_k-f}^2\dif x \leq \int_{\R^n}\abs {f_k-f}^2\dx = \norm {f_k-f}_{L^2}^2 \xrightarrow[k \to \pinfty]{} 0.\]
\end{ex}

\begin{thm} Soit $E \neq \emptyset$ convexe fermé dans $H$. Alors~:
\[\exists! x \in E \tq \norm x = \min_{z \in E}\norm z = \inf_{z \in E}\norm z \eqqcolon \delta.\]
\end{thm}

\begin{proof} \unic soient $x, y \in E \tq \norm x = \norm y = \delta$. Par la formule du parallélogramme~:
\[\norm {x+y}^2 + \norm {x-y}^2 = 2(\norm x^2+\norm y^2).\]

Par convexité de $E$, $\frac 12(x+y) \in E$. Donc $\norm {\frac 12(x+y)} \geq \delta$. On trouve alors~:
\[\norm {x-y}^2 \leq 2(\underbrace {\norm x^2 + \norm y^2}_{=2\delta^2}) - 4\delta^2 = 0.\]

On en déduit $\norm {x-y} = 0$, i.e. $x=y$.

\exis Soit $(y_k)_{k \in \N} \subset E \tq \norm {y_k} \xrightarrow[k \to \pinfty]{} \delta$ qui existe par définition de l'infimum. Par la règle du parallélogramme~:
\[\norm {y_k - y_m}^2 \leq 2(\underbrace {\norm {y_k}^2+\norm {y_m}^2)}_{\xrightarrow[k, m \to \pinfty]{} 2\delta^2} - 4\delta^2 \xrightarrow[k, m \to \pinfty]{} 0.\]
Donc $(y_k)$ est de Cauchy. Par complétude de $H$, $\exists x_0 \in H \tq y_k \xrightarrow[k \to \pinfty]{} x_0$, et par fermeture de $E$, $x_0 \in E$.

De plus, par continuité de la norme, $\norm {y_k} \xrightarrow[k \to \pinfty]{} \norm {x_0}$, et par unicité de la limite, $\norm {x_0} = \delta$.
\end{proof}

\begin{ex} Si $\Omega \subseteq \R^n$ est un ouvert, $u \in L^2(\Omega)$, $E = \{v \in L^2 \tq u=v \text{ sur } \Omega\}$ est un convexe fermé, donc par ce théorème,
il existe un unique $u^* \in E$ qui minimise la norme~: $u^* = u$ sur $\Omega$ et $u^*=0$ sur $\R^n \setminus \Omega$.
\end{ex}

\begin{thm}[Décomposition orthogonale] Soit $M \leq H$ fermé. Alors~:
\begin{enumerate}
	\item $\forall x \in H : \exists! (y, z) \in M \times M^\perp \tq x=y+z$~;
	\item ces valeurs $y, z$ sont les points les plus proches de $x$ dans $M$ et $M^\perp$ respectivement~;
	\item Les applications $P : x \mapsto y$ et $Q : x \mapsto z$ sont linéaires~;
	\item $\norm x^2 = \norm {Px}^2 + \norm {Qx}^2$ (et donc $P,Q$ sont continues)~;
	\item $P$ et $Q$ sont les projections orthogonales de $x$ sur $M$ et $M^\perp$ respectivement.
\end{enumerate}
\end{thm}

\begin{proof}~
\begin{enumerate}
	\item \unic si $x = y_1+z_1 = y_2+z_2$, pour $y_1,y_2 \in M$ et $z_1,z_2 \in M^\perp$, on a
	$\underbrace {y_1-y_2}_{\in M} = \underbrace {z_2-z_1}_{\in M^\perp}$. Or $M \cap M^\perp = \{0\}$. Donc $y_1=y_2$ et $z_1=z_2$.

 	\exis $x+M$ est convexe (trivial par le fait que $M \leq H$). Montrons que $x+M$ est fermé. Soit $u \in \overline {x+M}$.
	Il existe $(u_k)_{k \in \N} \subset x+M \tq u_k \xrightarrow[k \to \pinfty]{} u$. $\forall k \in \N : x+M \ni u_k = x + y_k$. On en déduit
	$y_k \xrightarrow[k \to \pinfty]{} u-x$. Par fermeture de $M$, on a $u-x \in M$, et donc $u \in x+M$ (i.e. $x+M$ est fermé).

	Soit $z \in x+M$ l'élément qui minimise la norme. On pose $y \coloneqq x-z \in M$. Montrons alors que $z \in x+M$. Soit $w \in M$~; WLOG, supposons $\norm w = 1$.
	Puisque $z \in x+M$, $\forall \alpha \in \C : z-\alpha w \in x+M$. Dès lors~:
	\[\norm z^2 \leq \norm {z-\alpha w}^2 = \norm z^2 - 2\Re(\alpha\scpr wz) + \abs\alpha^2,\]
	et donc $0 = 2\Re(\alpha\scpr wz) - \abs\alpha^2$. En particulier, pour $\alpha = \scpr zw$~: $0 = \abs {\scpr zw}^2$, donc $\scpr zw = 0$. Dès lors $z \in M^\perp$.

	\item Soit $Y \in M$. Montrons que $\norm {x-Y} \geq \norm {x-y} = \norm z$. Par Pythagore~:
	\[\norm {x-Y}^2 = \norm {y+z-Y}^2 = \norm {(y-Y) + z}^2 = \norm {y-Y}^2 + \norm z^2 \geq \norm z^2.\]

	Idem pour $Z \in M^\perp$~: $\norm {x-Z}^2 \geq \norm y^2$.

	\item Soient $x_1, x_2 \in H, \alpha_1, \alpha_2 \in \C$. On a $\alpha_1x_1 = \alpha_1Px_1 + \alpha_1Qx_1$, et $\alpha_2x_2 = \alpha_2Px_2 + \alpha_2Qx_2$. Donc~:
	\[P(\alpha_1x_1+\alpha_2x_2) + Q(\alpha_1x_1 + \alpha_2x_2) = \alpha_1x_1 + \alpha_2x_2 = \alpha_1Px_1 + \alpha_1Qx_1 + \alpha_2Px_2 + \alpha_2Qx_2,\]
	et donc~:
	\[\underbrace {P(\alpha_1x_1+\alpha_2x_2) - \alpha_1Px_1 - \alpha_2Px_2}_{\in M} = \underbrace {\alpha_1Qx_1 + \alpha_2Qx_2 - Q(\alpha_1x_1 + \alpha_2x_2)}_{\in M^\perp}.\]
	Or $M \cap M^\perp = \{0\}$, donc $P(\alpha_1x_1+\alpha_2x_2) = \alpha_1Px_1 + \alpha_2Px_2$, et $\alpha_1Qx_1 + \alpha_2Qx_2 = Q(\alpha_1x_1 + \alpha_2x_2)$.

	\item Par Pythagore $\norm x^2 = \norm {Px}^2 + \norm {Qx}^2$. Donc $\norm {Px} \leq \norm x$ et $\norm {Qx} \leq \norm x$, i.e. $P$ et $Q$ sont Lipschitziennes,
	donc en particulier continues.
\end{enumerate}
\end{proof}

\begin{cor}\label{cor:sous-espace vectoriel fermé admet un orthogonal} Si $M \leq H$ est fermé, avec $M \neq H$, il existe $y \in H \setminus \{0\} \tq y \perp M$.
\end{cor}

\begin{proof} Pour $x \in H \setminus M$, $x = Px+Qx$, où $Qx \neq 0$, et $Qx \perp M$.
\end{proof}

%\begin{rmq} TODO: Commenter l'hypothèse de fermeture...
%\end{rmq}

\begin{cor} Si $M \leq H$ est fermé, alors $M = {M^\perp}^\perp$.
\end{cor}

\begin{proof} La première inclusion est triviale~: si $x \in M$, alors $x \perp M^\perp$.

La seconde inclusion se démontre comme suit~: soit $x \in {M^\perp}^\perp \subseteq H$. $x = y+z$ où $y \in M$ et $z \in M^\perp$.
Or $0 = \scpr xz = \scpr {y+z}z = \scpr yz + \norm z^2$, et $\scpr yz = 0$ par définition d'orthogonalité. Donc $\abs z = 0$ et $z=0$, i.e. $x=y \in M$.
\end{proof}

\begin{lem}[Lemme de Riesz]\label{lem:Riesz} Soit $L : H \to \C$, une forme linéaire continue. Alors~:
\[\exists! y \in H \tq L = \scpr \cdot y.\]
\end{lem}

\begin{proof} \unic pour $y_1, y_2 \in H \tq \forall x \in H : \scpr x{y_1} = \scpr x{y_2}$, on a $\scpr x{y_1-y_2} = 0$, donc $y_1-y_2 \in H^\perp = \{0\}$, i.e. $y_1=y_2$.

\exis si $L \equiv 0$, alors $y=0$. Supposons alors que $L$ n'est pas identiquement nulle. $\Ker L \lneqq H$ et est fermé par continuité de $L$. Dès lors, il existe $z \in H$,
$z \neq 0 \tq z \perp \Ker L$. WLOG, supposons $\norm z = 1$. Posons $y \coloneqq (\overline {Lz})z$ et $u \coloneqq (Lx)z - (Lz)x$. Calculons~:
\[Lu = (Lx)Lz - (Lz)Lx = 0,\]
donc $u \in \Ker L$, et donc $0 = \scpr uz = (Lx)\scpr zz - (Lz)\scpr xz = Lx-(Lz)\scpr xz$. Dès lors, $Lx = (Lz)\scpr xz = \scpr xy$.
\end{proof}

\section{Systèmes orthonormaux}

\begin{déf} Pour $V$ un e.v. et $S \subseteq V$, on note $\Vect S = \Span S$ l'e.v. engendré par $S$.

$(e_\alpha)_{\alpha \in A} \subset V$ est appelé \textit{orthonormal} lorsque $\forall \alpha, \beta \in A : \scpr {e_\alpha}{e_\beta} = \delta_{\alpha,\beta}$.

Pour $x \in H$, on définit $\hat x(\alpha) \coloneqq \scpr x{e_\alpha}$.
\end{déf}

Les $\hat x(\alpha)$ sont les coefficients de Fourier relativement au système $(e_\alpha)_{\alpha \in A}$.

\begin{ex} Sur $L^2(\R^n)$, les $(e_\alpha)_{\alpha \in \Z}$ sont les $e_\alpha : [0, 2\pi) \to \C : t \mapsto \frac {e^{i\alpha t}}{\sqrt {2\pi}}$.
\end{ex}

\begin{thm} Pour $H$ un espace de Hilbert et $(e_\alpha)_{\alpha \in A}$ un système orthonormal, $F \subset A$ fini, et $M_F \coloneqq \Vect {\{e_\alpha\}}_{\alpha \in F}$, on a~:
\begin{enumerate}
	\item si $\varphi : A \to \C$ est nulle sur $A \setminus F$, pour $y = \sum_{\alpha \in F}\varphi(\alpha)e_\alpha$, alors~:
	\[\forall \alpha \in A : \varphi(\alpha) = \hat y(\alpha).\]
	De plus, $\norm y^2 = \sum_{\alpha \in F}\abs {\varphi(\alpha)}^2$.

	\item Si $x \in H$, $s_F(x) \coloneqq \sum_{\alpha \in F}\hat x(\alpha)e_\alpha \in M_F$. Si $s \in M_F \setminus \{s_F(x)\}$, alors~:
	\[\norm {x-s_F(x)} \lneqq \norm {x-s}.\]

	De plus~: $\sum_{\alpha \in F}\abs {\hat x(\alpha)}^2 \leq \norm x^2$ (inégalité de Bessel).
\end{enumerate}
\end{thm}

\begin{proof}~
\begin{enumerate}
	\item $\hat y(\alpha) = \scpr y{e_\alpha} = \sum_{\beta \in F}\varphi(\beta)\scpr {e_\beta}{e_\alpha} = \varphi(\alpha)$ et~:
	\[\norm y^2 = \norm {\sum_{\beta \in F}\varphi(\beta)e_\beta}^2 \stackrel {\text{Pythagore}}= \sum_{\beta \in F}\abs {\varphi(\beta)}^2\norm {e_\beta}^2 = \sum_{\beta \in F}\abs {\varphi(\beta)}^2.\]
	\item Soit $s \in M_F$. $\forall \alpha \in F : x-s_F(x) \perp e_\alpha$ et $x-s_F(x) \perp s_F(x)-s \in M_F$. En effet~:
	\[\scpr {x-s_F(x)}{e_\alpha} = \scpr x{e_\alpha} - \scpr {s_F(x)}{e_\alpha} = \hat x(\alpha) - \hat x(\alpha) = 0.\]

	Dès lors~:
	\[x-s = (x - s_F(x)) + (s_F(x) - s),\]
	et donc, par Pythagore~:
	\begin{align}\label{eq:x-s in norm}
		\norm {x-s}^2 = \norm {x-s_F(x)}^2 + \norm {s_F(x) - s}^2.
	\end{align}

	Cette norme est minimisée (strictement) en $s = s_F(x)$ et donc~:
	\[\norm {x-s_F(x)} \lneqq \norm {x-s}\]
	si $s \neq s_F(x)$. Ensuite~:
	\[\norm {s_F(x)}^2 \leq \norm {s_F(x)}^2 + \norm {x-s_F(x)}^2.\]
	Par l'équation~\ref{eq:x-s in norm}~: si $s = 0$~:
	\[\norm {s_F(x)}^2 \leq \norm {s_F(x)}^2 + \norm {x-s_F(x)}^2 = \norm x^2.\]
	Or~:
	\[\norm {s_F(x)}^2 = \sum_{\alpha \in F}\abs {\hat x(\alpha)}^2,\]
	et donc l'inégalité de Bessel est bien vérifiée.
\end{enumerate}
\end{proof}

\begin{rmq} Sur $A$, on a un espace mesuré canonique~: $(A, \mathcal P(A), \#)$ pour lequel on adopte les notations~:
\[\forall B \in \mathcal P(A) : \int_B\varphi\dif\# \eqqcolon \sum_{\alpha \in B}\varphi(\alpha).\]

Remarquons également que par définition de l'intégrale, si $\varphi : A \to [0, \pinfty]$~:
\[\sum_{\alpha \in A}\varphi(\alpha) = \sup_{\stackrel {B \subset A}{\abs B \lneqq \pinfty}}\sum_{\alpha \in B}\varphi(\alpha).\]

Et si $\varphi \in \ell^1(A)$ et $\varphi \geq 0$, alors pour $A_k = \{\varphi \geq k^{-1}\}$ ($k \geq 1$), on a $\abs {A_k} \lneqq \pinfty$ puisque $\varphi \in \ell^1(A)$.
Or $\bigcup_{k \geq 1}A_k = \{\varphi \gneqq 0\}$, et donc $\{\varphi \neq 0\}$ est au plus dénombrable.

Dans le cas général, pour $\varphi \in \ell^1(A)$, alors $(\Re \varphi)^\pm$ et $(\Im \varphi)^\pm$ sont non-nulles sur un ensemble au plus dénombrable, et donc $\{\varphi \neq 0\}$
est au plus dénombrable.
\end{rmq}

\begin{lem} Pour $(\Omega, \mathcal F, \mu)$ un espace mesuré, l'ensemble des fonctions simples mesurables nulles hors d'un ensemble de mesure finie est dense dans
$L^p(\Omega, \mathcal F, \mu)$ pour $p \in [1, \pinfty)$.
\end{lem}

\begin{lem}\label{lem:isométrie dense} Soient $X$ un espace métrique complet, $Y$ un espace métrique, et $X_0 \subset X$, un sous-ensemble dense.
Si $f \in C^0(X, Y)$ telle que $\restr f{X_0}$  une isométrie et $f(X_0)$ est dense dans $Y$, alors $f$ est surjective et est une isométrie.
\end{lem}

\begin{proof} Fixons $x, y \in X$. Il existe $(x_k)_{k \geq 0}, (y_k)_{k \geq 0} \subset X_0$ telles que $x_k \xrightarrow[k \to \pinfty]{} x$ et $y_k \xrightarrow[k \to \pinfty]{} y$.
Pour $k \geq 0$~:
\[d(x_k, y_k) \xrightarrow[k \to \pinfty]{} d(x, y)\]
car la distance est continue sur un espace métrique. De plus~:
\[d(x_k, y_k) = d\left(f(x_k), f(y_k)\right) \xrightarrow[k \to \pinfty]{} d\left(f(x), f(y)\right),\]
à nouveau par continuité de la métrique, et par continuité de $f$. Donc par unicité de la limite, on a $d(x, y) = d(f(x), f(y))$, et donc $f$ est une isométrie.

Il reste à montrer que $f$ est surjective. Soit $y \in Y$. $f(X_0)$ est dense dans $Y$, et donc par continuité de $f$, on sait~: $\exists (x_k)_{k \geq 0} \subset {X_0}$ telle que
$f(x_k) \xrightarrow[k \to \pinfty]{} y$. $(f(x_k))_k$ est de Cauchy dans $Y$, et puisque $f$ est une isométrie, $(x_k)_k$ est de Cauchy dans $X$. Par complétude, on sait
que $\exists x \in X \tq x_k \xrightarrow[k \to \pinfty]{} x$.

Finalement, par continuité de $f$~: $f(x_k) \xrightarrow[k \to \pinfty]{} f(x)$, et par construction $f(x_k) \xrightarrow[k \to \pinfty]{} y$. Par unicité de la limite dans les
espaces métriques, on a $f(x) = y$, et donc $y$ admet une préimage par $f$.
\end{proof}

\begin{thm}\label{thm:cefficients Fourier isométrie} Soit $(e_\alpha)_{\alpha \in A}$, un système orthonormal dans $H$. Soit $P = \Span \{e_\alpha\}_{\alpha \in A}$. Alors~:
\begin{enumerate}
	\item $\forall x \in H : \sum_{\alpha \in A}\abs {\hat x(\alpha)}^2 \leq \norm x^2 \qquad$ (inégalité de Bessel généralisée)~;
	\item $f : H \to \ell^2(A) : x \mapsto \hat x$ est linéaire, continue, et surjective~;
	\item $\restr f{\overline P}$ est une isométrie surjective $\overline P \to \ell^2(A)$.
\end{enumerate}
\end{thm}

\begin{proof}~
\begin{enumerate}
	\item Pour tout $F \subset A$ fini, on a~:
	\[\sum_{\alpha \in F}\abs {\hat x(\alpha)}^2 \leq \norm x^2.\]
	Or par la remarque précédente~:
	\[\sum_{\alpha \in A}\abs {\hat x(\alpha)}^2 = \sup_{\stackrel {B \subset A}{\abs B \lneqq \pinfty}}\sum_{\alpha \in B}\abs {\hat x(\alpha)}^2 \leq \norm x^2\]
	par passage au supremum (à la limite) et par l'inégalité de Bessel finie.
	\item Soit $x \in H$. Puisque $\norm x^2 \lneqq \pinfty$, par l'inégalité de Bessel, on sait $\sum_{\alpha \in A}\abs {\hat x(\alpha)}^2 \lneqq \pinfty$, i.e. $\hat x \in \ell^2(A)$.

	$f$ est linéaire par linéarité de $x \mapsto \scpr x{e_\alpha}$ pour tout $\alpha \in A$.

	$f$ est continue car Lipschitzienne~:
	\[\norm {f(x)-f(y)}_{\ell^2(A)}^2 = \sum_{\alpha \in A}\abs {\widehat {x-y}(\alpha)}^2 \leq \norm {x-y}_H^2.\]

	La surjectivité vient du point 3~: si $\restr f{\overline P}$ est surjective, alors en particulier $f$ est surjective.
	\item Pour $X = \overline P$, $X_0 = P$, $Y = \ell^2(A)$, remarquons que~:
	\[\underbrace {\left\{\chi \in \ell^2(A) \tq \chi(\alpha) = 0 \text{ si } \alpha \not \in F \subset A \text{ fini }\right\}}_{\text{dense dans } \ell^2(A)} \subset f(P).\]

	De plus $\restr fP$ est une isométrie. En effet, pour $x \in P$, on sait qu'il existe $F \subset A$ fini et $\lambda_\alpha \in \C$ ($\alpha \in F$) tels que
	$x = \sum_{\alpha \in F}\lambda_\alpha e_\alpha$. Dès lors~:
	\[\norm {f(x)}_{\ell^2(A)}^2 = \norm x_{\ell^2(A)}^2 = \sum_{\alpha \in A}\abs {\hat x(\alpha)}^2 = \sum_{\alpha \in F}\abs {\hat x(\alpha)}^2
	= \sum_{\alpha \in A}\abs {\sum_{\beta \in F}\lambda_\beta \scpr {e_\beta}{e_\alpha}}^2 = \sum_{\alpha \in F}\abs {\lambda_\alpha}^2,\]
	et~:
	\[\norm x_H^2 = \scpr xx_H = \scpr {\sum_{\alpha \in F}\lambda_\alpha e_\alpha}{\sum_{\beta \in F}\lambda_\beta e_\beta}
	= \sum_{\alpha \in F}\sum_{\beta \in F}\lambda_\alpha\overline {\lambda_\beta}\scpr {e_\alpha}{e_\beta} = \sum_{\alpha \in F}\abs {\lambda_\alpha}^2.\]

	Finalement, puisque $\overline P$ est complet (car sous-ensemble fermé de $H$), on peut ensuite appliquer le lemme~\ref{lem:isométrie dense} qui affirme que
	$\restr f{\overline P}$ est une isométrie surjective sur $Y$.
\end{enumerate}
\end{proof}

\begin{déf} Un \textit{système orthonormal maximal} (SOM) est un système orthonormal qui est maximal au sens de l'inclusion.
\end{déf}

\begin{thm}\label{thm:caractérisation SOM} Soit $(e_\alpha)_{\alpha \in A}$ un système orthonormal dans $H$. Alors les conditions suivantes sont équivalentes~:
\begin{enumerate}
	\item $(e_\alpha)_\alpha$ est un SOM.
	\item $M \coloneqq \Span \{e_\alpha\}_\alpha$ est dense dans $H$.
	\item $\forall x \in H : \norm {\hat x}_{\ell^2(A)}^2 = \sum_{\alpha \in A}\abs {\hat x(\alpha)}^2 = \norm x^2_H$.
	\item $\forall x, y \in H : \scpr {\hat x}{\hat y}_{\ell^2(A)} = \sum_{\alpha \in A}\hat x(\alpha)\overline {\hat y(\alpha)} = \scpr xy \qquad$ (Identité de Parseval).
	\item $\forall x \in H : \forall \varepsilon > 0 : \exists A_0 \subset A$ fini tel que $\forall A_1 \supset A_0$ : si $A_1$ est fini, alors $\norm {x - \sum_{\alpha \in A_1}\hat x(\alpha)e_\alpha} \leq \varepsilon$.
\end{enumerate}
\end{thm}

\begin{proof}~
\begin{itemize}
	\item[$1 \Rightarrow 2$] Par l'absurde, supposons que $\overline M \subsetneqq H$. Alors par le Corollaire~\ref{cor:sous-espace vectoriel fermé admet un orthogonal},
	il existe $y \in H \setminus \{0\}$ tel que $y \perp \overline M$ et donc le système $\{e_\alpha\}_{\alpha \in A}$ n'est pas maximal car on peut lui ajouter $\frac y{\norm y_H}$.
	\item[$2 \Rightarrow 3$] Par le Théorème~\ref{thm:cefficients Fourier isométrie} (point 3), on sait que $\overline M \to \ell^2(A) : x \mapsto \hat x$ est une isométrie,
	ce qui revient à dire que si $x \in \overline M$, alors l'inégalité de Bessel est une égalité, i.e.~:
	\[\norm x^2 = \sum_{\alpha \in A}\abs {\hat x(\alpha)}^2.\]
	\item[$3 \Rightarrow 4$] On veut montrer que $\norm {\hat \cdot}_{\ell^2(A)} = \norm \cdot_H \Rightarrow \scpr {\hat \cdot}{\hat \cdot}_{\ell^2(A)} = \scpr \cdot\cdot_H$.
	Dans $\mathcal H$, un espace de Hilbert quelconque (e.g. $\mathcal H = H$ ou $\mathcal H = \ell^2(A)$), on a~:
	\[4\scpr xy_{\mathcal H} = \norm {x+y}^2_{\mathcal H} - \norm {x-y}^2_{\mathcal H} + i\norm {x+iy}^2_{\mathcal H} - i\norm {x-iy}^2_{\mathcal H}.\]
	Or par hypothèse, $\norm \cdot_H = \norm {\hat \cdot}_{\ell^2(A)}$. Donc~:
	\begin{align*}
		4 \scpr {\hat x}{\hat y}_{\ell^2(A)} &= \norm {\hat x+\hat y}^2_{\ell^2(A)} - \norm {\hat x-\hat y}^2_{\ell^2(A)} + i\norm {\hat x+i\hat y}^2_{\ell^2(A)} - i\norm {\hat x-i\hat y}^2_{\ell^2(A)} \\
		&= \norm {x+y}^2_H - \norm {x-y}^2_H + i\norm {x+iy}^2_H - i\norm {x-iy}^2_H = 4\scpr xy_H.
	\end{align*}
	\item[$4 \Rightarrow 1$] Par l'absurde, supposons qu'il existe $u \neq 0$ tel que $\forall \alpha \in A : u \perp e_\alpha$. Alors $\forall \alpha \in A : \hat u(\alpha) = 0$.
	Or $0 \neq \norm u^2 = \sum_{\alpha \in A}\hat u(\alpha)\overline {\hat u(\alpha)} = \sum_{\alpha \in A}\abs {\hat u(\alpha)}^2 = 0$, ce qui est une contradiction.
	\item[$5 \Rightarrow 2$] Fixons $x \in H$ et $\varepsilon = \frac 1k$. Pour tout $k$, il existe $x_k = \sum_{\alpha \in A_1}\scpr x{e_\alpha}e_\alpha \in M$ tel que
	$\norm {x-x_k} \leq \frac 1k = \varepsilon$
	\item[$3 \Rightarrow 5$]
	\[\sum_{\alpha \in A}\abs {\hat x(\alpha)}^2 = \sup_{B \subset A \text{ fini}}\sum_{\beta \in B}\abs {\hat x(\beta)}^2.\]
	Par définition du $\sup$~: $\forall \varepsilon > 0 : \exists A_0$ fini $\subset A \tq \forall A_1$ fini $\supset A_0$~:
	\[\norm {x- \sum_{\alpha \in A_1}\hat x(\alpha)e_\alpha}^2 = \norm x^2 - \sum_{\alpha \in A_1}\abs {\hat x(\alpha)}^2 \leq \varepsilon^2.\]
\end{itemize}
\end{proof}

\begin{thm} Tout espace de Hiblert possède un système orthonormal maximal.
\end{thm}

\begin{proof} Soit $A$ l'ensemble des SOMs de $H$. $(A, \subseteq)$ est ordonné. Soit $\mathcal S \subset A$ une partie totalement ordonnée. On pose~:
\[\hat S \coloneqq \bigcup_{s \in \mathcal S}s.\]
Mq $\hat S \in A$.

Soient $a_1, a_2 \in \hat S$. Il existe $s_1, s_2 \in \mathcal S$ tels que $a_1 \in s_1$ et $a_2 \in s_2$, or $\mathcal S$ est totalement ordonné. Donc soit $s_1 \subseteq s_2$,
soit $s_2 \subseteq s_1$, donc $a_1, a_2 \in s_1$ ou $a_1, a_2 \in s_2$. En particulier, ils sont orthogonaux, et de plus $\hat S$ majore tout $s \in \mathcal S$.

Par le lemme de Zorn, on a l'existence d'un élément maximal pour l'inclusion, i.e. un SOM.
\end{proof}

\begin{thm}[Gram-Schmidt] Soit $\{x_1, x_2, \ldots\} \subseteq H$ une suite finie ou dénombrable de vecteurs linéairement indépendants. Alors il existe un système orthonormal
$\{u_1, u_2, \ldots,\}$ fini et de même cardinalité que $\{x_1, \ldots\}$ si ce dernier est fini ou dénombrable si $\{x_1, x_2, \ldots\}$ est dénombrable tel que
$\Span \{u_1, \ldots, \} = \Span \{x_1, \ldots\}$.
\end{thm}

\begin{proof} Les $x_j$ sont non-nuls car $\{x_1, x_2, \ldots\}$ est linéairement indépendant. Posons $y_1 \coloneqq x_1$ et $u_1 \coloneqq \frac {y_1}{\norm {y_1}}$ et pour tout $n > 1$
posons $y_n \coloneqq x_n - \sum_{j=1}^n\scpr {x_n}{u_j}u_j$ et $u_n \coloneqq \frac {y_n}{\norm {y_n}}$.

Il faut maintenant s'assurer que pour tout $n \geq 1$~: $y_n \neq 0$ afin que les $u_n$ soient bien définis. Par l'absurde, supposons qu'il existe $n \geq 1$ tel que $y_n = 0$. Alors~:
\[x_n \in \Span\{u_1, \ldots, u_{n-1}\} \subseteq \Span\{y_1, \ldots, y_{n-1}\} \subseteq \Span\{x_1, \ldots, x_{n-1}\}.\]
Or les $x_j$ sont linéairement indépendants.

Il est évident que $u_n \in \Span\{x_1, \ldots, x_n\}$ et $x_n \in \Span\{u_1, \ldots, u_n\}$ pour tout $n \geq 1$. Il reste alors uniquement à montrer que $u_n \perp u_j$ ($j < n$),
ou de manière équivalente $y_n \perp u_j$, et cette dernière formulation est évidente par définition de $y_n$.
\end{proof}

\begin{déf} Un espace topologique $E$ est dit \textit{séparable} s'il admet une partie dense dénombrable.
\end{déf}

\begin{thm} Un espace de Hilbert est séparable ssi il possède un système orthonormal maximal fini ou dénombrable.
\end{thm}

\begin{proof} \underline {$\Rightarrow$~:} Soit $\{a_1, a_2, \ldots\}$ une suite dense dans $H$. Soit $\{a'_1, a'_2, \ldots\}$ la suite partielle (possiblement finie) de
$\{a_1, \ldots\}$ consitituée des $a_i \not \in \Span\{a_1, \ldots, a_{i-1}\}$. Par définition, on a que les $a'_i$ sont indépendants et tous les $a_j$ sont combinaisons linéaires
des $a'_i$, i.e. $\{a_1, a_2, \ldots\} \subset \Span\{a'_1, a'_2, \ldots\}$.

On en déduit alors que $\Span\{a'_1, a'_2, \ldots\}$ est dense dans $H$ Par Gram-Schmidt sur $\{a'_1, a'_2, \ldots\}$, on a un système orthonormal $\{u_1, \ldots\}$ tel que
$\Span\{u_1, \ldots\}$ est dense dans $H$. Dès lors, par le Théorème~\ref{thm:caractérisation SOM}, $\{u_1, \ldots\}$ est un SOM.

\underline {$\Leftarrow$~:} Soit $(e_k)_{k \in \N}$ un SOM. Pour $N \in \N^*$, on pose~:
\[E_N \coloneqq \left\{\sum_{j=1}^Nq_je_j \tq q_j \in \Q[i]\right\}.\]

$E_N$ est dénombrable, et donc $E \coloneqq \bigcup_{N > 0}E_N$ est également dénombrable. Par le théorème~\ref{thm:caractérisation SOM}, on a
$\forall \varepsilon > 0 : \exists A_0 \subset A$ fini tel que~:
\[\forall A_1 \supset A_0 : \norm {x-\sum_{\alpha \in A_1}\hat x(\alpha)e_\alpha} \leq \varepsilon.\]

Montrons que $\forall \varepsilon > 0 : \exists y \in E \st \norm {x-y} \leq \varepsilon$.
Soit $(q_\alpha)_{\alpha \in A_1} \subset \Q[i] \st \sum_{\alpha \in A_1}\norm {q_\alpha - \hat x(\alpha)}^2 \leq \varepsilon^2$. De tels $q_\alpha$ existent bien par densité
de $\Q$ dans $\R$, et donc par densité de $\Q[i]$ dans $\C$.

\[\norm {x - \sum_{\alpha \in A_1}q_\alpha e_\alpha}^2 = \norm {x-\sum_{\alpha \in A_1}\hat x(\alpha)e_\alpha}^2 + \sum_{\alpha \in A_1}\norm {q_\alpha - \hat x(\alpha)} \leq 2\varepsilon^2.\]

Donc pour $y = \sum_{\alpha \in A_1}q_\alpha e_\alpha$, on a bien le résultat.
\end{proof}

\begin{ex}
$\C^N$ muni du produit scalaire usuel admet une base canonique. Cette dernière est orthonormale et maximale.
\end{ex}

\begin{ex}
Dans $\ell^2(\N_0)$, posons le suites $e_k \; (k \in \Z)$ telles que $e_{k,j} = 1$ si $k=j$ et $0$ sinon. $(e_k)_{k \in \Z}$ est un système orthonormal maximal car~:
\[\norm x^2 = \scpr xx = \sum_{j \geq 1}x_j\overline {x_j} = \sum_{j \geq 1}\abs {x_j}^2.\]
Par le point 3 du Théorème~\ref{thm:caractérisation SOM}, on a que $(e_k)_{k \in \Z}$ est un SOM.
\end{ex}

\begin{ex}
Dans $L^2[0, 2\pi)$, on définit (pour $k \in \Z$)~:
\[e_k : [0, 2\pi) \to \C : t \mapsto \frac {e^{ikt}}{\sqrt {2\pi}}.\]
Pour le produit scalaire usuel de $L^2$, on a~:
\[\scpr {e_k}{e_\ell}_{L^2} = \int_0^{2\pi}\frac {e^{i(k-\ell)t}}{2\pi}\dif t = \begin{cases}1 &\text{ si } k-\ell = 0 \\0 &\text{ sinon}\end{cases}.\]

Pour montrer que $(e_k)_{k \in \Z}$ est maximal, prenons $f \in L^2[0, 2\pi)$. $S_kf \coloneqq \sum_{m=-k}^k\scpr f{e_m}e_m.$ Montrons que $S_kf \xrightarrow[k \to \pinfty]{L^2} f$.
Pour $\varepsilon > 0, \exists \varphi \in C_0^\infty((0, 2\pi)) : \norm {f-\varphi}_{L^2} \leq \varepsilon$ (par densité de $C_0^\infty$ dans $L^2$).

On a $S_k\varphi \xrightarrow[k \to \pinfty]{\text{CVU}} \varphi$ (théorème de Dirichlet global), ce qui implique $S_k\varphi \xrightarrow[k \to \pinfty]{L^2} \varphi$.
Finalement, remarquons~:
\[\norm {f-S_kf}_{L^2} \leq \norm {f-S_k\varphi}_{L^2} \leq \norm {f-\varphi}_{L^2} + \norm {\varphi - S_k\varphi} \leq 2\varepsilon\]
si $k$ est assez grand.

Attention, $\{e_k\}_k$ n'est \textbf{pas} une base au sens algébrique car $\forall k \in \Z : e_k \in C^\infty$. Donc si $\{e_k\}_k$ est une base, toute fonction $f \in L^2$ est
égale à $\sum_{j=1}^Nc_je_{k_j} \in C^\infty$. Or $L^2 \not \subseteq C^\infty$.
\end{ex}

\section{Applications linéaires entre espaces vectoriels normés}
Soient $E, F$ deux espaces de Banach. Pour $T : E \to F$ linéaire, on dit que $T$ est \textit{bornée} lorsque~:
\[\sup_{\norm x \leq 1}\norm {Tx} \lneqq \pinfty.\]

\begin{rmq} \textit{borné} doit se comprendre \textit{borné sur la boule unité} car $T$ n'est pas borné puisque linéaire.
\end{rmq}

\begin{prp} Soit $T : E \to F$ linéaire.
\[\sup_{\norm x \leq 1}\norm {Tx} = \sup_{\norm x \lneqq 1}\norm {Tx} = \sup_{\norm x = 1}\norm {Tx}.\]
\end{prp}

\begin{proof} On note $A \coloneqq \{\norm {Tx} \st \norm x \leq 1\}$, $B \coloneqq \{\norm {Tx} \st \norm x \lneqq 1\}$, et $C \coloneqq \{\norm {Tx} \st \norm x = 1\}$.
Notons également $a = \sup A$, $b = \sup B$ et $c = \sup C$.

Puisque $A \supset B \cup C$, on sait que $a \geq b$ et $a \geq c$. Maintenant, si $x \neq 0 \st \norm x \leq 1$, alors~:
\[C \ni \norm {T\frac x{\norm x}} = \frac 1{\norm x}\norm {Tx} \geq \norm {Tx} \in A.\]
En particulier, $c \geq a$, et donc $c=a$.

Si $a \lneqq \pinfty$, alors $\forall \delta > 0 : \exists x \st \norm x = 1$ et $\norm {Tx} \geq a-\delta$ (par définition du $\sup$ et puisque $a=c$). Pour $\varepsilon \in (0, 1)$~:
\[\norm {T((1-\varepsilon)x)} = (1-\varepsilon)\norm {Tx} \geq (1-\varepsilon)(a-\delta) \geq a-\eta,\]
pour $\eta = \varepsilon a - \varepsilon \delta + \delta$. Pour $\delta, \varepsilon \to 0$, on a $\eta \to 0$ et donc $b \geq a$.

Finalement, si $a = \pinfty$, alors $\forall M > 0 : \exists x_M \st \norm {x_M} \leq 1$ et $\norm {Tx_M} \geq M$. Or par linéarité de $T$, on a $\norm {x_{M/2}} \lneqq 1$,
et finalement~:
\[\forall M > 0 : \exists \tilde x_M (= x_{M/2}) \st \norm {\tilde x_M} \leq 1 \text{ et } \norm {T\tilde x_M} \geq M.\]
On en déduit également que $a=b$.

Dès lors, on a bien $a = b = c$.
\end{proof}

\begin{déf} L'ensemble des applications linéaires bornées de $E$ dans $F$ est noté $\mathcal L(E, F)$. On munit cet ensemble de la norme~:
\[\norm \cdot_{\mathcal L(E, F)} : \mathcal L(E, F) \to \mathbb R^+ : T \mapsto \norm T_{\mathcal L(E, F)} \coloneqq \sup_{\norm x \leq 1}\norm {Tx}.\]

On note également $\mathcal L(E) \coloneqq \mathcal L(E, E)$.
\end{déf}

\begin{rmq} Il est à noter que cette norme sur $\mathcal L(E, F)$ dépend des normes sur $E$ et sur $F$~!
\end{rmq}

\begin{prp} $\norm \cdot_{\mathcal L(E, F)}$ est une norme.
\end{prp}

\begin{proof} \TODO: Exercice
\end{proof}

\begin{prp} Soit $T \in \mathcal L(E, F)$. Pour tout $y \in E$, on a $\norm {Ty} \leq \norm T\norm y$.
\end{prp}

\begin{proof} Si $y = 0$, alors $Ty = 0$, et donc ok. Sinon, $Ty = \norm yT\frac y{\norm y}$. Par passage à la norme dans $F$~:
\[\norm {Ty} = \norm y \underbrace {\norm {T\frac y{\norm y}}}_{\leq \norm T} \leq \norm y\norm T.\]
\end{proof}

On remarque également que $\norm {(T_2 \circ T_1)(x)} \leq \norm {T_2}\norm {T_1x} \leq \norm {T_2}\norm {T_1}\norm x$, et donc $\norm {T_2T_1} \leq \norm {T_2}\norm {T_1}$. Dès lors
si $T_1 \in \mathcal L(E, F)$ et $T_2 \in \mathcal L(F, G)$, alors $T_2T_1 \in \mathcal L(E, G)$.

\begin{thm}\label{thm:opérateur continu ssi borné} Soit $T : E \to F$ linéaire. Alors les conditions suivantes sont équivalentes~:
\begin{itemize}
	\item[$(i)$]   $T$ est bornée~;
	\item[$(ii)$]  $T$ est continue en tous points~;
	\item[$(iii)$] $\exists x_0 \in E \st T$ est continue en $x_0$.
\end{itemize}
\end{thm}

\begin{proof}~
\begin{itemize}
	\item[$(i) \Rightarrow (ii)$]   Soit $y \in E$. $\norm {Tx - Ty} = \norm {T(x-y)} \leq \norm T\norm {x-y}$. Dès lors, $T$ est Lipschitzienne et donc continue.
	\item[$(ii) \Rightarrow (iii)$] Trivial (je cite~: \textit{Si vous avez un problème ici, je crois qu'il y a un sérieux problème dans l'enseignement}).
	\item[$(iii) \Rightarrow (i)$]  Pour $\varepsilon > 0$, il existe $\delta > 0$ tel que si $\norm {x-x_0} < \delta$, alors $\norm {T(x-x_0)} < \varepsilon$.
		En posant $y \coloneqq x-x_0$, si $\norm y < \delta$, alors $\norm {Ty} < \varepsilon$. Autrement dit, $T(B(0, \delta)) \subset B(0, \varepsilon)$.
		Pour $z \in B(0, 1)$ (i.e. $\norm z < 1$), on a~:
		\[\norm {Tz} = \frac 1\delta\norm {T(\delta z)} < \frac 1\delta \varepsilon.\]
		Dès lors $\forall z : \norm z < 1 \Rightarrow \norm {Tz} < \varepsilon/\delta$, et donc $\norm T \lneqq \pinfty$, i.e. $T$ est bornée.
\end{itemize}
\end{proof}

\begin{ex}
Un opérateur linéaire $T : \C^n \to \C^m$ est déterminé par une matrice $(T_{k\ell})_{k,\ell}$ dans les bases canoniques de $\C^m$ et $\C^n$.
$T$ est continu\footnote{En dimension finie, toute application linéaire entre espaces vectoriels est continue.} donc bornée.
\end{ex}

\begin{ex}
$T : L^1(\Omega, \mathcal A, \mu) \to \C : f \mapsto \int f\dif\mu$ est borné car~:
\[\abs {Tf} = \abs {\int f\dif\mu} \leq \int\abs f\dif\mu = \norm f_{L^1}.\]
Dès lors l'opérateur $T$ d'intégration est continue.
\end{ex}

\begin{ex}
Si $1 \leq p, q \leq \pinfty$ sont conjugués, pour $g \in L^q(\Omega, \mathcal A, \mu)$ on définit~:
\[T_g : L^p(\Omega, \mathcal A, \mu) \to \C : f \mapsto \int fg\dif\mu.\]
$T_g$ est bien défini par l'inégalité de Hölder. De plus~: $\abs {T_gf} \leq \norm f_{L^p}\norm g_{L^q}$, et donc $\norm {T_g} \leq \norm g_{L^q}$.
\end{ex}

\begin{ex}
$\mathbb F : L^2(\R^n) \to L^2(\R^n)$ est un opérateur linéaire. De plus, par Plancherel, on a $\norm {\mathbb Ff}_{L^2} = (2\pi)^{n/2}\norm f_{L^2}$,
et donc $\norm {\mathbb F} = (2\pi)^{n/2}$, i.e. la transformée de Fourier est un opérateur borné (donc continu).
\end{ex}

\begin{ex}
Pour $H$, un espace de Hilbert quelconque et $M$ un sous-espace vectoriel fermé, $P : H \to M$, la projection orthogonale sur $M$ est bornée car Lipschitzienne
(et donc également continue).
\end{ex}

\begin{ex}
Dans $(\Omega, \mathcal A, \mu)$ un espace mesuré, on fixe $K \in L^2(\Omega \times \Omega, \mathcal A \otimes \mathcal A, \mu \otimes \mu)$, et on pose~:
\[T : L^2(\Omega, \mathcal A, \mu) \to L^2(\Omega, \mathcal A, \mu) : f \mapsto \int_\Omega K(\cdot, y)f(y)\dif\mu(y).\]
Par Fubini, $\forall x \in \Omega : \int_\Omega \abs {K(x, y)}^2\dif\mu(y)$ est bien défini, et pour presque tout $x \in \Omega :
\abs {K(x, \cdot)}^2 \in L^1(\Omega, \mathcal A, \mu)$. En particulier, $\abs {K(x, \cdot)} \in L^2(\Omega, \mathcal A, \mu)$.

Dès lors, à $x \in \Omega$ fixé, $y \mapsto K(x, y)f(y)$ est $L^1$ par Hölder. Donc il existe $N \in \mathcal A \st \mu(N) = 0$ et $\forall x \in \Omega \setminus N :
K(x, \cdot)f(\cdot) \in L^1(\Omega, \mathcal A, \mu)$. On pose alors~:
\[g : x \mapsto \begin{cases}\displaystyle \int_\Omega K(x, y)f(y)\dif\mu(y) &\text{ si $x \not \in N$,} \\0 &\text{ sinon.}\end{cases}\]

Montrons que $g = Tf \in L^2(\Omega, \mathcal A, \mu)$. Si $x \in \Omega \setminus N$, alors par Cauchy-Schwarz~:
\[\abs {g(x)}^2 \leq \left(\int\abs {K(x, y)}\abs {f(y)}\dif\mu(y)\right)^2 \leq \int_\Omega \abs {K(x, y)}^2\dif\mu(y)\int_\Omega \abs {f(y)}^2\dif\mu(y).\]
Or le second facteur ($= \norm f_{L^2}^2$) ne dépend pas de $x$ et est fini puisque $f \in L^2(\Omega, \mathcal A, \mu)$. Dès lors~:
\[\norm {Tf}_{L^2}^2 = \int\abs {g(x)}^2\dif\mu(x) \leq \norm f_{L^2}^2\int_\Omega\left(\int_\Omega \abs {K(x, y)}^2\dif\mu(y)\right)\dif\mu(y).\]
	Par Fubini (puisque $K \in L^2(\Omega \times \Omega, \mathcal A \otimes \mathcal A, \mu \otimes \mu)$), on a finalement~:
\[\int_\Omega\abs {g(x)}^2\dif\mu(x) \leq \norm f_{L^2}^2\int_{\Omega \times \Omega}\abs {K(x, y)}^2\dif(\mu \otimes \mu)(x, y) \lneqq \pinfty.\]
On en déduit également $\norm g_{L^2} \leq \norm f_{L^2}\norm K_{L^2}$, et donc $\norm T \leq \norm K_{L^2}$.
\end{ex}

\begin{déf} Un opérateur de la forme suivante~:
\[T : L^2(\Omega, \mathcal A, \mu) \to L^2(\Omega, \mathcal A, \mu) : f \mapsto \int_\Omega K(\cdot, y)f(y)\dif\mu(y),\]
pour $K \in L^2(\Omega \times \Omega, \mathcal A \otimes \mathcal A, \mu \otimes \mu)$ est appelé \textit{opérateur intégral de Hilbert-Schmidt}.
\end{déf}

\begin{déf} Soient $H$ un espace de Hilbert, $T \in \mathcal L(H)$, $x \in H$ et $\Phi : H \to \C : y \mapsto \scpr {Ty}x$, une forme linéaire bornée. Par Cauchy-Schwarz~:
$\abs {\Phi y} \leq \norm {Ty}\norm x \leq \norm T\norm y\norm x$. Par le lemme de Riesz (Lemme~\ref{lem:Riesz}), on sait qu'il existe un unique $z_x \in H$ tel que
$\forall y \in H : \Phi y = \scpr y{z_x}$. On a alors une application $x \mapsto z_x$. Notons-la $T^*$. Cet opérateur $T^*$ est appelé \textit{l'opérateur adjoint de $T$}.
\end{déf}


\begin{prp} $T^*$ est une application linéaire.
\end{prp}

\begin{proof} Soient $x_1, x_2 \in H, \lambda \in \C$. Alors~:
\[\scpr y{T^*(\lambda x_1 + x_2)} = \scpr {Ty}{\lambda x_1 + x_2} = \overline \lambda \scpr {Ty}{x_1} + \scpr {Ty}{x_2} = \overline \lambda\scpr y{T^*x_1} + \scpr y{T^*x_2}
= \scpr y{\lambda T^*x_1 + x_2}.\]
Or cette égalité vaut pour pour tous $x_1, x_2 \in H$, donc $T^*(\lambda x_1 + x_2) = \lambda T^*x_1 + T^*x_2$.
\end{proof}

\begin{lem} $\sup_{\norm x \leq 1}\norm {Tx} = \sup_{\norm x \leq 1, \norm y \leq 1}\abs {\scpr {Tx}y}$.
\end{lem}

\begin{proof} Par Cauchy-Schwarz, si $\norm y \leq 1$~: $\abs {\scpr {Tx}y} \leq \norm {Tx}\norm y \leq \norm {Tx}$. En particulier, par passage au $\sup$, on a
l'inégalité $\geq$.

Pour l'autre inégalité, Si $T \equiv 0$, le résultat est trivial. Donc supposons $T \not \equiv 0$. On sait alors que $\Ker T \neq H$, et donc $\exists x \in H \setminus \Ker T$.
En posant $z \coloneqq \frac {Tx}{\norm {Tx}}$, on observe~:
\[\sup_{\substack {\norm x \leq 1 \\ \norm y \leq 1}}\abs {\scpr {Tx}y} \geq \sup_{\norm x \leq 1}\abs {\scpr {Tx}z} = \sup_{\norm x \leq 1}\frac {\norm {Tx}^2}{\norm {Tx}}.\]
\end{proof}

\begin{thm} Pour $T \in \mathcal L(H)$, on a $\norm T = \norm {T^*}$.
\end{thm}

\begin{proof} Par le lemme précédent~:
\[\norm {T^*} = \sup_{\norm x \leq 1}\norm {T^*x} = \sup_{\substack {\norm x \leq 1 \\ \norm y \leq 1}}\abs {\scpr {T^*x}y}
	= \sup_{\substack {\norm x \leq 1 \\ \norm y \leq 1}}\abs {\scpr y{T^*x}} = \sup_{\substack {\norm x \leq 1 \\ \norm y \leq 1}}\abs {\scpr {Ty}x}
	= \sup_{\norm x \leq 1}\norm {Tx} = \norm T\]
\end{proof}

\begin{prp} Soient $S, T \in \mathcal L(H)$. Pour $\alpha, \beta \in \C$, on a~:
\begin{enumerate}
	\item $(\alpha S + \beta T)^*x = \overline \alpha S^* + \overline \beta T^*$~;
	\item $(ST)^* = T^*S^*$.
\end{enumerate}
\end{prp}

\begin{proof}~
\begin{enumerate}
	\item Fixons $x, y \in H$.
	\[\scpr x{(\alpha S + \beta T)^*y} = \scpr {(\alpha S + \beta T)x}y = \alpha \scpr {Sx}y + \beta \scpr {Tx}y = \alpha \scpr x{S^*y} + \beta \scpr x{T^*y}
	= \scpr x{\overline \alpha S^*y + \overline\beta T^*y}.\]
	\item Montrons que $\forall x \in H : (ST)^*x = T^*S^*x$. Soient $x, y \in H$.
	\[\scpr y{(ST)^*x} = \scpr {STy}x = \scpr {Ty}{S^*x} = \scpr y{T^*S^*x}.\]
\end{enumerate}
\end{proof}

\begin{déf} Si $T = T^*$, on dit que $T$ est auto-adjoint.
\end{déf}

\begin{ex}~
\begin{itemize}
	\item[(1)] Soit un opérateur linéaire $T \in \mathcal L(\C^n)$. $T$ est défini par une matrice $(T_{k\ell})_{k,\ell} \in \C^{n \times n}$. L'adjoint $T^*$ de $T$
	est également un opérateur linéaire de $\C^n$ et est donc également définit par une matrice $({T^*}_{k\ell})_{k,\ell}$. Fixons $z, w \in \C^n$ et calculons~:
	\[\sum_{j=1}^n\sum_{k=1}^n{T^*}_{jk}z_k\overline {w_j} = \scpr {T^*z}w = \scpr z{Tw} = \sum_{j=1}^n\sum_{k=1}^nz_k\overline {T_{kj}}\overline {w_j}.\]
	Cette égalité étant vraie $\forall z, w \in \C^n$, on en déduit ${T^*}_{jk} = \overline {T_{kj}}$, i.e. la matrice adjointe est la conjuguée de la transposée.

	D'ailleurs, si $T=T^*$, alors $(T_{jk})_{jk}$ est une matrice hermitienne.
	\item[(2)] Pour un opérateur intégral de Hilbert-Schmidt, fixons $K \in L^2(\Omega \times \Omega, \mathcal A \otimes \mathcal A, \mu \otimes \mu)$ et considérons~:
	\[T_K : L^2(\Omega, \mathcal A, \mu) \to L^2(\Omega, \mathcal A, \mu) : f \mapsto \int K(\cdot, y)f(y)\dif\mu(y).\]
	Soient $f, g \in L^2(\Omega, \mathcal A, \mu)$. Partons de $\scpr {T_Kf}g = \scpr f{T_K^*f}$ et calculons~:
	\[\scpr {Tf}g = \int_\Omega\overline g(x)\int_\Omega K(x, y)f(y)\dif\mu(y)\dif\mu(x) \stackrel {\text{Fubini}}=
		\int_\Omega f(y)\int_\Omega K(x, y)\overline g(x)\dif\mu(x)\dif\mu(y) = \scpr f{T_K^*g}.\]

	Donc~:
	\[T_K^*g(y) = \overline {\int_\Omega K(x, y)\overline g(x)\dif\mu(x)},\]
	ou en changeant simplement les variables $x$ et $y$~:
	\[T_K^*g(x) = \int_\Omega \overline K(y, x) g(y)\dif\mu(y).\]
	$T_K^*$ est donc également un opérateur intégral de Hilbert-Schmidt et on a bien \textit{transposé/conjugué} le noyau $K$ de $T_K$ pour trouver celui de $T_K^*$.

	\item[(2)] Reconsidérons $M$ un sous-espace fermé de $H$ et la projection orthogonale $P : H \to M$. Montrons que $P = P^*$.

	Soient $x_1, x_2 \in H$ et soit $Q$ la projection orthogonale sur $M^\perp$. Calculons~:
	\[\scpr {Px_1}{x_2} = \scpr {Px_1}{Px_2 + Qx_2} = \scpr {Px_1}{Px_2} + \underbrace {\scpr {Px_1}{Qx_2}}_{= 0} = \scpr {Px_1}{Px_2}.\]
	Et~:
	\[\scpr {x_1}{Px_2} = \scpr {Px_1 + Qx_1}{Px_2} = \scpr {Px_1}{Px_2} + \underbrace {\scpr {Qx_1}{Px_2}}_{= 0} = \scpr {Px_1}{Px_2}.\]
	On a donc $\forall x_1, x_2 \in H : \scpr {x_1}{P^*x_2} = \scpr {Px_1}{x_2} = \scpr {x_1}{Px_2}$. Dès lors $P = P^*$. La projection orthogonale est donc auto-adjointe.
\end{itemize}
\end{ex}

Pour $E, F$ espaces vectoriels normés, plusieurs normes semblent canoniques. À $p \geq 1$ fixé, on peut définir~:
\[\norm {(e, f)}_{E \times F; p} \coloneqq \left(\norm e_E^p + \norm f_F^p\right)^{1/p}.\]
De même, si $E$ et $F$ sont munis d'un produit scalaire, on a un produit scalaires canonique sur $E \times F$~:
\[\scpr {(e_1, f_1)}{(e_2, f_2)}_{E \times F} \coloneqq \scpr {e_1}{e_2}_E + \scpr {f_1}{f_2}_F,\]
et donc la norme $\norm \cdot_{E \times F; 2}$ semble particulièrement intuitive.

Il est cependant à noter que les normes $\norm \cdot_{E \times F; p}$ sont équivalentes pour toutes les valeurs de $p \geq 1$, et donc que les topologies induites par ces
normes sont homéomorphes (elles sont même strictement identiques, et cette topologie est la topologie produit). Dès lors, la norme $\norm \cdot_{E \times F; 1}$ va être
posée canoniquement sur $E \times F$, mais les résultats qui suivront seront également valables pour toute valeur de $p > 1$.

\begin{déf} Pour un opérateur linéaire $T : E \to F$, on note $\Gamma_T = \{(x, Tx)\}_{x \in E} \subset E \times F$ le graphe de $T$.
\end{déf}

\begin{thm} Si $T$ est bornée, alors $\Gamma_T$ est fermé dans $E \times F$.
\end{thm}

\begin{proof} Soit $(x, y) \in \overline {\Gamma_T}$. Prenons une suite $(x_n, y_n)_{n \in \N} \subset \Gamma_T$ telle que $(x_n, y_n) \xrightarrow[n \to \pinfty]{} (x, y)$.
De plus $T$ est continue car bornée. Dès lors~:
\[\begin{cases}
&Tx_n = y_n \\
&Tx_n \xrightarrow[n \to \pinfty]{} Tx \\
&y_n \xrightarrow[n \to \pinfty]{} y.
\end{cases}\]
Or $x \in E$ et par unicité de la limite, $Tx = y$. Dès lors, $(x, y) \in \Gamma_T$.
\end{proof}

\begin{déf} Une application $f : E \to F$ est \textit{ouverte} si l'image de tout ouvert de $E$ par $f$ est un ouvert de $F$.
\end{déf}

\begin{thm}[de Baire] Soit $X$ un espace métrique complet. Si $(V_n)_{n \geq 0}$ est une suite d'ouverts denses dans $X$, alors $\bigcap_{n \geq 0}V_n$ est également dense dans $X$.
\end{thm}

\begin{proof} Soient $(V_n)_n$ ouverts denses dans $X$. Si pour tout $W \subset X$ ouvert non vide, $W \cap \bigcap_{n \geq 0}V_n \neq \emptyset$, alors $\bigcap_{n \geq 0}V_n$
est dense dans $X$. Fixons donc $W \subset X$ ouvert de $X$ non vide et trouvons $x \in W \cap \bigcap_{n \geq 0}V_n$.

$V_1$ est dense. Donc $\exists x_1 \in W \cap V_1$ et $r_1 \in (0, 1)$ tel que $\overline {B(x_1, r_1)} \subset W \cap V_1$. $B(x_X1, r_1)$ est un ouvert de $X$, donc
$\exists x_2 \in B(x_1, r_1) \cap V_2$ et $r_2 \in (0, 1/2)$ tel que $\overline {B(x_2, r_2)} \subset B(x_1, r_1) \cap V_2$, etc. On construit ainsi une suite $(x_n)_{n \geq 1}$
et $(r_n)_{n \geq 1}$ tels que~:
\[\forall n \geq 2 : x_n \in B(x_{n-1}, r_{n-1}) \cap V_n \text{ et } r_n \in (0, 1/n) \st \overline {B(x_n, r_n)} \subset B(x_{n-1}, r_{n-1}) \cap V_n.\]

À $n$ fixé, pour $k \geq n$, $x_k \in B(x_n, r_n)$ et donc pour $k, \ell \geq n : d(x_k, x_\ell) \leq d(x_k, x_n) + d(x_\ell, x_n) \leq 2r_n = \frac 2n$.
Donc la suite $(x_n)_n$ est de Cauchy dans $X$. Dès lors $\exists x \in X \st x_n \xrightarrow[n \to \pinfty]{} x$. De plus, pour $k \geq n$~:
\[x_k \in \overline {B(x_n, r_n)} \subset B(x_{n-1}, r_{n-1}) \cap V_n \subset V_n.\]
Dès lors $\forall n \geq 1 : \forall k \geq n : x_k \in V_n$, et donc $\forall k \geq 1 : x_k \in \bigcap_{k=1}^nV_n$. En particulier~: $x \in \bigcap_{n \geq 1}V_n$.

Finalement, puisque $x \in \overline {B(x_1, r_1)} \subset W$, on a bien $x \in W \cap \bigcap_{n \geq 1}V_n$.
\end{proof}

\begin{rmq} Le théorème de Baire peut se formuler de la manière équivalente suivante: si dans $X$, $(F_n)_{n \geq 0}$ est une suite de fermés d'intérieur vide,
alors $\bigcup_{n \geq 0}F_n$ est également d'intérieur vide.
\end{rmq}

\begin{thm}[de l'application ouverte, Banach] Soient $E, F$ espaces de Banach, $T \in \mathcal L(E, F)$. Si $T$ est surjective, alors $T$ est ouverte.
\end{thm}

\begin{proof} Notons $B_E$ (resp. $B_F$) la boule unité dans $E$ (resp. dans $F$). Il suffit de montrer que $\exists \delta > 0 \st T(B_E) \supset \delta B_F$.
En effet, en supposant que cette inclusion est vérifiée, on a $T(x_0 + \lambda B_E) = Tx_0 + \lambda T(B_E)$. Dès lors, si $U$ est un voisinage de $x_0$ dans $E$,
alors $T(U)$ est un voisinage de $Tx_0$ dans $F$. Si de plus $U$ est ouvert, $U$ est un voisinage de tout $y \in U$, et donc $T(U)$ est un voisinage de tout $Ty \in T(U)$,
et donc $T(U)$ est ouvert dans $F$.

Montrons donc qu'il existe un tel $\delta > 0$. On sait que $E = \bigcup_{n \geq 0}n B_E$, et donc~:
\[F = T(E) = T\left(\bigcup_{n \geq 0}n B_E\right) = \bigcup_{n \geq 0}T(n B_E).\]

Supposons alors par l'absure que $\forall n \geq 0 : \mathring {\overline {T(nB_E)}} = \emptyset$. Par Baire, on a $F = \mathring F = \emptyset$, ce qui est une contradiction.

Dès lors, il existe $n > 0$ te que $T(nB_E) \supseteq \mathring {\overline {T(nB_E)}} \neq \emptyset$, et donc il existe un ouvert $W \subset \overline {T(nB_E)}$. Soient $y_0 \in W$
et $r > 0 \st y_0 + rB_F \subset W$. Fixons également $y \in F \st \norm y < r$. Soit également $(x'_k)_{k \geq 0} \subset nB_E \st Tx'_k \xrightarrow[k \to \pinfty]{} y_0$.

Puisque $y_0+y \in W \subset \overline {T(nB_E)}$, prenons une autre suite $(x''_k)_{k \geq 0} \subset nB_E \st Tx''_k \xrightarrow[k \to \pinfty]{} y_0+y$.

On note $x_k \coloneqq x''_k - x'_k$. $\norm {x_k} \leq \norm {x'_k} + \norm {x''_k} \leq 2n$ et $Tx_k = Tx''_k - Tx'_k \xrightarrow[k \to \pinfty]{} y_0+y-y_0 = y$.
Dès lors $\forall \varepsilon > 0 : \forall y \in rB_F : \exists x \in E \st \norm x \leq 2n$ et $\norm {Tx-y} \leq \varepsilon$. Ce qui est équivalent à~:
\begin{equation}\tag{$\Delta$}\label{eq:Delta}
	\forall y \in F : \forall \varepsilon > 0 : \exists x \in E \st \norm x \leq \frac {4n}r\norm y \text{ et } \norm {Tx - y} \leq \varepsilon.
\end{equation}
En effet, si $y = 0$, on prend $x=0$, et si $y \neq 0$, on pose $\widetilde y \coloneqq \frac r{2\norm y}y$ et on sait qu'il existe
$\widetilde x \in E \st \norm x \leq 2n$ et $\norm {T\tilde x - \widetilde y} \leq \frac {\varepsilon r}{2\norm y}$. On peut dès lors poser $x \coloneqq \frac {2\norm y}r\widetilde x$.
Dans ce cas, on a $\norm x = \frac {2\norm y}r\norm {\tilde x} \leq \frac {4n}r\norm y$ et~:
\[\norm {Tx-y} = \norm {T\left(\frac {2\norm y}y\widetilde x\right) - \frac {2\norm y}r\widetilde y} = \frac {2\norm y}r\norm {T\widetilde x - \widetilde y}
\leq \frac {2\norm y}r\frac {\varepsilon r}{2\norm y} = \varepsilon.\]

Posons $\delta \coloneqq \frac r{4n}$, et prenons $\omega > 0$. Montrons alors que~:
\begin{equation}\tag{$\sharp$}\label{eq:sharp}
	\forall y \in F : \norm y \leq \delta \Rightarrow \exists x \in E \st \norm x \leq 1+\omega \text{ et } Tx = y.
\end{equation}

Fixons $y \in \delta B_F$. Par~\eqref{eq:Delta}~:
\[\exists x_1 \in E \st \norm {x_1} \leq \frac 1\delta\norm y \leq 1 \text{ et } \norm {Tx_1-y} \leq \frac {\delta\omega}2.\]

De même~:
\[\exists x_2 \in E \st \norm {x_2} \leq \frac {4n}r\norm {Tx_1 - y} \leq \frac \omega2 \text{ et } \norm {(y-Tx_1) - Tx_2} \leq \frac {\delta\omega}{2^2}.\]
On construit ainsi $x_1, \ldots, x_n$ tels que $\norm {x_n} \leq 2^{-(n-1)}\omega$. Pour $\varepsilon = 2^{-n}\delta\omega$, on a l'existence de
$x_n \st \norm {x_n} \leq 2^{-(n-1)}\omega$ et $\norm {y-Tx_1-\ldots-Tx_n} \leq \varepsilon$.

De plus, puisque $(x_n)_n$ est de Cauchy, la suite $(\sum_{j=1}^nx_j)_n$ est également de Cauchy. Donc par complétude de $E$~:
\[\exists x \in E \st \sum_{j=1}^nx_j \xrightarrow[n \to \pinfty]{} x.\]

Par continuité de la norme $\norm \cdot$, on a également $\norm {\sum_{j=1}^nx_j} \xrightarrow[n \to \pinfty]{} \norm x$. Or~:
\[\norm {\sum_{j=1}^nx_j} \leq \sum_{j=1}^n\norm {x_j} \leq 1 + \sum_{j=2}^n\norm {x_j} \leq 1 + \sum_{j=1}^n\omega 2^{-(n-1)} = 1 + \omega(1 - 2^{-(+1)}) \leq 1+\omega,\]
donc $\norm x \leq 1+\omega$. Finalement, par continuité de $T$, on a
$T(\sum_{j=1}^nx_j) \xrightarrow[n \to \pinfty]{} Tx$. Or puisque $T(\sum_{j=1}^nx_j) = \sum_{j=1}^nTx_j \xrightarrow[n \to \pinfty]{} y$, par unicité de la limite
(dans les espaces métriques complets), on a $Tx=y$,
ce qui montre bien~\eqref{eq:sharp}.

Finalement, par~\eqref{eq:sharp}, on a bien $\delta B_F \subset T(B_E)$ puisque pour tout $y \in \delta B_F$ (i.e. $\norm y \leq \delta$), on a $x \in E \st \norm x \leq 1$
et $Tx = y$.
\end{proof}

\begin{thm}[du graphe fermé, Banach]\label{thm:graphe fermé} Soient $E, F$ espaces de Banach, $T : E \to F$ linéaire. Si $\Gamma_T$ est fermé dans $E \times F$, alors $T$ est bornée.
\end{thm}

\begin{proof} $\Gamma_T$ est un sous-espace vectoriel fermé de $E \times F$. Or $E \times F$ est un espace de Banach puisque le produit d'espaces de Banach en est un.
Dès lors $\Gamma_T$ est un espace de Banach également. On décompose $T$ en $T_1 : E \to \Gamma_T : x \mapsto (x, Tx)$ et $T_2 : E \times F \to F : (x, y) \mapsto y$ ($T = T_2T_1$).
$T_1$ est bijective par définition du graphe d'une application et ${T_1}^{-1} : (x, Tx) \mapsto x$ est bornée (donc continue). Par le théorème de l'application ouverte de Banach,
on déduit que ${T_1}^{-1}$ est ouverte, i.e. $T_1$ est continue.

De plus, $T_2$ est continue car les projections sont toujours continues pour la topologie produit. Donc $T = T_2T_1$ est composition d'applications continues, et est donc continue.
\end{proof}

\chapter{Équations aux dérivées partielles}
\section{Rappels}
Dans le cadre des EDOs, on cherche une fonction $u : [-T, T] \to \C$ telle que~:
\[F(t, u(t), u'(t), \ldots, u^{(m)}(t)) = 0.\]

On généralise à $u : \R^n \to \C$, et une EDP est donc sous la forme~:
\[F\left(x, \left(\partial^\alpha u(x)\right)_{\abs \alpha \leq m}\right) = 0.\]

Dans le cadre des EDOs, la solution du problème de Cauchy suivant~:
\[\begin{cases}&\od ut(t) + Au(t) = f(t) \\&u(0) = u^0,\end{cases}\]
pour $A \in \C^{n \times n}$, $f : \R \to \C^n$ continue, et $u^0 \in \C^n$ est donnée par~:
\[u(t) = u^0e^{-tA} + \int_0^te^{(s-t)A}f(s)\dif s,\]
où l'exponentielle d'une matrice est définie par la série~:
\[e^A \coloneqq \sum_{k \geq 0}\frac {A^k}{k!}.\]

\subsection{Normes et matrices}
Remarquons que les matrices définissent des opérateurs linéaires. Elles sont donc canoniquement munies de la norme associée~:
\[\norm A \coloneqq \sup_{\abs x = 1}\abs {Ax}.\]

\begin{prp} Les normes $\norm \cdot$ et $\norm \cdot_\infty : A \mapsto \norm A_\infty \coloneqq \sup_{1 \leq i,j \leq m}\abs {A_{ij}}$ sont équivalentes.
\end{prp}

\begin{proof} Soit $A \in \C^{m \times m}$. Pour tous $1 \leq i, j \leq m$, on a~:
\[\abs {A_{ij}} \leq \abs {Ae_j} \leq \norm A\]
car $\abs {e_j} = 1$. De plus~:
\[\norm A = \sup_{\abs x = 1}\abs {Ax} = \sup_{\abs x = 1}\left(\sum_{i=1}^m\abs {\sum_{j=1}^mA_{ij}x_j}^2\right)^{1/2}
\leq \sup_{\abs x = 1}\norm A_\infty\left(\sum_{i=1}^m\sum_{j=1}^m\abs {x_j}^2\right)^{1/2}.\]
Or $x \mapsto \sum_{i=1}^m\sum_{j=1}^m\abs {x_j}^2$ est continue sur $\{x \in \C^m \st \abs x = 1\}$ (qui est fermé), et donc $\exists M > 0$ tel que~:
\[\norm A \leq M\norm A_\infty.\]
\end{proof}

\begin{prp} Pour $A \in \C^{m \times m}$~: $\norm {\exp(A)} \leq \exp(\norm A)$.
\end{prp}

\begin{proof} Par propriété de la norme opérateur~:
\[\norm {e^A} = \norm {\sum_{k \geq 0}\frac {A^k}{k!}} \leq \sum_{k \geq 0}\frac {\norm {A^k}}{k!} \leq \sum_{k \geq 0}\frac {\norm A^k}{k!} = e^{\norm A}.\]

Plus précisément, puisque pour tout $k \in \N_0$, on a $\norm {A^k} \leq \norm A^k$ et par continuité de la norme opérateur~:
\[\norm {e^A} = \lim_{K \to \pinfty}\norm {\sum_{k=0}^K\frac {A^k}{k!}} \leq \lim_{K \to \pinfty}\sum_{k=0}^K\frac {\norm A^k}{k!} = e^{\norm A}.\]
\end{proof}

\section{Fonctions holomorphes}
\begin{déf} Pour $\Omega \subset \C$ ouvert, $A : \Omega \to \C^{m \times m} : z \mapsto A(z)$ est holomorphe dans $\Omega$ si $A_{ij}$ est holomorphe dans $\Omega$
pour tous $1 \leq i, j \leq m$. On note cela $A \in \mathscr H(\Omega)$.
\end{déf}

\begin{déf} Pour $A \in \mathscr H(\Omega)$ et $\gamma$ un chemin $C^1$ par morceaux dont l'image est dans $\Omega$, on définit $\int_\gamma A(z)\dif z$
comme étant la matrice telle que~:
\[\forall\, 1 \leq i, j \leq m : \left(\int_\gamma A(z)\dif z\right)_{ij} = \int_\gamma A_{ij}(z)\dif z.\]
\end{déf}

\begin{prp} Pour $S \in \C^{m \times m}$~:
\begin{enumerate}
	\item $\spectre S \subset \{z \in \C \st \abs z \leq \norm S\}$~;
	\item $\C \setminus \spectre S \to \C^{m \times m} : z \mapsto (z-S)^{-1} \coloneqq (zI - S)^{-1}$ est holomorphe dans $\C \setminus \spectre S$.
\end{enumerate}
\end{prp}

\begin{proof}~
\begin{enumerate}
	\item Soit $\lambda \in \spectre S$. Il existe $v \neq 0 \st Sv = \lambda v$. Dès lors $\scpr {Sv}v = \lambda\abs v^2$. De plus, par Cauchy-Schwarz~:
	$\abs {\scpr {Sv}v} \leq \abs {Sv}\abs v \leq \norm S\abs v\abs v = \norm S\abs v^2$. On en déduit $\lambda \leq \norm S$.
	\item ${(z-S)^{-1}}_{ij}$ est une fonction rationnelle complexe dont le dénominateur $z \mapsto \det(z-S)$ ne s'annule jamais sur $\C \setminus \spectre S$.
	${(z-S)^{-1}}_{ij}$ est donc holomorphe dans $\C \setminus \spectre S$, et donc $(z-S)^{-1}$ également.
\end{enumerate}
\end{proof}

\begin{proof}[\textnormal {\textbf {Démonstration alternative du point 1.}}]
Soit $z \in \C$ tel que $\abs z \geq \norm S$. On pose $T \coloneqq \sum_{k \geq 0}z^{-k-1}S^k$. Cette série converge (en norme opérateur) car les sommes
partielles forment une suite de Cauchy~:
\[
	\norm {\sum_{k=K_1}^{K_2}z^{-k-1}S^k} \leq \sum_{k=K_1}^{K_2}\abs z^{-k-1}\norm {S^k} \leq \sum_{k=K_1}^{K_2}\abs z^{-k-1}\norm S^k
	= \abs z^{-1}\sum_{k=K_1}^{K_2}\Bigg(\underbrace {\frac {\norm S}{\abs z}}_{\in [0, 1)}\Bigg)^k \xrightarrow[K_1,K_2 \to \pinfty]{} 0.
\]

De plus~:
\[(z-S)\sum_{k=0}^Ks^{-k-1}S^k = \sum_{k=0}^Kz^{-k}S^k - \sum_{k=1}^{K+1}z^{-k-1}S^{k+1} = I - z^{-K-1}S^{K+1}.\]

Or puisque~:
\[\norm {z^{-(K+1)}S^{K+1}} = \left(\frac {\norm S}{\abs z}\right)^{K+1} \xrightarrow[K \to \pinfty]{} 0,\]
on trouve $(z-S)\sum_{k=0}^Kz^{-k-1}S^k \xrightarrow[K \to \pinfty]{} (z-S)T$ et $I-z^{-K-1}S^{K+1} \xrightarrow[K \to \pinfty]{} I$. Par unicité de la limite
dans les espaces métriques, on a $T = (z-S)^{-1}$, i.e. $(z-S)$ est inversible. Donc $z$ ne peut être une valeur propre de $S$.
\end{proof}

\begin{cor}\label{cor:intégrales chemins homotopes} Soient $\Omega$ un disque ouvert, $S \in \C^{m \times m} \st \spectre S \subset \Omega$ et $f \in \mathscr H(\Omega)$.
Alors $z \mapsto f(z)(z-S)^{-1} \in \mathscr H(\Omega \setminus \spectre S)$ et pour $\gamma_1, \gamma_2$ chemins $C^1$ par morceaux à valeurs dans $\Omega \setminus \spectre S$
homotopes dans $\Omega \setminus \spectre S$~:
\[\int_{\gamma_1}f(z)(z-S)^{-1}\dif z = \int_{\gamma_2}f(z)(z-S)^{-1}\dif z.\]
\end{cor}

\begin{prp}[Forme matricielle de la forme intégrale de Cauchy] Soient $\Omega \subset \C$ un disque ouvert, $S \in \C^{n \times n}$ et $f(z) = \sum_{n \geq 0}\alpha_nz^n$,
une série de puissance qui converge dans $\Omega \supset \overline {B(0, \norm S)}$. Considérons le chemin $\gamma_1 : [0, 2\pi] \to \Omega : t \mapsto (\norm S + \varepsilon)e^{it}$.
Soit $\gamma$, chemin $C^1$ par morceaux, homotope à $\gamma_1$ dans $\Omega \setminus \spectre S$. Alors~:
\[\sum_{k =0}^K\alpha_kS^k \xrightarrow[K \to \pinfty]{} \frac 1{2\pi i}\int_\gamma f(z)(z-S)^{-1}\dif z.\]
\end{prp}

\begin{proof} $f(z) = \sum_{k \geq 0}\alpha_kz^k$ converge uniformément sur $\Imapp \gamma$, et donc~:
\[\sum_{k=0}^K\alpha_kz^k(z-S)^{-1} \xrightarrow[K \to \pinfty]{\text{CVU sur } \Imapp \gamma} f(z)(z-S)^{-1}.\]

En effet~:
\[\norm {f(z)(z-S)^{-1} - \sum_{k=0}^K\alpha_kz^k(z-S)^{-1}} \leq \abs {\sum_{k \geq K+1}\alpha_kz^k}\norm {(z-S)^{-1}}.\]

Dès lors~:
\[\sum_{k=0}^K\alpha_k\int_\gamma z^k(z-S)^{-1}\dif z \xrightarrow[K \to \pinfty]{} \int_\gamma f(z)(z-S)^{-1}\dif z.\]

Par le Corollaire~\ref{cor:intégrales chemins homotopes}~:
\[\int_\gamma z^k(z-S)^{-1}\dif z = \int_{\gamma_1}z^k(z-S)^{-1}\dif z.\]

Puisque $(z-S)^{-1} = \sum_{\ell \geq 0}z^{-\ell-1}S^\ell$ converge uniformément sur $\{z \in \C \st \abs z > \norm S + \varepsilon/2\}$, on a~:
\[\int_{\gamma_1}z^k(z-S)^{-1}\dif z = \int_{\gamma_1}z^k\sum_{\ell \geq 0}z^{-\ell-1}S^\ell\dif z = \lim_{L \to \pinfty}\sum_{\ell=0}^L\int_{\gamma_1}z^{k-\ell-1}\dif zS^\ell.\]

Or $\int_{\gamma_1}z^{k-\ell-1}\dif z = 2\pi i \delta_\ell^k$. Donc~:
\[\int_{\gamma_1}z^k(z-S)^{-1}\dif z = 2\pi iS^k.\]

On conclut alors par~:
\[\sum_{k=0}^K\alpha_kS^k = \frac 1{2\pi i}\sum_{k=0}^K\alpha_k\int_\gamma z^k(z-S)^{-1}\dif z \xrightarrow[K \to \pinfty]{} \frac 1{2\pi i}\int_\gamma f(z)(z-S)^{-1}\dif z.\]
\end{proof}

\begin{cor}\label{cor:Cauchy e^(mu S)} Pour $\mu \in \C$, $f(z) = e^{\mu z}$, $S \in \C^{m \times m}$, $\gamma$ un chemin $C^1$ par morceaux, homotope au cercle $\gamma_1$ dans $\C \setminus \spectre S$~:
\[e^{\mu S} = \frac 1{2\pi i}\int_\gamma e^{\mu z}(z-S)^{-1}\dif z.\]
\end{cor}

\begin{lem} Soient $A(z)$ holomorphe et $\gamma$ un chemin $C^1$ par morceaux. Alors~:
\[\norm {\int_\gamma A(z)\dif z} \leq \int_\gamma\norm {A(z)}\abs {\dif z} \coloneqq \int_\gamma \norm {A(\gamma(t))}\abs {\gamma'(t)}\dif t.\]
\end{lem}

\begin{proof} Par intégration de Riemann~:
\begin{align*}
	\int_\gamma A(z)\dif z &= \int_0^1 A(\gamma(t))\gamma'(t)\dif t
		= \lim_{k \to \pinfty}\underbrace {\sum_{j=0}^{n_k}A(\gamma(\tau_j^k))\gamma'(\tau_j^k)(t_j^k - t_{j-1}^k)}_{\eqqcolon a_k} \\
	\int_\gamma\norm {A(z)}\abs {\dif z} &= \lim_{k \to \pinfty}\underbrace {\sum_{j=0}^{n_k}\norm {A(\gamma(\tau_j^k))}\abs {\gamma'(\tau_j^k)}(t^k_j-t^k_{j-1})}_{\eqqcolon b_k}.
\end{align*}

$a_k \in \C^{m \times m}$ et $\forall k \geq 0 : \norm {a_k} \leq b_k$. Or $\norm {a_k} \xrightarrow[k \to \pinfty]{} \norm {\int_\gamma A(z)\dif z}$ et
$b_k \xrightarrow[k \to \pinfty]{} \int_\gamma \norm {A(z)}\abs {\dif z}$. Donc par passage à la limite~:
\[\norm {\int_\gamma A(z)\dif z} \leq \int_\gamma\norm {A(z)}\abs {\dif z}.\]
\end{proof}

\begin{lem}\label{lem:majoration inverse (z-S)} Soit $S \in \C^{m \times m}$. Alors $\forall \delta > 0 : \exists C_m > 0 \st \forall z \in \C$, si $\dist(z, \spectre S) \geq \delta$,
alors~:
\[\norm {(z-S)^{-1}} \leq C_m(1 + \abs z + \norm S)^{m-1}.\]
\end{lem}

\begin{proof} Fixsons $\delta > 0$ et soit $z \in \C \st \dist(z, \spectre S)$. Posons $T = z-S$ et $P(\lambda) = \det(\lambda - T) = \lambda^m + \sum_{j=1}^ma_j\lambda^{m-j}$.
Par Hamilton-Cayley~:
\[T^m + \sum_{j=1}^ma_jT^{m-j} = 0,\]
ou encore~:
\[T\left(T^{m-1} + a_1T^{m-2} + \ldots + a_{m-1}\right) + a_m = 0.\]

Or $T$ est inversible par hypothèse, donc~:
\begin{align*}
	T^{-1} &= \frac {-1}{a_m}\left(T^{m-1} + \ldots + a_{m-1}\right) \\
	\norm {T^{-1}} &\leq \abs {a_m}^{-1}\norm T^{m-1} + \abs {\frac {a_1}{a_m}}\norm T^{m-2} + \ldots + \abs {\frac {a_{m-1}}{a_m}}.
\end{align*}

De plus $\norm T = \norm {z-S} \leq \abs z + \norm S$.

Montrons alors qu'il existe $\rho > 0$ indépendant de $z$ et $S$ qui borne uniformément les coefficicents
$\frac 1{\abs {a_m}}, \abs {\frac {a_1}{a_m}}, \ldots, \abs {\frac {a_{m-1}}{a_m}}$.

En effet, par hypothèse, toutes les racines de $P$ satisfont $\abs \lambda \geq \delta$ puisque $\det(\lambda - T) = \det((\lambda-z) + S) = 0$, i.e. $\lambda-z \in \spectre S$,
et donc~:
\[\abs \lambda = \abs {z-(\lambda-z)} \geq \min_{s \in \spectre S}\abs {z-s} \geq \delta\]
par hypothèse.

Posons alors $Q(\tau) \coloneqq \tau^m + \frac {a_{m-1}}{a_m}\tau^{m-1} + \ldots + \frac 1{a_m}$, et observons que pour $\tau \neq 0$~:
\[P(\tau^{-1}) = \tau^{-m}+\sum_{j=1}^{m-1}a_j\tau^{j-m} + a_m\tau^{-m} = a_m\tau^{-m}\left(\frac 1{a_m} + \sum_{j=1}^{m-1}\frac {a_j}{a_m}\tau^j + 1\right) = a_m\tau^{-m}Q(\tau).\]
Donc $P(\tau^{-1}) = 0 \iff Q(\tau)$, et donc les racines de $Q$ satisfont $\abs \tau \leq \delta^{-1}$. On en déduit~:
\[Q(\tau) = C\prod_{j=1}^m(\tau-\tau_j),\]
où les $\tau_j \in \overline {B(0, 1/\delta)}$ sont les racines de $Q$ et sont des paramètres du polynôme. Par continuité sur le compact $\overline {B(0, 1/\delta)}$,
les coefficients de $Q$ sont uniformément bornés. Finalement, on conclut par~:
\[\norm {(z-S)^{-1}} = \norm {T^{-1}} \leq \sum_{j=0}^{m-1}\rho\norm T^j \leq \rho\sum_{j=0}^{m-1}(\abs z + \norm S)^j \leq C_m(1+\abs z + \norm S)^{m-1}.\]
\end{proof}

\section{Problème de Cauchy pour les EDPs}
On va étudier des problèmes sous la forme~\eqref{eq:PC EDP} avec $u = [u_1, \ldots, u_m]^\top$ où $u_j : \R \times \R^n \to \C$, $f = [f_1, \ldots, f_n]^\top$ où
$f_j : \R \times \R^n \to \C$, $A_j \in \C^{m \times m}$, et $u^0 : \R^n \to \C$.

\begin{equation}\label{eq:PC EDP}\tag{P.C.}
	\begin{cases}&\dpd ut + \displaystyle \sum_{j=1}^nA_j\dpd u{x_j} = f\\&\restr u{t=0} = u^0\end{cases}
\end{equation}

\begin{prp} Le problème de Cauchy suivant (équivalent à~\eqref{eq:PC EDP} pour $n=m=1$)~:
\[\begin{cases}&\dpd ut + a\dpd ux = f\\&\restr u{t=0} = u^0,\end{cases}\]
pour $a \in \C$, $u^0 \in C^1(\R)$ et $f \in C^1(\R^2)$ possède une unique solution $C^1(\R^2)$ si $a \in \R$ (et ne possède en général pas de solution $C^1(\R^2)$
si $a \in \C \setminus \R$).
\end{prp}

\begin{proof} Supposons que $a \in \R$. On procède au changement de variable $(t, x) \mapsto (t, y)$ (et $u(t, x) = U(t, y)$). On a alors~:
\[\dpd ut + a \dpd ux = \dpd Ut + \left(\dpd yt + a\dpd yx\right)\dpd Uy.\]
En particulier, pour $y = x - at$ (afin d'annuler la parenthèse), on a~:
\[\begin{cases}&\dpd Ut = f(t, y+at)\\&U(0, y) = u^0(y)\end{cases}\]
qui revient à résoudre une EDO pour tout $y \in \R$. On a donc la solution~:
\[U(t, y) = u^0(y) + \int_0^tf(s, y+as)\dif s,\]
ou encore, pour $u$ et non $U$~:
\[u(t, x) = u^0(x-at) + \int_0^tf(s, x-a(t-s))\dif s.\]

Dans le cas où $a \in \C \setminus \R$, justifions pourquoi trouver une solution est en général pas faisable.

On peut réécrire $a = \alpha + i\beta$ avec $\beta \neq 0$. En faisant le même changement de variable que précédemment ($y = x-\alpha t$), on trouve~:
\[\begin{cases}&\dpd Ut + i\beta\dpd Uy = f(t, y+\alpha t)\\&\restr U{t=0} = u^0.\end{cases}\]
on peut supposer WLOG que $a = i$ car en posant $y' \coloneqq \frac y\beta$ (bien défini car $\beta \neq 0$), on trouve~:
\[\dpd {U'}t + i\dpd {U'}{y'} = f(t, \beta y' + \alpha t).\]

Dans le cas le plus simple, supposons $f \equiv 0$ et regardons le P.C.~:
\[\begin{cases}&\dpd ut + i\dpd ux = 0\\&\restr u{t=0} = u^0\end{cases}\]

La première équation est celle de Cauchy-Riemann en $t + ix$, et donc $u$ doit être une fonction holomorphe de $t + ix$. Si une solution $u$ existe à ce problème, alors~:
\[u(t, x) = \sum_{k \geq 0}c_k(t+ix)^k,\]
qui converge uniformément en les variables d'origine. En particulier, pour $t=0$~:
\[u^0(x) = u(0, x) = \sum_{k \geq 0}c_k(ix)^k.\]

Dès lors $u^0$ est analytique, et donc $u^0 \in C^\infty(\R)$. On a donc une condition extrêmement restrictive sur le choix de $u^0$~: il est nécessaire (mais pas suffisant~!)
que $u^0 \in C^\infty(\R)$ pour que le système admette une solution.
\end{proof}

Définissons alors des espaces dans lesquelles on espère pouvoir résoudre~\eqref{eq:PC EDP}

\begin{déf} Pour $p \in \N$, on définit~:
\[C^p_b(\R^n) \coloneqq \left\{v \in C^p(\R^n) \st \forall \alpha \in \N^n : \abs \alpha \leq p \Rightarrow \partial^\alpha v \in L^\infty(\R^n)\right\}.\]
De manière similaire, pour $T > 0$, on introduit~:
\[C^p_b([-T, T] \times \R^n) \coloneqq \left\{v \in C^p([-T, T] \times \R^n) \st \forall \alpha \in \N^n : \abs \alpha \leq p \Rightarrow \partial^\alpha v \in L^\infty([-T, T] \times \R^n)\right\}.\]
\end{déf}

\begin{prp} L'application suivante est une norme sur $C^p_b(\R^n)$~:
\[\norm \cdot_{C^p_b(\R^n)} : C^p_b(\R^n) \to \R^+ : v \mapsto \norm v_{C^p_b(\R^n)} \coloneqq \sum_{\abs \alpha \leq p}\sup_{x \in \R^n}\abs {\partial^\alpha v(x)}
= \sum_{\abs\alpha \leq p}\norm {\partial^\alpha v}_{L^\infty}.\]
\end{prp}

\begin{prp}\label{prp:C^p_b Banach} $C^p_b(\R^n)$ est un espace de Banach.
\end{prp}

\begin{proof}
%\textbf{Cas $p=0$.} Soit $(v_k)_k \subset C^p_b(\R^n)$ de Cauchy, i.e. $\sup\abs {v_k-v_\ell} \xrightarrow[k,\ell \to \pinfty]{} 0$.
TODO
\end{proof}

\begin{prp} Soient $B \in \C^{m \times m}$, $\mu \in \C$ et $w \in \C^m$. Si $Bw = \mu w$, alors $e^Bw = e^\mu w$.
\end{prp}

\begin{proof} Par définition de l'exponentielle matricielle~:
\[e^Bw = \left(\sum_{k \geq 0}\frac {B^k}{k!}\right)w = \sum_{k \geq 0}\frac {B^kw}{k!} = \sum_{k \geq 0}\frac {\mu^kw}{k!} = e^\mu w.\]
\end{proof}

\section{Théorème de Petrowsky}
\begin{thm}[Petrowsky, 1937] Soient $A_1, \ldots, A_n \in \C^{m \times m}$. Si il existe $T > 0$, $p \in \N$ tels que~:
\[\forall F \in C^p_b([-T, T] \times \R^n), u^0 \in C^p_b(\R^n) : \exists! u \in C^1_b(\R^n) \st:\]
\[\begin{cases}\dpd ut + \sum_{j=1}^nA_j\dpd u{x_j} = F & \text{ si } \abs t \leq T, x \in \R^n\\u=u^0 &\text{ si }t=0, x \in \R^n.\end{cases}\]

Alors $\forall \xi \in \R^n : \spectre {\sum_{j=1}^m\xi_j A_j} \subset \R$.
\end{thm}

\begin{rmq} Pour simplifier les notations, on pose $L \coloneqq \pd {}t + \sum_{j=1}^mA_j\pd {}{x_j}$.
\end{rmq}

\begin{proof} Le problème de Cauchy induit une application (l'application \textit{solution})~:
\[\Phi : C^p_b([-T, T] \times \R^n) \times C^p_b(\R^n) \to C^1_b(\R^n) : (F, u^0) \mapsto \Phi(F, u^0) \coloneqq u.\]

Montrons que $\Phi$ est linéaire. Soient $(F, u^0), (\tilde F, \tilde u^0) \in C^p_b([-T, T] \times \R^n) \times C^p_b(\R^n)$.

On prend $C^1_b(\R^n) \ni z \coloneqq \Phi\left((F, u^0) + (\tilde F, \tilde u^0)\right) = \Phi(F+\tilde F, u^0 + \tilde u^0)$ qui satisfait $Lz = F+\tilde F$
(pour $\abs t \leq T, x \in \R^n$) et $\restr z{t=0} = u^0+\tilde u^0$ (pour $x \in \R$). Prenons également $z_1 = \Phi(F, u^0)$ et $z_2 = \Phi(\tilde F, \tilde u^0)$.
Or par linéarité de l'opérateur $L$~:
\[\begin{cases}&L(z_1+z_2) = Lz_1 + Lz_2 = F+\tilde F\\&\restr {(z_1+z_2)}{t=0} = u^0 + \tilde u^0.\end{cases}\]

Dès lors $z$ et $z_1+z_2$ satisfont le même problème de Cauchy et par hypothèse, cette solution est unique, i.e. $z = z_1 + z_2$.

Notons maintenant que $\Phi$ est une application linéaire entre des espaces de Banach (par la Proposition~\ref{prp:C^p_b Banach}). Notons $E = \dom \Phi$ et $G = \Imapp \Phi$

Montrons que $\Gamma_\Phi$, le graphe de $\Phi$, est fermé dans $E \times G$. Soit $(\zeta, \psi) \in \overline {\Gamma_\Phi}$. Il existe des suites $(\zeta_k)_k \subset E$
et $(\psi_k)_k \subset G$ telles que $\zeta_k \xrightarrow[k \to \pinfty]{E} \zeta$ et $\psi_k \xrightarrow[k \to \pinfty]{G} \psi$. En notant $\zeta_k = (F_k, u^0_k)$
et $\zeta = (F, u^0)$, on a $F_k \xrightarrow[k \to \pinfty]{C^p_b([-T, T] \times \R^n)} F$ et $u^0_k \xrightarrow[k \to \pinfty]{C^p_b(\R^n)} u^0$.

Par définition de l'application $\Phi$~: $L\Phi(\zeta_k) = F_k$ et $\restr{\Phi(\zeta_k)}{t=0} = u^0_k$. Or, puisque $\Phi(\zeta_k) \xrightarrow[k \to \pinfty]{G} \psi$~:
\[L\Phi(\zeta_k) \xrightarrow[k \to \pinfty]{C^0_b(\R^n)} L\psi \qquad \text{ et } \qquad \restr {\Phi(\zeta_k)}{t=0} \xrightarrow[k \to \pinfty]{C^1_b(\R^n)} \restr \psi{t=0},\]
et donc, en sachant $\Phi(\zeta_k) \xrightarrow[k \to \pinfty]{C^1_b(\R^n)} \psi$~:
\[\dpd {}t\Phi(\zeta_k) + \sum_{j=1}^nA_j\dpd {}{x_j}\Phi(\zeta_k) \xrightarrow[k \to \pinfty]{C^0_b(\R^n)} \dpd {}t\psi + \sum_{j=1}^nA_j\dpd {}{x_j}\psi.\]

En effet, puisque $\dpd {}t$ et $\dpd {}{x_j}$ sont des opérateurs bornés de $C^1_b([-T, T] \times \R^n)$ dans $C^0_b([-T, T] \times \R^n)$ et que $A_j$
est un opérateur borné de $C^0_b([-T, T] \times \R^n)$ dans lui-même, la convergence se fait terme à terme car borné $\equiv$ continu.

Montrons donc que $\dpd {}t$ est un opérateur borné ($\dpd {}{x_j}$ se fait de manière similaire). Pour $v \in C^1_b([-T, T] \times \R^n)$~:
\[\norm {\dpd vt}_{C^0_b([-T, T] \times \R^n)} = \sup_{t,x}\abs {\dpd vt(t, x)}
\leq \sup_{t,x}\abs {\dpd vt(t, x)} + \sup_{t,x}\abs {v(t, x)} + \sum_{j=1}^n\sup_{t,x}\abs {\dpd v{x_j}(t,x)} = \norm v_{C^1_b([-T, T] \times \R^n)}.\]

Dès lors, puisque $F_k = L\Phi(\zeta_k)$~:
\[F_k \xrightarrow[k \to \pinfty]{C^p_b(\R^n)} F \qquad \text{ et } \qquad L\Phi(\zeta_k) \xrightarrow[k \to \pinfty]{C^0_b(\R^n)} L\psi.\]

Or la convergence dans $C^p_b(\R^n)$ implique la convergence $C^0_b(\R^n)$ car $C^p_b(\R^n) \subseteq C^0_b(\R^n)$ et~:
\[\forall v \in C^p_b(\R^n) : \norm v_{C^0_b(\R^n)} \leq \norm v_{C^p_b(\R^n)}.\]

Donc par unicité de la limite dans les espaces métriques (ici, dans $C^0_b(\R^n)$)~: $F = L\psi$. De la même manière, on a~:
\[\restr{\Phi(\zeta_k)}{t=0} = \restr {\psi_k}{t=0} \xrightarrow[k \to \pinfty]{C^1_b(\R^n)} \psi
\qquad \text{ et } \qquad \restr {\psi_k}{t=0} = u^0_k \xrightarrow[k \to \pinfty]{C^p_b(\R^n)} u^0.\]

À nouveau, par unicité de la limite (cette fois dans $C^1_b(\R^n)$, puisque $p \geq 1$)~: $\restr {\psi}{t=0} = u^0$. Dès lors $\Phi(\zeta) = \Phi(F, u^0) = \psi$, et donc
$(\zeta, \psi) \in \Gamma_\Phi$. Dès lors par le théorème du graphe fermé de Banach (Théorème~\ref{thm:graphe fermé}), on sait que $\Phi$ est bornée, i.e. $\exists C > 0$
($C = \norm \Phi$) tel que pour tout $(F, u^0) \in E$, si $u$ dénote $\Phi(F, u^0)$~:
\begin{align}\label{eq:Phi bornée}
	\norm u_G &= \norm {\Phi(F, u^0)}_{C^1_b(\R^n)} = \sum_{\abs \alpha \leq 1}\;\sup_{\abs t \leq T,x \in \R^n}\abs {\partial^\alpha u(t, x)} \nonumber \\
		&\leq C\left[\sum_{\abs {\beta} \leq p}\left(\sup_{\abs t \leq T, x \in \R^n}\abs {\partial^\beta F(t, x)} + \sup_{x \in \R^n}\abs {\partial^\beta u^0(x)}\right)\right] \\
		&= C\norm {(F, u^0)}_{C^p_b([-T, T] \times \R^n) \times C^p_b(\R^n)} = C\norm {(F, u^0)}_E. \nonumber
\end{align}

Finalement, raisonnons par l'absurde. Posons $F \equiv 0$ et $u^0 : \R^n \to \C : x \mapsto e^{i\scpr x\xi}v^0$ pour $v^0 \in \C^m$ et $\xi \in \R^n$. Fabriquons alors $u$, une
solution du problème de Cauchy~:
\begin{equation}\label{eq:PC 1 Petrowsky}\tag{PC~1}
	\begin{cases}
		Lu = 0 &\text{ pour } \abs t \leq T, x \in \R^n \\
		\restr u{t=0} = u^0 &\text{ pour } x \in \R^n
	\end{cases}
\end{equation}

Par séparation des variables~:
\[u(t, x) = e^{i\scpr x\xi} v(t, \xi),\]
et pour $A(\xi) \coloneqq \sum_{j=1}^m\xi_jA_j$~:
\[Lu = e^{i\scpr x\xi}\left(\dpd vt + iA(\xi)v\right) = 0.\]

Donc on a l'EDO suivante~:
\begin{equation}\label{eq:PC 2 Petrowsky}\tag{PC~2}
	\begin{cases}
		&v' + iA(\xi)v = 0 \\
		&\restr v{t=0} = v^0
	\end{cases}
\end{equation}

\eqref{eq:PC 2 Petrowsky} admet pour solution~:
\[v(t, \xi) = e^{-itA(\xi)}v^0,\]
ce qui permet de déterminer la solution de~\eqref{eq:PC 1 Petrowsky}~:
\[u(t, x) = e^{i\scpr x\xi}e^{-itA(\xi)}v^0.\]

Or par~\eqref{eq:Phi bornée}, on a~:
\begin{equation}\label{eq:fin Petrowsky}
	\sup_{\abs t \leq T}\abs {u(t, 0)} \leq C\sum_{\abs \beta \leq p}\sup_{x \in \R^n}\abs {\partial^\beta u^0(x)}.
\end{equation}

De plus, puisque $\abs {\partial^\beta u^0(x)} = \abs {\partial^\beta e^{i\scpr x\xi}v^0} \leq C\abs \xi^{\abs \beta}\abs {v^0}$~:
\[\sup_{\abs t \leq T}\abs {u(t, 0)} \leq C(1+\abs \xi^p)\abs {v^0}.\]

Or $\abs {u(t, 0)} = \abs {e^{-itA(\xi)}v^0}$.

Supposons maintenant qu'il existe un $\xi^0 \in \R^n$ tel que $A(\xi^0)$ admette une valeur propre $\lambda = \lambda(\xi^0) \in \C \setminus \R$,
et notons le vecteur propre associé $v^0 = v(\xi^0)$. Pour $\sigma > 0$~:
\[A(\sigma\xi^0)v^0 = \sigma\lambda(\xi^0)v^0,\]
et donc~:
\[-itA(\sigma\xi^0)v^0 = -it\sigma\lambda(\xi^0)v^0.\]

Dès lors~:
\[e^{-itA(\sigma\xi^0)}v^0 = e^{-it\sigma\lambda(\xi^0)}v^0,\]
et en particulier~:
\[\abs {e^{-itA(\sigma\xi^0)}v^0} = \abs {e^{-it\sigma\lambda(\xi^0)}}\abs {v^0} = e^{t\sigma\Im\lambda(\xi^0)}\abs {v^0}.\]

\eqref{eq:fin Petrowsky} nous dit finalement que~:
\[\forall \sigma > 0 : \forall t \in [-T, T] : \abs {v^0}e^{t\sigma\Im\lambda(\xi^0)} \leq C\left(1 + \abs {\sigma\xi^0}\right)^p\abs {v^0}.\]
Or, pour $t^0$ tel que $t^0\Im\lambda(\xi^0) > 0$ (et un tel $t^0$ existe, par exemple $\frac T2\sgn(\Im\lambda(\xi^0))$ est bien dans $[-T, T]$), on a une contradiction car
$\sigma \mapsto e^{t^0\Im\lambda(\xi^0)\sigma}$ croît exponentiellement et ne peut pas être bornée par un polynôme en $\sigma$.
\end{proof}

\begin{déf} Si $L = \pd {}t + \sum_{j=1}^nA_j\pd {}{x_j}$ satisfait la condition de Petrowsky (i.e. $\forall \xi \in \R^n : \spectre {A(\xi)} \subset \R$), on dit que
$L$ est \textit{hyperbolique} dans la direction $\pd {}t$.
\end{déf}

\begin{rmq} Le terme \textit{hyperbolicité} vient du fait que les courbes de niveau de la forme quadratique associée à la transformée de Fourier de $L$ sont hyperbolique.
\end{rmq}

\begin{ex}
Pour $m=n=1$~: $L = \pd {}t + a\pd {}x$. Pour $\xi \in \R : A(\xi) = a\xi$. Donc la condition de Petrowsky revient à imposer que $a \in \R$.
\end{ex}

\begin{ex}
On prend $L = \partial^2_t + 2b\partial^2_{t,x} + c\partial^2_x$ et l'EDP associée $Lu = f$. En posant $v = \partial_t u$ et $w = \partial_x u$, l'EDP devient~:
\[\begin{cases}\dpd vt + 2b\dpd vx + c\dpd wx = f\\&\\\dpd vx - \dpd wt = 0\end{cases}\]
ou sous forme matricielle~:
\[\dpd {}t\begin{bmatrix}v \\ w\end{bmatrix} + \begin{bmatrix}2b & c \\-1 & 0\end{bmatrix}\dpd {}x\begin{bmatrix}v \\ w\end{bmatrix} = \begin{bmatrix}f \\ 0\end{bmatrix}.\]

La condition d'hyperbolicité dit alors que $\spectre {\begin{bmatrix}2b & c \\-1 & 0\end{bmatrix}} \subset \R$, i.e. $b,c \in \R$ et $b^2-c \geq 0$.

Le Laplacien est un cas particulier de cet exemple avec $b=0$ et $c=1$. Donc le Laplacien n'est pas hyperbolique car $b^2-c = -1 \not \geq 0$.
Par contre $\partial_t^2 - \partial_x^2$ est hyperbolique car $b^2 - c = 1 \geq 0$.
\end{ex}

\begin{ex}
Les équations de Maxwell~:
\[\begin{cases}\dpd Et = \rot H\\&\\\dpd Ht = \rot E\end{cases}\]
pour $E, H : \R^4 \to \C$ sont hyperboliques et peuvent s'écrire sous la forme~:
\[\dpd ut + \sum_{j=1}^3\dpd u{x_j},\]
pour $A_j = \begin{bmatrix}0 & \omega_j \\-\omega_j & 0\end{bmatrix}$ pour $\omega_1 = \begin{bmatrix}0 & 0 & 0\\0 & 0 & 1\\0 & -1 & 0\end{bmatrix}$,
$\omega_2 = \begin{bmatrix}0 & 0 & -1\\0 & 0 & 0\\1 & 0 & 0\end{bmatrix}$, $\omega_3 = \begin{bmatrix}0 & 1 & 0\\-1 & 0 & 0\\0 & 0 & 0\end{bmatrix}$, et
$u = \begin{bmatrix}E\\H\end{bmatrix}$. En effet, les $A_j$ sont symétriques réelles, donc pour tout $\xi \in \R^3$, les valeurs propres de $A(\xi) = \sum_{j=1}^3\xi_jA_j$
sont réelles car $A(\xi)$ est également symétrique réelle.
\end{ex}

\begin{ex}
Pour $\sigma_1 = \begin{bmatrix}0 & 1 \\1 & 0\end{bmatrix}$, $\sigma_2 = \begin{bmatrix}0 & i \\-i & 0\end{bmatrix}$, $\sigma_3 = \begin{bmatrix}1 & 0\\0 & -1\end{bmatrix}$,
on pose $A_j \coloneqq \begin{bmatrix}0 & -\sigma_j\\-\sigma_j & 0\end{bmatrix}$. L'opérateur~:
\[L = \dpd {}t + \sum_{j=1}^3A_j\dpd {}{x_j}\]
est l'\textit{opérateur de Dirac}.
$L$ est hyperbolique puisque pour $\xi \in \R^3 : \sum_{j=1}^3\xi_jA_j$ est hermitienne.
\end{ex}

\begin{déf} on pose l'ensemble~:
\[\widetilde {\mathcal S}([-T, T] \times \R^n) \coloneqq \left\{u \in C^\infty([-T, T] \times \R^n) \st \forall \alpha, \beta \in \N^n : \forall k \in \N :
	x^\alpha\partial_x^\beta\partial_t^ku \in L^\infty([-T, T] \times \R^n)\right\}.\]
\end{déf}

\begin{rmq} $\widetilde{\mathcal S}([-T, T] \times \R^n)$ est une variante de l'espace de Schwartz où la première variable n'est pas définie sur $\R^n$ mais sur un compact $[-T, T]$.
\end{rmq}

\begin{thm}[Réciproque de Petrowsky] Soit $L$ un opérateur hyperbolique dans la direction $\pd {}t$. Alors~:
\[\forall F \in \widetilde {\mathcal S}([-T, T] \times \R^n) : \forall u^0 \in \mathcal S(\R^n) : \exists! u \in \widetilde {\mathcal S}([-T, T] \times \R^n) \st\]
\[\begin{cases}
	Lu = F &\text{ pour } \abs t \leq T, x \in \R^n \\
	\restr u{t=0} = u^0 &\text{ pour } x \in \R^n
\end{cases}\]
\end{thm}

\begin{proof}
\exis Si une solution $u$ existe, alors à $t \in [-T, T]$ fixé, on peut appliquer Fourier en $x$~:
\[\mathcal F_\xi u(t, \xi) \coloneqq \widetilde u(t, \xi) = \int_{\mathbb R^n}e^{-i\scpr x\xi}u(t, x)\dif x,\]
et $\widetilde u$ doit satisfaire le problème de Cauchy suivant~:
\[\begin{cases}
	\dpd {}t\widetilde u + iA(\xi)\widetilde u = \widetilde F \\&\\
	\restr {\widetilde u}{t=0} = \widehat {u^0},
\end{cases}\]
qui admet pour solution~:
\[\widetilde u = e^{-itA(\xi)}\widehat {u^0} + \int_0^te^{-i(t-s)}\widetilde F(s, \xi)\dif s.\]

Posons~:
\begin{align*}
	P(t, \xi) &\coloneqq e^{-itA(\xi)}\widehat {u^0}(\xi) \\
	Q(t, \xi) &\coloneqq \int_0^te^{-i(t-s)}\widetilde F(s, \xi)\dif s.
\end{align*}

\begin{claim}\label{claim 1} $\forall k \geq 0 : \partial_t^kP, \partial_t^kQ \in \mathcal S_\xi(\R^n)$, où $f \in \mathcal S_\xi(\R^n)$ veut dire~:
\[\forall t \in [-T, T] : \xi \mapsto f(t, \xi) \in \mathcal S(\R^n).\]
\end{claim}

Par cette affirmation, $P+Q \in \mathcal S_\xi$. Posons alors $u \coloneqq \mathcal F_\xi^{-1}(P+Q)$. On trouve~:
\begin{align*}
	\dpd ut + \sum_{j=1}^nA_j\dpd u{x_j}
	&= \left(\dpd {}t + \sum_{j=1}^nA_j \dpd{}{x_j}\right)\mathcal F_\xi^{-1}P + \left(\dpd {}t + \sum_{j=1}^nA_j\dpd {}{x_j}\right)\mathcal F_\xi^{-1}Q \\
	&= \mathcal F_\xi^{-1}\left(\left(\partial_t + iA(\xi)\right)(P+Q)\right),
\end{align*}
où la dernière égalité découle directement de~:
\begin{align*}
\left(\partial_t + \sum_{j=1}^nA_j\partial_{x_j}\right)\mathcal F_\xi^{-1}P &= \left(\partial_t + \sum_{j=1}^nA_j\partial_{x_j}\right)\int_{\R^n}e^{i\scpr x\xi}P(t,\xi)\dif\xi\\
	&= \int_{\R^n}\left(\partial_t + \sum_{j=1}^nA_j\partial_{x_j}\right)\left(e^{i\scpr x\xi}P(t,\xi)\right)\dif\xi \\
	&= \int_{\R^n}\partial_t\left(e^{i\scpr x\xi}P(t,\xi)\right) + \left(\sum_{j=1}^nA_j\partial_{x_j}e^{i\scpr x\xi}\right)P(t,\xi)\dif\xi \\
	&= \int_{\R^n}e^{i\scpr x\xi}\partial_tP(t,\xi) + \sum_{j=1}^nA_ji\xi_je^{i\scpr x\xi}P(t,\xi)\dif \xi \\
	&= \int_{\R^n}e^{i\scpr x\xi}\left(\partial_t + i\sum_{j=1}^n\xi_jA_j\right)P(t,\xi)\dif\xi \\
	&= \mathcal F_\xi\left(\left(\partial_t + i\sum_{j=1}^n\xi_jA_j\right)P\right).
\end{align*}

Par symétrie, on a le même résultat sur $Q$. Donc $\partial_t u + \sum_{j=1}^nA_j\partial_{x_j}u = \mathcal F_\xi^{-1}\widetilde F = F$
(car à $t$ fixé, $\xi \mapsto F(t, \xi) \in \mathcal S$). Donc $u$ est bien solution de $Lu = F$, et par la formule d'inversion de Fourier (dans l'espace de Schwartz)~:
\[\restr {P+Q}{t=0} = \restr P{t=0} + \restr Q{t=0} = \widehat {u^0} + \int_0^0\ldots\dif s = \widehat {u^0},\]
et donc $\restr u{t=0} = u^0$.

$u$ est donc bien solution du problème de Cauchy. Il reste à montrer que $u \in \widetilde {\mathcal S}([-T, T] \times \R^n)$. Pour cela, il est suffisant de montrer que
$P, Q \in \widetilde {\mathcal S}([-T, T] \times \R^n)$ puisque la transformée de Fourier est une bijection sur $\mathcal S(\R^n)$.

\begin{claim}\label{claim 2} $(t, \xi) \mapsto e^{-itA(\xi)}$ est $C^\infty$.
\end{claim}

Donc $(t, \xi) \mapsto e^{-itA(\xi)}\widehat {u^0}(\xi)$ est $C^\infty$ également, i.e. $P \in C^\infty$.
Soient $\alpha, \beta \in \N^n, k \in \N$.

\begin{align*}
	\xi^\alpha\partial_t^kD_\xi^\beta P
	&= \xi^\alpha\partial_t^kD_\xi^\beta\left(e^{-itA(\xi)}\widehat {u^0}(\xi)\right) \\
	&= \xi^\alpha\partial_t^k\sum_{\beta'\leq\beta}\left[\binom \beta{\beta'}D_\xi^{\beta'}(e^{-itA(\xi)})D_\xi^{\beta-\beta'}\widehat {u^0}(\xi)\right] \\
	&= \xi^\alpha\sum_{\beta'\leq\beta}\binom\beta{\beta'}\underbrace {\partial_t^kD_\xi^{\beta'}e^{-itA(\xi)}}_{(*)}D_\xi^{\beta-\beta'}\widehat {u^0}(\xi) \\
	&= \xi^\alpha\sum_{\beta'\leq\beta}\binom\beta{\beta'}D_\xi^{\beta'}\left(\left((-iA(\xi)\right)^ke^{-itA(\xi)}\right)D_\xi^{\beta-\beta'}\widehat {u^0}(\xi) \\
	&= \xi^\alpha\sum_{\beta'\leq\beta}\binom \beta{\beta'}\left[\sum_{\gamma\leq\beta'}\binom{\beta'}\gamma D_\xi^\gamma\left(-iA(\xi)\right)^kD_\xi^{\beta'-\gamma}\left(e^{-itA(\xi)}\right)\right]D_\xi^{\beta-\beta'}\widehat{u^0}(\xi)
\end{align*}

{\tiny {$(*)$ les dérivées commutent puisque $(t, \xi) \mapsto e^{-itA(\xi)} \in C^\infty$.}}

\begin{claim}\label{claim 3} Pour $\mu \in \N^n$~: $\exists C > 0, e_\mu > 0 \st \abs {D_\xi^\mu e^{-itA(\xi)}} \leq C(1+\abs\xi)^{e_\mu}$.
\end{claim}

Avec cette affirmation, on peut finalement majorer~:
\begin{align*}
	\abs {\xi^\alpha\partial_t^kD_\xi^\beta P}
	&\leq \sum_{\beta',\gamma}C\abs \xi^\alpha\underbrace {\abs {D_\xi^\gamma (-iA(\xi))^k}}_{\text{polynôme} \leq C(1+\abs\xi)^{K_\gamma}}\underbrace {\abs {D_\xi^{\beta'-\gamma}e^{-itA(\xi)}}}_{\leq C(1+\abs\xi)^{e_\gamma}}\underbrace {\abs {D_\xi^{\beta-\beta'}\widehat {u^0}(\xi)}}_{\leq C_N(1+\abs\xi)^{-N}} \\
	&\leq C(1+\abs\xi)^{-K} \leq C
\end{align*}
pour $N$ suffisamment grand.

On a donc bien finalement $\xi^\alpha\partial_t^kD_\xi^\beta P$ borné. On a exactement la même chose pour $Q$. Dès lors, $u$ est une solution au problème de Cauchy
et $u \in \widetilde {\mathcal S}([-T, T] \times \R^n)$. L'existence est donc démontrée. De plus, cela montre l'Affirmation~\ref{claim 1}.

\unic Soient $u_{(1)}, u_{(2)}$, deux solutions du problème de Cauchy dans $\widetilde {\mathcal S}([-T, T] \times \R^n)$, alors on pose $w \coloneqq u_{(1)} - u_{(2)}$.
$w \in \widetilde {\mathcal S}([-T, T] \times \R^n))$ est solution du problème de Cauchy~:
\[\begin{cases}Lw = \\&\\\restr w{t=0} = 0\end{cases}\]

En en prenant la transformée de Fourier à $\abs t \leq T$ fixé~:
\[\begin{cases}\partial_t\widetilde w + iA(\xi)\widetilde w = 0\\&\\\restr {\widetilde w}{t=0} = 0\end{cases}\]
qui admet pour unique solution $\widetilde w = 0$, et donc $w = 0$ par formule d'inversion de Fourier, et donc $u_{(1)} = u_{(2)}$.
\end{proof}

\begin{prp} Il existe $C_m > 0$ tel que $\forall S \in \C^{m \times m} \st \spectre S \subset \R : \norm {e^{iS}} \leq C_m(1+\norm S)^m$.
\end{prp}

\begin{proof} Trouvons un chemin $\gamma$ tel que~:
\[\norm {e^{iS}} = \frac 1{2\pi i}\int_\gamma e^{iz}(z-S)^{-1}\dif z.\]

Prenons le chemin $\gamma = \left[(-\norm S-1 - i) \to (\norm S+1-i) \to (\norm S+1+i) \to (-\norm S-1+i) \to (-\norm S-1-i)\right]$. $\gamma$ est homotope à
$\gamma' : t \mapsto (\norm S + \varepsilon)e^{it}$. Par la forme intégrale de Cauchy pour $f(z) = e^{iz}$~:
\[e^{iS} = \frac 1{2\pi i}\int_\gamma e^{iz}(z-S)^{-1}\dif z,\]
et donc~:
\[\norm {e^{iS}} \leq \frac 1{2\pi}\int_\gamma\abs {e^{iz}}\norm {(z-S)^{-1}}\abs {\dif z}.\]

Par le Lemme~\ref{lem:majoration inverse (z-S)}, $\norm {(z-S)^{-1}} \leq C_m(1+\abs z + \norm S)^{m-1}$. De plus $\abs {e^{iz}} \leq \kappa$ sur le chemin $\gamma$.
De plus, pour $z$ dans le chemin $\gamma$~:
\[\abs z \leq \sqrt {1 + (1+\norm S)^2} \leq \sqrt 2(\norm S + 1).\]

Dès lors~:
\[\norm {e^{iS}} \leq \frac 1{2\pi}\int_\gamma\kappa C(1+\norm S)^{m-1}\abs {\dif z} = \frac {C(1+\norm S)^{m-1}\kappa}{2\pi}\int_\gamma\abs {\dif z}
	\leq \frac {C\kappa(\norm S + 1)^{m-1}}\pi(\norm S + 2) \leq C(1+\norm S)^m.\]
\end{proof}

\begin{proof}[\textnormal {\textbf {Démonstration} (de l'Affirmation~\ref{claim 2})}] Montrons que $(t, \xi) \mapsto e^{-itA(\xi)}$ est $C^\infty$ sur $[-T, T] \times \R^n$.

On fixe $\xi^0 \in \R^n$. Si $\spectre {A(\xi)} \subset (-\chi^0, \chi^0)$ où
$\chi^0 = \norm {A(\xi^0)}+1$, on peut prendre le chemin $\gamma_{\xi^0}$ défini par~:
\[\gamma_{\xi^0} = \left[(-\chi^0-i) \to (\chi^0-i) \to (\chi^0+i) \to (-\chi^0+i) \to (-\chi^0-i)\right].\]

Or puisque~:
\[\norm {A(\xi)} \leq \norm {A(\xi^0)} + \norm {A(\xi-\xi^0)} \leq \norm {A(\xi^0)} + \norm A\abs {\xi-\xi^0} \lneqq \norm {A(\xi^0)} + 1\]
pour $\abs {\xi-\xi^0}$ suffisament petit, on sait qu'à $\xi$ fixé, $\exists \xi^0 \st \spectre {A(\xi)} \subset (-\chi^0, \chi^0)$.

Donc, par le Corollaire~\ref{cor:Cauchy e^(mu S)}~:
\begin{equation}\label{eq:forme intégrale Cauchy e^(-itA(xi))}
	e^{-itA(\xi)} = \frac 1{2\pi i}\int_{\gamma_{\xi^0}}e^{-itz}(z-A(\xi))^{-1}\dif z.
\end{equation}

Il faut donc voir que $\xi \mapsto (z-A(\xi))^{-1}$ est bien $C^\infty$ pour $z \in \Imapp \gamma_{\xi^0}$. Or par choix de $\xi^0$, on sait que
$\forall z \in \Imapp \gamma : z \not \in \spectre {A(\xi)}$. Dès lors, pour $1 \leq j, k \leq m$~:
\[{(z-A(\xi))^{-1}}_{jk} = \frac {\text{polynôme en $\xi$ et $z$}}{\det(z-A(\xi))}\]
est une fonction rationnelle complexe dont le dénominateur ne s'annule pas. Dès lors elle est bien $C^\infty$.
\end{proof}

\begin{proof}[\textnormal {\textbf {Démonstration} (de l'Affirmation~\ref{claim 3})}] Il reste donc unoquement à vérifier que les dérivées de $\xi \mapsto z^{-itA(\xi)}$ sont à
croissance polynômiale.

Puisque~:
\[0 = \dpd {}{\xi_j}I = \dpd {}{\xi_j}\left((z-A(\xi))^{-1}(z-A(\xi))\right) = \left(\dpd {}{\xi_j}(z-A(\xi))^{-1}\right)(z-A(\xi)) + (z-A(\xi))^{-1}(-A_j),\]
on trouve~:
\begin{equation}\label{eq:partial_j (z-A(xi)) inverse}
	\dpd {}{\xi_j}(z-A(\xi))^{-1} = (z-A(\xi))^{-1}A_j(z-A(\xi_j))^{-1}.
\end{equation}

De plus, par l'équation~\eqref{eq:forme intégrale Cauchy e^(-itA(xi))} (et parce que l'intégration ne dépend pas de $\xi_j$)~:
\[\dpd {}{\xi_j}e^{-itA(\xi)} = \frac 1{2\pi i}\int_{\gamma_{\xi^0}}e^{-itz}(z-A(\xi))^{-1}A_j(z-A(\xi))^{-1}\dif z,\]
que l'on majore comme suit~:
\[\norm {\dpd {}{\xi_j}e^{-itA(\xi)}} \leq \frac 1{2\pi i}\int_{\gamma_{\xi^0}}\kappa_t\norm {(z-A(\xi))^{-1}}\norm {A_j}\norm {(z-A(\xi))^{-1}}\abs {\dif z}.\]

De plus, par le Lemme~\ref{lem:majoration inverse (z-S)}~:
\[\norm {(z-A(\xi))^{-1}} \leq C\left(1+\abs z + \norm {A(\xi)}\right)^{m-1},\]
et par choix du chemin d'intégration~:
\[\abs z \leq \left(\left(\norm {A(\xi^0)} + 1\right)^2 + 1\right)^{1/2} \leq C\left(\norm {A(\xi^0)}+1\right) \leq C\left(\abs {\xi^0}+1\right).\]

Dès lors $\norm {(z-A(\xi))^{-1}} \leq C(1+\abs {\xi^0}+\abs {\xi})$, et donc~:
\[\norm {\dpd {}{\xi_j}e^{-itA(\xi)}} \leq C\left(1+\abs {\xi^0}+\abs \xi\right)^{2(m-1)}\int_{\gamma_{\xi^0}}\abs {\dif z}
	\leq C\left(1 + \abs {\xi^0} + \abs \xi\right)^{2(m-1)} \cdot K \left(1+\abs {\xi^0}\right).\]

Et finalement, puisque~:
\[\abs {\xi^0} \leq \abs \xi + \abs {\xi-\xi^0} \leq \abs \xi + c \leq C(1+\abs \xi),\]
on peut conclure avec~:
\[\norm {\dpd {}{\xi_j}e^{-itA(\xi)}} \leq C(1+\xi)^{2(m-1)}(1+\xi) = C(1+\abs \xi)^{2m-1}.\]

De là, il ne reste plus qu'à effectuer une récurrence sur base du Lemme~\ref{lem:majoration inverse (z-S)} et de l'équation \eqref{eq:partial_j (z-A(xi)) inverse} pour montrer
que toutes les dérivées de $\xi \mapsto e^{-itA(\xi)}$ sont à croissance polynômiale.
\end{proof}

\chapter{Théorie des distributions}
On pose $\Omega \subseteq \R^n$, un ouvert quelconque.

\section{Introduction}
\begin{déf} Soit $u : C^\infty_0(\Omega) \to \C$ linéaire. On dit que $u$ est une \textit{distribution} sur $\Omega$ si pour toute suite $(\varphi_k)_k \subset C^\infty_0(\Omega)$
telle que $\exists K \subset\subset \Omega \st \forall k \geq 0 : \supp \varphi_k \subset K$ et~:
\[\forall \alpha \in \N^n : \partial^\alpha\varphi_k \xrightarrow[k \to \pinfty]{\text{CVU}} 0,\]
on a~:
\[\scpr u{\varphi_k} \coloneqq u(\varphi_k) \xrightarrow[k \to \pinfty]{} 0 \qquad\text{ dans } \C.\]

On note $\mathcal D'(\Omega)$ l'ensemble des distributions sur $\Omega$.\footnote{Cette notation vient du fait que $\mathcal D'(\Omega)$ est le \textit{dual} de $C^\infty_0(\Omega)$,
qui est aussi noté $\mathcal D(\Omega)$.}
\end{déf}

\begin{rmq} Cette condition correspond à une forme de \textit{continuité généralisée}. De fait, on peut construire une topologie sur $C^\infty_0(\Omega)$ telle que cette
condition correspond exactement à la notion topologique de continuité. Cependant, les résultats sont plus généraux car ce n'est pas une topologie d'espace normé.
\end{rmq}

\begin{prp} $\mathcal D'(\Omega)$ est un $\C$-e.v.
\end{prp}

\begin{ex}\label{ex:L^1_loc}
On introduit l'espace~:
\[L^1_\loc(\Omega) \coloneqq \left\{f : \Omega \to \C \st \forall K \subset\subset \Omega : f\chi_K \in L^1(\Omega)\right\}.\]
\textit{Note~:} $L^1(\Omega) \subseteq L^1_\loc(\Omega)$ trivialement, mais $L^1_\loc(\Omega) \neq L^1(\Omega)$. En effet $x \mapsto e^x \in L^1_\loc(\Omega) \setminus L^1(\Omega)$.

Pour $f \in L^1_\loc(\Omega)$, on définit l'application~:
\[u_f : C^\infty_0(\Omega) \to \C : \varphi \mapsto \int_\Omega\varphi f\dif x.\]

$u_f$ est bien une distribution. En effet, fixons pour une suite $(\varphi_k)_k$ satisfaisant les hypothèses. On sait que $\varphi_kf \xrightarrow[k \to \pinfty]{} 0$
ponctuellement. De plus, $\exists M > 0 \st \forall k \geq 0 : \abs {\varphi_kf} \leq M\abs f\chi_K \in L^1(\Omega)$. Par la convergence dominée~:
\[\lim_{k \to \pinfty}\int\varphi_k f\dif x = \int \lim_{k \to \pinfty}\varphi_k f\dif x = 0.\]

De plus, $f \mapsto u_f$ est injective, donc on peut identifier $f$ à $u_f$. Les distributions généralisent donc la notion de fonction.
\end{ex}

\begin{ex}
Soit $\mu$ une mesure borélienne sur $\Omega$ et finie sur les compacts. On définit~:
\[u_\mu : C^\infty_0(\Omega) \to \C : \varphi \mapsto \int_\Omega\varphi\dif\mu.\]

Montrons que $u_\mu$ est une distribution. La linéarité vient de la linéarité de l'intégrale. Soit $(\varphi_k)_k \subset C^\infty_0(\Omega)$ satisfaisant les hypothèses.
Le support de toutes les applications $\varphi_k$ est contenu dans un compact $K$, donc $\exists M > 0 \st \forall k \geq 0 : \varphi_k \leq M\chi_K \in L^1(\mu)$.
Dès lors par la convergence dominée~: $\int_\Omega\varphi_k\dif\mu \xrightarrow[k \to \pinfty]{} 0$.

À nouveau, $\mu \mapsto u_\mu$ est injective et donc on peut identifier $\mu$ avec $u_\mu$. La notion de distribution généralise donc également la notion de mesure.
\end{ex}

\begin{ex}
$u : C^\infty_0(\R) \to \C : \varphi \mapsto \varphi'(0)$ est une distribution puisque~:
\[u(\varphi_k) = \varphi_k'(0) \xrightarrow[k \to \pinfty]{} 0.\]
\end{ex}

\section{Dérivées de distributions}

\begin{déf} Soit $u \in \mathcal D'(\Omega)$. Pour $j \in \intint 1n$, on définit~:
\[\partial_ju : C^\infty_0(\Omega) \to \C : \varphi \mapsto \scpr {\partial_ju}\varphi \coloneqq -\scpr u{\partial_j\varphi}.\]
\end{déf}

\begin{prp}\label{prp:dérivation distribution} $\partial_ju \in \mathcal D'(\Omega)$ pour tout $u \in \mathcal D'(\Omega)$ et $j \in \intint 1n$.
\end{prp}

\begin{proof} $\partial_ju$ est bien une forme linéaire par linéarité de $u$ et de $\partial_j$. Pour $(\varphi_k)_k \subset C^\infty_0(\Omega)$, si il existe
$K \subset\subset \Omega \st \forall k \geq 0 : \supp \varphi_k \subseteq K$ et si $\forall \alpha \in \N^n : \partial^\alpha\varphi_k \xrightarrow[k \to \pinfty]{\text{CVU}} 0$,
on sait que $(\partial_j\varphi_k)_k$ satisfait les mêmes propriétés. Dès lors, par définition de $u$~:
\[-\scpr u{\partial_j\varphi_k} \xrightarrow[k \to \pinfty]{} 0.\]
\end{proof}

\begin{déf} Pour $\alpha \in \N^n$, on définit $\partial^\alpha u \coloneqq \partial_1^{\alpha_1}\ldots\partial_n^{\alpha_n}u$.
\end{déf}

\begin{prp} $\partial^\alpha u \in \mathcal D'(\Omega)$.
\end{prp}

\begin{proof} par récurrence avec la Proposition~\ref{prp:dérivation distribution}.
\end{proof}

\begin{prp} Pour $f \in C^1(\Omega)$, $u_{\partial_jf} = \partial_ju_f$ (au sens de l'exemple~\ref{ex:L^1_loc}).
\end{prp}

\begin{proof} Soit $\varphi \in C^\infty_0(\Omega)$. On calcule~:
\[\scpr {u_{\partial_jf}}\varphi = \int_\Omega \varphi\partial_jf\dif x,\]
et~:
\[\scpr {\partial u_f}\varphi = -\int_\Omega f\partial_j\varphi\dif x.\]

Par intégration par partie, par un raisonnement similaire à celui dans la démonstration du Théorème~\ref{thm:Fourier continu sur Schwartz}, on trouve~:
\[\int_\Omega\varphi\partial_jf\dif x + \int_\Omega\partial_j\varphi f\dif x = \int_\Omega \partial_j(\varphi f)\dif x = 0\]
car par Fubini~:
\[\int_K\partial(\varphi f)\dif x = \int_{-L_1}^{L_1}\ldots\underbrace {\int_{-L_n}^{L_n}\partial(\varphi f)\dif x_n}_{= 0}\ldots\dif x_1\]
où $\Omega \supset \prod_{j=1}^n [-L_j, L_j] = K \supseteq \supp \varphi$, donc $\varphi(x_1, \ldots, x_n) = 0$ si $\abs {x_j} \geq L_j$.
\end{proof}

\begin{ex} Pour $f = \chi_{\overline {\R^+}}$~:
\[\od {}xu_f = (u_f)' : \varphi \mapsto -\int_{\overline {\R^+}}\varphi'(x)\dif x = -\left(\lim_{x \to \pinfty}\varphi(x) - \varphi(0)\right) = \varphi(0),\]
puisque $\varphi$ est à support compact.

Donc $\od {}xu_{\chi_{\overline {\R^+}}} = \delta_0$ (la mesure de Dirac en 0).
\end{ex}

\begin{prp} Pour $u \in \mathcal D'(\Omega)$, $\partial_j\partial_ku = \partial_k\partial_ju$.
\end{prp}

\begin{proof} $u$ est définie sur $C^\infty_0(\Omega)$ donc pour $\varphi \in C^\infty_0(\Omega)$, les dérivées commutent. Donc~:
\[\scpr {\partial_j\partial_ku}\varphi = -\scpr {\partial_ku}{\partial_j\varphi} = \scpr u{\partial_k\partial_j\varphi} = \scpr u{\partial_j\partial_k\varphi}
= \scpr {\partial_k\partial_ju}\varphi.\]
\end{proof}

\begin{déf} Pour $a \in C^\infty_0(\Omega)$ et $u \in \mathcal D'(\Omega)$, on définit~:
\[au : C^\infty_0(\Omega) \to \C : \varphi \mapsto \scpr {au}\varphi \coloneqq \scpr u{a\varphi}.\]
\end{déf}

\begin{prp} Pour $a \in C^\infty_0(\Omega), u \in \mathcal D'(\Omega)$, $au$ est une distribution.
\end{prp}

\begin{proof} Par linéarité de $u$.
\end{proof}

\begin{prp} Soient $f \in L^1_\loc(\Omega)$ et $a \in C^\infty_0(\Omega)$. Alors $u_{af} = au_f$.
\end{prp}

\begin{proof} Soit $\varphi \in C^\infty_0(\Omega)$.
\[\scpr {u_{af}}\varphi = \int_\Omega \varphi (af)\dif x = \int_\Omega(a\varphi)f\dif x = \scpr {u_f}{a\varphi} = \scpr {au_f}\varphi.\]
\end{proof}

\begin{ex} Soient $\delta_0$ la mesure de Dirac en 0, $u$ la distribution associée et $C^\infty_0(\Omega) \ni a : x \mapsto x_j$. Par définition de $a$~:
\[\scpr {au}\varphi = (a\varphi)(0) = 0.\]
\end{ex}

\begin{prp} Les distributions vérifient la formule de Leibniz~: si $u \in \mathcal D'(\Omega)$ et $a \in C^\infty_0(\Omega)$, alors~:
\[\partial_j(au) = \partial_jau + a\partial_ju\]
\end{prp}

\begin{proof} Soit $\varphi \in C^\infty_0(\Omega)$.
\begin{align*}
	\scpr {\partial_j(au)}\varphi &= -\scpr {au}{\partial_j\varphi} = \scpr u{-a\partial_j\varphi} = \scpr u{-\partial_j(au)+\partial_ja\varphi}
		= \scpr {\partial_ju}{a\varphi} + \scpr{\partial_jau}\varphi \\
	&= \scpr {a\partial_ju}\varphi + \scpr {\partial_jau}\varphi = \scpr {\partial_jau + a\partial_ju}\varphi.
\end{align*}
\end{proof}

\section{Distributions tempérées}
On se place dans le cas $\Omega = \R^n$.

\begin{déf} Soit $u \in \mathcal D'(\R^n)$. $u$ est dite \textit{tempérée} si pour toute suite
$(\varphi_k)_k \subset C^\infty_0(\R^n) \st \varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0$ (au sens de la Définition~\ref{déf:convergence Schwartz}), on a~:
\[\scpr u{\varphi_k} \xrightarrow[k \to \pinfty]{} 0.\]

On note $\mathcal S'(\R^n)$ l'espace des distributions tempérées.
\end{déf}

\begin{rmq} Si $(\varphi_k)_k \subset C^\infty_0(\R^n)$ telle qu'il existe $K \subset\subset \R^n$ avec $\forall k \geq 0 : \supp \varphi_k \subset K$ et~:
\[\forall \alpha \in \N^n : \partial^\alpha \varphi_k \xrightarrow[k \to \pinfty]{\text{CVU}} 0,\]
alors $\varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0$, i.e. la topologie sur $C^\infty_0(\R^n)$ est plus fine que celle de $\mathcal S(\R^n)$.
\end{rmq}

\begin{prp}\label{prp:borné ssi continu distributions tempérées} Soit $u : C^\infty_0(\R^n) \to \C$ une forme linéaire. $u$ est tempérée ssi~:
\[\exists C, m > 0 \st \forall \varphi \in C^\infty_0(\R^n) : \abs {\scpr u\varphi} \leq C\sum_{\abs \alpha + \abs \beta \leq m}\norm {x^\alpha\partial^\beta\varphi}_{L^\infty(\R^n)}.\]
\end{prp}

\begin{rmq}Cette proposition est une généralisation du Théorème~\ref{thm:opérateur continu ssi borné}.
\end{rmq}

\begin{proof} \underline {$\Leftarrow$~:} Pour tout $\varphi \in C^\infty_0(\R^n)$, on note~:
\[p_m(\varphi) \coloneqq \sum_{\abs \alpha + \abs\beta \leq m}\norm {x^\alpha\partial^\beta\varphi}_{L^\infty(\R^n)}.\]

Soit $(\varphi_k)_k \subset C^\infty_0(\R^n)$ telle que $\varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0$. Par définition de la convergence dans $\mathcal S(\R^n)$, on sait
que $p_m(\varphi_k) \xrightarrow[k \to \pinfty]{} 0$. Dès lors, $\scpr u{\varphi_k} \leq Cp_m(\varphi_k) \xrightarrow[k \to \pinfty]{} 0$, et donc $u$ est tempérée.

\underline {$\Rightarrow$~:} Par contraposée, supposons que~:
\[\forall C > 0, m > 0 : \exists \varphi_{C,m} \in C^\infty_0(\R^n) \st \abs {\scpr u{\varphi_{C,m}}} > Cp_m(\varphi_{C,m}).\]

En particulier, si $C=m=j$, il existe $\varphi_j \in C^\infty_0(\R^n) \st \abs {\scpr u{\varphi_j}} \gneqq jp_j(\varphi_j)$. WLOG supposons $\abs {\scpr u{\varphi_j}}=1$
(quitte à normaliser $\varphi_j$). Alors $p_j(\varphi_j) < \frac 1j$, et donc pour $m \leq j$~: $p_m(\varphi_j) < \frac 1j$. On a donc construit une suite
$(\varphi_j)_j \subset C^\infty_0(\R^n)$ qui converge vers 0 dans $\mathcal S(\R^n)$. Cependant pour tout $j$, $\abs {\scpr u{\varphi_j}} = 1$, et donc $\scpr u{\varphi_j}$
ne converge pas vers 0. Donc $u$ n'est pas tempérée.
\end{proof}

\begin{prp}\label{prp:distributions tempérées e.v.} $\mathcal S'(\R^n) \subset \mathcal D'(\R^n)$ et $\mathcal S'(\R^n)$ est un $\C$-e.v.
\end{prp}

\begin{lem}\label{lem:C^infty_0 Lipschitz} Soit $\psi \in C^\infty_0(\R^n)$. Il existe $C > 0$ tel que~:
\[\forall y, z \in \R^n : \abs {\psi(y)-\psi(z)} \leq C\abs {y-z}.\]
\end{lem}

%PROOF?

\begin{lem}\label{lem:C infty 0 dense dans Schwartz} Si $\varphi \in \mathcal S(\R^n)$, alors
$\exists (\varphi_k)_k \subset C^\infty_0(\R^n) \st \varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \varphi$,
i.e. $C^\infty_0(\R^n)$ est dense dans $\mathcal S(\R^n)$ pour la topologie de $\mathcal S(\R^n)$.
\end{lem}

\begin{proof} Soit $\psi \in C^\infty_0(\R^n) \st \psi(x) = 1$ si $\abs x \leq 1$. Pour $k \geq 1$, on pose~:
\[\varphi_k : \R^n \to \C : x \mapsto \varphi(x)\psi\left(\frac xk\right).\]

On a alors $(\varphi_k-\varphi)(x) = \varphi(x)\left(\psi\left(\frac xk\right)-1\right)$. Par Leibniz~:
\begin{align*}
	\abs {x^\alpha\partial^\beta(\varphi_k-\varphi)}
	&= \abs {\sum_{\gamma \leq \beta}\binom \beta\gamma x^\alpha\partial^\gamma\varphi\partial^{\beta-\gamma}\left(\psi\left(\frac xk\right)-1\right)} \\
	&\leq \sum_{\gamma\leq\beta}\binom\beta\gamma\abs {x^\alpha}\abs {\partial^\beta\varphi}\abs {\partial^{\beta-\gamma}\left(\psi\left(\frac xk\right)-1\right)}
\end{align*}

Si $\beta - \gamma \neq 0$, un facteur $k^{-\abs{\beta-\gamma}}$ sort du dernier facteur, et si $\beta=\gamma$, par le Lemme~\ref{lem:C^infty_0 Lipschitz}, il existe $C$ tel que
$\abs {\psi\left(\frac xk\right)-1} = \abs {\psi\left(\frac xk\right) - \psi(0)} \leq C\abs {\frac xk} = C\frac {\abs x}k$.

Dès lors, tous les termes de la somme tendent uniformément vers 0 et~:
\[\varphi_k-\varphi \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0.\]
\end{proof}

\begin{thm} Soit $u \in \mathcal D'(\R^n)$. $u$ possède un unique prolongement $\widetilde u$ à $\mathcal S(\R^n)$ tel que~:
\begin{enumerate}
	\item $\widetilde u : \mathcal S(\R^n) \to \C$ est linéaire~;
	\item $\exists C, m > 0 \st \forall \varphi \in \mathcal S(\R^n) : \abs {\scpr {\widetilde u}\varphi} \leq Cp_m(\varphi)$.
\end{enumerate}
\end{thm}

\begin{proof} Par le Lemme~\ref{lem:C infty 0 dense dans Schwartz}, il existe
$(\varphi_k)_k \subset C^\infty_0(\R^n) \st \varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \varphi$. Pour tout $k$, $\scpr u{\varphi_k}$ est bien défini et
$(\scpr u{\varphi_k})_k$ est de Cauchy puisque~:
\[\abs {\scpr u{\varphi_k} - \scpr u{\varphi_\ell}} = \abs {\scpr u{\varphi_k-\varphi_\ell}} \leq Cp_m(\varphi_k-\varphi_\ell) \xrightarrow[k,\ell \to \pinfty]{} 0\]
par la Proposition~\ref{prp:borné ssi continu distributions tempérées}. On pose alors~:
\[\scpr {\widetilde u}\varphi \coloneqq \lim_{k \to \pinfty}\scpr u{\varphi_k}.\]

Pour montrer que $\widetilde u$ est linéaire, on fixe $\varphi, \psi \in \mathcal S(\R^n)$. On prend $(\varphi_k)_k, (\psi_k)_k \subset C^\infty_0(\R^n)$ telles que~:
\[\varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \varphi \qquad \text{ et } \qquad \psi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \psi.\]
Alors en particulier $\varphi_k + \psi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \varphi + \psi$, et donc~:
\[\scpr {\widetilde u}{\varphi + \psi} = \lim_{k \to \pinfty}\scpr u{\varphi_k+\psi_k} = \lim_{k \to \pinfty}\scpr u{\varphi_k}+\lim_{k \to \pinfty}\scpr u{\psi_k}
= \scpr {\widetilde u}\varphi + \scpr {\widetilde u}\psi.\]

De plus, pour $\lambda \in \R$, $\lambda\varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \lambda\varphi$, donc~:
\[\scpr {\widetilde u}{\lambda \varphi} = \lim_{k \to \pinfty}\scpr u{\lambda\varphi_k} = \lambda\lim_{k \to \pinfty}\scpr u{\varphi_k} = \lambda\scpr {\widetilde u}\varphi.\]

Montrons alors le deuxième point et fixons $\varepsilon > 0$. Pour $\varphi \in \mathcal S(\R^n)$,
$(\varphi_k)_k \subset C^\infty_0(\R^n) \st \varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \varphi$, on a à $k$ fixé~:
\[\abs {\scpr {\widetilde u}\varphi} \leq \abs {\scpr {\widetilde u}{\varphi_k}} + \abs {\scpr {\widetilde u}{\varphi-\varphi_k}}
= \abs {\scpr u{\varphi_k}} + \abs {\scpr {\widetilde u}{\varphi-\varphi_k}} \leq Cp_m(\varphi_k) + \abs {\scpr {\widetilde u}\varphi - \scpr u{\varphi_k}}.\]

Or par convergence des $\varphi_k$ vers $\varphi$ dans $\mathcal S$, on a~:
\[p_m(\varphi_k) = \sum_{\abs \alpha + \abs \beta \leq m}\norm {x^\alpha\partial^\beta\left(\varphi + [\varphi_k - \varphi]\right)}_{L^\infty}
\leq \sum_{\abs \alpha + \abs \beta \leq m}\norm {x^\alpha\partial^\beta\varphi}_{L^\infty}
	+ \sum_{\abs \alpha + \abs \beta \leq m}\norm {x^\alpha\partial^\beta(\varphi_k-\varphi)}_{L^\infty}
\leq p_m(\varphi) + \varepsilon\]
pour $k > K_1$ suffisamment grand. De plus, par définition de $\widetilde u$, pour $k > K_2$~: $\abs {\scpr {\widetilde u}\varphi - \scpr u{\varphi_k}} \leq \varepsilon$.
Pour $k > \max\{K_1, K_2\}$, on a donc~:
\[\abs {\scpr {\widetilde u}\varphi} \leq Cp_m(\varphi) + (C+1)\varepsilon.\]
Or le résultat tient pour tout $\varepsilon > 0$, donc~:
\[\abs {\scpr {\widetilde u}\varphi} \leq Cp_m(\varphi).\]

Il reste à montrer que $\widetilde u$ est unique. Soit $\bar u : \mathcal S(\R^n) \to \C$ telle que $\restr {\bar u}{C^\infty_0(\R^n)} = u$ et telle que $\exists C_1, m_1 \st
\forall \varphi \in C^\infty_0(\R^n) : \abs {\scpr {\bar u}\varphi} \leq C_1p_{m_1}(\varphi)$. Alors en posant $\hat C \coloneqq \max\{C, C_1\}$ et $\hat m \coloneqq \max\{m, m_1\}$,
à $\varepsilon > 0$ fixé, pour tout $k$, on trouve~:
\begin{align*}
	\abs {\scpr {\widetilde u}\varphi - \scpr {\bar u}\varphi} &= \abs {\scpr {\widetilde u}\varphi - \scpr u{\varphi_k} - \scpr {\bar u}\varphi + \scpr u{\varphi_k}}
	= \abs {\scpr {\widetilde u}{\varphi-\varphi_k} - \scpr {\bar u}{\varphi-\varphi_k}} \\
	&\leq Cp_m(\varphi-\varphi_k) + C_1p_{m_1}(\varphi-\varphi_k) \leq 2\hat Cp_{\hat m}(\varphi-\varphi_k)
\end{align*}
puisque $\varphi-\varphi_k \in \mathcal S(\R^n)$. De plus, puisque $\varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \varphi$, on sait qu'il existe $K > 0$ tel que si $k > K$,
alors~:
\[2\hat Cp_{\hat m}(\varphi - \varphi_k) < \varepsilon.\]
Dès lors~:
\[\forall \varepsilon > 0 : \abs {\scpr {\widetilde u}\varphi - \scpr {\bar u}\varphi} \leq \varepsilon,\]
i.e. $\scpr {\widetilde u}\varphi = \scpr {\bar u}\varphi$.
\end{proof}

\begin{prp}\label{prp:dérivée prolongement = prolongement dérivée} Pour $u \in \mathcal D'(\R^n)$ et $\widetilde u$ son prolongement, alors
$\widetilde {\partial_ju} = \partial_j\widetilde u$.
\end{prp}

\begin{proof}Soit $\varphi \in \mathcal S(\R^n)$ et soit $(\varphi_k)_k \subset C^\infty_0(\R^n) \st \varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \varphi$. Alors~:
\[\scpr {\widetilde {\partial_ju}}\varphi = \lim_{k \to \pinfty}\scpr {\partial_ju}{\varphi_k} = \lim_{k \to \pinfty}\scpr u{-\partial_j\varphi_k}
= \scpr {\widetilde u}{-\partial_j\varphi} = \scpr {\partial_j\widetilde u}\varphi\]
car $\partial_j\varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \partial_j\varphi$.
\end{proof}

\begin{ex} Pour $p \in [1, \pinfty]$, si $f \in L^p(\R^n)$, alors $f \in L^\infty(\R^n)$, et donc $f \in L^1_\loc(\R^n)$. Montrons que la distribution associée~:
\[u_f : C^\infty_0(\R^n) \to \C : \varphi \mapsto \int_{\R^n}f\varphi\dif x\]
est une distribution tempérée.

Soit $(\varphi_k)_k \subset C^\infty_0(\R^n) \st \varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0$. Pour $q$ conjugué à $p$~:
\[\abs {\scpr {u_f}{\varphi_k}} = \abs {\int_{\R^n}f\varphi_k\dif x} \leq \int_{\R^n}\abs {f\varphi_k}\dif x = \norm {f\varphi_k}_{L^1} \leq \norm f_{L^p}\norm {\varphi_k}_{L^q}\]
car $C^\infty_0(\R^n) \subset \bigcap_{q \in [1, \pinfty]}L^q(\R^n)$. Distinguons les deux cas~:
\begin{itemize}
	\item si $q = \pinfty$, alors $\norm {\varphi_k}_{L^\infty} \xrightarrow[k \to \pinfty]{} 0$ puisque $\varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0$,
	\item et si $q \in [1, \pinfty)$, alors~:
	\[\norm {\varphi_k}_{L^q}^q = \int_{\R^n}\abs {\varphi_k}^q\dif x
	= \int_{\R^n}\underbrace {\abs {(1+\abs x)^m\varphi_k}^q}_{\xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0}\underbrace {(1+\abs x)^{-mq}}_{\in L^1(\R^n) \text{ si } mq > n}\dif x.\]
	Donc pour $m > n/q$~: $\norm {\varphi_k}_{L^q} \xrightarrow[k \to \pinfty]{} 0$.
\end{itemize}

Dans tous les cas, $\abs {\scpr {u_f}{\varphi_k}} \xrightarrow[k \to \pinfty]{} 0$.
\end{ex}

\begin{ex} Pour $\alpha \in \N^n$ un multi-indice, $f : x \mapsto x^\alpha \in L^1_\loc(\R^n)$. Montrons que $u_f \in \mathcal S'(\R^n)$. Soit $(\varphi_k)_k \subset C^\infty_0(\R^n)$.
Par un raisonnement similaire~:
\[\abs {\scpr {u_f}{\varphi_k}} \leq \int_{\R^n} \left(x^\alpha(1+\abs x)^{-N}\right)\left((1+\abs x)^N\varphi_k\right)\dif x \xrightarrow[k \to \pinfty]{} 0\]
pour $N > \abs \alpha + n$.
\end{ex}

\begin{rmq} Par la Proposition~\ref{prp:distributions tempérées e.v.} et par l'exemple précédent, on trouve que la distribution associée à un polynôme quelconque est tempérée.
\end{rmq}

\begin{ex} Soient $\delta_0$ la mesure de Dirac en $0$ et $u_\delta : C^\infty_0(\R^n) \to \C : \varphi \mapsto \varphi(0)$, sa distribution associée. Montrons que $u_\delta$
est tempérée. Soit $(\varphi_k)_k \subset C^\infty_0(\R^n) \st \varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0$.
\[\scpr {u_\delta}{\varphi_k} = \varphi_k(0) \leq \sup_{x \in \R^n}\varphi_k(x) = \norm {\varphi_k}_{L^\infty} \xrightarrow[k \to \pinfty]{} 0\]
car $\varphi_k$ est continue.
\end{ex}

\begin{prp} Si $u \in \mathcal S'(\R^n)$, alors pour tout $\alpha \in \N^n$~: $\partial^\alpha u \in \mathcal S'(\R^n)$ et $x^\alpha u \in \mathcal S'(\R^n)$.
\end{prp}

\begin{proof} La preuve repose sur le fait que si $\varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0$, alors pour tout $\alpha \in N^n$~:
$\partial^\alpha \varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0$ et $x^\alpha \varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0$ par définition. Dès lors~:
\[\scpr {\partial^\alpha u}{\varphi_k} = (-1)^{\abs \alpha}\scpr u{\partial^\alpha \varphi_k} \xrightarrow[k \to \pinfty]{} 0,\]
et~:
\[\scpr {x^\alpha u}{\varphi_k} = \scpr u{x^\alpha \varphi_k} \xrightarrow[k \to \pinfty]{} 0.\]
\end{proof}

\section{Transformations de Fourier de distributions tempérées}

\begin{déf} Soit $u \in \mathcal S'(\R^n)$. On définit~:
\[\mathcal Fu : C^\infty_0(\R^n) \to \C : \varphi \mapsto \scpr {\widetilde u}{\widehat \varphi},\]
où $\widehat \varphi$ est la transformée de Fourier \textit{classique} (au sens de la définition~\ref{déf:Fourier dans L^1}) et est dans $\mathcal S(\R^n)$ car
par la Proposition~\ref{prp:C^infty_0 dans Schartz dans l'intersection des L^p}, on sait que $C^\infty_0(\R^n) \subset \mathcal S(\R^n)$.
\end{déf}

\begin{prp} Pour $u \in \mathcal S'(\R^n)$, $\mathcal Fu \in \mathcal S'(\R^n)$.
\end{prp}

\begin{proof} $\mathcal Fu$ est bien une application linéaire par linéarité de la transformée de Fourier sur $\mathcal S(\R^n)$ et par linéarité de $\widetilde u$.

De plus, pour $(\varphi_k)_k \subset C^\infty_0(\R^n) \st \varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0$~:
\[\abs {\scpr {\widetilde u}{\widehat {\varphi_k}}} \leq Cp_m(\widehat {\varphi_k}) \xrightarrow[k \to \pinfty]{} 0,\]
car $\widehat {\varphi_k} \xrightarrow[k \to \pinfty]{} \widehat 0 = 0$ par le Théorème~\ref{thm:Fourier continu sur Schwartz}.
\end{proof}

\begin{prp} Soit $u \in \mathcal S'(\R^n)$. Alors $\mathcal FD_ju = x_j\mathcal Fu$.
\end{prp}

\begin{proof} Soit $\varphi \in C^\infty_0(\R^n)$. Alors~:
\[\scpr {\mathcal FD_ju}\varphi = \scpr {\widetilde {D_ju}}{\widehat \varphi} = \scpr {\widetilde u}{-D_j\widehat \varphi} = \scpr {\mathcal Fu}{x_j\varphi}
= \scpr {x_j\mathcal Fu}\varphi\]
par la Propriété~\ref{prp:dérivée prolongement = prolongement dérivée}.
\end{proof}

\begin{prp} pour $f \in L^2(\R^n) \subset L^1_\loc(\R^n)$~: $u_{\mathbb Ff} = \mathcal Fu_f$.
\end{prp}

\begin{proof} Soit $\varphi \in C^\infty_0(\R^n)$. Pour $(\psi_k)_k \subset C^\infty_0(\R^n) \st \psi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \widehat {\varphi}$.
\[\scpr {\mathcal Fu_f}\varphi = \scpr {\widetilde {u_f}}{\widehat \varphi} = \lim_{k \to \pinfty}\scpr {u_f}{\psi_k} = \lim_{k \to \pinfty}\int_{\R^n}f\psi_k\dif x.\]

Or~:
\[\abs {\int f\psi_k\dif x - \int f\widehat \varphi\dif x} \leq \int\abs {f(\psi_k - \widehat \varphi)}\dif x
	\leq \norm f_{L^2}\norm {\psi_k-\widehat \varphi}_{L^2} \xrightarrow[k \to \pinfty]{} 0.\]

En effet, puisque $\psi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \widehat \varphi$, pour $N > \frac n2$~:
\begin{align*}
	\norm {\psi_k - \widehat \varphi}_{L^2}^2 &= \int\abs {\psi_k-\widehat \varphi}^2\dif x = \int\left((1+\abs x)^N\abs {\psi_k-\widehat \varphi}\right)^2(1+\abs x)^{-2N}\dif x \\
	&\leq \int\norm {(1+\abs x)^N(\psi_k-\widehat\varphi)}_{L^\infty}^2(1+\abs x)^{-2N}\dif x \\
	&= \underbrace {\norm {(1+\abs x)^N(\psi_k-\widehat\varphi)}_{L^\infty}^2}_{\xrightarrow[k \to \pinfty]{} 0}\underbrace{\int(1+\abs x)^{-2N}\dif x}_{< \pinfty}
		\xrightarrow[k \to \pinfty]{} 0.
\end{align*}

Donc~:
\[\scpr {\mathcal Fu_f}\varphi = \int f\widehat \varphi\dif x.\]

De même~:
\[\scpr {u_{\mathbb Ff}}\varphi = \int f\widehat \varphi\dif x.\]

En effet, pour $(f_k)_k \subset \mathcal S(\R^n) \st f_k \xrightarrow[k \to \pinfty]{L^2(\R^n)} f$~:
\[\int\mathbb Ff_k\varphi\dif x = \int\widehat {f_k}\varphi\dif x = \int f_k\widehat \varphi\dif x,\]
et par Hölder~:
\[\abs {\int f_k\widehat\varphi\dif x - \int f\widehat\varphi\dif x} \leq \int\abs {f_k\widehat \varphi - f\widehat \varphi}\dif x = \int\abs {f_k-f}\abs{\widehat \varphi}\dif x
\leq \norm {f_k-f}_{L^2}\norm {\widehat \varphi}_{L^2} \xrightarrow[k \to \pinfty]{} 0.\]

De plus, puisque $\widehat {f_k} = \mathbb Ff_k \xrightarrow[k \to \pinfty]{L^2(\R^n)} \mathbb Ff$~:
\[\abs {\int \mathbb Ff_k\varphi\dif x - \int\mathbb Ff\varphi\dif x} \leq \int\abs {\mathbb Ff_k-\mathbb Ff}\abs \varphi\dif x
\leq \norm {\mathbb Ff_k-\mathbb Ff}_{L^2}\norm \varphi_{L^2} \xrightarrow[k \to \pinfty]{} 0.\]

Dès lors, par unicité de la limite dans $L^1(\R^n)$~:
\[\int\mathbb Ff\varphi\dif x = \int f\widehat \varphi\dif x.\]
\end{proof}

\begin{déf} Pour $\varphi \in \mathcal S(\R^n)$, on définit l'application~:
\[\widecheck \varphi : \R^n \to \C : x \mapsto \varphi(-x).\]
Pour $u \in \mathcal D'(\R^n)$, On définit également~:
\[\widecheck u : C^\infty_0(\R^n) \to \C : \varphi \mapsto \scpr u{\widecheck \varphi}.\]
\end{déf}

\begin{rmq} $\widecheck \varphi \in \mathcal S(\R^n)$.
\end{rmq}

\begin{prp} Si $u \in \mathcal D'(\R^n)$, alors $\widecheck u \in \mathcal D'(\R^n)$. De plus, si $u \in \mathcal S'(\R^n)$, alors $\widecheck u \in \mathcal S'(\R^n)$.
\end{prp}

\begin{proof} Soient $K \subset\subset \R^n, (\varphi_k)_k \subset C^\infty_0(\R^n) \st K \supset \bigcup_{k \in \N}\supp\varphi_k$. Alors on pose
$\widecheck K \coloneqq -K$ tel que $\widecheck K \supset \bigcup_{k \in \N}\supp\widecheck {\varphi_k}$. Alors, pour $\alpha \in \N^n$~:
\[\norm {\partial^\alpha\widecheck{\varphi_k}}_{L^\infty} = \sup_{x \in \widecheck K}\partial^\alpha\widecheck {\varphi_k}(x) = \sup_{x \in K}\partial^\alpha\varphi_k(x)
= \norm {\partial^\alpha\varphi_k}_{L^\infty} \xrightarrow[k \to \pinfty]{} 0.\]

De même, si $u \in \mathcal S'(\R^n)$, alors pour $(\varphi_k)_k \subset \mathcal S(\R^n) \st \varphi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0$~:
\[\scpr u{\widecheck {\varphi_k}} \xrightarrow[k \to \pinfty]{} 0\]
puisque $\widecheck {\varphi_k} \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} 0$ également.
\end{proof}

\begin{lem}\label{lem:Fourier distribution prolongée} Soient $u \in \mathcal S'(\R^n), \psi \in \mathcal S(\R^n)$.
Alors $\scpr {\widetilde {\mathcal Fu}}\psi = \scpr {\widetilde u}{\widehat \psi}$
\end{lem}

\begin{proof} Soit $(\psi_k)_k \subset C^\infty_0(\R^n) \st \psi_k \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \psi$. Alors~:
\[\scpr {\widetilde {\mathcal Fu}}\psi = \lim_{k \to \pinfty}\scpr {\mathcal Fu}{\psi_k} = \lim_{k \to \pinfty}\scpr u{\widehat {\psi_k}} = \scpr u{\widehat \psi}\]
car $\widehat {\psi_k} \xrightarrow[k \to \pinfty]{\mathcal S(\R^n)} \widehat \psi$.
\end{proof}

\begin{prp} Pour $u \in \mathcal S'(\R^n)$~:
\[\mathcal F^2u = \mathcal F\mathcal Fu = (2\pi)^n\widecheck u.\]
\end{prp}

\begin{proof} Soit $\varphi \in \mathcal S(\R^n)$. Par le Lemme~\ref{lem:Fourier distribution prolongée}~:
\[\scpr {\mathcal F^2u}\varphi = \scpr {\widetilde u}{\widehat {\widehat \varphi}}.\]

Or par la formule d'inversion de Fourier~:
\[\widecheck \varphi(x) = \varphi(-x) = (2\pi)^{-n}\int_{\R^n}e^{-i\scpr x\xi}\widehat \varphi(\xi)\dif \xi = (2\pi)^{-n}\widehat {\widehat \varphi}(x).\]

Donc~:
\[\scpr {\mathcal F^2u}\varphi = (2\pi)^n\scpr {\widetilde u}{\widecheck \varphi} = (2\pi)^n\scpr u{\widecheck \varphi}\]
car $\varphi \in C^\infty_0(\R^n)$ et $\widetilde u = u$ sur $C^\infty_0(\R^n)$.
\end{proof}

\begin{cor} $\mathcal F$ est une bijection de $\mathcal S'(\R^n)$ sur $\mathcal S'(\R^n)$.
\end{cor}

\begin{proof}~
\begin{description}
	\item[Injectivité] $\mathcal F$ est linéaire, donc il est suffisant de montrer que $\Ker\mathcal F = \{0\}$. Soit $u \in C^\infty_0(\R^n) \st \mathcal Fu = 0$. Alors~:
	\[\widecheck u = \mathcal F^2u = \mathcal F0 = 0,\]
	et donc $u = 0$.
	\item[Surjectivité] Soit $f \in \mathcal S'(\R^n)$. On pose $u \coloneqq \mathcal F\left((2\pi)^{-n}\widecheck f\right)$. On a alors~:
	\[\mathcal Fu = \mathcal F^2(2\pi)^{-n}\widecheck f = (2\pi)^{-n}\mathcal F^2\widecheck f = (2\pi)^{-n}(2\pi)^n\widecheck {\widecheck f} = f.\]
\end{description}
\end{proof}

\begin{ex} Soit $\delta$ la mesure de Dirac en 0 sur $\R^n$. Pour $\varphi \in C^\infty_0(\R^n)$~:
\[\scpr {\mathcal Fu_\delta}\varphi = \scpr {\widetilde {u_\delta}}{\widehat \varphi} = \widehat \varphi(0) = \int_{\R^n}\varphi(x)\dif x = \scpr {u_\mathbf 1}\varphi.\]

De plus~:
\[\mathcal Fu_{\mathbf 1} = \mathcal F\mathcal Fu_\delta = \mathcal F^2u_\delta = (2\pi)^n \widecheck {u_{\delta}} = (2\pi)^n u_{\delta}\]
\end{ex}

\section{Convolution et distributions}

\begin{déf} Pour $f \in L^1(\R^n), \varphi \in C^\infty_0(\R^n)$, on définit la \textit{convolution} de $f$ et $\varphi$ par~:
\[(f * \varphi) : \R^n \to \C : x \mapsto \int_{\R^n}f(y)\varphi(x-y)\dif y.\]
\end{déf}

\begin{déf} Pour $u \in \mathcal D'(\R^n)$ et $\varphi \in C^\infty_0(\R^n)$, on définit l'application convolution par~:
\[u*\varphi : \R^n \to \C : x \mapsto \scpr u{\varphi(x-\cdot)}.\]
\end{déf}

\begin{ex} Pour $\delta$ la mesure de Dirac en 0 sur $\R^n$ et $\varphi \in C^\infty_0(\R^n)$~:
\[\forall x \in \R^n : (\delta * \varphi)(x) = \varphi(x),\]
i.e. $\delta * \varphi = \varphi$.
\end{ex}

\begin{lem}\label{lem:C^infty_0 Lipschitz^2} Si $\psi \in C^\infty_0(\R^n)$, alors pour $x, y \in \R^n$~:
\[\abs {\psi(x) - \psi(y) - \sum_{j=1}^n\partial_j\psi(y)(x_j-y_j)} \leq C\abs {x-y}^2.\]
\end{lem}

\begin{rmq} Ce lemme généralise le Lemme~\ref{lem:C^infty_0 Lipschitz}.
\end{rmq}

\begin{thm} Soient $u \in \mathcal D'(\R^n), \varphi \in C^\infty_0(\R^n)$. Alors $u * \varphi \in C^\infty(\R^n)$ et~:
\[\forall \alpha \in \N^n : \partial^\alpha(u * \delta) = (\partial^\alpha u) * \varphi = u * (\partial^\alpha\varphi).\]
\end{thm}

\begin{proof} Montrons tout d'abord la continuité de $u * \varphi$. Soit $(x_k)_k \subset \R^n \st x_k \xrightarrow[k \to \pinfty]{} x$ et posons
$\varphi_k \coloneqq \varphi(x_k-\cdot)$.
\[(u * \varphi)(x_k) = \scpr u{\varphi_k}\]

De plus, comme la suite $(x_k)_k$ est convergente, elle est bornée, i.e. $\exists > 0 \st \forall k \in \N : \abs {x_k} \leq M$, et donc~:
\[\bigcup_{k \in \N}\supp\varphi_k = \bigcup_{k \in \N}\left(x_k-\supp\varphi\right) = \bigcup_{k \in \N}\{x_k\} - \supp\varphi \subseteq \overline {B(0, M)} - \supp\varphi.\]
Donc $K \coloneqq \overline {B(0, M)} - \supp\varphi = \left\{x-y \st x \in \overline {B(0, M)}, y \in \supp\varphi\right\}$ est un compact (car $\overline {B(0, M)}$
et $\supp\varphi$ sont compacts) qui contient le support de tous les $\varphi_k$.

De plus, $\forall \alpha \in \N^n : \partial^\alpha\varphi_k \xrightarrow[k \to \pinfty]{\text{CVU}} \partial^\alpha\varphi(x-\cdot)$. En effet, par le
Lemme~\ref{lem:C^infty_0 Lipschitz}, pour $\alpha \in \N^n$~:
\[\norm {\partial^\alpha\varphi_k - \partial^\alpha\varphi(x-\cdot)}_{L^\infty} = \sup_{y \in K}\abs {\partial^\alpha\varphi(x_k-y) - \partial^\alpha\varphi(x-y)}
	\leq \sup_{y \in K}C\abs {x_k-y-x+y} = C\abs {x_k-x} \xrightarrow[k \to \pinfty]{} 0.\]
Dès lors, par définition des distributions~:
\[\scpr u{\varphi_k} - \scpr u{\varphi(x-\cdot)} = \scpr u{\varphi_k-\varphi(x-\cdot)} \xrightarrow[k \to \pinfty]{} 0,\]
i.e.
\[(u*\varphi)(x_k) = \scpr u{\varphi_k} \xrightarrow[k \to \pinfty]{} \scpr u{\varphi(x-\cdot)} = (u*\varphi)(x).\]

Montrons alors que $u * \varphi$ est de classe $C^1$. Soient $(h_\ell)_\ell \subset \R \st h_\ell \xrightarrow[\ell \to \pinfty]{} 0$ et $k \in \{1, \ldots, n\}$.
\[\frac {(u*\varphi)(x+h_\ell e_k) - (u*\varphi)(x)}{h_\ell} = \scpr u{\underbrace {\frac {\varphi(x+h_\ell e_k - \cdot) - \varphi(x-\cdot)}{h_\ell}}_{\eqqcolon \varphi_{k,h_\ell}}}.\]
Par le Lemme~\ref{lem:C^infty_0 Lipschitz^2}~:
\begin{align*}
	\abs {\varphi(x+h_\ell e_k - y) - \varphi(x-y) - \partial_k\varphi(x-y)h_\ell}
			&= \abs {\varphi(x+h_\ell e_k - y) - \varphi(x-y) - \sum_{j=1}^n\partial_j\varphi(h_\ell e_k)(h_\ell e_k)_j} \\
	&\leq C\abs {h_\ell e_k}^2 = Ch_\ell^2 \xrightarrow[\ell \to \pinfty]{} 0,
\end{align*}
i.e.~:
\begin{equation}\label{eq:borne varphi_k,h ell - partial_k varphi}
	\abs {\varphi_{k,h_\ell}(y) - \partial_k(x-y)} = \abs {\frac {\varphi(x+h_\ell e_k - y) - \varphi(x-y)}{h_\ell} - \partial_k\varphi(x-y)}
	\leq \frac {Ch_\ell^2}{h_\ell} = Ch_\ell \xrightarrow[\ell \to \pinfty]{} 0,
\end{equation}
ou encore~:
\[\varphi_{k,h_\ell} \xrightarrow[\ell \to \pinfty]{\text{CVU}} \partial_k\varphi.\]

En effet, la convergence est uniforme puisque dans l'équation~\eqref{eq:borne varphi_k,h ell - partial_k varphi}, $\abs {\varphi_{k,h_\ell}(y)-\partial_k\varphi(x-y)}$ est borné
(uniformément pour $y$) par $Ch_\ell$. De même, par le même raisonnement que pour la continuité~:
\[\norm {\partial^\alpha\varphi_{k,h_\ell} - \varphi(x-\cdot)}_{L^\infty} \leq Ch_\ell \xrightarrow[\ell \to \pinfty]{} 0.\]
Donc par définition des distributions, on a~:
\[\scpr u{\varphi_{k,h_\ell}} \xrightarrow[\ell \to \pinfty]{} \scpr u{\partial_k\varphi(x-\cdot)}.\]

Il reste à montrer que $\partial_k(u*\varphi) = (\partial_ku) * \varphi$, or cela suit directement de la définition de $\partial_ku$~:
\[((\partial_ku)*\varphi)(x) = \scpr {\partial_ku}\varphi = -\scpr u{\pd{}{y_k}\varphi(x-y)} = \scpr u{\partial_k\varphi(x-y)} = (u * \partial_k\varphi)(x).\]

On fonctionne ensuite par récurrence pour montrer que $u*\varphi$ est bien $C^\infty$~: exercice.
\end{proof}
\end{document}
