\documentclass{report}

\usepackage[french]{babel}
\usepackage{commath}
\usepackage{palatino, eulervm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{mathtools}
\usepackage{cases}
\usepackage{stmaryrd}
\usepackage[bottom]{footmisc}
\usepackage[parfill]{parskip}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{hyperref}

\title{Calcul différentiel et intégral II}
\author{R. Petit}
\date{année académique 2016 - 2017}

% amsthm
\newtheorem{thm}{Théorème}[chapter]
\newtheorem{prp}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollaire}
\newtheorem{lem}[thm]{Lemme}
\addto\captionsfrench{\renewcommand\proofname{\underline{Démonstration}}}
\theoremstyle{definition}
\newtheorem{déf}[thm]{Définition}
\theoremstyle{remark}
\newtheorem*{rmq}{Remarque}
\newtheorem{ex}{Exemple}[chapter]

\numberwithin{equation}{section}

% link amsthm and mdframed
\iftrue
%\iffalse
	% pre-amsthm
	\mdfdefinestyle{resultstyle}{%
		hidealllines=true,%
		leftline=true,%
		rightline=true,%
		innerleftmargin=10pt,%
		innerrightmargin=10pt,%
		innertopmargin=10pt,%
		innerbottommargin=8pt,%
	}

	\surroundwithmdframed[style=resultstyle]{thm}
	\surroundwithmdframed[style=resultstyle]{prp}
	\surroundwithmdframed[style=resultstyle]{cor}
	\surroundwithmdframed[style=resultstyle]{lem}
\fi

\newcommand{\K}{\mathbb K}
\newcommand{\C}{\mathbb C}
\newcommand{\R}{\mathbb R}
\newcommand{\Rp}{\R^{+}}
\newcommand{\Rm}{\R^{-}}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Z}{\mathbb Z}
\newcommand{\N}{\mathbb N}
\newcommand{\Ns}{\N^{*}}
\newcommand{\tq}{\text{ t.q. }}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Imf}{Im}
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\MatBC}{\Mat_{B_C}}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\adh}{adh}
\DeclareMathOperator{\intr}{int}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Vect}{Vect}
\DeclareMathOperator{\Angle}{Angle}
\DeclareMathOperator{\Arg}{Arg}
\DeclareMathOperator{\Ind}{Ind}
\DeclareMathOperator{\Conv}{Conv}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

% sequence; parameters: sequence letter, sequence index, index set
\newcommand{\seq}[3]{\left(#1_{#2}\right)_{#2 \in #3}}
% sequence of instanciated function; parameters: sequence function letter, sequence index, function variable name, index set
\newcommand{\seqf}[4]{\left(#1_{#2}\left(#3\right)\right)_{#2 \in #4}}
% metric set convergence; parameters: variable name, variable limit, distance function
\newcommand{\mconv}[3]{\xrightarrow[#1 \to #2]{#3}}
% Convergence
\newcommand{\CONV}[5]{\xrightarrow[#2 \to #3]{#4 \text{ #5 } #1}}
% Simple convergence
\newcommand{\CVS}[3]{\CONV{#1}{#2}{#3}{CVS}{sur}}
% Simple convergence on compacts
\newcommand{\CVSc}[3]{\CONV{#1}{#2}{#3}{CVS}{sur tout cpct de}}
% uniform convergence
\newcommand{\CVU}[3]{\CONV{#1}{#2}{#3}{CVU}{sur}}
% uniform convergence on compacts; parameters: convergence set, index variable, destination value of index variable
\newcommand{\CVUc}[3]{\CONV{#1}{#2}{#3}{CVU}{sur tout cpct de}}

\newcommand{\El}[2]{\mathcal E\!\left(#1, #2\right)}

\newcommand{\restr}[2]{\left.#1\vphantom{\big|}\right|_{#2}}
\newcommand{\intint}[2]{\left\llbracket#1, #2\right\rrbracket}
\newcommand{\scpr}[2]{\left\langle #1, #2\right\rangle}

\newcommand{\evfn}[3]{\left(#1\left(#2, #3\right), \norm {\cdot}_{\infty}\right)}
\newcommand{\evnC}[3]{{\evfn {C^{#1}}{#2}{#3}}}
\newcommand{\evnCb}[3]{{\evfn {C_b^{#1}}{#2}{#3}}}
\newcommand{\toC}[1]{\xrightarrow{C^{#1}}}
\newcommand{\tocont}{\toC 0}
\newcommand{\minfty}{{-\infty}}
\newcommand{\pinfty}{{+\infty}}
\newcommand{\grantedproof}{\begin{proof} \underline{Admis.} \end{proof}}
\newcommand{\evn}{espace vectoriel normé }
\newcommand{\evnc}{{\evn} complet }
\newcommand{\CDII}{{CDI 1}}
\newcommand{\CDIII}{{CDI 2}}
\newcommand{\subscpct}{\subset\subset}

\newcommand{\CmT}[2]{C^{#1,m}_{#2}}
\newcommand{\CzmT}[1]{\CmT 0{#1}}
\newcommand{\Czm}{\CzmT{2\pi}}
\newcommand{\ComT}[1]{\CmT 1{#1}}
\newcommand{\Com}{\ComT {2\pi}}

\newcommand{\poles}{{\{\text{pôles}\}}}

\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}

\begin{document}

\pagenumbering{Roman}
\maketitle
\tableofcontents
\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

\part{Fonctions, séries et intégrales}
\chapter{Suites et séries de fonctions}
	\section{Rappels}
		\subsection{Topologie métrique}
			\subsubsection{Espaces métriques}
				\begin{déf} Soit $X$ un ensemble. Une \textit{distance} sur $X$ est une application $d : X \times X \to \Rp$ telle que~:

				\begin{enumerate}
					\item $\forall x, y \in X : d(x, y) = d(y, x)$ (symétrie)~;
					\item $\forall x, y, z \in X : d(x, z) \leq d(x, y) + d(y, z)$ (inégalité triangulaire)~;
					\item $\forall x, y \in X : \left(d(x, y) = 0 \iff x = y\right)$ (séparation\footnote{Également appelé
						  \textit{principe d'identité des indiscernables.}}).
				\end{enumerate}
				\end{déf}

				\begin{déf}  On appelle \textit{espace métrique} $(X, d)$ un espace $X$ muni d'une distance $d$ sur $X$. \end{déf}

				\begin{déf} Soient $(X, d)$ un espace métrique, $\seq xn\N$ et $x \in X$ La suite $(x_n)$ converge vers $x$ dans $(X, d)$ lorsque~:
				\begin{equation}
					\forall \varepsilon > 0 : \exists N \in \N \tq \forall n \geq \N : d(x_n, x) < \varepsilon.
				\end{equation}

				Cela se note~:
				\begin{equation}
					x_n \mconv n\pinfty d x.
				\end{equation}
				\end{déf}

				\begin{prp} Soit $\seq xn\N$ une suite dans $(X, d)$, un espace métrique. Soient $x, y \in X$. Si~:
				\[x_n \mconv n\pinfty d x \qquad\qquad \text{et} \qquad\qquad x_n \mconv n\pinfty d y\]
				alors $x = y$. \end{prp}

				\begin{proof} Soit $\varepsilon > 0$. Puisque $x_n \to x$ et $x_n \to y$, on sait qu'il existe $N_1, N_2 \in \N$ tels que~:
				\[\forall n \geq N_1 : d(x_n, x) < \frac \varepsilon2 \qquad\qquad \text{et} \qquad\qquad \forall n \geq N_2 : d(x_n, y) < \frac \varepsilon2.\]

				Dès lors, soit $N \coloneqq \max\{N_1, N_2\}$. On peut dire~:
				\begin{equation}
					\forall n \geq N : d(x, y) \leq d(x, x_n) + d(x_n, y) < \frac \varepsilon2 + \frac \varepsilon2 = \varepsilon.
				\end{equation}

				On en déduit $d(x, y) = 0$ et donc $x = y$ par séparation. \end{proof}

			\subsubsection{Espaces vectoriels}
				\begin{déf} Soit $\K$, un sous-corps de $\C$. On appelle \textit{norme} sur le $\K$-e.v. $E$ toute application $n : E \to \Rp$ telle que~:

				\begin{enumerate}
					\item $\forall x \in E : \left(n(x) = 02 \iff x = 0\right)$~;
					\item $\forall x \in E : \forall \lambda \in \K : n(\lambda x) = \abs \lambda n(x)$~;
					\item $\forall x, y \in E : n(x + y) \leq n(x) + n(y)$.
				\end{enumerate}
				\end{déf}

				\begin{prp} Soit $(E, n)$ un $\K$-\evn. L'application $d$ suivante est une distance sur $E$ (on l'appelle la
				\textit{distance associée à la norme $n$})~:
				\begin{equation}
					d : E \times E \to \Rp : (x, y) \mapsto n(y-x).
				\end{equation}
				\end{prp}

				\begin{proof} EXERCICE.
				\end{proof}

				\begin{rmq} Si $(E, n)$ est un \evn, $\seq xn\N$ est une suite de $E$, et si $x \in E$, alors on dit~:
				\begin{equation}
					x_n \mconv n\pinfty n x
				\end{equation}
				lorsque~:
				\begin{equation}
					x_n \mconv n\pinfty{} x
				\end{equation}
				au sens de la distance associée à la norme $n$.
				\end{rmq}

				\begin{ex} $\R$ est un $\R$-e.v. normé avec pour norme $n : x \mapsto \abs x$. \end{ex}

				\begin{ex} Soient $d \in \Ns$, $p \in [1, \pinfty)$. Pour $x = (x_i)_{1 \leq i \leq d} \in \C^d$, on définit~:
				\begin{equation}
					n(x) = \norm x_p \coloneqq \left(\sum_{k=0}^d\abs {x_i}^p\right)^{\frac 1p}.
				\end{equation}
				On a alors $(\C^d, n)$ est un $\C$-\evn. Également $(\C^d, n)$ et $(\R^d, n)$  sont des $\R$-espaces vectoriels normés.
				\end{ex}

				\begin{déf} Soit $x \in \C^d$. On définit la \textit{norme infinie} de $x$ dans $\C^d$ par~:
				\begin{equation}
					\norm x_\infty \coloneqq \max_{1 \leq i \leq d}\abs {x_i}.
				\end{equation}
				\end{déf}

				\begin{ex} Soit $d \in \Ns$. $(\C^d, \norm \cdot_\infty)$ est un $\C$-\evn. Également, $(\R^d, \norm \cdot_\infty)$
				et $(\C^d, \norm \cdot_\infty)$ sont des $\R$-espaces vectoriels normés. \end{ex}

				\begin{proof} EXERCICE.
				\end{proof}

				\begin{déf} Soit $\seq xn\N$ une suite. On dit que la suite $(x_n)$ est \textit{presque nulle} s'il existe $N \in \N$ tel que
				$\forall n \geq N : x_n = 0$. \end{déf}

				\begin{ex} Soient $P \in \C[x]$ et $\seq ak\N$ la suite presque nulle des coefficients de $P$. On pose~:
				\begin{equation}
					\norm P_\infty \coloneqq \sup_{k \in \N}\abs {a_k} = \max_{k \in \N}\abs {a_k}.
				\end{equation}

				Alors $\norm \cdot_\infty$ est une norme sur $\C[x]$.
				\end{ex}

				\begin{proof} EXERCICE.
				\end{proof}

			\subsubsection{Ouverts, fermés, compacts}
				\begin{déf} Soit $(X, d)$ un espace métrique. On appelle \textit{boule ouverte} de centre $x \in X$ et de rayon $r \gneqq 0$ l'ensemble~:
				\begin{equation}
					B(x, r[ \coloneqq \left\{y \in X \tq d(x, y) \lneqq r\right\}.
				\end{equation}

				On définit également la \textit{boule fermée} de centre $x$ et de rayon $r$ l'ensemble~:
				\begin{equation}
					B(x, r] \coloneqq \{y \in X \tq d(x, y) \leq r\}.
				\end{equation}
				\end{déf}

				\begin{déf} Soit $(X, d)$ un espace métrique et soit $O \subset X$. On dit que $O$ est une partie \textit{ouverte} dans $X$ lorsque~:
				\begin{equation}
					\forall x \in O : \exists r \gneqq 0 \tq B(x, r[  \subset O.
				\end{equation}
				\end{déf}

				\begin{rmq} Pour tout $X$, les ensembles $\emptyset$ et $X$ sont tous deux des ouverts de $X$. \end{rmq}

				\begin{déf} Soit $(X, d)$ un espace métrique. Une partie $F \subset X$  de $X$ est dite \textit{fermée} dans $X$ lorsque $X \setminus F$
				est ouvert. \end{déf}

				\begin{prp} Dans un espace métrique $(X, d)$, soit $\seq OiI$ une famille d'ouverts de $X$ indicés par un ensemble $I \neq \emptyset$.
				Alors $\left(\bigcup_{i \in I}O_i\right)$ est un ouvert de $X$. Si de plus $I$ est fini, alors $\left(\bigcap_{i \in I}\right)$ est
				un ouvert de $X$. \end{prp}

				\begin{ex} Prenons $X = \R$ et $O_i = (-1-\frac 1i, 1 + \frac 1i)$. Alors $\left(\bigcap_{i \in \Ns}O_i\right) = [-1, 1]$ qui n'est pas
				un ouvert de $X$. \end{ex}

				\begin{proof} EXERCICE.
				\end{proof}

				\begin{déf}[Compacts par Borel-Lebesgue] Soit $(X, d)$ un espace métrique. Une partie $K \subset X$ est dite \textit{compacte} si
				$K \neq \emptyset$ et si, de tout recouvrement de $K$ par des ouverts de $X$, on peut extraire un sous-recouvrement fini.

				C'est-à-dire lorsque~:
				\begin{enumerate}
					\item $K \neq \emptyset$~;
					\item $\forall I \neq \emptyset : \forall \seq OiI$ ouverts de $X \tq K \subset \left(\bigcup_{i \in I}O_i\right) : \exists J \subset I$
						  fini $\tq K \subset \left(\bigcup_{j \in J}O_j\right)$.
				\end{enumerate}
				\end{déf}

				\begin{prp}[Compacts par Bolzano-Weierstrass] Soit $(X, d)$ un espace métrique. Une partie $K$ de $X$ est compacte si et seulement si~:

				\begin{enumerate}
					\item $K \neq \emptyset$~;
					\item de toute suite de points de $K$, on peut extraire une sous-suite convergente dans $K$.
				\end{enumerate}
				\end{prp}

				\grantedproof

				\begin{ex} L'ensemble $[0, 1]$ est un compact de $\R$. \end{ex}

				\begin{prp} Soit $(X, d)$, un espace métrique et $K \subset X$, une partie compacte. Alors $K$ est fermé et borné. \end{prp}

				\begin{proof} EXERCICE. (Absurde)
				\end{proof}

				\begin{prp} Soit $(E, n)$ un $\K$-e.v. normé de dimension finie. Alors les parties compactes de $E$ sont les parties fermées
				bornées non nulles. \end{prp}

				\grantedproof

			\subsubsection{Suites de Cauchy}
				\begin{déf} Soit $(X, d)$, un espace métrique. On dit que $\seq xn\N$ est \textit{de Cauchy} dans $X$ lorsque~:
				\begin{equation}
					\forall \varepsilon > 0 : \exists N \in \N \tq \forall m, n \geq N : d(x_n, x_m) < \varepsilon.
				\end{equation}
				\end{déf}

				\begin{prp} Si $\seq xn\N$ est convergente dans l'espace métrique $(X, d)$, alors elle est de Cauchy. \end{prp}

				\begin{proof} Si $x$ est la limite de la suite $(x_n)$, on pose $\varepsilon > 0$. Il existe $N \in \N$ tel que~:
				\begin{equation}
					\forall n \geq \N : d(x, x_n) < \frac \varepsilon2.
				\end{equation}

				Donc $\forall m, n \geq N : d(x_m, x_n) \leq d(x_m, x) + d(x, x_n) < \varepsilon$.
				\end{proof}

				\begin{déf} Un espace métrique $(M, d)$ est dit \textit{complet} quand toute suite de Cauchy de points de $X$ converge dans $X$. \end{déf}

				\begin{déf} Un espace vectoriel $E$ est dit \textit{de Banach} lorsque toute suite de Cauchy de vecteurs de $E$ converge dans $E$. \end{déf}

				\begin{rmq} On remarque que dans un espace métrique complet, une suite converge si et seulement si elle est de Cauchy
				(ce qui est entre autres le cas de $\R$).

				De plus, les suites de Cauchy permettent, dans des espaces complets, de montrer que des suites convergent sans connaitre leur limite. \end{rmq}

				\begin{ex} Les espaces métriques $(\R, \abs \cdot)$ et $(\C, \abs \cdot)$ sont des espaces de Banach. Et pour tout $p \in \Ns$ et
				$q \in \N$, les espaces métriques $(\R^q, \norm \cdot_p)$ et $(\C^q, \norm \cdot_p)$ sont des espaces de Banach. \end{ex}

			\subsubsection{Continuité}
				\begin{déf} Soient $(X, d_X)$ et $(Y, d_Y)$ deux espaces métriques. Une application $f : X \to Y$ est dite continue en $x_0 \in X$ lorsque~:
				\begin{equation}
					\forall \varepsilon > 0 : \exists \delta \gneqq 0 \tq \forall x \in X :
					\left(d_X(x, x_0) < \delta \Rightarrow d_Y(f(x), f(x_0)) < \varepsilon\right).
				\end{equation}

				On dit que $f$ est continue sur $A \subset X$ lorsque $f$ est continue en tout $a \in A$.
				\end{déf}

				\begin{prp} Une fonction $f : (X, d) \to (Y, d)$ est continue sur $X$ lorsque l'image réciproque par $f$ de $(Y, d)$ est un ouvert de $(X, d)$.
				\end{prp}

				\grantedproof

				\begin{prp} Une fonction $f : (X, d) \to (Y, d)$ est continue en $x_0 \in X$ si et seulement si l'image par $f$ de toute suite de points de
				$X$ convergente en $x_0$ est une suite convergente en $f(x_0)$. \end{prp}

				\grantedproof

				\begin{déf} Soit $f : (X, d) \to (Y, d)$. $f$ est dite \textit{lipschitzienne} de constante $K \geq 0$ lorsque~
				\begin{equation}
					\forall (x, y) \in X^2 : d(f(x), f(y)) \leq Kd(x, y).
				\end{equation}
				\end{déf}

				\begin{prp} Si $f : (X, d) \to (Y, d)$ est lipschitzienne, alors elle est continue sur $X$. \end{prp}

				\begin{proof} EXERCICE.
				\end{proof}

				\begin{déf} Soit $\seq ak\N$, une suite dans un espace métrique $(X, d)$. On dit que $(a_k)$ est \textit{presque nulle} lorsqu'il existe
				$N \in \N$ tel que $\forall n \geq N : a_n = 0$. \end{déf}

				\begin{ex}~
				\begin{itemize}
					\item Pour tout $i \in \N$, l'application $c_i : \C[x] \to \C : P = \sum_{k=0}^\pinfty a_kx^k \mapsto a_i$ est continue de
						  $(\C[x], \norm \cdot_\infty)$ dans $(\C, \abs \cdot)$. En effet, pour $i \in \N$, $P = \sum_{k=0}^\pinfty a_kx^k$, et
						  $Q = \sum_{k=0}^\pinfty b_kx^k$, on a~:
						  \begin{equation}
							\abs{c_i(P) - c_i(Q)} = \abs{a_i - b_i} \leq \norm{P-Q}_\infty = \max_{k \in \N}\abs{a_k - b_k}.
						\end{equation}

						  On en déduit que $c_i$ est lipschitzienne sur $\C[x]$ et donc continue sur $\C[x]$.
					\item Soit $n \in \N$. Posons~:
						  \begin{equation}
							P_n = \sum_{k=0}^n\frac 1{k!}x^k \in \C[x].
						\end{equation}

						  On observe que $\seq Pn\N$ est de Cauchy dans $(\C[x], \norm \cdot_\infty)$ car~:
						  \begin{equation}
							\norm{P_n - P_m}_\infty = \norm{\sum_{k=0}^n\frac 1{k!}x^k - \sum_{k=0}^m\frac 1{k!}x^k}_\infty.
						\end{equation}

						  On a alors~:
						  \begin{equation}
							\norm{P_n - P_m}_\infty = \norm {\sum_{k=\min\{m, n\}+1}^{\max\{m, n\}}\frac 1{k!}x^k}_\infty =
						  	\max_{\min\{m, n\}+1 \leq k \leq \max\{m, n\}}\frac 1{k!} = \frac 1{(\min\{m, n\}+1)!}.
						\end{equation}

						  Montrons que $\seq Pn\N$ est de Cauchy. Supposons (par l'absurde) que $\seq Pn\N$ converge vers $P \in (\C[x], \norm \cdot_\infty)$.
						  Notons $(a_k) \subset \C$, la suite presque nulle des coefficients de $P$. Pour $i \in \N$, on a $c_i(P) = \frac 1{i!}$ quand
						  $n \geq i$. Or par la propriété de Lipschitz, on sait que $c_i(P_n) \mconv n\pinfty{} c_i(P) = a_i$. Or $(a_k)$ est presque nulle et
						  $a_i = \frac 1{i!}$. Il y a donc contradiction. Donc $(P_n)$ ne converge pas dans $(\C[x], \norm \cdot_\infty)$. Dès lors,
						  $(\C[x], \norm \cdot_\infty)$ n'est pas complet.
				\end{itemize}
				\end{ex}

	\section{Convergence de suites de fonctions}
		\subsection[Convergence simple]{Convergence simple\protect\footnote{La convergence simple est la notion de convergence «~minimale~» que l'on va exiger.
		Il existe des convergences encore plus élémentaires (voir théorie de l'intégration de Lebesgue), mais qui se trouvent en dehors des objectifs du cours.}}
			\begin{déf} Soit $X$ un ensemble et $(Y, d)$ un espace métrique. On dit que la suite $\seqf fnx\N$ où $f_n : X \to (Y, d)$
			\textit{converge simplement} sur $X$ lorsque~:
			\begin{equation}
				\forall x \in X : \seqf fnx\N \text{converge dans } (Y, d).
			\end{equation}
			\end{déf}

			\begin{déf} Dans ce cas, la suite a pour limite simple la fonction~:
			\begin{equation}
				f : X \to (Y, d) : x \mapsto \lim_{n \to \pinfty}f_n(x)
			\end{equation}
			et est bien définie. Cela se note~:
			\begin{equation}
				f_n \xrightarrow[n \underset{X}{\to} \pinfty]{CVS} f \qquad\qquad \text{ou} \qquad\qquad f_n \CVS Xn\pinfty f.
			\end{equation}
			\end{déf}

			\begin{ex} Soient $X = [0, 1]$ et $Y = \R$. On pose $f_n(x) = x^n$ pour tout $n \in \N$.
			\begin{itemize}
				\item Si $x \in [0, 1)$, alors la suite $\seqf fnx\N$ est une suite géométrique de raison $x$ avec $\abs x < 1$ donc la suite converge vers 0~;
				\item si $x = 1$,a lors $f_n(x) = 1$ pour tout $n \in \N$. Donc la suite $\seqf fnx\N$ converge simplement sur $[0, 1]$ vers la fonction~:
					\begin{equation}
						f : [0, 1] \to \R : x \mapsto \begin{cases}0 &\text{ si } x  < 1 \\ 1 &\text{ si } x = 1\end{cases}.
					\end{equation}
			\end{itemize}
			\end{ex}

			\begin{rmq}~
			\begin{itemize}
				\item On a «~perdu~» la continuité des fonctions $f_n$ par passage à la limite~;
				\item ici, la convergence simple peut s'écrire ainsi, à l'aide de quantificateurs~:
					\begin{equation}
						\forall \varepsilon > 0 : \forall x \in X : \exists N \in \N \tq \forall n \geq N : d(f_n(x), f(x)) < \varepsilon.
					\end{equation}

					On remarque donc que $N$ dépend de $x$ (ordre des quantificateurs).
			\end{itemize}
			\end{rmq}

		\subsection{Convergence uniforme}
			\begin{déf} Soient $X$ un ensemble, $(Y, d)$ un espace métrique, et $f_n : X \to (Y, d)$. On dit que $(f_n)$ \textit{converge uniformément} sur $X$
			vers $f : X \to (Y, d)$ lorsque~:
			\begin{equation}
				\forall \varepsilon > 0 : \exists N \in \N \tq \forall n \geq N : \forall x \in X : d(f_n(x), f(x)) < \varepsilon.
			\end{equation}

			Cela se note~:
			\begin{equation}
				f_n \CVU Xn\pinfty f.
			\end{equation}
			\end{déf}

			\begin{rmq} La définition est très proche de la convergence simple. La différence étant que pour une convergence uniforme, il faut que
			$N \in \N$ ne dépende pas de la valeur de $x$. \end{rmq}

			\begin{prp} Soient $X$ un ensemble, $(Y, d)$ un espace métrique, $\seqf fnx\N$ une suite de fonctions de $X$ dans $(Y, d)$ et $f : X \to (Y, d)$.
			Si $(f_n)$ converge uniformément sur $X$ vers $f$, alors $(f_n)$ converge simplement sur $X$ vers $f$. \end{prp}

			\begin{proof} EXERCICE.
			\end{proof}

			\begin{ex} Prenons $X = \R = Y$ et pour tout $n \geq 1$, définissons $f_n(x) = \sqrt{x^2 + \frac 1n}$. Fixons $x \in \R$. On trouve alors~:
			\begin{equation}
				\seqf fnx\N = \left(\sqrt{x^2 + \frac 1n}\right)_{n \in \N} \to \sqrt {x^2} = \abs x.
			\end{equation}

			Donc~:
			\begin{equation}
				f_n \CVS Xn\pinfty \abs \cdot.
			\end{equation}
			\end{ex}

			\begin{thm} Soient $(X, d)$, $(Y, d)$ deux espaces métriques. Soient $f_n : X \to Y$, $a \in X$. On suppose~:
			\begin{itemize}
				\item $\exists f \tq f_n \CVU Xn\pinfty f$~;
				\item $\forall n \in \N : f_n$ est continue en $a$.
			\end{itemize}

			Alors $f$ est continue en $a$.
			\end{thm}

			\begin{proof}  Soit $\varepsilon > 0$. Par convergence uniforme des $f_n$, on sait~:
			\begin{equation}
				\exists N \in \N \tq \forall n \geq N : \forall x \in X : d(f_n(x), f(x)) < \frac \varepsilon3.
			\end{equation}

			De plus, la fonction $f_N$ est continue en $a$ par hypothèse. Dès lors, on sait qu'il existe $\delta$ tel que~:
			\begin{equation}
				\forall x \in X : d(x, a) < \delta \Rightarrow d(f_N(x), f_N(a)) < \frac \varepsilon3.
			\end{equation}

			Ainsi, prenons $x \in X$ tel que $d(x, a) < \delta$. On a alors~:
			\begin{equation}
				d(f(x), f(a)) \leq d(f(x), f_N(x)) + d(f_N(x), f(a)) \leq d(f(x), f_N(x)) + d(f_N(x), f_N(a)) + d(f_N(a), f(a)) \leq 3\frac \varepsilon3 = \varepsilon.
			\end{equation}
			\end{proof}

			\begin{cor} Si $f_n \in C^0(X, Y)$ et $f_n \CVU Xn\pinfty f$, alors $f \in C^0(X, Y)$. \end{cor}

			\begin{proof} Les fonctions $f_n$ sont continues en tout point et $f_n \CVU Xn\pinfty$ par hypothèse. Dès lors, pour tout point $a \in X$, par le
			théorème précédent, on peut dire $f$ continue en $a$. Dès lors $f \in C^0(X, Y)$.
			\end{proof}

		\subsection{L'espace $B(X, E)$}
			\begin{déf} Soient $X \neq \emptyset$ et $(E, \norm \cdot_E)$ un \evn. On note~:
			\begin{equation}
				B(X, E) \coloneqq \left\{f : X \to E \tq f \text{ est bornée sur } X\right\}.
			\end{equation}

			Pour $f \in B(X, E)$, on définit~:
			\begin{equation}
				\norm f_\infty \coloneqq \sup_{x \in X}\norm {f(x)}_E.
			\end{equation}
			\end{déf}

			\begin{prp} $\left(B(X, E), \norm \cdot_\infty\right)$ est un \evn. \end{prp}

			\begin{proof} EXERCICE.
			\end{proof}

			\begin{thm}\label{thm:BXEcpltssiEcplt} $\left(B(X, E), \norm \cdot_\infty\right)$ est complet si et seulement si $(E, \norm \cdot_E)$ est complet.
			\end{thm}

			\begin{proof} Supposons d'abord $\left(B(X, E), \norm \cdot_\infty\right)$ complet et montrons que $(E, \norm \cdot_E)$ est complet.

			Soit $(x_n)_n$ une suite de Cauchy d'éléments de $E$. Soit $(f_n)$ une suite de fonctions de $B(X, E)$ telle que~:
			\begin{equation}
				\forall n \in \N : \forall x \in X : f_n(x) = x_n.
			\end{equation}

			Puisque $(x_n)$ est de Cauchy, on sait que~:
			\begin{equation}
				\forall \varepsilon > 0 : \exists N \in \N \tq \forall m, n \geq N : d(x_m, x_n) < \varepsilon.
			\end{equation}
			Or, avec $a \in X$ fixé, on peut alors dire $\forall m, n \geq N : d(f_m(a), f_n(a)) < \varepsilon$, et ce peu importe le $a$ choisi (car les
			$f_n$ sont constantes). On a donc $(f_n)$ une suite de Cauchy dans $B(X, E)$ car $d(f_m(a), f_n(a)) = \norm {f_m - f_n}_\infty$. Or, par
			complétude de $B(X, E)$, on sait qu'il existe $f \in B(X, E)$ telle que $f_n \to f$. La fonction $f$ est également constante. Posons $L$ la seule
			image de $f$. Soit $\varepsilon > 0$. On sait qu'il existe $n \in \N$ tel que $\forall n \geq N : \norm {f_n - f}_\infty < \varepsilon.$

			Or~:
			\begin{equation}
				\varepsilon > \norm {f_n - f}_\infty = \sup_{x \in X} \norm {f_n(x) - f(x)}_E = \norm {f_n(a) - f(a)}_E = \norm {x_n - L}.
			\end{equation}
			Dès lors, on sait que $(x_n)$ converge dans $E$.

			Montrons maintenant que si $(E, \norm \cdot_E)$ est complet, alors $(B(X, E), \norm \cdot_\infty)$ est complet également.

			Soit $(f_n)_n$ une suite de Cauchy de fonctions de $(B(X, E), \norm \cdot_\infty)$. Fixons $\varepsilon > 0$. Il existe alors $N \in \N$ tel que~:
			\begin{equation}
				\forall m, n \geq N : \norm {f_m - f_n}_\infty < \varepsilon.
			\end{equation}

			Soit $x \in X$. On observe que~:
			\begin{equation}
				\forall m, n \geq N : \norm {f_n(x) - f_m(x)}_E \leq \norm {f_n - f_m}_\infty < \varepsilon.
			\end{equation}
			La suite $(f_n(x))_n$ est donc une suite de Cauchy dans $(E, \norm \cdot_E)$. Par complétude de $E$, on sait qu'il existe $f(x) \in E$ tel que
			$f_n(x) \to f(x)$. Montrons maintenant que $f \in B(X, E)$.

			La suite $(f_n)_n$ est de Cauchy et donc bornée. Soit $M \gneqq 0$ tel que $\forall n \in \N : \norm {f_n}_\infty < M$. Passons à la limite dans
			$(B(X, E)$. On a alors~:
			\begin{equation}
				\forall n \in \N : \forall x \in X : \norm {f(x)}_E < M.
			\end{equation}

			Ainsi, $f \in B(X, E)$ par définition.

			Soit alors $\varepsilon > 0$. Pour tout $m, n \in \N$ et pour tout $x \in X$, on a~:
			\begin{equation}
				\norm {f_n(x) - f_m(x)}_E  \leq \norm {f_n - f_m}_\infty \leq \varepsilon.
			\end{equation}

			Passons alors à la limite en $m$, ce qui donne~:
			\begin{equation}
				\norm {f_n(x) - f(x)}_E \leq \norm {f_n - f}_\infty \leq \varepsilon.
			\end{equation}
			Dès lors~:
			\begin{equation}
				\forall n \geq N : \norm {f_n - f}_\infty \leq \varepsilon.
			\end{equation}
			\end{proof}

			\begin{rmq} Quand $X \neq \emptyset$ et $Y = E$ est un \evn, on a~:
			\begin{equation}
				f_n \CVU Xn\pinfty f \iff
				\begin{cases}&\exists N \in \N \tq \forall n \geq N : f_n - f \in B(X, E) \\ &f_n - f \xrightarrow[n \to \pinfty]{\norm \cdot_\infty} 0\end{cases}.
			\end{equation}
			\end{rmq}

		\subsection{Convergence uniforme sur tout compact}
			\begin{déf} Soit $X$, une partie non-vide d'un \evn de dimension finie $(E, \norm \cdot_E)$. Soit $(Y, d)$ un espace métrique.
			Une suite $f_n : X \to Y$ converge uniformément vers $f : X \to Y$ sur tout compact lorsque~:
			\begin{equation}
				\forall \text{ compact } K \subset X : \restr {f_n}K \CVU Kn\pinfty \restr fK.
			\end{equation}

			Cela se note~:
			\begin{equation}
				f_n \CVUc Xn\pinfty f.
			\end{equation}
			\end{déf}

			\begin{rmq} ici, la notation $\restr fK$ désigne la restriction de la fonction $f$ au sous-domaine $K$.
			\end{rmq}

			\begin{prp}\label{prp:cvuccontinues} Si la suite $f_n$ converge uniformément sur tout compact de $X$ et si toutes les fonctions $f_n$ sont continues
			en $a \in X$, alors $f$ est continue en $a$.
			\end{prp}

			\begin{proof} EXERCICE.
			\end{proof}

			\begin{ex} Prenons $X = Y = \R$. On définit $f_n(x) = \sum_{k=0}^n\frac {x^k}{k!}$. On a alors $f_n \CVS Xn\pinfty \exp$.

			De plus~:
			\begin{equation}
				\norm {f_n - \exp}_\infty = \sup_{x \in \R} \abs {\sum_{k=0}^n\frac {x^k}{k!} - \exp(x)} = \pinfty.
			\end{equation}
			Donc $f_n$ ne converge pas uniformément vers $\exp$. Montrons maintenant que $f_n$ converge uniformément vers $\exp$ sur tout compact de $\R$.
			Soit $K \subset \R$ un compact. On sait qu'il existe $a, b \in \R, a < b$ tels que $K \subset [a, b]$. Pour $x \in [a, b]$, par Lagrange, on a~:
			\begin{equation}
				\exp(x) - \sum_{k=0}^n\frac {x^k}{k!} = \frac {x^{n+1}}{(n+1)!}\exp(c_x),
			\end{equation}
			avec $c_x \in [a, b]$.

			Ainsi~:
			\begin{equation}
				\abs {\exp(x) - \sum_{k=0}^n\frac {x^k}{k!}} \leq \frac {(b-a)^{n+1}}{(n+1)!}\sup_{x \in [a, b]}\exp(x) \xrightarrow[n \to \pinfty]{} 0.
			\end{equation}

			D'où $f_n \xrightarrow[n \to \pinfty]{[a, b]} f$ et donc la convergence uniforme sur tout compact de $f_n$ vers $f$.
			\end{ex}

	\section{Suites de fonctions et opérations d'intégration et de dérivation}
		\subsection{Passage à la limite dans une intégrale de Riemann}
			Soit $X$ un pavé de $\R^d$ (donc $X = \prod_{i=1}^d[a_i, b_i]$ avec $a_i < b_i \forall i \in \{1, \ldots, d\}$).

			\begin{thm} Soit $f_n : X \to \R$ intégrables au sens de Riemann sur $X$. Supposons $f_n \CVU Xn\pinfty f$. Alors~:
			\begin{itemize}
				\item $f$ est intégrable au sens de Riemann~;
				\item la suite $\left(\int_X f_n(x)\dif x\right)_n$ converge vers $\int_Xf(x)\dif x$.\footnotemark
			\end{itemize}
			\end{thm}		\footnotetext{Cela veut dire que~:\begin{equation}\lim_{n \to \pinfty}\int_X f_n(x)\dif x = \int_X \lim_{n \to \pinfty} f_n(x)\dif x.\end{equation}}

			\begin{proof} On note $\mathcal E(X, \R) \coloneqq \{f : X \to \R \tq f \text{ est élémentaire}\}$.

			Soit $\varepsilon > 0$. Par la convergence uniforme, on sait qu'il existe $N \in \N$ tel que~:
			\begin{equation}
				\forall n \geq N : \norm {f_n - f}_\infty \leq \frac \varepsilon{4\abs X},
			\end{equation}
			où $\abs X = \prod_{i=1}^d(b_i - a_i)$.

			Par intégrabilité de $f_N$, on sait qu'il existe $\varphi, \psi \in \mathcal E(X, \R)$ telles que~:
			\begin{equation}
				\psi \leq f_N \leq \varphi \qquad\qquad \text{ et } \qquad\qquad \int_X(\varphi - \psi) < \frac \varepsilon2.
			\end{equation}

			On a alors~:
			\begin{equation}
				\psi - f_N \leq f \leq \varphi + f_N,
			\end{equation}
			ou encore~:
			\begin{equation}
				\psi - \frac \varepsilon{4\abs X} \leq f \leq \varphi + \frac \varepsilon{4\abs X}.
			\end{equation}

			En posant $\overline \psi \coloneqq \psi - \frac \varepsilon{4\abs X}$ et $\overline \varphi \coloneqq \varphi + \frac \varepsilon{4\abs X}$, on a
			$\overline \psi, \overline \varphi \in \mathcal E(X, \R)$. De plus~:
			\begin{equation}
				\int_X(\overline \varphi - \overline\psi) = \int_X\left(\varphi + \frac \varepsilon{4\abs X} - \left(\psi - \frac \varepsilon{4\abs X}\right)\right)
				= \frac \varepsilon{2\abs X}\abs X + \int_X (\psi - \varphi) < 2\frac \varepsilon2 = \varepsilon.
			\end{equation}

			Dès lors, on en déduit $f$ intégrable au sens de Riemann.

			Fixons $\varepsilon > 0$. Par convergence uniforme de $f_n$ vers $f$ sur $X$, on sait que~:
			\begin{equation}
				\exists N \in \N \tq \forall n \geq N : \norm {f_n - f}_\infty < \frac \varepsilon{\abs X}
			\end{equation}

			Et donc~:
			\begin{equation}
				\abs {\int_X f_n(x)\dif x - \int_X f(x)\dif x} = \abs {\int_X (f_n-f)(x)\dif x} \leq \abs {\int_X\norm {f_n-f}_\infty\dif x}
				= \abs X\norm{f_n-f}_\infty \leq \abs X\frac \varepsilon{\abs X} = \varepsilon.
			\end{equation}

			Finalement, la suite $\left(\int_X f_n(x)\dif x\right)_n$ converge dans $\R$ vers $\int_X f(x)\dif x$.
			\end{proof}

			\begin{rmq}~
			\begin{enumerate}
				\item Il est possible d'avoir les résultats sans vérifier les hypothèses. Par exemple, $X = [0, 1] \subset \R = Y$, avec $f_n(x) = x^n$.
					On sait que $f_n \CVS Xn\pinfty 1_{\{x=1\}}$ et que la convergence n'est pas uniforme sur $[0, 1]$. On remarque alors~:
					\begin{equation}
						\lim_{n\to\pinfty}\int_0^1f_n(x)\dif x = \lim_{n\to\pinfty}\frac 1{n+1} = 0 = \int_0^11_{\{x=1\}}(x)\dif x
					  	= \int_0^1\lim_{n\to\pinfty}f_n(x)\dif x~;
					\end{equation}
				\item si les hypothèses ne sont pas vérifiées, la conclusion peut être fausse. Par exemple, $X = [0, 1] \subset \R = Y$. On définit ($n \geq 1$)~:
					\begin{equation}
						f_n(x) = \begin{cases}
					  	2n\alpha_nx &\text{ si } 0 \leq x < \frac 1{2n} \\
						2\alpha_n - 2n\alpha_nx &\text{ si }\frac 1{2n} \leq x < \frac 1n \\
						0 &\text{ sinon}\end{cases},
					\end{equation}
					  où $\alpha_n \in \R^+_0 \tq \forall n \in \N^* : \int_0^1f_n(x)\dif x = 1$, donc $\alpha_n = 2n$.

					  On a alors $f_n \CVS Xn\pinfty 0 = f$. La fonction nulle $0(x)$ est intégrable au sens de Riemann sur $[0, 1]$.

					  Finalement, on a~:
					\begin{equation}
						\int_0^1\lim_{n \to \pinfty}f_n(x)\dif x = \int_0^1f(x)\dif x = 0 \qquad \text{ et } \qquad \lim_{n \to \pinfty}\int_0^1f_n(x)\dif x = 1.
					\end{equation}

					  Dans ce cas précis, on ne peut pas passer à la limite.
			\end{enumerate}
			\end{rmq}

		\subsection{Passage à la limite dans une dérivation ordinaire ou partielle}
			\begin{thm} Soit $\emptyset \neq \Omega \subset \R^d$, un ouvert. Soient $f_n : \Omega \to \R$, toutes de classe $C^1$ sur $\Omega$. Supposons~:
			\begin{itemize}
				\item $f_n \CVSc \Omega n\pinfty f$~;
				\item $\forall i \in \intint 1d : \pd {f_n}{x_i} \CVU \Omega n\pinfty g_i$.
			\end{itemize}

			Alors~:
			\begin{enumerate}
				\item $f \in C^1(\Omega, \R)$~;
				\item $\forall i \in \intint 1d : \pd f{x_i} = \lim_{n \to \pinfty} \pd {f_n}{x_i}$ dans $\Omega$~;
				\item $f_n \CVUc \Omega n\pinfty f$.
			\end{enumerate}
			\end{thm}

			\begin{proof} Soit $x \in \Omega$. Par ouverture de $\Omega$, on sait qu'il existe $\delta \gneqq 0$ tel que $B(x, \delta[ \subset \Omega$.
			On en déduit que $B(x, \frac \delta2]$ est incluse dans $B(x, \delta[$. Or $B(x, \frac \delta2]$ est fermé et borné par définition.
			$B(x \frac \delta2]$ est donc un compact de $\Omega$.

			Soient $i \in \intint 1d$ et $h \in [\pm \frac \delta2]$\footnote{Pour $\delta \gneqq 0$, les notations $[\pm \delta]$ et $[\gamma \pm \delta]$
			désignent respectivement les ensembles $[-\delta, +\delta]$ et $[\gamma-\delta, \gamma+\delta]$}. On a alors~:
			\begin{equation}
				\forall n \in \N : f_n(x + he_i) = f_n(x) + \int_0^h\pd f{x_i}(x+s e_i)\dif s.
			\end{equation}

			Or comme $f_n \CVSc \Omega n\pinfty f$ et pour tout $i$, $\pd {f_n}{x_i}$ converge uniformément vers $g_i$ sur $B(x, \frac \delta2]$, il vient~:
			\begin{equation}
				f_n(x + he_i) = f_n(x) + \int_0^hg_i(x+se_i)\dif s,
			\end{equation}
			où $\{e_1, \ldots e_d\}$ est la base canonique de $\R^d$.

			On en déduit alors que $f$ admet une dérivée partielle par rapport à $x_i$ en $x$~:
			\begin{equation}
				\pd f{x_i}(x) = g_i(x).
			\end{equation}

			De plus, les $f_n$ sont $C^1(\Omega, \R)$, et donc les dérivées partielles $\pd {f_n}{x_i}$ sont $C^0(\Omega, \R)$ pour tout $i$ et par convergence
			uniforme sur les compacts, $g_i \in C^0(\Omega, \R)$ (Proposition~\ref{prp:cvuccontinues}).

			On en déduit alors $f \in C^1(\Omega, \R)$ avec $\pd f{x_i} = g_i$ pour tout $i$ dans $\intint 1d$ (points 1 et 2 à montrer).

			Il reste donc à montrer le point 3.

			Soit $K \subset \Omega$, un compact. Par ouverture de $\Omega$, on sait que pour tout $a \in K$, on a~:
			\begin{equation}
				\exists r_a \gneqq 0 \tq B(a, r_a[ \subset \Omega.
			\end{equation}

			Dès lors, on sait que~:
			\begin{equation}
				K \subset \bigcup_{a \in K}B\left(a, \frac {r_a}2\right[.
			\end{equation}

			Par compacité, on sait qu'il existe un sous-recouvrement fini de $K$, c'est-à-dire $p \in \N^*$ et $(a_i)_{i \in \intint 1p} \in K^p$ tel que~:
			\begin{equation}
				K \subset \bigcup_{i=1}^pB\left(a_i, \frac {r_{a_i}}2\right[.
			\end{equation}

			Par convergence simple de $f_n$ vers $f$, et puisque les $a_i$ sont en nombre fini, on peut alors exprimer~:
			\begin{equation}
				\forall \varepsilon > 0 : \exists N \in \N \tq \forall n \geq N : \forall i \in \intint 1p : \abs {f_n(a_i) - f(a_i)} < \varepsilon.
			\end{equation}

			Fixons donc $\varepsilon > 0$, soit $N$ correspondant et soit $x \in K$. Il existe $k \in \intint 1p$ tel que $x \in B(a_k, \frac {r_{a_k}}2[$ car
			les boules ouvertes forment un recouvrement de $K$. On a alors~:
			\begin{equation}
				f_n(x) = f_n(a_k) + \int_0^1\scpr {\nabla f_n(a_k + t(x - a_k))}{(x - a_k)}\dif t,
			\end{equation}
			et~:
			\begin{equation}
				f(x) = f(a_k) + \int_0^1\scpr {\nabla f(a_k + t(x - a_k))}{(x - a_k)}\dif t.
			\end{equation}

			Par différence, on a~:
			\begin{equation}
				\abs {f_n(x) - f(x)} \leq \abs {f_n(a_k) - f(a_k)} + \int_0^1\norm {\nabla f_n(a_k + t(x - a_k))
				- \nabla f(a_k + t(x - a_k))}\dif t \cdot \norm {x - a_k}.
			\end{equation}

			Par convergence uniforme sur $\left(\bigcup_{i=1}^pB(a_i, \frac {r_{a_i}}2]\right)$ de $\nabla f_n$ vers $\nabla f$, on sait que~:
			\begin{equation}
				\exists N \in \N \tq \forall n \geq N : \norm {\nabla f_n - \nabla f}_{\infty, \bigcup_{i=1}^pB\left(a_i, \frac {r_{a_i}}2\right]} <
				\frac {2\varepsilon}{\displaystyle \max_{i \in \intint 1p} r_{a_i}}.
			\end{equation}

			Finalement, on a~:
			\begin{equation}
				\forall x \in K : \forall n \geq N : \norm {f_n(x) - f(x)} \leq
				\varepsilon + \frac {r_{a_k}}{2} \cdot \frac {2\varepsilon}{\displaystyle \max_{i \in \intint 1d}r_{a_i}} \leq 2\varepsilon.
			\end{equation}

			Ainsi, pour $n \geq N$, on a~:
			\begin{equation}
				\norm {f_n - f}_{\infty, K} \leq 2\varepsilon.
			\end{equation}
			\end{proof}

			\begin{rmq} Ce théorème est vrai en particulier pour $d = 1$, et $\Omega$ un segment de $\R$. \end{rmq}

			\begin{ex}[Contre-exemples ne vérifiant pas les hypothèses donc ne pouvant faire passer la limite dans la dérivation]
			\begin{enumerate}
				\item $f_n(x) = \frac {\sin(n^2x)}n, n \geq 1, x \in X = \R$. On a donc $f_n \CVU \R n\pinfty 0 = f$ car $\abs {f_n(x) - f(x)} \leq \frac 1n \to 0$
					  avec $\frac 1n$ ne dépendant pas de $x$. Les $f^n$ sont $C^\infty(\R)$ et sont donc dérivables~:
					  \begin{equation}
						\od {f_n}x = n\cos(n^2x),
					\end{equation}
					  et donc~:
					  \begin{equation}
						\od {f_n}x\sVert[3]_{x=0} = n \to \pinfty.
					\end{equation}

					  On en déduit~:
					  \begin{equation}
						\lnot\left(\od {f_n}x \xrightarrow[n \to \pinfty]{} \od fx\right).
					\end{equation}

				\item $f_n(x) = \frac {x^n}n, n \geq 1, x \in X = [0, 1]$. On a $f_n \CVU Xn\pinfty 0$. Puisque les $f_n$ sont $C^\infty(X, \R)$, on a~:
					  \begin{equation}
						\od {f_n}x = x^{n-1} \xrightarrow[n \to \pinfty]{} 1_{\{x=1\}},
					\end{equation}
					  qui n'est pas une dérivée. À nouveau, la suite des dérivées des $f_n$ ne tend pas vers la dérivée de $f$.
			\end{enumerate}
			\end{ex}

			\begin{cor} Soient $p \in \N^*, \Omega \subset \R^d$, un ouvert non-vide. Soit $f_n : \Omega \to \R$ de classe $C^p(\Omega, \R)$. Supposons~:
			\begin{itemize}
				\item $\displaystyle \forall q \in \intint 0{p-1} : \forall (i_1, \ldots, i_q) \in \intint 1d^q :
					\frac {\partial^qf_n}{\partial x_{i_1}\ldots\partial x_{i_q}} \CVU \Omega n\pinfty g_{i_1, \ldots, i_q}$~;
				\item $\displaystyle \forall (i_1, \ldots i_p) \in \intint 1d^p :
					\frac {\partial^pf_n}{\partial x_{i_1}\ldots\partial x_{i_p}} \CVU \Omega n\pinfty g_{i_1, \ldots i_q}$.
			\end{itemize}
			Alors~:
			\begin{enumerate}
				\item $f = g_\emptyset \in C^p(\Omega, \R)$~;
				\item $\displaystyle \forall q \in \intint 1p : \forall (i_1, \ldots, i_q) \in \intint 1d^q :
					\frac {\partial^qf}{\partial x_{i_1}\ldots\partial x_{i_q}} = g_{i_1, \ldots, i_q}$~;
				\item $\forall q \in \intint 0{p-1} : \forall (i_1, \ldots, i_q) \in \intint 1d^q :
					\frac {\partial^qf_n}{\partial x_{i_1}\ldots\partial x_{i_q}} \CVUc \Omega n\pinfty g_{i_1, \ldots, i_q}$.
			\end{enumerate}
			\end{cor}

			\begin{proof} EXERCICE. (Récurrence sur $p$ par le résultat précédent)
			\end{proof}

	\section{Séries de fonctions}
		\subsection{Retranscription des résultats sur les suites}
			\begin{déf} Soit $u_n : X \to Y$ où $X \neq \emptyset$ et $Y$ est un \evn. On appelle \textit{somme partielle d'ordre $n$ de la série de terme
			général $u_n$} la fonction suivante~:
			\begin{equation}
				S_n : X \to Y : x \mapsto \sum_{k=0}^nu_k(x).
			\end{equation}

			On dit que la série de terme général $u_n$ \textit{converge simplement} sur $X$ lorsque $S_n$ converge simplement sur $X$. De même pour la
			\textit{convergence uniforme} sur $X$ et la \textit{convergence uniforme sur tout compact} de $X$.
			\end{déf}

			\begin{thm}\label{thm:fncvusériecvg} Soient $(X, d)$ un espace métrique et $Y$ un \evn. Soit $u_n : X \to Y$. Si $\forall n \in \N : u_n$ est continue en
			$a \in X$ et si la série de terme général $u_n$ converge uniformément sur $X$, alors~:
			\begin{equation}
				S \coloneqq \lim_{n \to \pinfty}S_n \text{ est continue en } a.
			\end{equation}
			\end{thm}

			\begin{proof} EXERCICE.
			\end{proof}

			\begin{thm} Soit $X \neq \emptyset$, un pavé de $\R^d$ et soit $u_n : X \to Y \tq \sum_{n \geq 0}u_n \CVU Xn\pinfty S$ avec $u_n$ intégrable au
			sens de Riemann pour tout $n$. Alors~:
			\begin{enumerate}
				\item $S$ est intégrable au sens de Riemann sur $X$~;
				\item la suite $\int_XS_n(x)\dif x$ converge vers $\int_XS(x)\dif x$.
			\end{enumerate}
			\end{thm}

			\begin{proof} EXERCICE.
			\end{proof}

			\begin{thm} Soit $\Omega \subset \R^d$, un ouvert non-nul et soit $u_n : \Omega \to \R$ de classe $C^p(\Omega, \R)$ avec $p \in \N^*$.
			Supposons~:
			\begin{itemize}
				\item $\sum_{n \geq 0}u_n \CVS \Omega n\pinfty S$~;
				\item $\displaystyle \forall \alpha \in \N^d \tq \abs \alpha \coloneqq \sum_{i=1}^d\alpha_i \leq p :
					\sum_{n \geq 0}\frac {\partial^{\abs \alpha}}{\partial x_1^{\alpha_1}\ldots\partial x_d^{\alpha_d}}u_n \CVS \Omega n\pinfty s_{\alpha}$~;
				\item lorsque $\abs \alpha = p$, la convergence ci-dessus est uniforme sur les compacts de $\Omega$.
			\end{itemize}

			Alors~:
			\begin{enumerate}
				\item $S \in C^p(\Omega, \R)$~;
				\item $\displaystyle \forall \alpha \in \N^d :
					\frac {\partial^{\abs \alpha}}{\partial x_1^{\alpha_1}\ldots\partial x_d^{\alpha_d}}S =
						\sum_{n \geq 0}\frac {\partial^{\abs \alpha}}{\partial x_1^{\alpha_1}\ldots\partial x_d^{\alpha_d}}u_n$~;
				\item Il y a convergence uniforme sur les compacts de $\Omega$ des séries de dérivées partielles d'ordre $0$ à $p-1$.
			\end{enumerate}
			\end{thm}

		\subsection{Convergence normale}
			\begin{déf} Soient $X \neq \emptyset$ et $Y$ un \evn. On dit que la série de terme général $u_n : X \to Y$ converge normalement sur $X$ lorsque~:
			\begin{equation}
				\sum_{n \geq 0}\norm {u_n}_{\infty, X} < \pinfty.
			\end{equation}
			\end{déf}

			\begin{déf} On dit que la série de terme général $u_n : X \to Y$ vérifie le critère de Weierstrass lorsqu'il existe $\seq Mn\N$ telle que~:
			\begin{itemize}
				\item $\displaystyle \forall n \in \N : \forall x \in X : \norm {u_n(x)}_E \leq M_n$~;
				\item $\displaystyle \sum_{n \geq 0}M_n < \pinfty$.
			\end{itemize}
			\end{déf}

			\begin{rmq} $\sum_{n \geq 0}u_n$ converge normalement sur $X$ si et seulement si elle vérifie le critère de Weierstrass.
			\end{rmq}

			\begin{prp} Si $(E, \norm \cdot_E)$ est un \evnc, et si $\sum_{n \geq 0}u_n$ converge normalement sur $X$ alors $\sum_{n \geq 0}$
			converge uniformément sur $X$.
			\end{prp}

			\begin{proof} Écrivons $S_n = \sum_{k=0}^nu_k \in B(X, E)$. Par convergence normale, la suite $\sigma_n = \sum_{k \geq 0}\norm {u_k}_{\infty, X}$
			converge. De plus, $(\sigma_n)$ est de Cauchy dans $\Rp$. Donc~:
			\begin{equation}
				\forall \varepsilon > 0 : \exists N \in \N \tq \forall n, m \geq N : \abs {\sigma_n - \sigma_m} < \varepsilon.
			\end{equation}

			Ainsi~:
			\begin{align}
				\norm {S_n - S_m}_{\infty, X} &= \norm {\sum_{k=\min(m, n)+1}^{\max(m, n)}u_k}_{\infty, X} \leq \sum_{k=\min(m, n)+1}^{\max(m, n)}\norm {u_k}_{\infty, X} \\
				&\leq \abs {\sigma_n - \sigma_m} < \varepsilon.
			\end{align}

			Donc $(S_n)_n$ est de Cauchy dans $\left(B(X, E), \norm \cdot_{\infty, X}\right)$. Cet espace est complet car $(E, \norm \cdot_E)$ l'est
			(Théorème~\ref{thm:BXEcpltssiEcplt}). Et donc, $(S_n)_n$ converge uniformément sur $X$.
			\end{proof}

			\begin{rmq} On peut écrire~:
			\begin{equation}
				CVN \underset {\text{complet}}\Rightarrow CVU \Rightarrow CVS,
			\end{equation}
			mais les réciproques sont habituellement fausses.
			\end{rmq}
% 3rd class
			\begin{cor} Si $f_n : X \to Y$ (avec $Y$ un \evnc) est $\tq$~:
			\begin{align}
				\begin{cases}
					&\forall n \in \N : \exists M_n \geq 0 \tq \norm {f_{n+1} - f_n}_{\infty, X} \leq M_n \\
					&\sum_{m \geq 0}M_m < \pinfty,
				\end{cases}
			\end{align}
			alors $\seq fn\N$ converge uniformément sur $X$.
			\end{cor}

			\begin{proof} La série de terme général $u_n = f_{n+1}-f_n$ converge normalement sur $X$ car elle vérifie le critère de Weierstrass sur $X$. Par
			complétude de $Y$, la série $\sum u_n$ converge uniformément sur $X$.

			Pour $n \in \N$, on calcule~:
			\begin{equation}
				S_n = \sum_{k=0}^nu_k = f_{n+1}-f_0.
			\end{equation}
			Donc la suite $\seq fn\N$ converge uniformément sur $X$.
			\end{proof}

		\subsection{Transformation d'Abel}
			\begin{thm} Soient $Y$ un \evnc, $\seq gn\N \in Y^{\N}$, et $\seq fn\N \in (\Rp)^{\N}$. Supposons~:
			\begin{numcases}
				\exists M \gneqq 0 \tq \forall n \in \N : \norm {\sum_{k=0}^ng_k}_Y \leq M \\
				f_n  \mconv n\pinfty{} 0\text{ en décroissant.}
			\end{numcases}
			Alors $f_ng_n$ est le terme général d'une série convergente.
			\end{thm}

			\begin{proof} On pose~:
			\begin{equation}
				\forall k \in \Ns : G_k = \sum_{m=0}^kg_m
			\end{equation}

			Calculons, pour $n, p \in \Ns$~:
			\begin{align}
				S_{n+p} - S_n &= \sum_{k=n+1}^{n+p}f_kg_k = \sum_{k=n+1}^{n+p}f_k(G_k - G_{k-1}) \\
				&= \sum_{k=n+1}^{n+p}f_kG_k - \sum_{k=n+1}^{n+p}f_kG_{k-1} = \sum_{k=n+1}^{n+p}f_kG_k - \sum_{k=n}^{n+p-1}f_{k+1}G_k \\
				&= \sum_{n+1}^{n+p-1}G_k(f_k - f_{k+1}) + f_{n+p}G_{n+p} - f_{n+1}G_n.
			\end{align}

			Ainsi~:
			\begin{align}
				\norm {S_{n+p} - S_n}_Y &\leq M\sum_{k=n+1}^{n+p}(f_k-f_{k+1}) + Mf_{n+p} + Mf_{n+1} = M(f_{n+1}-f_{n+p}) + M(f_{n+p} + f_{n+1}) \\
				&= 2Mf_{n+1} \xrightarrow [n \to \pinfty]{} 0,
			\end{align}
			et la convergence ne dépend pas de $p$. On a alors que la suite $\seq Sn\N$ est de Cauchy, et par complétude de $Y$, $S_n$ converge, ce qui implique que la
			série de terme général $f_ng_n$ converge.
			\end{proof}

			\begin{thm} Soient $Y$ un \evnc et $X \neq \emptyset$. Soient~:
			\begin{align}
				g_n &: X \to Y, \\
				f_n &: X \to \Rp.
			\end{align}

			Supposons~:
			\begin{itemize}
				\item qu'il existe $M \gneqq 0$ tel que $\forall n \in \N : \norm {\sum_{k=1}^ng_k}_{\infty, X} \leq M$~;
				\item que $f_n \CVU Xn\pinfty 0$ en décroissant.
			\end{itemize}

			Alors $f_ng_n$ est le terme général d'une série qui converge uniformément sur $X$.
			\end{thm}

			\begin{proof} Par la preuve précédente, on a~:
			\begin{equation}
				\norm {S_{n+p} - S_n}_Y \leq 2Mf_{n+1}(x) \leq 2M\norm {f_{n+1}}_{\infty, X}.
			\end{equation}
			On déduit donc~:
			\begin{equation}
				\norm {S_{n+p}-S_n}_{\infty, X} \leq 2M\norm {f_{n+1}}_{\infty, X}.
			\end{equation}

			On sait donc que $\seq Sn\N$ est de Cauchy dans $B(X, Y)$. Par complétude de $Y$, la série de terme général $f_ng_n$ converge uniformément sur $X$.

			On remarque en effet que les $S_n$ sont bornés car $f_n \CVU Xn\pinfty 0$, ce qui implique $\norm {f_n}_{\infty, X}$ bornée, au moins à partir d'un certain
			$n \in \N$. De plus, $\norm {\sum_kg_k} < M$ assure que $g_k$ est uniformément bornée.
			\end{proof}

		\subsection{Exemple d'une fonction continue sur $\R$ nulle part dérivable}
			Considérons la fonction $\varphi : \R \to \R : x \mapsto \abs x$ sur $[-1, 1]$ et 2-périodique. La fonction $\varphi$ est continue sur $\R$. Posons~:
			\begin{equation}
				\forall k \in \N : u_k : \R \to \R : x \mapsto \left(\frac 34\right)^k\varphi(4^kx).
			\end{equation}

			On sait que $\forall k \in \N : \norm {u_k}_\infty = \left(\frac 34\right)^k \in [0, 1]$. Ainsi, la série de terme général $u_k$ converge normalement
			sur $\R$ par le critère de Weierstrass. Par le Théorème~\ref{thm:fncvusériecvg}, la fonction~:
			\begin{equation}
				f : \R \to \R : x \mapsto \sum_{k \geq 0}u_k(x)
			\end{equation}
			est continue sur $\R$.

			Montrons maintenant la fonction $f$ n'est jamais dérivable.

			Construisons $\alpha_n$ et $\beta_n$ tels que~:
			\begin{align}\begin{cases}
				&\forall n \in \N : \alpha_n \leq x \leq \beta_n, \\
				&\beta_n - \alpha_n \xrightarrow[]{} 0, \\
				&\forall n \in \N : \abs {\frac {f(\beta_n) - f(\alpha_n)}{\beta_n - \alpha_n}} \geq \frac 123^n.
			\end{cases}\end{align}

			Soit $n \in \Ns$. Choisissez $p \in \Z$ tel que $p = \floor {4^nx}$ (et donc $p \leq 4^nx < p+1$). Posons $\alpha_n = \frac p{4^n}$ et
			$\beta_n = \frac {p+1}{4^n}$. On a alors~:
			\begin{equation}
				f(\beta_n) - f(\alpha_n) = \sum_{k \geq 0}\left(\varphi(4^k\beta_n)\left(\frac 34\right)^k - \varphi(4^k\alpha_n)\left(\frac 34\right)^k\right)
				= \sum_{k \geq 0}\left(\frac 34\right)^k\left(\varphi(4^k\beta_n) - \varphi(4^k\alpha_n)\right)
			\end{equation}

			On observe que~:
			\begin{itemize}
				\item si $k \lneqq n$, alors $4^k\beta_n = 4^{k-n}(p+1)$ et $4^k\alpha_n = 4^{k-n}p$. Puisque $\varphi$ est lipschitzienne de constante 1, on a~:
					\begin{equation}
						\varphi(4^k\beta_n) - \varphi(4^k\alpha_n) \leq 4^{k-n}(p+1-p) = 4^{k-n}~;
					\end{equation}
				\item si $k = n$, alors $\abs {\varphi(4^k\beta_n) - \varphi(4^k\alpha_n)} = 1$~;
				\item si $k \gneqq n$, alors $4^k\alpha_n = 4^{k-n}p \in 4\Z \subset 2\Z$ donc $\varphi(4^k\alpha_n) = 0$.
					De même, on a $\varphi(4^k\beta_n) = 0$.
			\end{itemize}

			Ainsi~:
			\begin{equation}
				f(\beta_n) - f(\alpha_n) = \sum_{k=0}^{n-1}\left(\frac 34\right)^k\left(\varphi(4^k\beta_n) - \varphi(4^k\alpha_n)\right) +
				\left(\frac 34\right)^n\left(\varphi(4^n\beta_n) - \varphi(4^n\alpha_n)\right).
			\end{equation}

			Or, par inégalité triangulaire inversée, on a~:
			\begin{equation}
				\abs {f(\beta_n) - f(\alpha_n)} \geq \left(\frac 34\right)^n\abs {\varphi(4^n\beta_n) - \varphi(4^k\alpha_n)}
				- \sum_{k=0}^{n-1}\left(\frac 34\right)^k4^{k-n} \geq \frac 12\left(\frac 34\right)^n.
			\end{equation}

			Et puisque $\beta_n - \alpha_n = 4^{-n}$, il vient~:
			\begin{equation}
				\abs {\frac {f(\beta_n) - f(\alpha_n)}{\beta_n - \alpha_n}} \geq \frac 123^n,
			\end{equation}
			ce qui contredit la dérivabilité en $x$.
	\section{Séries de puissances}\label{sec:séries de puissances}
		\subsection{Théorie du rayon}
			On se donne $(Y, \norm \cdot)$, un $\C$-ev complet.

			\begin{déf} On appelle \textit{série de puissance} toute série de fonctions~:
			\begin{equation}
				u_n : \C \to Y,
			\end{equation}
			dont le terme général est sous la forme $u_n(z) = a_n(z-z_0)^n$, avec $z_0 \in \C$ fixé et $(a_n) \subset Y$.
			\end{déf}

			\begin{rmq} $Y = \Mat_{n \times n}(\C)$.
			\end{rmq}

			\begin{déf} Définissons $\overline {\Rp} \coloneqq \Rp \cup \{\pinfty\}$.
			\end{déf}

			\begin{thm}\label{thm:rayondeconvergence} Soit $R \coloneqq \left(\limsup_{n \to \pinfty}\norm {a_n}_Y^{\frac 1n}\right)^{-1}$. Quel que soit
			$z \in \C$~:
			\begin{itemize}
				\item si $\abs {z - z_0} \lneqq R$, alors $\sum_{n \geq 0}u_n(z)$ converge absolument~;
				\item si $\abs {z - z_0} \gneqq R$, alors $\sum_{n \geq 0}u_n(z)$ diverge grossièrement (le terme général ne tend pas vers 0 pour $n \to \pinfty$
				en norme dans $Y$).
			\end{itemize}
			\end{thm}

			\begin{proof} Soit $z \in \C$ tel que $\abs {z - z_0} < R$. Alors il existe $R' \gneqq 0 \tq \abs {z - z_0} < R' < R$ et~:
			\begin{equation}
				\frac 1R + \frac 1{R'} < \frac 1{\abs {z - z_0}}.
			\end{equation}

			Puisque $R^{-1} = \limsup_{n \to \pinfty}\norm{a_n}_Y^{\frac 1n}$, il existe $N \in \N \tq$~:
			\begin{equation}
				\forall n \geq N : \norm {a_n}_Y^{\frac 1n} \leq \limsup_{n \to \pinfty} \norm {a_n}_Y^{\frac 1n} = \frac 1R \leq \frac 1{R'}.
			\end{equation}

			Dès lors~: $\norm {a_n}_Y \leq \frac 1{(R')^n}$, ou encore $\abs {z - z_0}^n\norm {a_n}_Y \leq \frac {\abs {z - z_0}^n}{(R')^n}$. On a donc~:
			\begin{equation}
				\norm {(z-z_0)^na_n}_Y \leq \left(\frac {\abs {z - z_0}}{R'}\right)^n.
			\end{equation}

			Et comme $\abs {\frac {\abs {z - z_0}}{R'}} < 1$, on sait que la série de terme général $\frac {\abs {z - z_0}}{R'}$ converge et donc de terme
			général $\norm {(z - z_0)^na_n}_Y$ converge aussi.

			Soit maintenant $z \in \C \tq \abs {z - z_0} > R$. Il existe $R' > 0$ tel que $\abs {z - z_0} > R' > R$ et $\abs {z - z_0}^{-1} < (R')^{-1} + R^{-1}$.
			Soit $\varphi: \N \to \N$ strictement croissante telle que~:
			\begin{equation}
				\forall n \in \N : \frac 1{R'} \leq \norm {a_{\varphi(n)}}_Y^{\frac 1{\varphi(n)}}.
			\end{equation}

			On en déduit~:
			\begin{equation}
				\norm {a_{\varphi(n)}(z - z_0)^{\varphi(n)}}_Y = \abs {z - z_0}^{\varphi(n)}\norm {a_{\varphi(n)}}_Y
				\geq \left(\frac {\abs {z - z_0}}{R'}\right)^{\varphi(n)} \xrightarrow[n \to \pinfty]{} \pinfty.
			\end{equation}

			Dès lors, $\sum_{n \geq 0}a_n(z - z_0)^n$ diverge grossièrement.
			\end{proof}

			\begin{thm}\label{thm:cvnsurrayondeconvergence} Soit $a_n(z - z_0)^n$, le terme général d'une série de puissance. Alors~:
			\begin{itemize}
				\item lorsque $0 < R < \pinfty, \forall r \in (0, R) :$ la série de fonctions de terme général~: $z \mapsto a_n(z - z_0)^n$ converge normalement
				sur $B(z_0, r]$~;
				\item lorsque $R = \pinfty$, la série de fonctions de terme général $z \mapsto a_n(z - z_0)^n$ converge normalement sur $B(z_0, r]$ pour tout $r$.
			\end{itemize}
			\end{thm}

			\begin{proof} Si $0 < r < R < \pinfty$, observons que $\abs {z_0 + r - z_0} < R$. Ainsi, avec le Théorème~\ref{thm:rayondeconvergence}, la série
			de terme général $a_n(z_0 + r)$ converge absolument. Or~:
			\begin{equation}
				\norm {a_n(z_0 + r)}_Y = \norm {a_n}_Y\abs {z_0 + r - z_0}^n = \norm {a_n}_Yr^n.
			\end{equation}
			Donc la série de terme général $\norm {a_n}_Yr^n$ converge. Observons que~:
			\begin{equation}
				\forall n \in \N : \forall z \in B(z_0, r] : \norm {u_n(z)}_Y = \norm {a_n}_Y\abs {z - z_0}^n \leq \norm {a_n}_Yr^n.
			\end{equation}

			Ainsi $\norm {u_n}_{\infty, B(z_0, r]} \leq \norm {a_n}_Yr^n$. Or $\norm {a_n}_Yr^n$ est le terme général d'une série qui converge. Par le critère
			de Weierstrass, la série de terme général $u_n$ converge normalement $B(z_0, r]$.

			Si maintenant $R = \pinfty$, on prend $r \in \Rp_0$, $\abs {z_0 + r - z_0} = r < R = \pinfty$. Avec le Théorème~\ref{thm:rayondeconvergence}, on a~:
			que $\sum_{n \geq 0}u_n(z_0 + r)$ converge absolument. Or $\norm {u_n(z_0 + r)}_Y = \norm {a_n}_Rr^n$. Donc la série de terme général
			$\norm {a_n}_Yr^n$ converge. Puisque l'on a toujours~:
			\begin{equation}
				\norm {u_n}_{\infty, B(z_0, r]} \leq \norm {a_n}_Yr^n,
			\end{equation}
			on a donc $\sum_{n \geq 0}u_n$ converge normalement sur $B(z_0, r]$ par le critère de Weierstrass.
			\end{proof}

			\begin{cor} Soit $a_n(z - z_0)^n$ une série de puissance dans $Y$ complet et $R$ le rayon associé. Lorsque $R \gneqq 0$, la série converge normalement
			sur tout compact de $B(0, r[$.
			\end{cor}

			\begin{proof} Si $0 < R < \pinfty$, soit $K \subset B(z_0, R[$ un compact. Il existe $r \in (0, R)$ tel que $K \subset B(z_0, r] \subset B(z_0, R[$.
			La convergence normale sur $B(z_0, r]$ implique la convergence normale sur $K$.

			Si $R = \pinfty$, on a $B(z_0, R[ = \C$ Soit $K$, un compact de $\C$. Il existe $r > 0$ tel que $K \subset B(z_0, r]$, et donc la convergence
			normale sur $B(z_0, r]$ implique la convergence normale sur $K$.
			\end{proof}

			\begin{cor} Lorsque $R > 0$, la fonction $S(z) = \sum_{k \geq 0}a_k(z - z_0)^k$ est une fonction continue sur $B(z_0, R[$.
			\end{cor}

			\begin{proof} Les fonctions $u_n : B(z_0, R[ \to Y : z \mapsto a_n(z - z_0)^n$ sont continues sur l'ouvert $B(z_0, R[ \subset \C$, et il y a
			convergence normale (et donc uniforme) de $\sum_{n \geq 0}u_n$ sur les compacts de $B(z_0, R[$. Par le Théorème~\ref{thm:cvnsurrayondeconvergence},
			on sait que $S \in C^0(B(z_0, R[, Y)$.
			\end{proof}
		\subsection{Étude sur le cercle de convergence}
			\begin{déf} On définit le cercle centré en $z_0 \in \C$ et de rayon $R > 0$ par~:
			\begin{equation}
				\mathcal C(z_0, R] = \{z \in \C \tq \abs {z - z_0} = R\}.
			\end{equation}
			\end{déf}

			\begin{thm}Lorsque $0 < R < \pinfty$, s'il existe $z \in \mathcal C(z_0, R]$ tel que $\sum_{n \geq 0}a_n(z - z_0)^n$ converge absolument,alors la
			série de fonctions~: $u_n(z) = a_n(z - z_0)^n$ converge normalement sur $B(z_0, R]$.
			\end{thm}

			\begin{proof} Soit $z \in \mathcal C(z_0, R]$ tel que $\sum_{n \geq 0}a_n(z - z_0)^n$ converge absolument. On a~:
			\begin{equation}
				\norm {a_n(z - z_0)^n}_Y = \abs {z - z_0}^n\norm {a_n}_Y = R^n\norm {a_n}_Y.
			\end{equation}
			Puisque $\forall z \in B(z_0, R] : \forall n \in \N : \norm {u_n(z)}_Y = \abs {z - z_0}\norm {a_n}_Y \leq R^n\norm {a_n}_Y,$
			il vient que~:
			\begin{equation}
				\forall n \geq 0 : \norm {u_n}_{\infty, B(z_0, R]} \leq R^n\norm {a_n}_Y.
			\end{equation}
			Et donc $\sum_{n \geq 0}u_n$ converge normalement sur $B(z_0, R]$ par le critère de Weierstrass.
			\end{proof}

			\begin{ex} $Y = \C, \sum_{n \geq 1}\frac {z^n}{n^2}$. On a alors $a_n = \frac 1{n^2}$, donc~:
			\begin{equation}
				\abs {a_n}^{\frac 1n} = n^{\frac {-2}n} = \exp\left(-2\frac {\ln n}n\right) \xrightarrow[n \to \pinfty]{} 1,
			\end{equation}
			d'où $R = 1$, et il y a convergence en $z = 1$, donc il y a convergence absolue de la série~:
			\begin{equation}
				\sum_{n \geq 1}\frac {\exp(in\theta)}{n^2} \; \forall \theta \in \R,
			\end{equation}
			et la convergence de $\sum_{n \geq 1}\frac {z^n}{n^2}$ est normale sur $B(0, 1]$.
			\end{ex}

			\begin{thm}[Théorème d'Abel]\label{thm:cvgsurcercleimpliquecvusursegment} Si $R \in (0, \pinfty)$ et
			$\exists z \in \mathcal C(z_0, R] \tq \sum_{n \geq 0} a_n(z - z_0)^n$ converge, alors la série de fonctions de terme général
			$z \mapsto a_n(z - z_0)^n$ converge uniformément sur le segment reliant $z_0$ à $z$.
			\end{thm}

			\begin{proof} Prenons $z_0 = 0$ et $z \in \Rp_0$. Prenons $x \in [0, z], n, p \in \Ns$. Écrivons~:
			\begin{equation}
				\sum_{k=n}^{n+p}a_kx^k = \sum_{k=0}^{n+p}a_kz^k\left(\frac xz\right)^k.
			\end{equation}
			Notons alors $S_m \coloneqq \sum_{k=0}^ma_kz^k$, pour tout $m$. On obtient alors~:
			\begin{align}
				\sum_{k=n}^{n+p}a_kx^k &= \sum_{k=n}^{n+p}(S_k - S_{k-1})\left(\frac xz\right)^k
					= \sum_{k=n}^{n+p}(S_k - S_{n-1})\left(\frac xz\right)^k - \sum_{k=n}^{n+p}(S_{k-1} - S_{n-1})\left(\frac xz\right)^k \\
				&= \sum_{k=n}^{n+p}(S_k-S_{n-1})\left(\frac xz\right)^k - \sum_{k=n-1}^{n+p-1}(S_k-S_{n-1})\left(\frac xz\right)^{k+1} \\
				&= -(S_{n-1}-S_{n-1})\left(\frac xz\right)^n + \sum_{k=n}^{n+p-1}(S_k-S_{n-1})\left(\left(\frac xz\right)^k - \left(\frac xz\right)^{k+1}\right)
					+ (S_{n+p}-S_{n-1})\left(\frac xz\right)^{n+p}.
			\end{align}

			Puisque la série de terme général $z \mapsto a_k\abs {z-z_0}^k$ converge, la suite $(S_m)_n$ est de Cauchy dans $Y$. Soit $\varepsilon > 0$. On sait
			qu'il existe $N \in \N \tq$~:
			\begin{equation}
				\forall k, n > N : \norm {S_k - S_{n-1}}_Y \leq \varepsilon.
			\end{equation}

			Soit un $n \geq N$, et $p \in \Ns$. Prenons $x \in [0, z]$. On a~:
			\begin{align}
				\norm {\sum_{k=n}^{n+p}a_kx^k}_Y &\leq \sum_{k=n}^{n+p-1}\norm {(S_k-S_{n-1})\left(\left(\frac xz\right)^{k+1} - \left(\frac xz\right)^k\right)}_Y
					+ \norm {(S_{n+p}-S_{n-1}\left(\frac xz\right)^{n+p}} \\
				&\leq \sum_{k=n}^{n+p-1}\norm {S_k - S_{n-1}}\left(\left(\frac xz\right)^k - \left(\frac xz\right)^{k+1}\right)
					+ \norm {S_{n+p}-S_{n-1}}\left(\frac xz\right)^{n+p} \\
				&\leq \varepsilon\sum_{k=n}^{n+p-1}\left(\left(\frac xz\right)^k - \left(\frac xz\right)^{k+1}\right) + \varepsilon\left(\frac xz\right)^{n+p} \\
				&\leq \varepsilon\left(\left(\frac xz\right)^n - \left(\frac xz\right)^{n+p}\right) + \varepsilon\left(\frac xz\right)^{n+p} \\
				&\leq \varepsilon\left(\frac xz\right)^{n}.
			\end{align}

			Par la suite, on peut dire que pour $n \geq N, p \in \Ns$~:
			\begin{equation}
				\norm {\sum_{k=n}^{n+p}a_k \cdot^k}_{\infty, [0, z]} \leq \varepsilon.
			\end{equation}

			On en déduit que la série de terme général $x \mapsto a_kx^k$ est de Cauchy dans $B([0, z], Y)$, et donc, par complétude de $Y$, convergente.
			\end{proof}

			\begin{rmq} Soient $(a_n), (b_n) \subset \C$. On appelle la \textit{série de Cauchy} de $(a_n)$ et $(b_n)$ la série de terme général~:
			\begin{equation}
				c_n \coloneqq \sum_{k=0}^na_kb_{n-k}.
			\end{equation}
			\end{rmq}

			\begin{thm}[Théorème de Cauchy, version \CDII]\label{thm:CauchyCDI1} Si $\sum_{n \geq 0}a_n$ et $\sum_{n \geq 0}b_n$ convergent absolument,
			alors $\sum_{n \geq 0}c_n$ converge absolument, et on a~:
			\begin{equation}
				\left(\sum_{n \geq 0}a_n\right)\left(\sum_{n \geq 0}b_n\right) = \sum_{n \geq 0}c_n.
			\end{equation}
			\end{thm}

			\begin{thm}[Théorème de Cauchy, version \CDIII] Si $\sum_{n \geq 0}a_n$, $\sum_{n \geq 0}b_n$, et $\sum_{n \geq 0}c_n$ convergent, alors~:
			\begin{equation}
				\left(\sum_{n \geq 0}a_n\right)\left(\sum_{n \geq 0}b_n\right) = \sum_{n \geq 0}c_n.
			\end{equation}
			\end{thm}

			\begin{proof} Par hypothèse de convergence des séries, on a~:
			\begin{equation}
				R_a \coloneqq R\left(\sum_{n \geq 0}a_nz^n\right), R_b \coloneqq R\left(\sum_{n \geq 0}b_nz^n\right), R_c \coloneqq R\left(\sum_{n \geq 0}c_nz^n\right)
				\geq 1.
			\end{equation}
			Posons~:
			\begin{align}
				&A : [0, 1] \to \R : x \mapsto \sum_{n \geq 0}a_nx^n, \\
				&B : [0, 1] \to \R : x \mapsto \sum_{n \geq 0}b_nx^n, \\
				&C : [0, 1] \to \R : x \mapsto \sum_{n \geq 0}c_nx^n.
			\end{align}

			Si les $R_\cdot$ sont $> 1$, alors $[0, 1]$ est un compact de $B(0, R[$ et donc la somme de la série de terme général $\cdot_nz^n$ est $C^0$ sur
			$[0, 1]$, et si $R=1$, alors la série de puissance converge en $1 \in \mathcal C(0, 1]$ et donc la série de terme général $\cdot_nz^n$ converge
			uniformément sur $[0, 1]$.

			Puisque $z \mapsto a_nz^n$ (pareil pour $b_n$, $c_n$) est $C^0$ sur $[0, 1]$, il vient que $A, B, C \in C^0([0, 1], \C)$.

			Pour $x \in [0, 1)$, les séries $\sum_{n \geq 0}\cdot_n$ convergent \textbf{absolument}. De plus~:
			\begin{equation}
				\sum_{k=0}^na_kx^k\cdot b_{n-k}x^{n-k} = x^n\sum_{n=0}^na_kb_{n-k} = x^nc_n.
			\end{equation}

			Par le Théorème~\ref{thm:CauchyCDI1}, on a~:
			\begin{equation}
				\forall x \in [0, 1) : A(x)B(x) = C(x).
			\end{equation}
			De même, en passant à la limite (continuité) $x \to 1$, il vient~:
			\begin{equation}
				\left(\sum_{n \geq 0}a_n\right)\left(\sum_{n \geq 0}b_n\right) = A(1)B(1) = C(1) = \sum_{n\geq 0}c_n.
			\end{equation}
			\end{proof}

		\subsection{Fonctions réelles analytiques}
			On considère la série de puissances $u_n(x) = a_n(x - x_0)^n$, avec $x, x_0 \in \R$ et $x_0$ fixé.

			\begin{déf} On appelle \textit{série dérivée formelle} de $u_n$ la série de terme général~:
			\begin{equation}
				u_n'(x) = na_n(x-x_0)^{n-1}, \qquad n \geq 1
			\end{equation}
			\end{déf}

			\begin{rmq} La série dérivée formelle est toujours une série de puissances.
			\end{rmq}

			\begin{prp} Soient~:
			\begin{align}
				&R_1 \coloneqq R\left(\sum_{n \geq 0}a_n(x-x_0)^n\right), \\
				&R_2 \coloneqq R\left(\sum_{n \geq 1}na_n(x-x_0)^{n-1}\right).
			\end{align}

			Alors $R_1 = R_2$.
			\end{prp}

			\begin{proof} On observe aisément que~:
			\begin{equation}
				R_1^{-1} = \limsup_{n \to \pinfty}\norm {a_n}_Y^{\frac 1n},
			\end{equation}
			et donc~:
			\begin{equation}
				R_2^{-1} = \limsup_{n \to \pinfty}\norm {na_n}_Y^{\frac 1n} = \limsup_{n \to \pinfty}n^{\frac 1n}\norm {a_n}_Y^{\frac 1n}
				= \limsup_{n \to \pinfty}\norm {a_n}_Y^{\frac 1n} = R_1^{n-1},
			\end{equation}
			car $n^{\frac 1n} \xrightarrow[n \to \pinfty]{} 1$.
			\end{proof}

			\begin{prp}\label{prp:formuledérivéepsdp} Soit $x_0 \in \R$. Supposons $R \gneqq 0$, et notons~:
			\begin{equation}
				f : (x_0-R, x_0+R) \to Y : x \mapsto \sum_{n \geq 0}a_n(x-x_0)^n.
			\end{equation}

			Alors la fonction $f$ est continue sur $(x_0 \pm R)$, et on a~:
			\begin{align}
				\forall p \in \N : \forall x \in (x_0 \pm R) : f^{(p)}(x) &= \sum_{n \geq p}\left(n(n-1)(n-2)\ldots(n-p+1)\right)a_n(x-x_0)^{n-p} \\
				&= \sum_{n \geq p}\frac {n!}{(n-p)!}a_n(x-x_0)^{n-p}.
			\end{align}
			\end{prp}

			\begin{proof} On observe que le terme général $u_n(x) = a_n(x-x_0)^n$ est de classe $C^\infty$ sur $(x_0 \pm R)$. La série
			$u_n'(x) = na_n(x-x_0)^{n-1}$ converge normalement sur les compacts de $(x_0 \pm R)$ par l'égalité des rayons. Donc $f \in C^1\left((x_0 \pm R)\right)$
			et $f'(x) = \sum_{n \geq 1}na_n(x-x_0)^{-1}$.

			Par récurrence, on obtient le résultat désiré.
			\end{proof}

			\begin{cor} Si $f$ est une somme d'une série de puissances $\sum_{n \geq 0}a_n(x-x_0)^n$ de rayon $R \gneqq 0$ sur $(x_0 \pm R)$, alors~:
			\begin{equation}
				\forall n \in \N : a_n = \frac {f^{(n)}(x_0)}{n!}.
			\end{equation}
			\end{cor}

			\begin{proof} Si $f$ est somme de la série de puissance de terme général $a_n(x-x_0)^n$, alors $f \in C^\infty$ sur $(x_0 \pm R)$. Par la
			Proposition~\ref{prp:formuledérivéepsdp}, on trouve~:
			\begin{equation}
				\forall p \in \N : f^{(p)}(x_0) = \sum_{n \geq p}\frac {n!}{(n-p)!}a_n(x_0-x_0)^{n-p} = \frac {p!}{0!}a_p(x_0-x_0)^{p-p} + 0 = p!a_p1 = p!a_p,
			\end{equation}
			et donc $a_p = \frac {f^{(p)}(x_0)}{p!}$.
			\end{proof}

			\begin{rmq} Les notations suivantes sont dues à Landau~:
			\begin{align}
				&u_n \sim v_n \iff \forall \varepsilon > 0 : \exists N \in \N \tq \forall n \geq N : \abs {u_n - v_n} < \varepsilon \abs{u_n} \\
				&u_n = o(v_n) \iff \forall \varepsilon > 0 : \exists N \in \N \tq \forall n \geq N : \abs {u_n} < \varepsilon \abs {v_n} \\
				&u_n = O(v_n) \iff \exists N \in \N \tq \forall M \gneqq 0 : \forall n \geq N : u_n < M \abs {v_n}
			\end{align}
			\end{rmq}

			\begin{déf} Soit $U \subset \R$, un ouvert. Une fonction $f : U \to \R$ est dite \textit{réelle analytique} lorsque~:
			\begin{equation}
				\forall x_0 \in U : \exists \varepsilon > 0, (a_n) \subset \R \tq (x_0 \pm \varepsilon) \subset U \text{ et } \sum_{k=0}^na_k(x-x_0)^k
			\text{ converge simplement sur } (x_0 \pm \varepsilon).
			\end{equation}
			\end{déf}

			\begin{déf}[Définition équivalente] $f : U \subset U \to \R$ est dite \textit{réelle analytique} lorsque $f$ est somme de sa série de Taylor sur un
			voisinage de chaque point de $U$.
			\end{déf}

			\begin{déf} Pour $\emptyset \neq U \subset \R$, on pose $\mathcal A(U) \coloneqq \{f : U \to \R \tq f \text{ est réelle analytique sur } U\}$.
			\end{déf}

			\begin{prp} Soit $\emptyset \neq U \subset \R$. Alors $\mathcal A(U) \subsetneqq C^\infty(U, \R)$.
			\end{prp}

			\begin{proof} Montrons d'abord l'inclusion. Soit $x_0 \in U$ et soit $\varepsilon > 0$ tel que~:
			\begin{equation}
				f(x) = \sum_{k \geq 0}\frac {f^{(k)}(x_0)}{k!}(x-x_0)^k \text{ sur } (x_0 \pm \varepsilon).
			\end{equation}
			On a donc $\restr f {(x_0 \pm \varepsilon)} \in C^\infty\left((x_0 \pm \varepsilon), \R\right)$.

			Pour montrer l'inclusion stricte, soit~:
			\begin{equation}
				f : \R \to \R : x \mapsto \begin{cases}\exp(-x^{-1}) &\text{ si } x > 0 \\0 &\text{ sinon}\end{cases}.
			\end{equation}
			On sait que $f \in C^\infty(\R, \R)$, et $\forall k \in \N : f^{(k)}(0) = 0$. Donc $f$ n'est somme de sa série de Taylor sur aucun voisinage de $0$.
			On a donc $f \not \in \mathcal A(\R)$.
			\end{proof}

			\begin{rmq} $f \in \mathcal A(\R)$ peut avoir, en certains points, un rayon fini. Par exemple $f(x) = \frac 1{1 + x^2}$. Pour $\abs x < 1$, on a~:
			\begin{equation}
				f(x) = \frac 1{1 - (-x^2)} = \sum_{k \geq 0}(-1)^kx^{2k},
			\end{equation}
			et $R\left(\sum_{k \geq 0}(-1)^kx^{2k}\right) = \left(\limsup_{n \to \pinfty}((-1)^n)^{\frac 1n}\right)^{-1} = 1$.
			\end{rmq}

\chapter{Intégration}
	\section{Intégrales absolument convergentes}
		\subsection{Rappels concernant l'intégrale de Riemann}
			\begin{déf} On se place sur un segment $[a, b] \subset \R$. On note~:
			\begin{equation}
				\El {[a, b]}\R \coloneqq \left\{\varphi : [a, b] \to \R \tq \varphi \text{ est en escaliers sur} [a, b]\right\}.
			\end{equation}
			\end{déf}

			\begin{rmq} $\int$ est bien définie sur $\El {[a, b]}\R$.
			\end{rmq}

			\begin{déf} La fonction $f : [a, b] \to \R$ est \textit{R-int} (\textit{Riemann intégrable}, ou encore \textit{intégrable au sens de Riemann})
			lorsque~:
			\begin{align}
				\forall \varepsilon > 0 : &\exists \varphi, \psi \in \El {[a, b]}\R \tq \\
				&(i) \quad \varphi \leq f \leq \psi \\
				&(ii) \quad \int(\psi-\varphi) < \varepsilon
			\end{align}
			\end{déf}

			\begin{prp} De manière équivalente, $f : [a, b] \to \R$ est R-int sur $[a, b]$ lorsque~:
			\begin{equation}
				\overline \int f \coloneqq \inf_{f \leq \psi \in \El {[a, b]}\R}\int \psi
				= \sup_{f \geq \varphi \in \El {[a, b]}\R}\int \varphi \eqqcolon \underline \int f.
			\end{equation}
			\end{prp}

			\begin{déf} On note dans ce cas~:
			\begin{equation}
				\int_a^b f(x)\dif x = \overline {\int_a^b} f(x)\dif x = \underline {\int_a^b} f(x)\dif x.
			\end{equation}
			\end{déf}

			\begin{prp} Si $f : [a, b] \to \R$ est R-int sur $[a, b]$, alors $f$ est bornée sur $[a, b]$.
			\end{prp}

			\begin{prp} Soient $f, g$ R-int, et $\lambda, \mu \in \R$. Alors les fonctions suivantes sont R-int~:
			\begin{align}
				&x \mapsto \left(\lambda f + \mu g\right)(x) \\
				&x \mapsto \min(f, g)(x) \\
				&x \mapsto \max(f, g)(x) \\
				&x \mapsto \abs f(x)
			\end{align}
			Et on a~:
			\begin{itemize}
				\item[$(i)$]
					\begin{equation}
						\abs {\int_a^b f(x)\dif x} \leq \int_a^b\abs {f(x)}\dif x~;
					\end{equation}
				\item[$(ii)$]
					\begin{equation}
						\int_a^b\left(\lambda f + \mu g\right)(x)\dif x = \lambda \int_a^b f(x)\dif x + \mu \int_a^b g(x)\dif x.
					\end{equation}
			\end{itemize}
			\end{prp}

			\begin{proof} montrons que $\min(f, g)$ est R-int sur $[a, b]$.

			Fixons $\varepsilon > 0$. Soient $\varphi_f, \varphi_g, \psi_f, \psi_g \in \El {[a, b]}\R$ tels que~:
			\begin{align}
				&\varphi_f \leq f \leq \psi_f, \qquad\qquad \int_a^b(\psi_f - \varphi_f) < \varepsilon \\
				&\varphi_g \leq g \leq \psi_g, \qquad\qquad \int_a^b(\psi_g - \varphi_g) < \varepsilon.
			\end{align}

			Prenons $x \in [a, b]$, et remarquons que~:
			\begin{equation}
				\min(\varphi_f, \varphi_g) \leq f\qquad\qquad\min(\varphi_f, \varphi_g) \leq g,
			\end{equation}
			et donc $\min(\varphi_f, \varphi_g) \leq \min(f, g)$.

			Posons $\El {[a, b]}\R \ni \widetilde \varphi \coloneqq \min(\varphi_f, \varphi_g), \widetilde \psi \coloneqq \min(\psi_f, \psi_g)$. On remarque alors~:
			\begin{equation}
				\widetilde \varphi \leq \min(f, g) \leq \widetilde \psi.
			\end{equation}

			Prenons $x \in [a, b]$. On remarque~:
			\begin{itemize}
				\item si $\varphi_f(x) \leq \varphi_g(x)$, on a~:
				\begin{equation}
					\widetilde \psi(x) - \widetilde \varphi(x) \leq \psi_f(x) - \varphi_f(x)~;
				\end{equation}
				\item si $\varphi_g(x) < \varphi_f(x)$, on a~:
				\begin{equation}
					\widetilde \psi(x) - \widetilde \varphi(x) \leq \psi_g(x) - \varphi_g(x).
				\end{equation}
			\end{itemize}

			Ainsi, en séparant les intégrales en un nombre fini où on a soit $(i)$, soit $(ii)$, on a~:
			\begin{equation}
				\int_a^b (\widetilde \psi - \widetilde \varphi) \leq \int_a^b (\psi_f - \varphi_f) + \int_a^b (\psi_g - \varphi_g) \leq 2\varepsilon.
			\end{equation}
			\end{proof}

			\begin{cor} $\abs {\int_a^b (\lambda f + \mu g)(x)\dif x} \leq \abs \lambda \int_a^b \abs {f(x)}\dif x + \abs \mu \int_a^b\abs {g(x)}\dif x$.
			\end{cor}

			\begin{proof} On calcule~:
			\begin{equation}
				\abs {\int_a^b \lambda f(x) + \mu g(x)\dif x} \leq \int_a^b\abs {\lambda f(x) + \mu g(x)}\dif x
				= \abs \lambda\int_a^b\abs {f(x)}\dif x + \abs \mu\int_a^b\abs {g(x)}\dif x.
			\end{equation}
			\end{proof}

		\subsection{Fonctions absolument intégrables sur un intervalle}
			\begin{déf} Soit $I \neq \emptyset$, un intervalle de $\R$, et $f : I \to \R$. On dit que $f$ est \textit{abs-int} (\textit{absolument intégrable})
			sur $I$ lorsque~:
			\begin{itemize}
				\item[$(i)$] $\forall [a, b] \subset I : \restr f{[a, b]}$ est R-int sur $[a, b]$~;
				\item[$(ii)$] $\sup_{[a, b] \subset I} \int_a^b \abs f \lneqq \pinfty$.
			\end{itemize}
			\end{déf}

			\begin{rmq} La condition $(ii)$ revient à dire que $\exists M > 0 \tq \forall [a, b] \subset I : \int_a^b \abs f \leq M$.
			\end{rmq}

			\begin{déf} Soit $I \subset \R$, un intervalle. On appelle \textit{suite exhaustive de segments de $I$} toute suite $([a_n, b_n])_{n \in \N}$ de
			segments de $I$ tels que~:
			\begin{itemize}
				\item[$(i)$]  la suite est croissante (c-à-d $\forall n \in \N : [a_{n+1}, b_{n+1}] \supseteq [a_n, b_n]$)~;
				\item[$(ii)$] $\bigcup_{n \in \N}[a_n, b_n] = I$.
			\end{itemize}
			\end{déf}

			\begin{prp} Soit $I \neq \emptyset$, un intervalle de $\R$. $I$ admet une suite exhaustive.
			\end{prp}

			\begin{proof} Si $I$ est un fermé, prenons $a, b \in \R$ tels que $I = [a, b]$. La suite $([a_n, b_n])_n = ([a, b])_n$ est exhaustive.

			Si $I$ est un ouvert, prenons $a, b \in \R$ tels que $I = (a, b)$. La suite $([a_n, b_n])_n = ([a + \frac 1n, b - \frac 1n])_n$ est exhaustive.

			Si $I$ est ouvert d'un côté, et fermé de l'autre, les suites exhaustives $([a, b-\frac 1n])_n$ et $([a + \frac 1n, b])_n$ sont exhaustives.
			\end{proof}

			\begin{prp}\label{prp:abs-int => convergence} Soit $I \neq \emptyset$ un intervalle de $\R$. Soit $f : I \to \R$ abs-int sur $I$. Soit $([a_n, b_n])_n$
			une suite exhaustive de 	segments de $I$. Alors~:
			\begin{itemize}
				\item[$(i)$]  la suite définie par~:
				\begin{equation}
					\left(\int_{a_n}^{b_n}f(x)\dif x\right)_n \subset \R
				\end{equation}
				est convergente~;
				\item[$(ii)$] la limite de cette suite ne dépend pas de la suite exhaustive de segments de $I$ choisie.
			\end{itemize}
			\end{prp}

			\begin{déf} On appelle \textit{intégrale de $f$ sur $I$} cette valeur, et on la note~:
			\begin{equation}
				\int_I f(x)\dif x.
			\end{equation}
			\end{déf}

			\begin{proof} Soit $([a_n, b_n])$ une suite exhaustive de segments de $I$. Posons pour $n \in \N$~: $\alpha_n = \int_{a_n}^{b_n}\abs f$.
			La suite $(\alpha_n)_n$ est croissante et majorée donc $(\alpha_n)$ converge vers un certain $\ell \in \Rp$. En particulier, $(\alpha_n)$ est de
			Cauchy dans $\R$. Considérons maintenant $(\beta_n)_n$, où $\beta_n \coloneqq \int_{a_n}^{b_n}f$. Observons que pour $p, n \in \N$~:
			\begin{equation}
				\beta_{n+p} - \beta_n = \int_{a_{n+p}}^{b_{n+p}}f - \int_{a_n}^{b_n}f
				= \int_{a_{n+p}}^{a_n}f + \int_{a_n}^{b_n}f + \int_{b_n}^{b_{n+p}}f - \int_{a_n}^{b_n}f = \int_{a_{n+p}}^{a_n}f + \int_{b_n}^{b_{n+p}}f.
			\end{equation}

			Ainsi~:
			\begin{equation}
				\abs {\beta_{n+p}-\beta_n} \leq \int_{a_{n+p}}^{a_n}\abs f + \int_{b_n}^{b_{n+p}} \abs f
				\leq \int_{a_{n+p}}^{a_n}\abs f + \int_{a_b}^{b_n} \abs f + \int_{b_n}^{b_{n+p}} \abs f - \int_{a_n}^{b_n}\abs f
				= \alpha_{n+p} - \alpha_n.
			\end{equation}

			La suite $(\beta_n)_n$ est donc bornée par une suite de Cauchy (et est donc de Cauchy) dans $\R$. Par complétude de $\R$, $(\beta_n)_n$ converge
			dans $\R$.

			Montrons maintenant que cette limite ne dépend pas de la suite exhaustive. Soit $[\widetilde a_n, \widetilde b_n]$ une suite exhaustive de $I$. On
			sait que $\widetilde \beta_n = \int_{\widetilde a_n}^{\widetilde b_n} f$ converge. On veut montrer que $\widetilde \beta_n$ a la même limite que
			$\beta_n$. On construit donc une nouvelle suite exhaustive de $I$. On choisit $[\bar a_0, \bar b_0] = [a_0, b_0]$. Il existe
			$N_1 \gneqq 0 \tq \forall n \geq N_1 : [a_0, b_0] \subset [\widetilde a_n, \widetilde b_n]$. On pose ensuite
			$[\bar a_1, \bar b_1] = [\widetilde a_{N_1}, \widetilde b_{N_1}]$. Il existe
			$N_2 \gneqq N_1 \tq \forall n \geq N_2 : [\bar a_1, \bar b_1] \subset [a_n, b_n]$. On pose donc $[\bar a_2, \bar b_2] = [a_{N_2}, b_{N_2}]$.

			On construit donc $\varphi : \N \to \N$ strictement croissante telle que pour tout $p$~:
			\begin{align}
				&[\bar a_{2p}, \bar b_{2p}] = [a_{\varphi(2p)}, b_{\varphi(2p)}], \\
				&[\bar a_{2p+1}, \bar b_{2p+1}] = [\widetilde a_{\varphi(2p+1)}, \widetilde b_{\varphi(2p+1)}].
			\end{align}

			Donc la suite $([\bar a_p, \bar b_p])_p$ est exhaustive. La suite $\left(\int_{\bar a_p}^{\bar b_p}f\right)_p$ converge vers $\bar \beta$ dans $\R$.

			Puisque~:
			\begin{align}
				&\int_{\bar a_{2p}}^{\bar b_{2p}}f = \int_{a_{\varphi(2p)}}^{b_{\varphi(2p)}}f = \beta_{\varphi(2p)} \xrightarrow[p \to \pinfty]{} \beta = \bar \beta \\
				&\int_{\bar a_{2p+1}}^{\bar b_{2p+1}} f = \int_{\widetilde a_{\varphi(2p+1)}}^{\widetilde b_{\varphi(2p+1)}} f
					= \widetilde \beta_{\varphi(2p+1)} \xrightarrow[p \to \pinfty]{} \widetilde \beta = \bar \beta,
			\end{align}
			on déduit $\beta = \bar \beta = \widetilde \beta$. Les limites sont donc les mêmes, peu importe les suites exhaustives choisies.
			\end{proof}

			\begin{prp} Soit $f : [a, b] \to \R$ R-int sur $[a, b]$. Alors $f$ est abs-int sur $[a, b]$, et on a~:
			\begin{equation}
				\int_{[a, b]} f = \int_a^b f.
			\end{equation}
			\end{prp}

			\begin{proof} $f$ est R-int, et donc est R-int sur tout segment de $[a, b]$. Soit $([a_n, b_n])_n$, une suit exhaustive de segments de $[a, b]$. Il existe
			$N \in \N \tq \forall n \geq N : [a_n, b_n] = [a, b]$. Ainsi, pour $n \geq N$, on a~:
			\begin{equation}
				\int_{a_n}^{b_n} f = \int_a^b f.
			\end{equation}
			En passant à la limite pour $n \to \pinfty$, on a~:
			\begin{equation}
				\int_{[a, b]} f = \int_a^b f.
			\end{equation}
			\end{proof}

			\begin{prp} L'ensemble $L^1(I) \coloneqq \{f : I \to \R \tq f \text{ est abs-int sur } I\}$ est un $\R-ev$. De plus, l'application~:
			\begin{equation}
				\int : L^1(I) \to \R : f \mapsto \int_I f
			\end{equation}
			est une forme linéaire sur $L^1(I)$.
			\end{prp}

			\begin{proof} EXERCICE.
			\end{proof}

		\subsection{Fonctions absolument intégrables vues comme fonction des bornes}
			\begin{prp} Soit $I \subset \R$, un intervalle non-vide de $\R$. Si la fonction $f : I \to \R$ est abs-int sur $I$, alors elle l'est sur
			$I \cap (\minfty, a]$ et $[a, \pinfty)$ pour tout $a \in \R$, et on a~:
			\begin{equation}
				\int_I f = \int_{I \cap (\minfty, a]} f + \int_{I \cap [a, \pinfty)} f.
			\end{equation}
			\end{prp}

			\begin{proof} Soit $[\alpha, \beta] \subset [a, \pinfty) \cap I$. $[\alpha, \beta]$ est un segment de $I$ et $f$ est abs-int sur $I$. $f$ est donc abs-int
			sur tout segment de $I$, en particulier sur $[\alpha, \beta]$. De plus, il existe $M \gneqq 0$ tel que pour tout segment $[u, v]$ de $I$, on a~:
			\begin{equation}
				\int_u^v \abs f \leq M.
			\end{equation}

			Ainsi~:
			\begin{equation}
				\int_\alpha^\beta \abs f \leq M.
			\end{equation}
			$f$ est donc abs-int sur $I \cap [a, \pinfty)$. On raisonne de manière similaire pour $(\minfty, a]$.

			Montrons maintenant l'égalité. Soit $[\alpha_n, \beta_n]$, une suite de segments de $I$. Il existe $N \in \N$ tel que~:
			\begin{equation}
				\forall n \geq N : \alpha_n \leq a \leq \beta_n.
			\end{equation}
			Il vient alors que $([\alpha_n, a])_n$ est une suite exhaustive de $I \cap (\minfty, a]$, et $([a, \beta_n])_n$ est une suite exhaustive de
			$I \cap [a, \pinfty)$. Pour $n \geq N$, on a alors~:
			\begin{equation}
				\int_{\alpha_n}^{\beta_n} f = \int_{\alpha_n}^a f + \int_a^{\beta_n} f.
			\end{equation}
			En passant à la limite pour $n \to \pinfty$, on trouve~:
			\begin{equation}
				\int_I f= \int_{I \cap (\minfty, a]} f + \int_{I \cap [a, \pinfty)} f.
			\end{equation}
			\end{proof}

			\begin{prp} Soient $I \subset \R$, $a \in I$, et $f : I \to \R$. Si $f$ est abs-int sur $I \cap (\minfty, a]$ et sur $I \cap [a, \pinfty)$, alors $f$
			est abs-int sur $I$, et on a~:
			\begin{equation}
				\int_I f = \int_{I \cap (\minfty, a]} f + \int_{I \cap [a, \pinfty)} f.
			\end{equation}
			\end{prp}

			\begin{proof} Soit $[\alpha, \beta]$ un segment de $I$. Si $a < \alpha$, ou $a > \beta$, c'est trivial.

			Supposons alors $\alpha \leq a \leq \beta$. $f$ est R-int sur $[\alpha, a]$ et sur $[a, \beta]$. $f$ est donc R-int sur $[\alpha, \beta]$. De plus, il existe
			$M^+, M^- > 0$ tels que~:
			\begin{equation}
				\sum_{[u, v] \subset (\minfty, a] \cap I}\int_u^v f \leq M^-\qquad\qquad\text{ et }\qquad\qquad\sup_{[u, v] \subset I \cap [a, \pinfty)}\int_u^v\abs f
				\leq M^+.
			\end{equation}
			On peut donc dire que $\int_\alpha^\beta\abs f \leq M^+ + M^-$. On a alors $f$ abs-int sur $I$ et on peut appliquer la proposition précédente pour~:
			\begin{equation}
				\int_I f = \int_{I \cap (\minfty, a]} f + \int_{I \cap [a, \pinfty)} f.
			\end{equation}
			\end{proof}

			\begin{prp} Soient $I \subset \R$, un intervalle non-vide, et $f : I \to \R$ abs-int. La fonction $F : I \to \R : x \to \int_{I \cap (\minfty, x]} f$ est
			localement lipschitizenne.
			\end{prp}

			\begin{proof} Soit $x_0 \in I$. Supposons que $x_0$ n'est pas un bord de $I$ (sinon EXERCICE). Supposons qu'il existe
			$\delta > 0 \tq (x_0 \pm \delta) \subset I$. La fonction $f$ est R-int sur $\left[x_0 \pm \frac \delta2\right]$ et donc sa valeur absolue est bornée sur
			ce segment par $M(x_0, \delta)$. Pour $x, y \in \left[x_0 \pm \frac \delta2\right]$, avec $x < y$, on déduit~:
			\begin{equation}
				F(y) - F(x) = \int_{I \cap (\minfty, y]} f - \int_{I \cap (-\infty, x]} f
				= \int_{I \cap (\minfty, x]} f + \int_x^y f - \int_{I \cap (\minfty, x]} f = \int_x^y f.
			\end{equation}

			D'où~:
			\begin{equation}
				\abs {F(y) - F(x)} = \int_x^y\abs f \leq M(x_0, \delta)(y-x).
			\end{equation}
			\end{proof}

			\begin{cor} Si $f$ est abs-int sur $I$, alors $F$ est continue sur $I$.
			\end{cor}

			\begin{prp} Si $f$ est abs-int sur $I$, et continue en $x_0 \in I$, alors $F$ est dérivable en $x_0$ et on a~:
			\begin{equation}
				F'(x_0) = f(x_0).
			\end{equation}
			\end{prp}

			\begin{cor} Si $f$ est abs-int sur $I$ et de classe $C^k$ sur un voisinage de $x_0 \in I$, alors $F$ est de classe $C^{k+1}$ sur un voisinage de
			$x_0$ et on a, sur ce voisinage~:
			\begin{equation}
				\forall p \in \{0, \ldots, k\} : F^{(p+1)}(x) = f^{(p)}(x).
			\end{equation}
			\end{cor}

			\begin{rmq} Si le voisinage est ouvert pour $f$, alors on a le même voisinage pour $F$.
			\end{rmq}

			\begin{proof} $F(x) = F(x_0) + \int_{x_0}^x f(t)\dif t$.
			\end{proof}

			\begin{rmq} C'est donc le théorème fondamental du calcul différentiel et intégral qui est revu ici.
			\end{rmq}

		\subsection{Critères d'intégration absolue}
			\begin{prp}[Critère de comparaison]\label{prp:critèredecomparaison} Soient $I \subset \R$ un intervalle non-vide et $f, g : I \to \R$ avec~:
			\begin{itemize}
				\item $\forall x \in I : \abs {f(x)} \leq g(x)$~;
				\item $f, g$ R-int sur tout segment de $I$.
			\end{itemize}

			Si $g$ est abs-int sur $I$, alors $f$ l'est aussi, et on a~:
			\begin{equation}
				\int_I\abs f \leq \int_I g.
			\end{equation}
			\end{prp}

			\begin{proof} Il existe $M_g \gneqq 0 \tq$~:
			\begin{equation}
				\forall [u, v] \subset I : \int_u^vg \leq M_g.
			\end{equation}
			Ainsi, si $[a, b] \subset I$ est un segment, on a $f$ R-int sur $[a, b]$ et~:
			\begin{equation}
				\int_a^b\abs f \leq \int_a^b g \leq M_g,
			\end{equation}
			avec $M_g$ donc indépendant de $[a, b]$. Ceci montre que $f$ est abs-int sur $I$ et que~:
			\begin{equation}
				\int_I \abs f \leq \int_I g.
			\end{equation}
			\end{proof}

			\begin{rmq} Soient $f, g : [a, b) \to \R$. On dit que $f$ est \textit{équivalent} à $g$ en $b^-$ lorsque~:
			\begin{equation}
				\forall \varepsilon > 0 : \exists \eta > 0 \tq \forall x \in [b-\eta, b) : \abs {f(x)-g(x)} \leq \varepsilon \abs {f(x)}.
			\end{equation}
			\end{rmq}

			\begin{prp} Soit $I = [a, b)$, et soit $f, g : I \to \R$, R-int sur tout segment de $I$. Alors~:
			\begin{enumerate}
				\item si $\abs f \underset {b^-}\sim g$, alors $f$ est abs-int sur $I$ si et seulement si $g$ l'est~;
				\item dans le cas abs-int, on a~:
				\begin{equation}
					\int_x^b\abs f \underset {b^-}\sim \int_x^b g.
				\end{equation}
				Dans le cas non-abs-int, on a~:
				\begin{equation}
					\int_a^x\abs f \underset {b^-}\sim \int_a^x g.
				\end{equation}
			\end{enumerate}
			\end{prp}

			\begin{proof}~
			\begin{itemize}
				\item Supposons $f$ abs-int. Pour $\varepsilon = 1$, il existe $\eta > 0$ tel que~:
				\begin{equation}
					\forall x \in [b - \eta, b) : \abs {\abs {f(x)} - g(x)} \leq \varepsilon \abs {f(x)} = \abs {f(x)}.
				\end{equation}
				On en déduit $(0 \leq g(x) \leq 2\abs {f(x)}$ sur $[b-\eta, b)$. Ainsi, par le critère de comparaison, $g$ est abs-int sur $[b-\eta, b)$. De plus,
				$g$ est abs-int sur $[a, b-\eta]$ pour tout $\eta$, et donc $g$ est abs-int sur $[a, b)$. On montre que si $g$ est abs-int, alors $f$ est abs-int, de la
				même manière (critère de comparaison).
				\item Dans le cas abs-int, fixons $\varepsilon > 0$. Il existe $\eta > 0$ tel que pour $x \in [b-\eta, b)$, on a
				$\abs {\abs {f(x)}-g(x)} \leq \varepsilon g(x)$. Pour $x > b-\eta$, il vient~:
				\begin{equation}
					\abs {\int_x^b\abs f - \int_x^b g} \leq \int_x^b\abs {\abs f - g}.
				\end{equation}

				Dans le cas non-abs-int, fixons $\varepsilon > 0$. Il existe $\eta_1$ tel que pour $x \in [b-\eta_1, b)$, on a~:
				\begin{equation}
					\abs {\abs {f(x)} - g(x)} \leq \varepsilon g(x).
				\end{equation}
				Pour $x \geq b-\eta_1$, on a~:
				\begin{equation}
					\abs {\int_a^x \abs f - \int_a^x g} \leq \int_a^{b- \eta}\abs {\abs {f(t)} - g(t)}\dif t + \int_{b-\eta}^b\abs {\abs {f(t)} - g(t)}\dif t.
				\end{equation}
				Puisque $g$ n'est pas abs-int sur $[a, b)$, il existe $\eta_2 \in (0, \eta_1)$ tel que pour $x \geq b-\eta_2$, on a~:
				\begin{equation}
					\frac {\int_a^{b-\eta_1}\abs {\abs f - g}}{\int_a^x g} \leq \varepsilon.
				\end{equation}
				Par suite, on a pour $x \geq b - \eta_2$~:
				\begin{equation}
					\abs {\int_a^x \abs f - \int_a^x g} \leq \varepsilon \int_a^xg + \varepsilon\int_{b-\eta_1}^x g \leq 2\varepsilon \int_a^x g.
				\end{equation}
			\end{itemize}
			\end{proof}

		\subsection{Fonctions de référence de Riemann}
			\begin{prp} Soit $\alpha \in \R$. $x \mapsto x^{-\alpha}$ est abs-int sur $[1, \pinfty)$ si et seulement si $\alpha > 1$.
			\end{prp}

			\begin{proof} Remarquons que $x \mapsto x^{-\alpha}$ est continue sur $[1, \pinfty)$, et donc R-int sur tout segment de $[1, \pinfty)$. De plus,
			elle est positive sur $[1, \pinfty)$. Pour $X \gneqq 1$, on a~:
			\begin{equation}
				\int_1^X\abs {\frac 1{x^\alpha}}\dif x = \int_1^X \frac {\dif x}{x^\alpha} = \int_1^Xx^{-\alpha}\dif x.
			\end{equation}
			\begin{itemize}
				\item si $\alpha \neq 1$, alors~:
				\begin{equation}
					\int_1^Xx^{-\alpha}\dif x = \frac 1{1-\alpha}\left[x^{1-\alpha}\right]_1^X = \frac {X^{1-\alpha}}{1-\alpha} - \frac 1{1-\alpha}~;
				\end{equation}
				\item si $\alpha \gneqq 1$, alors~:
				\begin{equation}
					\int_1^X\frac {\dif x}{x^\alpha} \xrightarrow[X \to \pinfty]{} \frac 1{1-\alpha},
				\end{equation}
				et~:
				\begin{equation}
					\int_1^X\abs {x^{-\alpha}}\dif x \leq \frac 1{1-\alpha}.
				\end{equation}
				Donc l'intégrale de $x \mapsto x^{-\alpha}$ sur les segments de $[1, \pinfty)$ est majorée indépendamment du segment, donc cette fonction est abs-int sur
				$[1, \pinfty)$.
				\item si $\alpha \lneqq 1$, alors~:
				\begin{equation}
					\int_1^X \frac {\dif x}{x^\alpha} \xrightarrow[X \to \pinfty]{} \pinfty.
				\end{equation}
				Donc l'intégrale de $x \mapsto x^{-\alpha}$ sur les segments de $[1, \pinfty)$ n'est pas majorée indépendamment du segment, donc cette fonction n'est pas
				abs-int sur $[1, \pinfty)$.
			\end{itemize}

			Finalement, si $\alpha = 1$, alors~:
			\begin{equation}
				\int_1^X\frac {\dif x}x = \left[\ln x\right]_1^X = \ln X \xrightarrow[X \to \pinfty]{} \pinfty.
			\end{equation}
			À nouveau, l'intégrale n'est pas bornée sur les segments de $[1, \pinfty)$, indépendamment du segment, et donc $x \mapsto x^{-1}$ n'est pas abs-int sur
			$[1, \pinfty)$.
			\end{proof}

			\begin{prp} Soit $\alpha \in \R$. $x \mapsto x^{-\alpha}$ est abs-int sur $(0, 1]$ si et seulement si $a \lneqq 1$.
			\end{prp}

			\begin{proof} EXERCICE.
			\end{proof}

			\begin{ex} La fonction $x \mapsto \frac 1{\sqrt x} = x^{-\frac 12}$ est absolument intégrable sur $(0, 1]$.
			\end{ex}

		\subsection{Théorème du changement de variable}
			\begin{thm} Soient $I, J$, deux intervalles non-vides de $\R$ et non réduits à un point. Soit $\varphi : I \to J$ bijective et strictement croissante de
			classe $C^1$ sur $I$. Soit $f : J \to \R$ R-int sur tout segment de $J$. La fonction $f$ est abs-int sur $J$ si et seulement si
			$(f \circ \varphi) \cdot \varphi' : I \to \R$ est abs-int sur $I$, et on a~:
			\begin{equation}
				\int_I\left((f \circ \varphi)\varphi'\right)(x)\dif x = \int_Jf(y)\dif y.
			\end{equation}
			\end{thm}

			\begin{proof} Soit $([a_n, b_n])_n$ une suite exhaustive de segments de $I$. La suite $([\varphi(a_n), \varphi(b_n)])_n$ est une suite exhaustive de
			segments de $J$ (car $\varphi$ est bijective). Puisque~:
			\begin{equation}
				\forall n \in \N : \int_{a_n}^{b_n}\abs {(f \circ \varphi)\varphi'} = \int_{a_n}^{b_n}\abs {(f \circ \varphi)}\varphi'
				= \int_{\varphi(a_n)}^{\varphi(b_n)}\abs {f},
			\end{equation}
				par \CDII, on conclut que $(f \circ \varphi)\varphi'$ est abs-int sur $I$ si $f$ l'est sur $J$.

				On raisonne de manière similaire avec $\varphi^{-1}$ (qui existe car $\varphi$ est une bijection) pour montrer que $f$ est abs-int sur $J$ si
				$(f \circ \varphi)\varphi\prime$ l'est sur $I$.

				Dans ce cas, si $([a_n, b_n])_n$ est une suite exhaustive de segments de $I$, on a~:
				\begin{equation}
					\forall n \geq 0 : \int_{a_n}^{b_n}(f \circ \varphi)\varphi\prime = \int_{\varphi(a_n)}^{\varphi(b_n)}f.
				\end{equation}
				En passant à la limite pour $n \to \pinfty$, on obtient~:
				\begin{equation}
					\int_I(f \circ \varphi)\varphi\prime = \int_J f,
				\end{equation}
				car $([\varphi(a_n), \varphi(b_n)])_n$ est une suite exhaustive de $J$.
			\end{proof}

	\section{Intégrales convergentes}
		\subsection{Définitions et exemples}
			\begin{déf} Soit $I \subset \R$, un intervalle non-vide, et soit $f : I \to \R$ R-int sur tous les segments de $I$. On dit que l'intégrale de $f$ sur $I$
			converge lorsque~:
			\begin{itemize}
				\item[$(i)$]  $\forall ([a_n, b_n])_n$ exhaustive de $I$ : $\left(\int_{a_n}^{b_n}f\right)_n$ converge dans $\R$~;
				\item[$(ii)$] la limite ne dépend pas de la suit exhaustive choisie.
			\end{itemize}

			On note cette limite $\int_I f$.
			\end{déf}

			\begin{prp} Si $f$ est abs-int sur $I$, alors son intégrale sur $I$ converge et on a~:
			\begin{equation}
				\int_I f(x)\dif x = \int_I f(x)\dif x,
			\end{equation}
			c-à-d, les deux notions ont le même sens pour la même notation.
			\end{prp}

			\begin{proof} Par la Proposition~\ref{prp:abs-int => convergence}
			\end{proof}

			\begin{ex} La fonction $\mapsto \frac {sin x}x$ a une intégrale convergente sur $[1, \pinfty)$ mais n'est pas abs-int sur $[1, \pinfty)$.

			On remarque que $\frac {\sin x}x$ est continue sur $[1, \pinfty)$ et donc R-int sur tout segment de $I$. Soit $([a_n, b_n])_n$, une suite exhaustive de
			segments de $[1, \pinfty)$. Pour $n \in \N$, écrivons~:
			\begin{equation}
				\int_{a_n}^{b_n}\frac {\sin x}x\dif x = \left[\frac{-\cos x}x\right]_{a_n}^{b_n} + \int_{a_n}^{b_n}\frac{\cos x}{x^2}\dif x.
			\end{equation}
			On sait que $\abs {\frac{\cos x}{x^2}} \leq \frac 1{x^2}$ sur $[1, \pinfty)$. Par le critère de comparaison, puisque $x \mapsto \frac 1{x^2}$ est abs-int
			sur $[1, \pinfty)$, on sait que $x \mapsto \frac{\cos x}{x^2}$ l'est également. Ainsi, la suite~:
			\begin{equation}
				\left(\int_{a_n}^{b_n}\frac{\cos x}{x^2}\dif x\right)_n
			\end{equation}
			converge dans $\R$ vers une limite qui ne dépend pas de la suite $([a_n, b_n])_n$ choisie. Par ailleurs~:
			\begin{equation}
				\left[-\frac {\cos x}x\right]_{a_n}^{b_n} = \frac{-\cos b_n}{b_n} + \frac{\cos a_n}{a_n} \xrightarrow[{[a_n, b_n] \to [1, \pinfty)}]{} 0 + \cos 1.
			\end{equation}
			Donc la suite $\left(\int_{a_n}^{b_n}\frac{\sin x}x\dif x\right)_n$ converge vers une limite indépendante de la suite exhaustive de segments choisie.
			Donc l'intégrale de $x \mapsto \frac{\sin x}x$ converge dans $[1, \pinfty)$.

			Montrons maintenant que la fonction n'est pas abs-int. Soit $n \in \Ns$. On sait~:
			\begin{align}
				\int_\pi^{n\pi}\abs {\frac{\sin x}x}\dif x &= \sum_{k=1}^{n-1}\int_{k\pi}^{(k+1)\pi}\abs {\frac{\sin x}x}\dif x
				\geq \sum_{k=1}^{n-1}\int_{k\pi}^{(k+1)\pi}\frac {\abs {\sin x}}{(k+1)\pi}\dif x \geq \sum_{k=1}^{n-1}\frac {\int_0^\pi\abs {\sin x}\dif x}{(k+1)\pi} \\
				&= \frac {\int_0^\pi\abs {\sin x}\dif x}{\pi}\sum_{k=1}^{n-1} \frac 1{k+1}\xrightarrow[n \to \pinfty]{} \pinfty.
			\end{align}

			On a donc bien $x \mapsto \frac{\sin x}x$ non-abs-int sur $[1, \pinfty)$.
			\end{ex}

			\begin{ex} $\sign : x \mapsto \begin{cases}1 &\text{ si } x > 0 \\0 &\text{ si }x = 0\\-1 &\text{ si } x < 0\end{cases}$ n'admet pas d'intégrale convergente
			dans $\R$. Par exemple~:
			\begin{equation}
				\int_{-n}^{n^2}\sign(x)\dif x = n^2 - n \xrightarrow[n \to \pinfty]{} \pinfty.
			\end{equation}
			\end{ex}

			\begin{prp} Soient $I \subset \R$, un intervalle non-vide, $f : I \to \R$, R-int sur tout segment de $I$. L'intégrale de $f$ converge sur $I$ si et seulement
			si pour toute suite exhaustive de segments $([a_n, b_n])_n$ de $I$, la suite $\left(\int_{a_n}^{b_n}f(x)\dif x\right)_n$ est de Cauchy.
			\end{prp}

			\begin{proof} Toute suite convergente est de Cauchy dans $\R$.

			Pour montrer que si la suite d'intégrales sur une suite exhaustive de segments est de Cauchy, alors l'intégrale est convergente, on sait que toute
			suite réelle de Cauchy converge, et par la Proposition~\ref{prp:abs-int => convergence}, on a la non-dépendance de la suite exhaustive.
			\end{proof}

		\subsection{Rappel : deuxième formule de la moyenne}
			Pour rappel, la première formule de la moyenne et donnée par~:

			Soient $f, g : [a, b] \to \R$, de classe $C^0$ sur $[a, b]$, avec $g \geq 0$. Il existe $c \in [a, b]$ tel que~:
			\begin{equation}
				\int_a^b(fg)(t)\dif t = f(c)\int_a^bg(t)\dif t.
			\end{equation}

			\begin{prp} Soit $[a, b]$, un segment de $\R$, et soient $f, g : [a, b] \to \R$, avec $f$ R-int sur $[a, b]$, et $g \geq 0$, décroissante sur $[a, b]$.
			Alors il existe $c \in [a, b]$ tel que~:
			\begin{equation}
				\int_a^b(fg)(t)\dif t = g(a)\int_a^cf(t)\dif t.
			\end{equation}
			\end{prp}

			\begin{proof} On fixe $N \in \Ns$, et on pose $t_n \coloneqq a + n\frac {b-a}N$ pour $0 \leq n \leq N$. Écrivons~:
			\begin{equation}
				I_N \coloneqq \sum_{k=0}^{N-1}\int_{t_k}^{t_{k+1}}f(t)g(t_k)\dif t.
			\end{equation}
			On observe alors~:
			\begin{equation}
				I_N - \int_a^b(fg)(t)\dif t = \sum_{k=0}^{N-1}f(t)(g(t_k) - g(t)).
			\end{equation}
			On trouve donc~:
			\begin{align}
				\abs {I_N - \int_a^b(fg)(t)\dif t} &\leq \sum_{k=0}^{N-1} \int_{t_k}^{t_{k+1}}\abs {f(t)}\abs {g(t_k) - g(t)}\dif t
					= \sum_{k=0}^{N-1}\int_{t_k}^{t_{k+1}}\abs {f(t)}(g(t_k) - g(t))\dif t \\
				&\leq \sum_{k=0}^{N-1}\int_{t_k}^{t_{k+1}}\abs {f(t)}(g(t_k)-g(t_{k+1}))\dif t.
			\end{align}

			Rappelons que la fonction $K : [a, b] \to \R : x \mapsto \int_a^x\abs {f(t)}\dif t$ est lipschitzienne de constante $\norm f_{\infty, [a, b]} \eqqcolon M$.
			Ainsi~:
			\begin{align}
				\abs {I_N - \int_a^b(fg)(t)\dif t} &\leq \sum_{k=0}^{N-1}\abs {K(t_{k+1}) - K(t_k)}(g(t_k)-g(t_{k+1}))
					\leq M\sum_{k=0}^{N-1}(t_{k+1}-t_k)(g(t_k)-g(t_{k+1})) \\
				&= \frac {M(b-a)}N\sum_{k=0}^{N-1}(g(t_k) - g(t_{k+1})) \leq \frac {M(b-a)}N(g(a)-g(b)) \xrightarrow[N \to \pinfty]{} 0.
			\end{align}

			On en déduit que $(I_N)_{N > 1}$ converge, et~:
			\begin{equation}
				\lim_{N \to \pinfty}I_N = \int_a^b(fg)(t)\dif t.
			\end{equation}

			Par ailleurs, pour $N \geq 1$, on a~:
			\begin{equation}
				I_N = \sum_{k=0}^{N-1}\int_{t_k}^{t_{k+1}}f(x)\dif xg(t_k).
			\end{equation}

			Posons ensuite~:
			\begin{equation}
				F : [a, b] \to \R : x \mapsto \int_a^xf(t)\dif t.
			\end{equation}
			On sait que $F$ est continue sur le segment $[a, b]$, donc minorée par $m$ et majorée par $M$. En appliquant une transformation d'Abel, on trouve~:
			\begin{align}
				I_N &= \sum_{k=0}^{N-1}\left(F(t_{k+1}) - F(t_k)\right)g(t_k) = \sum_{k=0}^{N-1}F(t_{k+1})g(t_k) - \sum_{k=0}^{N-1}F(t_k)g(t_k) \\
					&= \sum_{k=1}^NF(t_k)g(t_{k-1}) - \sum_{k=0}^{N-1}F(t_k)g(t_k) \\
					&= -F(t_0)g(t_0) + \sum_{k=1}^{N-1}F(t_k)\left(g(t_{k-1})-g(t_k)\right) + F(t_N)g(t_{N-1}).
			\end{align}

			Par suite, on sait que $F(t_0) = F(a) = \int_a^a f = 0$, on peut donc exprimer~:
			\begin{equation}
				m\sum_{k=1}^{N-1}\left(g(t_k-1)-g(t_k)\right)g(t_k) + mg(t_{N-1}) \leq I_N \leq M\sum_{k=1}^{N-1}\left(g(t_{k-1}) - g(t_k)\right) + Mg(t_{N-1}).
			\end{equation}

			En développant les sommes, on trouve~:
			\begin{equation}
				mg(a) \leq I_N \leq Mg(a).
			\end{equation}

			En passant à la limite pour $N \to \pinfty$, on trouve~:
			\begin{equation}
				mg(a) \leq \int_a^b(fg)(t)\dif t \leq Mg(a).
			\end{equation}

			Distinguons alors deux cas~:
			\begin{itemize}
				\item si $g(a) = 0$, alors $g \equiv 0$ sur $[a, b]$, et donc tout $c \in [a, b]$ convient~;
				\item si $g(a) \neq 0$, alors~:
				\begin{equation}
					m \leq \frac 1{g(a)}\int_a^b(fg)(t)\dif t \leq M.
				\end{equation}

				La fonction $F$ étant continue sur le segment $[a, b]$, par le théorème des valeurs intermédiaires, il existe $c \in [a, b]$ tel que~:
				\begin{equation}
					F(c) = \frac 1{g(a)}\int_a^b(fg)(t)\dif t,
				\end{equation}
				et donc en remultipliant par $g(a)$ de part et d'autre, on obtient~:
				\begin{equation}
					g(a)F(c) = g(a)\int_a^cf(t)\dif t = \int_a^b(fg)(t)\dif t.
				\end{equation}
			\end{itemize}
			\end{proof}

		\subsection{Critère d'Abel}
			\begin{prp}[Critère d'Abel pour la convergence des intégrales] Soient $\minfty < a < b \leq \pinfty$, $f, g : [a, b] \to \R$ telles que~:
			\begin{itemize}
				\item $f$ est R-int sur tout segment de $[a, b)$ et il existe $M \gneqq 0$ tel que~:
				\begin{equation}
					\forall x \in [a, b) : \abs {\int_a^x f(t)\dif t} \leq M~;
				\end{equation}
				\item $g$ est positive et décroissante vers $0$ en $b^-$ sur $[a, b)$.
			\end{itemize}

			Alors l'intégrale de $fg$ converge sur $I$.
			\end{prp}

			\begin{proof} Soit $([a_n, b_n])_n$ une suite exhaustive de segments de $[a, b)$. Posons, pour $n \in \N$~:
			\begin{equation}
				\alpha_n \coloneqq \int_{a_n}^{b_n}(fg)(t)\dif t.
			\end{equation}
			Observons pour $n, p \in \N$~:
			\begin{equation}
				\alpha_{n+p} - \alpha_n = \int_{a_{n+p}}^{b_{n+p}}(fg)(t)\dif t - \int_{a_n}^{b_n}(fg)(t)\dif t.
			\end{equation}

			Puisque $a_n = a$ à partir d'un certain $N \in \N$, on peut écrire~:
			\begin{equation}
				\alpha_{n+p} - \alpha_n = \int_{b_n}^{b_{n+p}}(fg)(t)\dif t = g(b_n)\int_a^{c_{n+p}}f(t)\dif t,
			\end{equation}
			pour un certain $c_{n+p} \in [b_n, b_{n+p}]$ par la deuxième formule de la moyenne. On trouve finalement~:
			\begin{equation}
				\abs {\alpha_{n+p} - \alpha_n} \leq g(b_n) \cdot \abs {\int_a^{c_{n+p}}f(t)\dif t - \int_a^{b_n}f(t)\dif t} \leq 2Mg(b_n) \xrightarrow[n \to \pinfty]{} 0,
			\end{equation}
			par décroissance de $g$ vers $0$.

			La suite $(\alpha_n)_n$ est donc une suite de Cauchy, ce qui implique que l'intégrale de $(fg)$ converge sur $[a, b)$, par le critère de Cauchy.
			\end{proof}

			\begin{ex} Prenons $\beta \in (0, 1)$. La fonction $x \mapsto \frac {\sin x}{x^\beta}$ est d'intégrale convergente sur $[1, \pinfty)$ car~:
			\begin{itemize}
				\item $f : [1, \pinfty) : x \mapsto \sin x$ est R-int sur tous les segments de $[1, \pinfty)$, et pour $x \geq 1$, on a~:
				\begin{equation}
					\abs {\int_1^x\sin t\dif t} \leq 2~;
				\end{equation}
				\item $g$ est positive et décroissante vers $0$ en $\pinfty$.
			\end{itemize}

			La convergence est assurée par le critère d'Abel.
			\end{ex}

			\begin{ex}Les fonctions $x \mapsto \sin(x^2)$ et $x \mapsto \cos(x^2)$ sont d'intégrale convergente sur $\Rp$. Soit $([\alpha_n, \beta_n])_n$, une suite
			exhaustive de segments de $\Rp$. Pour $n \geq 0$, écrivons~:
			\begin{equation}
				\int_{\alpha_n}^{\beta_n}\sin(x^2)\dif x = \int_{\alpha_n^2}^{\beta_n^2}\sin t\frac {\dif t}{\sqrt t}
			\end{equation}
			pour $t = x^2, \dif t = 2x\dif x$. On a donc~:
			\begin{equation}
				\frac 12\int_{\alpha_n^2}^{\beta_n^2}\sin t\frac {\dif t}{\sqrt t}
				= \frac 12\int_{\alpha_n^2}^{\beta_n^2}\sin t \frac {\dif t}{\sqrt t} + \frac 12\int_{\alpha_n^2}^{\beta_n^2}\sin t\frac {\dif t}{\sqrt t}.
			\end{equation}

			La fonction $t \mapsto \frac {\sin t}{\sqrt t}$ est $C^0$ sur $(0, 1]$ et prolongeable en $0$ par continuité. Elle est donc abs-int sur $(0, 1]$. Donc
			$\int_{\alpha_n^2}^1\sin t \frac {\dif t}{\sqrt t}$ converge et la limite ne dépend pas de la suite $\alpha_n \to 0$ choisie.
			La fonction $t \mapsto \sin t$ est R-int sur tout segment de $[1, \pinfty)$, avec~:
			\begin{equation}
				\abs {\int_1^x\sin t\dif t} \leq 2.
			\end{equation}
			La fonction $t \mapsto \frac 1{\sqrt t}$ est décroissante vers $0$ en $\pinfty$. Alors par le critère d'Abel,
			$\int_1^{\beta_n^2}\sin t\frac {\dif t}{\sqrt t}$ converge et la limite ne dépend pas de la suite $\beta \to \pinfty$ choisie.

			On en déduit que $x \mapsto \sin(x^2)$ est d'intégrale convergente sur $\Rp$. On raisonne de manière similaire pour $x \mapsto \cos(x^2)$.
			\end{ex}

\chapter{Intégrales à paramètres}
	\section{Fonctions définies par une intégrale sur un segment fixe}
		\subsection{Un résultat de continuité}
			\begin{déf} L'ensemble $X$ est dit \textit{localement} compact lorsque~:
			\begin{equation}
				\forall x \in X : \forall O \text{ ouvert } : x \in O \Rightarrow \exists V \text{ compact } \in \mathcal V(x) \tq O \subset V.
			\end{equation}
			\end{déf}

			\begin{prp}\label{prp:intparamcontinuesidefsurcpct} Soit $[a, b]$ un segment de $\R$, et $(X, d)$ un espace métrique localement compact.
			Si $f : X \times [a, b] \to \R : (x, t) \mapsto f(x, t)$ est continue sur $X \times [a, b]$, alors~:
			\begin{equation}
				F : X \to \R : x \mapsto \int_a^bf(x, t)\dif t
			\end{equation}
			est définie, et continue sur $X$.
			\end{prp}

			\begin{proof} Soit $x \in X$. $t \mapsto f(x, t)$ est continue sur $[a, b]$, donc R-int sur $[a, b]$. La fonction $F$ est donc en effet définie.

			Soit $x \in X$ et soit $V \in \mathcal V(x)$ compact dans $X$. La fonction~:
			\begin{equation}
				V \times [a, b] \to \R : (x, t) \mapsto f(x, t)
			\end{equation}
			est continue sur $V \times [a, b]$. En particulier, par le théorème de Heine, elle est uniformément continue sur $V \times [a, b]$, car $V \times [a, b]$
			est compact. Soit $\varepsilon > 0$ et soit $x_0 \in X$. Il existe $\eta > 0$ et $\delta > 0$ tels que~: $\forall x, y \in V : \forall t, t' \in [a, b]$,
			si $d(x, y) < \eta$ et $\abs {t'-t} < \delta$, et $B(x_0, \eta[ \subset V$, alors $\abs {f(x, t) - f(y, t')} \leq \varepsilon$.

			En particulier, pour $x \in B(x_0, \eta[$ et $t \in [a, b]$, on a~:
			\begin{equation}
				\abs {f(x, t) - f(x_0, t)} < \varepsilon.
			\end{equation}

			Par intégration, on trouve~:
			\begin{equation}
				\abs {F(x) - F(x_0)} \leq \int_a^b\abs {f(x, t) - f(x_0, t)}\dif t \leq \varepsilon (b-a).
			\end{equation}
			\end{proof}

			\begin{rmq} Cette proposition est encore vraie pour un compact $\prod_{i=1}^n[a_i, b_i]$, au lieu de $[a, b]$.
			\end{rmq}

			\begin{ex} Que dire de $\lim_{x \to 0^+} \int_0^1\sin(\exp(-xt) - 1)\dif t$~?

			$f : [0, 1] \times [0, 1]  \to \R : (x, t) \mapsto \sin(\exp(-xt) - 1)$ est continue sur $[0, 1] \times [0, 1]$, d'où $F(x) = \int_0^1f(x, t)\dif t$
			est de classe $C^0$ sur $[0, 1]$, avec la proposition précédente. Donc $F(x) \xrightarrow[x \to 0^+]{} 0$.
			\end{ex}

		\subsection{Un résultat de dérivabilité}
			\begin{prp}\label{prp:dérivabilitésegmentfixe} Soient $X \subset \R^d$, un ouvert non-vide, $[a, b] \subset \R$, un segment, et
			$f : X \times [a, b] \to \R$, continue sur $X \times [a, b]$ admettant une dérivée partielle par rapport à tout $x_i$ en tout point de $X$ tel que~:
			\begin{equation}
				\forall i \in \intint 1d : X \times [a, b] \to \R : (x, t) \mapsto \pd f{x_i}(x, t) \in C^0(X \times [a, b], \R).
			\end{equation}
			Alors la fonction $F : X \to \R : x \mapsto \int_a^b f(x, t)\dif t$ est de classe $C^1$ sur $X$, et on a~:
			\begin{equation}
				\forall i \in \intint 1d : \pd F{x_i}(x) = \int_a^b \pd f{x_i}(x, t)\dif t.
			\end{equation}
			\end{prp}

			\begin{proof} Pour $x_0 \in X, \delta > 0 \tq B(x_0, \delta[ \subset X$, puisque $x \mapsto f(x, t)$ est de classe $C^1$ sur $B(x_0, \delta[$,
			pour $t \in [a, b]$, on peut écrire~:
			\begin{equation}
				f(x_0 + he_i, t) - f(x_0, t) = \int_0^h \pd f{x_i}(x_0 + se_i, t)\dif s.
			\end{equation}

			Ces fonctions étant continues, elles sont R-int, et on a~:
			\begin{equation}
				\int_a^b\left(f(x_0 + he_i, t) - f(x_0, t)\right)\dif t = \int_a^b\int_0^h \pd f{x_i}(x_0 + se_i, t)\dif s\dif t
				= \int_a^b h\int_0^1\pd f{x_i}(x_0 + hse_i, t)\dif s\dif t.
			\end{equation}

			Pour $h \in \left[-\frac \delta2, \frac \delta2\right] \setminus \{0\}$, et en posant~:
			\begin{equation}
				g : \left[-\frac \delta2, \frac \delta2\right] \times \left([0, 1] \times [a, b]\right) : (h, (s, t)) \mapsto \pd f{x_i}(x_0 + hse_i, t),
			\end{equation}
			on a~:
			\begin{equation}
				\frac 1h\int_a^b \left(f(x_0 + he_i, t) - f(x_0, t)\right)\dif t = \int_a^b\int_0^1g(h, (s, t))\dif s\dif t.
			\end{equation}

			Or la fonction $g$ est continue sur son compact de définition, par continuité de $f$. Par la Proposition~\ref{prp:intparamcontinuesidefsurcpct}, on
			sait que la fonction~:
			\begin{equation}
				h \mapsto \int_a^b\int_0^1g(h, (s, t))\dif s\dif t \in C^0([-\frac \delta2, \frac \delta2], \R).
			\end{equation}
			Par cette continuité, on déduit que la fonction $h \mapsto \frac 1h\left(F(x_0 + he_i) - F(x_0)\right)$ admet une limite finie en $h=0$ qui est
			$\int_a^b\int_0^1g(0, (s, t))\dif s\dif t$. On en déduit que $F$ admet une dérivée partielle par rapport à $x_i$ en $x_0$, et on a~:
			\begin{equation}
				\pd F{x_i}(x) = \int_a^b\int_0^1\pd f{x_i}(x_0 + 0, t)\dif s\dif t = \int_a^b\pd f{x_i}(x_0, t)\int_0^1\dif s\dif t = \int_a^b\pd f{x_i}(x_0, t)\dif t.
			\end{equation}

			À nouveau, par la Proposition~\ref{prp:intparamcontinuesidefsurcpct} appliquée à $(x, t) \mapsto \pd f{x_i}(x, t)$, on obtient
			$\pd F{x_i} \in C^0(X, \R)$, ce qui implique $F \in C^1(X, \R)$, et on a bien la formule ci-dessus.
			\end{proof}

	\section{Fonction définies par des intégrales sur un segment variable}
		\subsection{Un résultat de continuité}
			\begin{thm}\label{thm:continuitésegmentvariable} Soit $f : [\alpha, \beta] \times [a, b] \tocont \R$. La fonction
			$F : [\alpha, \beta] \times [a, b] \to \R : (x, T) \mapsto \int_a^Tf(x, t)\dif t$ est continue sur $[\alpha, \beta] \times [a, b]$.
			\end{thm}

			\begin{proof} Soient $(x, T) \in [\alpha, \beta] \times [a, b]$, $(\Delta x, \Delta T) \in \R^2$ tels que~:
			\begin{equation}
				(x + \Delta x, T + \Delta T) \in [\alpha, \beta] \times [a, b].
			\end{equation}
			Écrivons alors~:
			\begin{align}
				F(x + \Delta x, T + \Delta T) - F(x, T) &= \int_a^{T+\Delta T}f(x+\Delta x, t)\dif t - \int_a^Tf(x, t)\dif t \\
				&= \int_a^Tf(x+\Delta x, t)\dif t + \int_T^{T+\Delta T}f(x +\Delta x, t)\dif t - \int_a^Tf(x, t)\dif t \\
				&= \int_a^T\left(f(x+\Delta x, t) - f(x, t)\right)\dif t + \int_T^{T + \Delta T}f(x+\Delta x, t)\dif t.
			\end{align}

			Par continuité de $f$ sur le compact $[\alpha, \beta] \times [a, b]$ et le théorème de Heine, on peut dire que $f$ est uniformément continue sur
			ce compact. On a alors pour $\varepsilon > 0$, il existe $\eta > 0$ tel que si $\abs {\Delta x} \lneqq \eta$, alors~:
			\begin{equation}
				\forall t \in [a, b] : \abs {f(x+\Delta x, t) - f(x, t)} < \frac \varepsilon{b-a},
			\end{equation}
			et donc~:
			\begin{equation}
				\abs {\int_a^T\left(f(x+\Delta x, t) - f(x, t)\right)\dif t} \leq (T-a)\max_{t \in [a, b]}\abs {f(x+\Delta x, t) - f(x, t)} \leq \varepsilon.
			\end{equation}

			Par ailleurs, la fonction $f$ est bornée sur son compact de définition. Donc il existe $M > 0$ tel que
			$\forall (y, t) \in [\alpha, \beta] \times [a, b] : \abs {f(y, t)} \leq M$. Si $\abs {\Delta T} < \frac \varepsilon M$ , on observe~:
			\begin{equation}
				\abs {\int_T^{T + \Delta T}f(x + \Delta x, t)\dif t} \leq M\abs {\Delta T} < \varepsilon.
			\end{equation}

			Dès lors, pour $\abs {\Delta x} < \eta$ et $\abs {\Delta T} < \frac \varepsilon M$, on a~:
			\begin{equation}
				\abs {F(x+\Delta x, T + \Delta T) - F(x, T)} < \varepsilon.
			\end{equation}

			On a en effet trouvé un voisinage de $(x, T)$ qui est envoyé sur un voisinage de $F(x, T)$. La fonction $F$ est donc continue.
			\end{proof}

			\begin{cor} Soit $f : [\alpha, \beta] \times [a, b] \to \R$ continue. Soient $\phi, \psi = [\alpha, \beta] \to [a, b]$ continues sur $[\alpha, \beta]$.
			La fonction $F : [\alpha, \beta] \to \R : x \mapsto \int_{\phi(x)}^{\psi(x)}f(x, t)\dif t$ est continue sur $[\alpha, \beta]$.
			\end{cor}

			\begin{proof} Posons $G : [\alpha, \beta] \times [a, b] \to \R : (x, T) \mapsto \int_a^T f(x, t)\dif t$. Par le théorème précédent, on peut dire
			$G \in C^0([\alpha, \beta] \times [a, b], \R)$. De plus, pour $x \in [a, b]$, on peut écrire~:
			\begin{equation}
				F(x) = \int_a^{\psi(x)}f(x, t)\dif t - \int_a^{\phi(x)}f(x, t)\dif t = G(x, \psi(x)) - G(x, \phi(x)).
			\end{equation}

			Par continuité des fonctions $G, \psi, \phi$, on sait que $x \mapsto G(x, \psi(x))$ et $x \mapsto G(x, \phi(x))$ sont continues sur $[\alpha, \beta]$
			par composition de fonctions continues. Ensuite, on peut dire que $F$ est continue sur $[\alpha, \beta]$ par différence de fonctions continues.
			\end{proof}

		\subsection{Un résultat de dérivabilité}
			\begin{thm}\label{thm:dérivabilitésegmentvariable} Soit $f : [\alpha, \beta] \times [a, b] \to \R$ continue sur $[\alpha, \beta] \times [a, b]$
			telle que $\pd fx$ existe en tout point de $[\alpha, \beta] \times [a, b]$ et $(x, t) \mapsto \pd fx(x, t)$ est continue sur
			$[\alpha, \beta] \times [a, b]$. Alors la fonction $F : [\alpha, \beta] \times [a, b] \to \R : (x, T) \mapsto \int_a^Tf(x, t)\dif t$ est de classe
			$C^1$ sur $[\alpha, \beta] \times [a, b]$ avec~:
			\begin{equation}
				\pd Fx(x, T) = \int_a^T\pd fx(x, t)\dif t \qquad\qquad\text{ et }\qquad\qquad \pd FT(x, T) = f(x, T).
			\end{equation}
			\end{thm}

			\begin{proof} Avec le résultat de dérivabilité sur segment fixe (Proposition~\ref{prp:dérivabilitésegmentfixe}), on sait que $\pd Fx(x, T)$ existe
			en tout point et vaut $\pd Fx(x, T) = \int_a^T\pd fx(x, t)\dif t$, avec $\pd fx(x, T) \in C^0([\alpha, \beta] \times [a, b], \R)$ par le
			Théorème~\ref{thm:continuitésegmentvariable}.

			De plus, par \CDII, on sait que $\pd FT(x, T)$ existe et vaut $f(x, T)$ (continue par hypothèse) car $f(x, \cdot) \in C^0([a, b], \R)$. On sait
			également que $T \mapsto \int_a^T \pd fT(x, t)\dif t \in C^1([\alpha, \beta] \times [a, b], \R)$.

			On a donc bien $F \in C^1([\alpha, \beta] \times [a, b], \R)$.
			\end{proof}

			\begin{prp} Soit $f : [\alpha, \beta] \times [a, b] \tocont \R$ telle que $\pd fx$ existe et est continue en tout point du compact de définition de
			$f$. Soient $\psi, \phi : [\alpha, \beta] \toC1 [a, b]$. La fonction $F : [\alpha, \beta] \to \R : x \mapsto \int_{\phi(x)}^{\psi(x)}f(x, t)\dif t$
			est de classe $C^1$ sur $[\alpha, \beta]$ et on a~:
			\begin{equation}
				\forall x \in [\alpha, \beta] : F'(x) = \psi'(x)f(x, \psi(x)) - \phi'(x)f(x, \phi(x)) + \int_{\phi(x)}^{\psi(x)}\pd fx(x, t)\dif t.
			\end{equation}
			\end{prp}

			\begin{proof} La fonction $G : [\alpha, \beta] \times [a, b] \to \R : (x, T) \to \int_a^Tf(x, t)\dif t$ est de classe $C^1$ par le
			Théorème~\ref{thm:dérivabilitésegmentvariable}, et on a~:
			\begin{equation}
				\pd Gx(x, T) = \int_a^T\pd Gx(x, t)\dif t\ \qquad\qquad\text{ et }\qquad\qquad \pd GT(x, T) = f(x, T).
			\end{equation}

			Par composition de fonctions de classe $C^1$, on sait que $x \mapsto G(x, \phi(x))$ et $x \mapsto G(x, \psi(x))$ sont de classe $C^1$ sur
			$[\alpha, \beta]$. Par ailleurs, pour $x \in [\alpha, \beta]$, on a~:
			\begin{equation}
				F(x) = G(x, \psi(x)) - G(x, \phi(x)).
			\end{equation}

			Par différence, on trouve donc $F \in C^1([\alpha, \beta], \R)$, et pour $x \in [\alpha, \beta]$, on trouve~:
			\begin{align}
				F'(x) &= \pd Gx(x, \psi(x)) + \pd GT(x, \psi(x))\psi'(x) - \pd Gx(x, \phi(x)) - \pd GT(x, \phi(x))\phi'(x) \\
				&= \int_a^{\psi(x)}\pd fx(x, t)\dif t + \psi'(x)f(x, \psi(x)) - \int_a^{\phi(x)}\pd fx(x, t)\dif t - \phi'(x)f(x, \phi(x)) \\
				&= \int_{\phi(x)}^{\psi(x)}\pd fx(x, t)\dif t + \psi'(x)f(x, \psi(x)) - \phi'(x)f(x, \phi(x)).
			\end{align}
			\end{proof}

	\section{Fonctions définies par des intégrales convergentes}
		\subsection{Exemple}\label{subsec:exempleintconv}
			Soit $f : \Rp \times \Rp \to \Rp : (x, t) \mapsto x\exp(-xt)$. Que dire de $F(x) = \int_0^\pinfty f(x, t)\dif t$ ?

			Fixons $x \in \Rp$. La fonction $t \mapsto x\exp(-xt)$est R-int sur tout segment de $\Rp$. De plus, $\abs {f(x, t)}t^2 \xrightarrow[t \to \pinfty]{} 0$
			si $x > 0$.Donc il existe $A_n \gneqq 0$ tel que~:
			\begin{equation}
				\forall t \geq A_n : \abs {f(x, t)} \leq \frac 1{t^2}.
			\end{equation}

			Par le critère de comparaison, la fonction $t \mapsto f(\cdot, t)$ est abs-int sur $[A_n, \pinfty)$. Si la fonction est abs-int sur $[0, A_n]$, alors
			elle l'est sur $[0, \pinfty)$. Lorsque $x = 0$, alors la fonction $t \mapsto f(x, t) = 0$, donc $F(0) = 0$ est bien défini.

			Pour tout $x \gneqq 0$, on trouve~:
			\begin{equation}
				F(x) = \lim_{T \to \pinfty}-\int_0^Tx\exp(-xt)\dif t = \lim_{T \to \pinfty}-\left[\exp(-xt)\right]_0^T
				= \lim_{T \to \pinfty} -\left(\exp(-xT) - 1\right) = 1.
			\end{equation}

			On en déduit que la fonction $F$ est discontinue en $0$. La fonction $f$ est en fait de classe $C^\infty$, mais
			$F(x) \coloneqq \int_0^\pinfty f(x, t)\dif t$ admet un point de discontinuité en $x = 0$.

		\subsection{Notion d'intégrales uniformément convergentes}
			\begin{déf} Soient $X \neq \emptyset, I \subset \R$, un intervalle non-vide. Soit $f : X \times I \to \R$ telle que~:
			\begin{equation}
				\forall x \in X : t \mapsto f(x, t) \text{ est d'intégrale convergente sur } I,
			\end{equation}
			à savoir~:
			\begin{equation}
				\forall x \in X : \forall \varepsilon > 0 : \exists K_{\varepsilon, x} \subset I \text{ segment } \tq
				\forall \widetilde K \subset I \text{ segment } :
					\left[K_{\varepsilon, x} \subset \widetilde K \Rightarrow \abs {\int_{\widetilde K}f(x, t)\dif t - \int_If(x, t)\dif t} < \varepsilon.\right]
			\end{equation}

			On dit que l'intégrale de $f$ sur $I$ converge uniformément sur $X$ lorsque $K_{\varepsilon, x}$ ne dépend pas de $x$, c'est-à-dire lorsque~:
			\begin{equation}
				\forall \varepsilon > 0 : \exists K_\varepsilon \subset I \text{ segment } \tq \forall \widetilde K \subset I \text{ segment } :
				\left[K_\varepsilon \subset \widetilde K \Rightarrow
					\forall x \in X : \abs {\int_{\widetilde K}f(x, t)\dif t - \int_I f(x, t)\dif t} < \varepsilon\right].
			\end{equation}
			\end{déf}

			\begin{rmq} Lorsque $I = [a, \pinfty)$, cela revient à avoir~:
			\begin{equation}
				\forall \varepsilon > 0 : \exists Y > 0 \tq \forall x \in X : \abs {\int_Y^\pinfty f(x, t)\dif t} < \varepsilon.
			\end{equation}

			Lorsque $I = [a, b]$, avec $a < b < \pinfty$, cela revient à avoir~:
			\begin{equation}
				\forall \varepsilon > 0 : \exists \delta \in (0, b-a) \tq \forall x \in X : \abs {\int_{b-\delta}^bf(x, t)\dif t} < \varepsilon.
			\end{equation}

			Dans l'exemple~\ref{subsec:exempleintconv}, $f(x, t) = x\exp(-xt)$, $X = I = \Rp$, on pouvait dire~:
			\begin{equation}
				\int_Y^\pinfty f(x, t)\dif t = \begin{cases}-\exp(-xt) &\text{ si } x > 0, \\0 &\text{ sinon}.\end{cases}
			\end{equation}

			On en déduit~:
			\begin{equation}
				\sup_{x \in X}\abs {\int_Y^\pinfty f(x, t)\dif t} = 1.
			\end{equation}
			On en déduit que l'intégrale de $t \mapsto x\exp(-xt)$ n'est pas convergente uniformément sur $\Rp$.
			\end{rmq}

			\begin{thm}\label{thm:intparamcontinuesiconvunif} Soient $(X, d)$ un espace métrique, $I \subset \R$, un intervalle non-vide Soit
			$f : X \times I \tocont \R$. On suppose que l'intégrale de $t \mapsto f(x, t)$ converge sur $I$ uniformément sur $X$. Alors la fonction
			$F : X \to \R : x \mapsto \int_I f(x, t)\dif t$ est continue sur $X$.
			\end{thm}

			\begin{proof} Soit $([a_n, b_n])_n$, une suite exhaustive de segments de $I$. Pour tout $n \in \N$, on pose~:
			\begin{equation}
				F_n : X \to \R : x \mapsto \int_{a_n}^{b_n}f(x, t)\dif t.
			\end{equation}
			On observe que ces $F_n$ sont continues par le Théorème~\ref{prp:intparamcontinuesidefsurcpct} car $[a_n, b_n]$ est un segment fixe (compact)
			et $f \in C^0(X \times I, \R)$ par hypothèse.

			Prenons, $n, p \in \N, x \in X$. Calculons~:
			\begin{align}
				F_{n+p}(x) - F_n(x) &= \int_{a_{n+p}}^{b_{n+p}}f(x, t)\dif t - \int_{a_n}^{b_n}f(x, t)\dif t \\
				&= \int_{a_{n+p}}^{b_{n+p}}f(x, t)\dif t - \int_If(x, t)\dif t + \int_If(x, t)\dif t - \int_{a_n}^{b_n}f(x, t)\dif t.
			\end{align}

			Pour $\varepsilon > 0$, il existe $N_\varepsilon \in \N$ tel que pour $n, p \in \N$ et $x \in X$, par continuité des $F_n$ et puisque
			$[a_n, b_n]~\xrightarrow[n \to \pinfty]{}~I$, on peut dire~:
			\begin{equation}
				\abs {F_{n+p}(x) - F_n(x)}
				\leq \abs {\int_{a_{n+p}}^{b_{n+p}}f(x, t)\dif t - \int_If(x, t)\dif t} + \abs {\int_If(x, t)\dif t - \int_{a_n}^{b_n}f(x, t)\dif t}
				\leq 2\varepsilon.
			\end{equation}

			La suite $(F_n)_n$ est donc uniformément de Cauchy, on en déduit qu'elle converge uniformément sur $X$. Sa limite simple (limite de convergence
			simple) étant $F$, il vient que $F \in C^0(X, \R)$.
			\end{proof}

			\begin{thm} Soient $X \subset \R^d$, un ouvert non-vide, , $I \subset \R$, un intervalle non-vide, et $f : X \times I \tocont \R$ telle que pour
			tout $i \in \intint 1d$, $f$ admet une dérivée partielle continue par rapport à $x_i$ pour tout point de $X \times I$. Si l'intégrale sur $I$ de
			$t \mapsto f(x, t)$ converge uniformément sur $X$ et si pour tout $i \in \intint 1d$, l'intégrale sur $I$ de $t \mapsto \pd f{x_i}(x, t)$ converge
			uniformément sur $X$, alors la fonction définie par~:
			\begin{equation}
				F : X \times \R : x \mapsto \int_If(x, t)\dif t
			\end{equation}
			est de classe $C^1$ sur $X$, et on a~:
			\begin{equation}
				\pd F{x_i}(x) = \int_I\pd f{x_i}(x, t)\dif t.
			\end{equation}
			\end{thm}

			\begin{proof} EXERCICE.
			\end{proof}

		\subsection{Théorème de Fubini}
			\begin{thm}[Théorème de Fubini, version \CDII] Soit $f : [\alpha, \beta] \times [a, b] \tocont \R$. Les fonctions~:
			\begin{equation}
				\begin{cases}
					&\displaystyle F : [\alpha, \beta] \to \R : x \mapsto \int_a^bf(x, t)\dif t \\
					&\displaystyle G : [a, b] \to \R : t \mapsto \int_\alpha^\beta f(x, t)\dif x
				\end{cases}
			\end{equation}
			sont continues sur leur segment de définition, et on a~:
			\begin{equation}
				\int_\alpha^\beta F(x)\dif x = \int_a^b G(t)\dif t.
			\end{equation}
			\end{thm}

			\begin{proof} Par la Proposition~\ref{prp:intparamcontinuesidefsurcpct}, on sait que $F, G$ sont continues sur leur segment de définition. On pose~:
			\begin{numcases}
				\displaystyle H_1 : [\alpha, \beta] \to \R : X \mapsto \int_\alpha^X F(x)\dif x \\
				\displaystyle H_2 : [\alpha, \beta] \to \R : X \mapsto \int_a^b\int_\alpha^X f(x, t)\dif x\dif t
			\end{numcases}
			Par \CDII, on sait que $H_1~\in~C^1([\alpha, \beta], \R)$ avec $H_1'(x) = F(x)$. Par la Proposition~\ref{prp:dérivabilitésegmentfixe}, on trouve
			$H_2~\in~C^1([\alpha, \beta], \R)$ avec $H_2'(X) = \int_a^b\pd {}{x_i}\int_\alpha^X f(x, t)\dif x\dif t = \int_a^b f(X, t)\dif t = F(X)$.

			On en déduit $H_1'(x) = H_2'(x)$, ou encore $H_1'(x)-H_2'(x) = 0$, ce qui indique que la fonction $H_1-H_2$ est constante sur le segment
			$[\alpha, \beta]$. De plus, on trouve~:
			\begin{equation}
				H_1(\alpha) = \int_\alpha^\alpha F(x)\dif x = 0 = \int_a^b0\dif t = H_2(\alpha).
			\end{equation}
			On a donc $H_1-H_2 = 0$, ou encore $H_1=H_2$ sur $[\alpha, \beta]$. En particulier, $H_1(\beta) = H_2(\beta)$, ce qui est précisément~:
			\begin{equation}
				\int_\alpha^\beta F(x) = H_1(\beta) = H_2(\beta) = \int_a^b\int_\alpha^\beta f(x, t)\dif x\dif t = \int_a^b G(t)\dif t.
			\end{equation}
			\end{proof}

			\begin{thm}[Théorème de Fubini, version \CDIII]\label{thm:FubiniCDI1} Soient $[\alpha, \beta]$, un segment, $I \subset \R$, un intervalle non-vide, et
			$f : [\alpha, \beta] \times I \tocont \R$ telle que l'intégrale sur $I$ de $t \mapsto f(x, t)$ converge uniformément sur $[\alpha, \beta]$. Alors~:
			\begin{enumerate}
				\item la fonction $F : [\alpha, \beta] \to \R : x \mapsto \int_I f(x, t)\dif t$ est continue sur $[\alpha, \beta]$~;
				\item la fonction $G : I \to \R : t \mapsto \int_\alpha^\beta f(x, t)\dif x$ est continue sur $I$~;
				\item $G$ est d'intégrale convergente sur $I$.
			\end{enumerate}

			De plus, on a~:
			\begin{equation}
				\int_\alpha^\beta F(x)\dif x = \int_I G(t)\dif t.
			\end{equation}
			\end{thm}

			\begin{proof} $F$ et $G$ sont continues sur $[\alpha, \beta]$ par la Proposition~\ref{prp:intparamcontinuesidefsurcpct} et le
			Théorème~\ref{thm:intparamcontinuesiconvunif}.Fixons $\varepsilon > 0$. Soit $([a_n, b_n])_n$ une suite exhaustive de segments de $I$. Par convergence
			sur $I$ uniforme sur $[\alpha, \beta]$ de l'intégrale de $t \mapsto f(x, t)$, il existe $N_\varepsilon \in \N$ tel que~:
			\begin{equation}
				\forall n \geq N_\varepsilon : \forall x \in [\alpha, \beta] : \abs {\int_{a_n}^{b_n} f(x, t)\dif t - \int_I f(x, t)\dif t}
				\leq \frac \varepsilon{\beta-\alpha}.
			\end{equation}

			On en déduit~:
			\begin{equation}
				\abs {\int_\alpha^\beta\int_{a_n}^{b_n} f(x, t)\dif t\dif x - \int_\alpha^\beta\int_I f(x, t)\dif t\dif x}
				\leq \int_\alpha^\beta\abs {\int_{a_n}^{b_n}f(x, t)\dif t - \int_I f(x, t)\dif t}\dif x
				\leq (\beta-\alpha)\frac \varepsilon{\beta-\alpha} = \varepsilon.
			\end{equation}

			Par le théorème~\ref{thm:FubiniCDI1}, on peut écrire~:
			\begin{equation}
				\int_\alpha^\beta\int_{a_n}^{b_n}f(x, t)\dif t\dif x = \int_{a_n}^{b_n}\int_\alpha^\beta f(x, t)\dif x\dif t.
			\end{equation}

			Prenons alors $n \geq N_\varepsilon$, on observe~:
			\begin{align}
				\varepsilon &\geq \abs {\int_\alpha^\beta\int_{a_n}^{b_n} f(x, t)\dif t\dif x - \int_\alpha^\beta\int_I f(x, t)\dif t\dif x}
				= \abs {\int_{a_n}^{b_n}\int_\alpha^\beta f(x, t)\dif x\dif t - \int_\alpha^\beta\int_I f(x, t)\dif t\dif x} \\
				&= \abs {\int_{a_n}^{b_n}G(t)\dif t - \int_\alpha^\beta F(x)\dif x}.
			\end{align}

			Cela fournit la convergence de l'intégrale de $G$ sur $I$, et le fait que~:
			\begin{equation}
				\int_I G(t)\dif t = \int_\alpha^\beta F(x)\dif x.
			\end{equation}
			\end{proof}

		\subsection{Critères de convergence uniforme d'intégrales}
			\begin{thm}[Équivalent du critère de Weierstrass des séries sur les intégrales] Soient $X \neq \emptyset$, $I \subset \R$, un intervalle non-vide,
			et $f : X \times I \to \R$ telle que pour tout $x \in X$, on a $t \mapsto f(x, t)$ R-int sur tout segment de $I$. On suppose qu'il existe une
			fonction $\varphi : I \to \R$ abs-int sur $I$ telle que~:
			\begin{equation}
				\forall (x, t) \in X \times I : \abs {f(x, t)} \leq \varphi(t).
			\end{equation}

			Alors l'intégrale de $t \mapsto f(x, t)$ converge sur $I$ uniformément sur $X$.
			\end{thm}

			\begin{proof} Fixons $x \in X$. La fonction $t \mapsto f(x, t)$ est abs-int sur $I$ par le critère de comparaison avec $\varphi$
			(Proposition~\ref{prp:critèredecomparaison}). Puisque $\varphi$ est abs-int sur $I$, pour $\varepsilon > 0$ fixé, il existe $K_\varepsilon \subset I$
			segment tel que~:
			\begin{equation}
				\forall K \subset I \text{ segment } : \left(K_\varepsilon \subset K \Rightarrow \abs {\int_K \varphi(t)\dif t - \int_I \varphi(t)\dif t}
				\leq \varepsilon\right).
			\end{equation}

			Pour $x \in X, K \subset I$ segment $\tq K_\varepsilon \subset K$, écrivons~:
			\begin{align}
				\abs {\int_{K_\varepsilon} f(x, t)\dif t - \int_I f(x, t)\dif t} &= \abs {\int_{a_\varepsilon}^{b_\varepsilon} f(x, t)\dif t - \int_I f(x, t)\dif t}
					= \abs {\int_{\inf I}^{a_\varepsilon}f(x, t)\dif t + \int_{b_\varepsilon}^{\sup I}f(x, t)\dif t} \\
				&\leq \int_{\inf I}^{a_\varepsilon}\abs {f(x, t)}\dif t + \int_{b_\varepsilon}^{\sup I}\abs {f(x, t)\dif t}
					\leq \int_{\inf I}^{a_\varepsilon}\varphi(t)\dif t + \int_{b_\varepsilon}^{\sup I}\varphi(t)\dif t \\
				&= \abs {\int_I\varphi(t)\dif t - \int_{K_\varepsilon}\varphi(t)\dif t} \leq \varepsilon,
			\end{align}
			par choix de $K_\varepsilon$. On a donc bien la convergence sur $I$ uniforme sur $X$ de $t \mapsto f(x, t)$.
			\end{proof}

			\begin{thm}[Équivalent du critère d'Abel des séries sur les intégrales] Soit $I = [a, b)$, où $a < b \leq \pinfty$. Soient $X \neq \emptyset$, et
			$I \subset \R$, un intervalle non-vide, et $f, g : X \times I \to \R$ tels que~:
			\begin{itemize}
				\item $\forall x \in X : t \mapsto f(x, t)$ et $t \mapsto g(x, t)$ sont R-int sur tout segment de $I$~;
				\item $\exists M \gneqq 0, a \in I \tq \forall T \in I : \forall x \in X : \abs {\int_a^T f(x, t)\dif t} \leq M$~;
				\item $t \mapsto g(x, t)$ converge vers $0$ en décroissant en $b^-$ uniformément par rapport à $x$.
			\end{itemize}

			Alors $t \mapsto f(x, t)g(x, t)$ est d'intégrale convergente sur $I$ uniformément sur $X$.
			\end{thm}

			\begin{proof} Soit $([a_n, b_n])_n$ une suite exhaustive de segments de $I$. Puisque $I = [a, b)$, il existe $N \in \N$ tel que pour $n \geq N$, on
			a $a_n \equiv a$. Pour $x \in X, n, p \in \N$, écrivons~:
			\begin{equation}
				\int_a^{b_{n+p}}f(x, t)g(x, t)\dif t - \int_a^{b_n}f(x, t)g(x, t)\dif t = \int_{b_n}^{b_{n+p}}f(x, t)g(x, t)\dif t
				= g(x, b_n)\int_{b_n}^{c_{n, p}(x)}f(x, t)\dif t,
			\end{equation}
			par la seconde formule de la moyenne. On en déduit alors~:
			\begin{equation}
				\abs {\int_a^{b_{n+p}}f(x, t)g(x, t)\dif t - \int_a^{b_n}f(x, t)g(x, t)\dif t} \leq 2g(x, b_n)M.
			\end{equation}

			Par hypothèse, on sait que $g(\cdot, b_n)$ converge vers $0$ uniformément par rapport à $x$. On sait donc qu'il existe $N_\varepsilon \in \N$
			tel que pour $x \in X, n \geq N_\varepsilon$, on a $0 \leq g(x, b_n) \leq \frac \varepsilon{2M}$. Finalement, on en déduit~:
			\begin{equation}
				\forall n \geq N_\varepsilon : \forall p \in \N : \forall x \in X : \abs {\int_a^{b_{n+p}}f(x, t)g(x, t)\dif t - \int_a^{b_n}f(x, t)g(x, t)\dif t}
				\leq \varepsilon.
			\end{equation}

			En faisant tendre $p \to \pinfty$, on obtient bien~:
			\begin{equation}
				\forall n \geq N_\varepsilon : \forall x \in X : \abs {\int_If(x, t)g(x, t)\dif t - \int_a^{b_n}f(x, t)g(x, t)\dif t} \leq \varepsilon.
			\end{equation}

			On en déduit alors que $t \mapsto f(x, t)g(x, t)$ admet une intégrale convergente.
			\end{proof}

	\section{Application à la régularisation et à l'approximation à une dimension}
		\subsection{Fonctions à support compact}
			\begin{déf} Soient $\Omega \subset \R$ ouvert non-vide, $f : \Omega \to \R$. On appelle le \textit{support de $f$} l'ensemble~:
			\begin{equation}
				\supp f \coloneqq \adh \left\{x \in \Omega \tq f(x) \neq 0\right\}
			\end{equation}
			\end{déf}

			\begin{rmq} Le support est le plus petit fermé contenant tous les points où $f$ ne s'annule pas. De même, $\Omega \setminus \supp f$ est le plus
			grand ouvert inclus dans l'ensemble dans l'ensemble des points de $\Omega$ où $f$ s'annule.
			\end{rmq}

			\begin{prp} Il existe une fonction $\varphi : \R \to \R$ de classe $C^\infty$ sur $\R$, définie positive sur $\R$, de support $[-1, 1]$ et telle que~:
			\begin{equation}
				\int_R \varphi(x)\dif x = 1.
			\end{equation}
			\end{prp}

			\begin{proof} Soit la fonction $h$ définie par~:
			\begin{equation}
				h : \R \to \R : x \mapsto \begin{cases}\exp\left(-x^{-2}\right) &\text{ si } x > 0 \\0 &\text{ sinon }\end{cases}.
			\end{equation}
			On observe que $h$ est de classe $C^\infty$ sur $\Rp_0$ et $\Rm_0$. Également, on a~:
			\begin{equation}
				h^{(k)}(x) = \frac {P_k(x)}{x^{3k}}\exp(-x^{-2}),
			\end{equation}
			avec $P_k \in \R[x]$, et donc les dérivées sont telles que~:
			\begin{equation}
				\forall k \geq 1 : h^{(k)}(x) \xrightarrow[x \to 0^+]{} 0 = h^{(k)}(0^-)
			\end{equation}

			De plus, on a $\supp h = \Rp$. Posons alors~:
			\begin{equation}
				\rho : \R \to \R : x \mapsto h(1-x)h(1+x).
			\end{equation}

			$\rho$ est toujours $C^\infty$ sur $\R$, et est de support $[-1, 1]$. Par positivité de $\rho$ et par minoration de $\rho$ par une fonction $\gneqq 0$
			sur un fermé contenu dans $[-1, 1]$, on peut dire que~:
			\begin{equation}
				\alpha \coloneqq \int_{[-1, 1]}\rho(x)\dif x \gneqq 0.
			\end{equation}

			Il suffit ensuite de poser~:
			\begin{equation}
				\varphi \coloneqq \frac \rho\alpha.
			\end{equation}
			\end{proof}

			\begin{cor}\label{cor:varphi_k}Soit $\alpha > 0$. La fonction $\varphi_\alpha$ définie par~:
			\begin{equation}
				\varphi_\alpha : \R \to \R : x \mapsto \alpha\varphi(\alpha x)
			\end{equation}
			est positive, de support $\left[-\frac 1\alpha, \frac 1\alpha\right]$, de classe $C^\infty$, et d'intégrale valant $1$.
			\end{cor}

		\subsection{Produit de convolution}
			\begin{prp} Soient $f, g : \R \to \R$ telles que~:
			\begin{itemize}
				\item $f$ est R-int sur tout segment de $\R$~;
				\item $g$ est $C^0$ sur $\R$ et à support compact.
			\end{itemize}

			Alors, pour tout $x$ réel, les fonctions~:
			\begin{equation}
				t \mapsto f(x-t)g(t)\qquad\text{ et }\qquad t \mapsto f(t)g(x-t)
			\end{equation}
			sont abs-int, et on a~:
			\begin{equation}
				\int_\R f(x-t)g(t)\dif t = \int_\R f(t)g(x-t)\dif t.
			\end{equation}
			\end{prp}

			\begin{proof} La fonction $g$ est de support compact. Donc il existe un segment $[a, b]$ tel que~:
			\begin{equation}
				\forall x \in \R \setminus [a, b] : g(x) = 0.
			\end{equation}
			La fonction $g$ est de plus continue sur un compact, donc bornée par $M \gneqq 0$. On peut alors écrire pour tout $x, t \in \R$~:
			\begin{equation}
				\abs {f(x-t)g(t)} \leq M\abs {f(x-t)}I_{\left[a \leq t \leq b\right]}.
			\end{equation}

			Par comparaison, on en déduit que $\abs {g(t)f(x-t)}$ est R-int sur $\R$. On sait alors que $f(x-t)g(t)$ est abs-int sur $\R$, et par changement de
			variable, on trouve~:
			\begin{equation}
				\int_\R f(x-t)g(t)\dif t = \int_\R f(t)g(x-t)\dif t
			\end{equation}

			\end{proof}

			\begin{déf} On appelle \textit{produit de convolution de $f$ par $g$} la fonction définie par~:
			\begin{equation}
				f * g : \R \to \R : x \mapsto \int_\R f(x-t)g(t)\dif t = \int_\R f(t)g(x-t)\dif t.
			\end{equation}
			\end{déf}

			\begin{prp}\label{prp:(f*varphi)'=f*varphi'} Soient $k \in \Ns$ et $f : \R \toC0 \R$, R-int sur tout segment. La fonction $(f * \varphi_k)$ est
			de classe $C^\infty$ sur $\R$, et on a~:
			\begin{equation}
				\forall n \in \N : (f * \varphi_k)^{(n)} = f * (\varphi_k)^{(n)}.
			\end{equation}
			\end{prp}

			\begin{proof} Fixons $[a, b]$ un segment de $\R$. Prenons $x \in [a, b]$ et $k \in \Ns$. On peut écrire~:
			\begin{equation}
				(f * \varphi_k)(x) = \int_\R f(x-t)\varphi_k(t)\dif t = \int_\R f(t)\varphi_k(x-t)\dif t = \int_{a-\frac 1k}^{b+\frac 1k}f(t)\varphi_k(x-t)\dif t.
			\end{equation}

			On sait que $t \mapsto f(t)\varphi_k(x-t)$ est de classe $C^0$ car $f$ est $C^0$ par hypothèse, et $\varphi_k$ est de classe $C^\infty$. De plus, on
			sait que $t \mapsto f(t)\varphi_k(x-t)$ est dérivable en $x$, ce qui donne~:
			\begin{equation}
				\od {}x(f(t)\varphi_k(x-t))\sVert[2]_x = f(t)\varphi_k'(x-t).
			\end{equation}
			Par la Proposition~\ref{prp:dérivabilitésegmentfixe}, on sait que $(f*\varphi_k)$ est de classe $C^1$, et on peut écrire~:
			\begin{equation}
				\od {}x\left(\int_{a-\frac 1k}^{b+\frac 1k}f(t)\varphi(x-t)\dif t\right) = \int_{a-\frac 1k}^{b+\frac 1k}f(t)\varphi_k'(x-t)\dif t = (f * \varphi_k')(x).
			\end{equation}

			En appliquant le résultat par récurrence, on obtient $(f*\varphi_k)$ de classe $C^\infty$ et~:
			\begin{equation}
				\left(f*\varphi_k\right)^{(n)}(x) = \left(f*\left(\varphi_k^{(n)}\right)\right)(x).
			\end{equation}
			\end{proof}

			\begin{prp}\label{prp:f*varphi_k CVUc f} Soit $f : \R \toC0 \R$. Alors~:
			\begin{equation}
				f*\varphi_k \CVUc \R k\pinfty f.
			\end{equation}
			\end{prp}

			\begin{proof} Fixons $[a, b]$, un segment de $\R$. Prenons $x \in [a, b]$ et $k \geq 1$, et calculons~:
			\begin{equation}
				(f*\varphi_k)(x)-f(x) = \int_\R f(x-t)\varphi_k(t)\dif t - f(x)\int_\R\varphi_k(t)\dif t,
			\end{equation}
			car $\varphi_k(t)$ est d'intégrale valant $1$ par le Corollaire~\ref{cor:varphi_k}. On sait donc~:
			\begin{equation}
				(f*\varphi_k)(x) - f(x) = \int_\R \left(f(x-t)-f(x)\right)\varphi_k(t)\dif t.
			\end{equation}

			On sait que $f$ est $C^0$ sur $[a-1, b+1] \ni x-t$ par hypothèse. Par le théorème de Heine, on sait que $f$ est uniformément continue sur $[a-1, b+1]$.
			Soit $\varepsilon > 0$. Il existe $\eta > 0$ tel que~:
			\begin{equation}
				\forall z_1, z_2 \in [a-1, b+1] : \abs {z_1-z_2} < \eta \Rightarrow \abs {f(z_1)-f(z_2)} < \varepsilon.
			\end{equation}

			On observe ensuite~:
			\begin{equation}
				\abs {(f*\varphi_k)(x)-f(x)} = \abs {\int_{-\frac 1k}^{\frac 1k}\left(f(x-t)-f(x)\right)\varphi_k(t)\dif t}.
			\end{equation}

			Pour $k$ tel que $\frac 1k < \eta$, on a~:
			\begin{equation}
				\forall t \in \left(-\frac 1k, \frac 1k\right) : \abs {f(x)-f(x-t)} < \varepsilon.
			\end{equation}

			Dès lors, pour de tels valeurs de $k$, on trouve~:
			\begin{equation}
				\abs {(f*\varphi_k)(x)-f(x)} = \int_{-\frac 1k}^{\frac 1k}\abs {f(x, t)-f(x)}\varphi_k(t)\dif t
				\leq \varepsilon\int_{-\frac 1k}^{\frac 1k}\varphi_k(t)\dif t = \varepsilon.
			\end{equation}

			Ainsi, quel que soit $[a, b] \subset \R$, on sait~:
			\begin{equation}
				\forall \varepsilon > 0 : \exists K_\varepsilon \in \N \tq \forall k \geq K_\varepsilon : \sup_{x \in [a, b]}\abs {(f*\varphi_k)(x)-f(x)} \leq \varepsilon.
			\end{equation}
			\end{proof}

			\begin{prp} Soit $f : \R \toC K \R$. Alors~:
			\begin{equation}
				\forall s \in \intint 0K : \left(f*\varphi_k\right)^{(s)} \CVUc \R k\pinfty f^{(s)}.
			\end{equation}
			\end{prp}

			\begin{proof} Remarquons par un raisonnement similaire à la Proposition~\ref{prp:(f*varphi)'=f*varphi'} que $(f*\varphi_k)^{(s)} = f^{(s)}*\varphi_k$, et
			appliquons la Proposition~\ref{prp:f*varphi_k CVUc f}.
			\end{proof}

			\begin{thm} Soit $[a, b] \subseteq \R$, un segment et soit $f \in C^0([a, b], \R)$. Alors~:
			\begin{equation}
				\forall \varepsilon > 0 : \exists P \in \R[x] \tq \sup_{x \in [a, b]}\abs {f(x)-P(x)} < \varepsilon.
			\end{equation}
			\end{thm}

			\begin{proof} Premièrement, on étend $f$ sur $[a-1, b+1]$ en y ajoutant les segments définis par les couples $\left((a-1, 0), (a, f(a))\right)$ et
			$\left((b, f(b)), (b+1, 0)\right)$. Ensuite, par translation et homothétie, on envoie $f$ sur le segment $\left[\pm \frac 12\right]$.

			Pour $k \geq 1$, on pose~:
			\begin{equation}
				g_k : \R \to \R : x \mapsto \begin{cases}{(1-x^2)}^k &\text{ si } \abs x < 1 \\0 &\text{ sinon}\end{cases}.
			\end{equation}
			On remarque que $g_k \in C^0(\R, \R)$ à support compact et $\int_\R g_k(x)\dif x \eqqcolon \alpha_k \gneqq 0$.

			On peut alors définir~:
			\begin{equation}
				h_k \coloneqq \frac {g_k}{\alpha_k}.
			\end{equation}

			$h_k$ est continue sur $\R$, à support compact et d'intégrale valant 1. Étant donné que pour $x \in [-1, 1]$, on a $x^2 \leq x$, et donc $-x^2 \geq -x$,
			on peut écrire~:
			\begin{equation}
				\alpha_k = \int_{-1}^1(1-x^2)^k\dif x = 2\int_0^1(1-x^2)^k\dif x \geq 2\int_0^1(1-x)^k\dif x = \frac 2{k+1}.
			\end{equation}

			De plus~:
			\begin{equation}
				\forall \delta \in (0, 1) : h_k \xrightarrow[{[-1, 1] \setminus [-\delta, \delta]}]{CVU} 0
			\end{equation}

			En effet, pour $\abs x \in [\delta, 1]$, on a~: $x^2 \in [\delta^2, 1^2] = [\delta^2, 1] \supset [\delta, 1]$, et donc $1-x^2 \in [0, 1-\delta^2]$.
			Et donc~:
			\begin{equation}
				h_k(x) = \frac {(1-x^2)^k}{\alpha_k} \leq \frac {(1-\delta^2)^k}{\alpha_k} \leq \frac {k+1}2(1-\delta^2)^k \xrightarrow[k \to \pinfty]{} 0,
			\end{equation}
			avec le majorant $\frac {k+1}2(1-\delta^2)^k$ ne dépendant pas de $x$. La convergence est donc uniforme.

			Pour $x \in \R, k \in \Ns$, posons~:
			\begin{equation}
				f_k(x) = f * h_k(x).
			\end{equation}

			Observons que si $x \in \left[\pm \frac 12\right]$, alors~:
			\begin{equation}
				\forall t \in \left[\pm \frac 12\right] : (x-t) \in [-1, -1],
			\end{equation}
			et donc~:
			\begin{equation}
				\forall t, x \in \left[\pm \frac 12\right] : h_k(x-t) = \frac {{(1-(x-t)^2)}^k}{\alpha_k} = \sum_{k=0}^{2p}a_{k\,p}(t)x^p,
			\end{equation}
			avec les $a_{k\,p}(t)$ venant des coefficients du binôme de Newton.

			Ainsi, $\restr {f_k}{\left[\pm\frac 12\right]}$ est une fonction polynômiale de degré inférieur ou égal à $2k$.

			$f$ est continue sur $\R$ et à support dans $\left[\pm \frac 12\right]$, donc elle est~:
			\begin{itemize}
				\item bornée par $M \gneqq 0$ sur $\R$~;
				\item uniformément continue sur $\R$\footnotemark.
			\end{itemize}

			Fixons $\varepsilon > 0$. Il existe $\eta > 0$ tel que~:
			\begin{equation}
				\forall x, y \in \R : \abs {x-y} < \eta \Rightarrow \abs {f(x)-f(y)} \leq \varepsilon.
			\end{equation}

			Pour $x \in \left[\pm\frac 12\right]$ et $k \geq 1$, écrivons~:
			\begin{equation}
				f_k(x)-f(x) = \int_\R f(x-t)h_k(t)\dif t - f(x)\int_\R h_k(t)\dif t = \int_\R \left(f(x-t)-f(x)\right)h_k(t)\dif t.
			\end{equation}

			En prenant la valeur absolue, on trouve~:
			\begin{align}
				\abs {f_k(x)-f(x)}
					&\leq \int_\minfty^{-\eta}\abs {f(x-t)-f(x)}h_k(t)\dif t + \int_{-\eta}^{\eta}\abs {f(x-t)-f(x)}h_k(t)\dif t + \int_\eta^\pinfty\abs {f(x-t)-f(x)}h_k(t)\dif t \\
				&= \int_{-1}^{-\eta}\abs {f(x-t)-f(x)}h_k(t)\dif t + \int_{-\eta}^{\eta}\abs {f(x-t)-f(x)}h_k(t)\dif t + \int_\eta^1\abs {f(x-t)-f(x)}h_k(t)\dif t \\
				&\leq 2M\norm {h_k}_{\infty, [-1, -\eta]} + \varepsilon\int_{-\eta}^\eta h_k(t)\dif t + 2M\norm {h_k}_{\infty, [\eta, 1]} \\
				&\leq 4M\norm {h_k}_{\infty, [\eta, 1]} + \varepsilon,
			\end{align}
			et le majorant ne dépend pas de $x \in \left[\pm\frac 12\right]$.

			Dès lors, en choisissant $k_\varepsilon$ tel que $\forall k \geq k_\varepsilon : \norm {h_k}_{\infty, [\eta, 1]} \leq \frac \varepsilon{4M}$, alors il vient que~:
			\begin{equation}
				\forall k \geq k_\varepsilon : \sup_{x \in \left[-\frac 12, \frac 12\right]}\abs {f_k(x)-f(x)} \leq 2\varepsilon.
			\end{equation}
			\end{proof}		\footnotetext{$\sim$ théorème de Heine.}

			% Exercice dans le cours

\chapter{Critère de compacité en dimension infinie~: le théorème d'Arzela-Ascoli}
	\section{Rappels de topologie métrique}
		\subsection{Densité et séparabilité}
			\begin{déf} Soit $(X, d)$ un espace métrique, et soit $A \subseteq X$. On appelle \textit{adhérence de $A$} l'ensemble~:
			\begin{equation}
				\adh A \coloneqq \bigcap_{\overset {F \text{ fermé }}{F \supset A}} F.
			\end{equation}

			$\adh A$ est, par construction, le plus petit (au sens de l'inclusion) fermé de $X$ qui contient $A$.
			\end{déf}

			\begin{rmq}~
			\begin{itemize}
				\item $x \in X$ est dans $\adh A$ si et seulement si $\forall \varepsilon > 0 : B(x, \varepsilon[ \cap A \neq \emptyset$~;
				\item l'ensemble $\adh A$ peut également être défini par l'ensemble des limites de suites de $A$ qui convergent dans $X$.
			\end{itemize}
			\end{rmq}

			\begin{déf} Soient $X \neq \emptyset$ et $A \subseteq X$. On dit que $A$ est \textit{dense} dans $X$ lorsque $\adh A = X$.
			\end{déf}

			\begin{déf} L'ensemble $A$ est dit \textit{dénombrable} lorsqu'il existe $\varphi : \N \to A$ bijective.
			\end{déf}

			\begin{déf} Soit $(x_n)_n$ une suit de $X$. On dit que $x^*$ est une \textit{valeur d'adhérence de $(x_n)$} lorsqu'il existe une sous-suite
			$(x_{\psi(n)})_n$ de $(x_n)$ qui converge en $x^*$.
			\end{déf}

			\begin{déf}Une suite $(x_n)_n$ est dite \textit{dense dans $X$} lorsque l'ensemble de ses valeurs d'adhérence dans $X$ est $X$.
			\end{déf}

			\begin{déf} Un espace métrique $(X, d)$ est dit \textit{séparable} lorsqu'il possède une suite dense.
			\end{déf}

			\begin{prp} Soit $(X, d)$ un espace métrique. Il est séparable si et seulement si il admet une partie dense finie ou dénombrable
			\end{prp}

			\begin{proof} Soit $(x_n)_n$ une suite dense dans $X$. La partie $A$ définie par~:
			\begin{equation}
				A \coloneqq \bigcup_{n \in \N}\{x_n\} = \left\{x_n \tq n \in \N\right\}
			\end{equation}
			est finie ou dénombrable, dense dans $X$ par définition.

			Soit maintenant $A$ dense dans $X$. Différencions les cas où $A$ est finie et où $A$ est dénombrable.
			\begin{itemize}
				\item si $A = \{x_1, \ldots, x_N\}$ est finie, alors $X = \{x_0, \ldots, x_N\} = A$, et la suite $(x_0, \ldots, x_N, x_0, \ldots, x_N, x_0, \ldots)$
				est dense dans $X$~;
				\item si $A = \{x_1, \ldots, x_N, \ldots\} = \bigcup_{n \in \N}\{x_n\}$ est dénombrable, alors la suite $(x_0, x_0, x_1, x_0, x_1, x_2, x_0, \ldots)$
				est dense dans $X$.
			\end{itemize}
			\end{proof}

			\begin{ex}~
			\begin{itemize}
				\item $A = \Q$ est dénombrable (et dense) dans $\R$, et donc $(\R, \abs .)$ est séparable~;
				\item $A = \Q^d$ est dénombrable (et dense) dans $\R^d$, et donc $(\R, \norm .)$ est séparable\footnote{La norme n'est pas précisée ici car dans
				$\R^n$, toutes les normes sont équivalentes.}.
			\end{itemize}
			\end{ex}

			\begin{déf} Soit $X$ dénombrable. On appelle \textit{énumération} toute bijection $\varphi : X \to \N$
			\end{déf}

			\begin{prp} Soit $A \subset \R^d$. Alors $A$ est séparable.
			\end{prp}

			\begin{proof} Montrons qu'il existe une partie dense dans $A$ finie ou dénombrable. Soit $(x_q)_{q \in \N}$, une énumération de $\Q^d$. Pour $n \geq 1$,
			on a~:
			\begin{equation}
				A \subset \bigcup_{q \in \N}B(x_q, n^{-1}[ = \R^d,
			\end{equation}
			par densité de $\Q^d$ dans $\R^d$.

			Pour $n \geq 1$, notons
			\begin{equation}
				C_n \coloneqq \left\{q \in \N \tq B(x_q, n^{-1}[ \, \cap A \neq 0\right\} \subseteq \N.
			\end{equation}
			On sait donc que $C_n$ est fini ou dénombrable. Pour tout $n \geq 1$, et $q \in C_n$, on peut choisir~:
			\begin{equation}
				y_{n\,q} \in B(x_q, n^{-1}[ \, \cap A.
			\end{equation}

			Pour $n \geq 1$, on pose alors~:
			\begin{equation}
				X_n \coloneqq \bigcup_{q \in C_n}\left\{y_{n\,q}\right\} \neq \emptyset,
			\end{equation}
			fini ou dénombrable, et donc~:
			\begin{equation}
				X \coloneqq \bigcup_{n \in \N}X_n
			\end{equation}
			est non-nul, fini ou dénombrable.

			Il reste à montrer que $X$ est dense dans $A$.

			Soient $x \in A$ et $\varepsilon > 0$ fixés. Il existe $n_0 \in \Ns$ tel que $n_0 > \frac 2\varepsilon$. Ainsi~:
			\begin{equation}
				x \in A \subset \bigcup_{q \in \N}B(x_q, {n_0}^{-1}[.
			\end{equation}

			Donc, il existe $q_0 \in \N \tq \norm {x-x_{q_0}} < \frac 1{n_0} < \frac \varepsilon2$. De plus, on peut dire que $y_{n_0\,q_0} \in X_n \subset X$.
			De plus, pour $y_{n\,q} \in X$~:
			\begin{equation}
				\norm {x-y_{n\,q}} \leq \norm {x-x_{q_0}} + \norm {x_{q_0} - y_{n\,q}} < \frac \varepsilon2 + \frac 1{n_0} < \varepsilon.
			\end{equation}
			\end{proof}

	\section{L'espace $C_b^0(X, \R)$}
		\begin{déf} Soit $X \subset \R^d$ non-nul. On définit~:
		\begin{equation}
			C^0_b(X, \R) \coloneqq \{f \in C^0(X, \R) \tq f \text{ est bornée}\}.
		\end{equation}
		\end{déf}

		\begin{prp}~
		\begin{enumerate}
			\item $\evnCb0X\R$ est un \evn~;
			\item $C^0_b(X, \R)$ est un fermé de $\left(B(X, \R), \norm \cdot_\infty\right)$~;
			\item $C^0_b(X, \R)$ est complet.
		\end{enumerate}
		\end{prp}

		\begin{proof}~
		\begin{enumerate}
			\item EXERCICE.
			\item Soit $f_n \in C^0_b(X, \R) \tq$~:
			\begin{equation}
				\exists f \in B(X, \R) \tq f_n \xrightarrow[n \to \pinfty]{\norm \cdot_\infty} f.
			\end{equation}
			$f$ est limite sur $X$ d'une suite de fonctions continues sur $X$. Donc $f \in C^0_b(X, \R)$, et donc $C^0_b(X, \R)$ est fermé dans $B(X, \R)$.
			\item $C^0_b(X, \R)$ est fermé dans $\left(B(X, \R), \norm \cdot_\infty\right)$ qui est complet, donc $C^0_b(X, \R)$ est complet.
		\end{enumerate}
		\end{proof}

		\begin{prp} Soit $X \subset \R^d$. Si $X$ est compact, alors $C_b^0(X, \R) = C^0(X, \R)$.
		\end{prp}

		\begin{proof} On sait que $C^0_b(X, \R) \subseteq C^0(X, \R)$ pour tout ensemble $X$. Prenons $f \in C^0(X, \R)$. Une fonction continue sur un compact
		est bornée, du coup $f \in C^0_b(X, \R)$, et donc $C^0(X, \R) \subseteq C^0_b(X, \R)$.
		\end{proof}

		\begin{déf} On note $\mathcal P([a, b])$ l'ensemble des fonctions polynômiales définies sur $[a, b]$ à valeur dans $\R$.
		\end{déf}

		\begin{prp}~
		\begin{enumerate}
			\item $\mathcal P([a, b]) \subset C^0([a, b], \R) = C^0_b([a, b], \R)$~;
			\item $\mathcal P([a, b])$ est dense dans $\evnC0{[a, b]}\R$.
		\end{enumerate}
		\end{prp}

		\begin{proof}~
		\begin{enumerate}
			\item EXERCICE.
			\item Weierstrass.
		\end{enumerate}
		\end{proof}

		\begin{cor} $\evnC0{[a, b]}\R$ est séparable.
		\end{cor}

		\begin{proof} $\Q[x]$ est dénombrable et dense dans $\evnC0{[a, b]}\R$. En effet, si $f \in C^0([a, b], \R)$ et $\varepsilon > 0$ sont fixés, alors par
		Weierstrass, il existe $P \in \R[x]$ tel que~:
		\begin{equation}
			\norm {f-P}_\infty < \frac \varepsilon2.
		\end{equation}

		On peut écrire $P$ sous la forme~:
		\begin{equation}
			P = \sum_{k=0}^da_kx^k,\qquad\qquad a_k \in \R.
		\end{equation}

		Par densité de $\Q$ dans $\R$, il existe $b_0, \ldots, b_k \in \Q$ tels que~:
		\begin{equation}
			\max_{i \in \intint 0d}\abs {a_i-b_i} \leq \frac \varepsilon{2\sum_{x \in [a, b]}\sum_{\gamma = 0}^d\abs x^k}.
		\end{equation}

		Posons~:
		\begin{equation}
			Q \coloneqq \sum_{k=0}^db_kx^k,
		\end{equation}
		le polynôme associé à ces coefficients. On trouve alors~:
		\begin{align}
			\norm {P-Q}_\infty &\leq \sum_{x \in [a, b]}\abs {a_k-b_k}\abs x^k
				\leq \sup_{x \in [a, b]}\sum_{k=0}^d\frac \varepsilon{2\sup_{x' \in [a, b]}\sum_{\gamma=0}^d\abs {x'}^\gamma}\abs {x}^k \\
			&= \frac \varepsilon{2\sup_{x \in [a, b]}\sum_{k=0}^d\abs x^k}\sup_{x \in [a, b]}\sum_{k=0}^d\abs x^k = \varepsilon.
		\end{align}

		Et finalement, on a~:
		\begin{equation}
			\norm {f-Q}_\infty \leq \norm {f-P}_\infty + \norm {Q-P}_\infty \leq \varepsilon.
		\end{equation}
		\end{proof}

	\section{Théorème d'Arzela-Ascoli}
		\subsection{Motivation}
			Soit $X \subset \R^d$ non-vide. $X$ est compact si et seulement si il est fermé et borné.

			Dans l'\evn $(E, \norm \cdot) = \evnC0{[a, b]}\R$, la suite~:
			\begin{equation}
				f_k : [0, 1] \to \R : x \mapsto x^k
			\end{equation}
			est bornée car pour tout $k \in \N$, on a $\norm {f_k}_\infty \leq 1$. Cependant, elle n'a pas de sous-suite convergente dans $\evnC0{[a, b]}\R$.
			En effet, s'il existe $\varphi : \N \to \N$ strictement croissante, $f \in C^0$ telle que~:
			\begin{equation}
				f_{\varphi(k)} \xrightarrow[k \to \pinfty]{\norm \cdot_\infty} f,
			\end{equation}
			alors $f : [0, 1] \to \R : x \mapsto I_{[x=1]} \not \in C^0([0, 1])$, ce qui est une contradiction.

			L'objectif du théorème d'Arzela-Ascoli est de donner un critère (condition suffisante) pour qu'une partie de $\evnC {0}{[0, 1]}{\R}$ soit d'adhérence
			compacte.

		\subsection{Énoncé et démonstration}
			\begin{déf} Soient $X \subset \R^d$ non-vide, $B \subset \mathcal F(X, \R)$, une partie de l'ensemble des fonctions $X \to \R$. On dit que $B$ est
			\textit{équicontinue} sur $X$ lorsque~:
			\begin{equation}
				\forall \varepsilon > 0 : \exists \eta > 0 \tq \forall f \in B : \forall x, y \in X :
				\left(\norm {x-y} < \eta \Rightarrow \abs {f(x)-f(y)} < \varepsilon\right).
			\end{equation}
			\end{déf}

			\begin{rmq} Lorsque $X = [0, 1]$ et $B \subset \evnC1X\R$, si $\exists M \gneqq 0 \tq \forall f \in B : \norm {f'}_\infty < M$, alors $B$ est
			équicontinue sur $X$. En effet~:
			\begin{equation}
				\forall f \in B : \forall x, y \in X : \abs {f(x)-f(y)} \leq M\abs {x-y},
			\end{equation}
			par le théorème des accroissements finis. Dès lors, $\eta = \frac \varepsilon M$ convient.
			\end{rmq}

			\begin{thm}[Théorème d'Arzela-Ascoli]\label{thm:Arzela-Ascoli} Soient $A \subset \R^d$ compact et $B \subset \evnC0A\R$ non-nul. Si $B$ est bornée
			(pour $\norm \cdot_{\infty, A}$) et équicontinue, alors $B$ est d'adhérence compacte.
			\end{thm}

			\begin{rmq} Cela amène que pour toute suite de points de $B$, on peut extraire une sous-suite qui converge dans $\adh B \subset C^0(A, \R)$.
			\end{rmq}

			\begin{proof} $A \subset \R^d$ est compacte et donc séparable. Soit $C$ une partie dénombrable ou finie dense dans $A$.

			\begin{itemize}
				\item Si $C$ est finie, alors $C = \{x_1, \ldots, x_k\}$ pour $k \in \Ns$ et $A = C$. Soit $(f_n)_n \subset B$. La suite $(f_n(x_1))_n \subset \R$
				est bornée (car $B$ est bornée) et admet une sous-suite $(f_{\varphi_1(n)}(x_1))_n \subset \R$ convergente dans $\R$.

				La suite $(f_{\varphi_1(n)}(x_2))_n \subset \R$ est bornée donc admet une sous-suite $(f_{(\varphi_1 \circ \varphi_2)(n)}(x_2))$ convergente
				dans $\R$. En réitérant jusque $k$, on trouve $\varphi_1, \ldots, \varphi_k : \N \to \N$ strictement croissantes et il existe $f(x_1), \ldots,
				f(x_n) \in \R$ tels que~:
				\begin{equation}
					\forall i \in \intint 1k : f_{(\varphi_1 \circ \ldots \circ \varphi_k)(n)}(x_i) \xrightarrow[n \to \pinfty]{} f(x_i).
				\end{equation}

				On a alors $f \in C^0(A, \R)$, et on a bien~:
				\begin{equation}
					\norm {f_{(\varphi_1 \circ \ldots \circ \varphi_k)(n)} - f}_{\infty, A} =
					\max_{i \in \intint 1n}\abs {f_{(\varphi_1 \circ \ldots \circ \varphi_k)(n)}(x_i) - f(x_i)}
					\xrightarrow[n \to \pinfty]{} 0.
				\end{equation}
				\item Si $C$ est dénombrable, on pose $(x_n)$ une énumération de $C$. Soit $(f_n) \subset B$. La suite $(f_n(x_0))$ est bornée dans $\R$ car
				$B$ est bornée et donc admet une sous-suite convergente , que l'on note $(f^{(0)}_n(x_0))_n$. Cette sous-suite est réelle et bornée donc admet
				une sous-suite convergente que l'on note $(f^{(1)}_n(x_1))_n$. En réitérant, on trouve une suite d'extractions $(f^{(k)}_n)_k$ telle que
				$f^{(k)}_n = f^{(k-1)}_{\varphi_k(n)}$, avec $\varphi_i$ strictement croissante pour $i \geq 0$. Pour tout $k$ naturel, on pose~:
				\begin{equation}
					g_k = f^{(k)}_k.
				\end{equation}
				$(g_k)_k$ est une extraction diagonale de Cantor. De plus, la suite $(g_k)_k$ est une suite extraite de $(f_n)_n$. Pour tout $p$ naturel, la
				suite $(g_k(x_p))_k$ converge donc vers $f(x_p)$ car~:
				\begin{equation}
					\forall \ell \geq p : g_\ell(x_p) = f^{(\ell)}_\ell(x_p),
				\end{equation}
				et donc $(g_k(x_p))_k$ est extraite de $(f_k^{(p)}(x_p))_k$ avec~:
				\begin{equation}
					f_k^{(p)}(x_p) \xrightarrow[k \to \pinfty]{} f(x_p).
				\end{equation}

				Montrons maintenant que la suite $(g_k)_k$ est uniformément de Cauchy sur $A$. Fixons $\varepsilon > 0$. Soit $\eta > 0$ le module d'équicontinuité
				de $B$ pour $\varepsilon$. Écrivons~:
				\begin{equation}
					A \subset \bigcup_{y \in A}B\left(y, \frac \eta2\right[.
				\end{equation}

				Par compacité de $A$, on sait qu'il existe un recouvrement fini, et donc $q \in \N$ et $y_1, \ldots y_q \in A$ tels que~:
				\begin{equation}
					A \subset \bigcup_{j=1}^qB\left(y_j, \frac \eta2\right[.
				\end{equation}

				Par définition de $C$ (séparabilité de $A$), on sait~:
				\begin{equation}
					\forall j \in \intint 1q : \exists x_{p_j} \tq \norm {x_{p_j} - y_j} \leq \frac \eta2.
				\end{equation}

				Les suites $(g_k(x_{p_i}))_k$ convergent dans $\R$ pour $i \in \intint 1q$ et donc de Cauchy. Puisqu'elles sont en nombre fini, il existe
				$N \in \N$ tel que~:
				\begin{equation}
					\forall m, n \geq N : \abs {g_m(x_{p_j})-g_n(x_{p_j})} \leq \varepsilon.
				\end{equation}

				Soit $x \in A$. Par compacité de $A$, il existe $j \in \intint 1q$ tel que $\norm {x-y_j} \leq \frac \eta2$, et~:
				\begin{equation}
					\norm {x-x_{p_j}} \leq \norm {x - y_j} + \norm {y_j - x_{p_j}} \leq 2\frac \eta2 = \eta.
				\end{equation}

				Pour $m, n > N$, on trouve donc~:
				\begin{equation}
					\abs {g_m(x)-g_n(x)} \leq \abs {g_m(x) - g_m(x_{p_j})} + \abs {g_m(x_{p_j}) - g_n(x_{p_j})} + \abs {g_n(x_{p_j}) - g_n(x)} \leq 3\varepsilon,
				\end{equation}
				par Cauchy et équicontinuité. On en déduit que la suite $(g_n)_n$ est de Cauchy dans $\evnC0A\R$.
			\end{itemize}
			\end{proof}

\part{Équations différentielles}
\chapter{Conditions suffisantes d'existence et d'unicité de solutions}
	\section{Équations différentielles - forme normale - réduction à l'ordre 1}
		\subsection{Généralités}
			\begin{déf} On appelle \textit{équation différentielle} toute relation de la forme~:
			\begin{equation}\label{eq:equadiffF}
				F(t, y(t), y^{(1)}(t), \ldots, y^{(p)}(t)) = 0\qquad\qquad t \in I,
			\end{equation}
			où~:
			\begin{itemize}
				\item $I$ est un intervalle de $\R$~;
				\item $F : I \times \Omega_0 \times \Omega_1 \times \ldots \Omega_p \to \R$ est une fonction~;
				\item $\Omega_0, \Omega_1, \ldots, \Omega_p$ sont des ouverts de $\R^d$.
			\end{itemize}
			$y : J \subset I \to \R^d$ est une fonction inconnue définie sur un intervalle $J$ inconnu également et $p$ fois dérivable sur $J$, telle que~:
			\begin{equation}
				\forall t \in J : \forall j \in \intint 0p : y^{(j)}(t) \in \Omega_j,
			\end{equation}
			et dont les dérivées sont liées par l'équation~\eqref{eq:equadiffF}.
			\end{déf}

			\begin{déf} Une équation différentielle est dite \textit{résoluble} lorsqu'elle peut être mise de manière équivalente sous forme normale~:
			\begin{equation}\label{eq:equadiffnormale}
				y^{(p)}(t) - f(t, y(t), \ldots, y^{(p-1)}(t)) = 0,
			\end{equation}
			où $f : I \times \Omega_0 \times \ldots \times \Omega_{p-1} \to \Omega_p$.
			\end{déf}

		\subsection{Réduction à l'ordre 1}
			\begin{rmq} Une équation sous forme normale~\eqref{eq:equadiffnormale} est équivalente à l'équation d'ordre 1 $Y'(t) - G(t, Y(t)) = 0$, où~:
			\begin{equation}
				Y(t) = \begin{bmatrix}y(t) \\y^{(1)}(t) \\ \vdots \\ y^{(p-1)}(t)\end{bmatrix} \qquad\qquad \text{ et } \qquad\qquad
			G(t, Y(t)) = \begin{bmatrix}Y_0(t) \\ Y_1(t) \\ \vdots \\ Y_{p-1}(t) \\ f(t, Y_0(t), \ldots, Y_{p-1}(t))\end{bmatrix}.
			\end{equation}
			\end{rmq}

			\begin{rmq} Toute équation différentielle résoluble étant équivalente à une équation différentielle d'ordre 1, on étudiera uniquement ces dernières,
			et cela permettra de résoudre les autres, sans perte de généralité.
			\end{rmq}

		\subsection{Problème de Cauchy}
			\begin{déf} On se donne un équation différentielle (ED) d'ordre 1 résoluble~:
			\begin{equation}
				y'(t) = f(t, y(t)),
			\end{equation}
			avec~:
			\begin{itemize}
				\item $f : I \times \Omega \to \R^d$~;
				\item $I \subset \R$ un intervalle~;
				\item $\Omega \subset \R^d$, un ouvert.
			\end{itemize}

			On appelle \textit{donnée de Cauchy} tout couple $(t_0, y_0) \in I \times \Omega$.

			On appelle \textit{problème de Cauchy} le fait de chercher $J \subset I$ un intervalle et $y : J \to \R^d$ tels que~:
			\begin{equation}\label{eq:equadiffPC}\tag{PC}
				\left\{\begin{aligned}
				&\forall t \in J : y'(t) = f(t, y(t)) \\
				&y(t_0) = y_0
			\end{aligned}\right.
			\end{equation}
			\end{déf}

		\subsection{Formulation intégrale}
			\begin{prp}\label{prp:PCintégrale} Si $f \in C^0(I \times \Omega, \R^d)$, alors $y : J \tocont \Omega$, avec $t_0 \in J$ est solution
			de~\eqref{eq:equadiffPC} si et seulement si~:
			\begin{equation}\label{eq:prppcintégrale}
				\forall t \in J : y(t) = y_0 + \int_{t_0}^tf(s, y(s))\dif s.
			\end{equation}
			\end{prp}

			\begin{proof} Supposons d'abord $y$ solution de~\eqref{eq:equadiffPC}. Alors~:
			\begin{equation}
				\forall t \in J : y'(t) = f(t, y(t)).
			\end{equation}
			Par dérivabilité de $y$ sur $J$, on sait que $y \in C^0(J)$. Puisque $y$ est à valeurs dans $\Omega$, on sait que $f \in C^0(I \times \Omega)$ avec
			$J \subset I$. La fonction $t \mapsto f(t, y(t))$ est donc continue sur $J$. Ainsi, $y'$ est continue sur $J$, et donc $y \in C^1(J, \R^d)$, et on a~:
			\begin{equation}
				y(t) - y(t_0) = \int_{t_0}^ty'(s)\dif s,
			\end{equation}
			ou encore~:
			\begin{equation}
				y(t) = y_0 + \int_{t_0}^t f(s, y(s))\dif s.
			\end{equation}

			Maintenant, supposons que $y$ vérifie~\eqref{eq:prppcintégrale}. Puisque $y \in C^0(J, \Omega)$, la fonction $s \mapsto f(s, y(s))$ est continue
			sur $J$. Elle est donc intégrable, et $t \mapsto \int_{t_0}^t f(s, y(s))\dif s$ est de classe $C^1$ sur $J$. On a alors~:
			\begin{equation}
				\od {}t\int_{t_0}^tf(s, y(s))\dif s = f(t, y(t)).
			\end{equation}
			Par hypothèse, on a~:
			\begin{equation}
				y(t) = y_0 + \int_{t_0}^tf(s, y(s))\dif s.
			\end{equation}
			Dès lors, en dérivant terme à terme, on trouve~:
			\begin{equation}
				y'(t) = f(t, y(t)),
			\end{equation}
			et on a de plus~:
			\begin{equation}
				y(t_0) = y_0 + \int_{t_0}^{t_0}f(s, y(s))\dif s = y_0 + 0 = y_0.
			\end{equation}
			$y$ est donc bien solution de~\eqref{eq:equadiffPC}.
			\end{proof}

	\section{Existence et unicité locales}
		\subsection{Théorème du point fixe de Banach}
			\begin{déf} Soient $(X, d_X)$ et $(Y, d_Y)$ deux espaces métriques. $f : X \to Y$ est dite \textit{contractante} lorsque~:
			\begin{equation}
				\exists k \in [0, 1) \tq \forall x, y \in X : d_Y(f(x), f(y)) \leq kd_X(x, y).
			\end{equation}
			\end{déf}

			\begin{thm}[Théorème du point fixe de Banach] Soient $(X, d)$ un espace métrique, $A \subset X$ une partie complète non-vide et $f : A \to A$
			contractante sur $A$. Alors~:
			\begin{itemize}
				\item $f$ admet un unique point fixe $a^* \in A$~;
				\item $\forall x_0 \in A$, la suite $x_n = f(x_{n-1})$ converge dans $A$ en $a^*$.
			\end{itemize}
			\end{thm}

			\begin{proof}~
			\begin{itemize}
				\item Montrons d'abord l'existence de $a^*$. Fixons $x_0 \in A$.  La suite $x_n = f(x_{n-1})$ est bien définie dans $A$ (car $f(A) \subseteq A$).
				Observons que~:
				\begin{equation}
					\forall n \in \N : x_n = f^n(x_0).
				\end{equation}

				Soient $n, p \in \N$. On calcule~:
				\begin{align}
					d(x_{n+p}, x_n) &\leq d(x_{n+p}, x_{n+p-1}) + d(x_{n+p-1}, x_{n+p-2}) + \ldots + d(x_{n+1}, x_n) \\
					&\leq k^{p-1}d(x_{n+1}, x_n) + k^{p-2}d(x_{n+1}, x_n) + \ldots + d(x_{n+1}, x_n) \\
					&\leq \frac {1-k^p}{1-k}d(x_{n+1}, x_n) \leq \frac 1{1-k}d(x_{n+1}, x_n) \\
					&\leq \frac 1{1-k}k^nd(x_1, x_0) \xrightarrow[n \to \pinfty]{} 0,
				\end{align}
				et ce, indépendamment de $p$. La suite $(x_n)_n$ est donc de Cauchy, et par complétude de $A$ (hypothèse), on sait que $(x_n)_n$ converge dans $A$.
				Appelons cette limite $a^*$. On a alors~:
				\begin{equation}
					a^* = \lim_{n \to \pinfty} x_n = \lim_{n \to \pinfty}f(x_{n-1}) = f(a^*).
				\end{equation}
				Le point $a^*$ est donc un point fixe.

				Montrons ensuite l'unicité de ce point fixe. Soient $x, y \in A$ deux points fixes de $f$. On sait alors~:
				\begin{equation}
					x = f(x) \qquad\qquad \text{ et } \qquad\qquad y = f(y).
				\end{equation}
				Or, puisque $f$ est contractante, on sait~:
				\begin{equation}
					d(x, y) = d(f(x), f(y)) \leq kd(x, y),
				\end{equation}
				ou encore~:
				\begin{equation}
					(1-k)d(x, y) \leq 0.
				\end{equation}

				Or on sait que $1-k \gneqq 0$. Donc on a $d(x, y) \leq 0$, et donc $d(x, y) = 0$, ce qui par séparabilité des points d'une métrique implique $x=y$.
				\item On a vu que $x_n = f(x_{n-1})$ était convergente pour toute valeur initiale de $x_0$. Or, on sait également que le point fixe de $f$ est
				unique, et donc pour tout $x_0$, la suite $x_n = f(x_{n-1})$ converge vers $a^*$ cet unique point fixe.
			\end{itemize}
			\end{proof}

			\begin{cor} Soit $A$ une partie non-vide et complète d'un espace métrique. Soit $f : A \to A$. Si $f$ admet une puissance contractante, alors $f$
			admet un unique point fixe $a^*$ dans $A$.
			\end{cor}

			\begin{proof} Soit $n \in \Ns$ tel que $f^n$ est contractante. Par le théorème de Banach, on sait que $f^n$ admet un unique point fixe $a^*$ sur $A$.
			On peut alors écrire $f^n(f(a^*)) = f(f^n(a^*)) = f(a^*)$. Donc $f(a^*)$ est un point fixe de $f^n$. Et par unicité, on sait que $f(a^*) = a^*$.

			Soit $a \in A$ un point fixe de $f$. Cela veut dire $a = f(a) = f(f(a)) = \ldots = f^n(a)$. Donc $a$ est un point fixe de $f^n$. À nouveau, par
			unicité, $a = a^*$.
			\end{proof}

		\subsection{Cylindres en espace-temps}
			\begin{déf} Soit $f : I \times \Omega \to \R^d$, où $I \subset \R$ est un intervalle et $\Omega \subset \R^d$ est un ouvert.

			On dit que $f$ est lipschitzienne en espace sur $I \times \Omega$ lorsqu'il existe $M \gneqq 0$ tel que~:
			\begin{equation}
				\forall t \in I : \forall x, y \in \Omega : \norm {f(t, x) - f(t, y)} \leq M\norm {x-y}.
			\end{equation}

			On dit que $f$ est localement lipschitzienne en espace sur $I \times \Omega$ lorsque~:
			\begin{equation}
				\forall J \times K \subset I \times \Omega \text{ compact }\exists M(J, K) \tq \forall t \in J :
				\forall x, y \in K : \norm {f(t, x) - f(t, y)} \leq M(J, K)\norm {x-y}.
			\end{equation}
			\end{déf}

			\begin{déf}[Définition équivalente de localement lipschitzien] $f : I \times \Omega \to \R^d$ est localement lipschitzienne en espace lorsque~:
			\begin{align}
				&\forall (t, x) \in I \times \Omega : \exists M \gneqq 0, \widetilde I \times \widetilde \Omega \text{ compacts } \subset I \times \Omega \tq \\
				&(t_0, x_0) \in \widetilde I \times \widetilde \Omega \text{ et } \forall t \in \widetilde I : \forall x, y \in \widetilde \Omega :
				\norm {f(t, x) - f(t, y)} \leq M\norm {x-y}.
			\end{align}
			\end{déf}

			\begin{prp}\label{prp:C1=>loclipschitz} Si $f \in C^1(I \times \Omega, \R)$, avec $I \subset \R$, et $\Omega \subset \R^d$, tous deux ouverts,
			alors $f$ est localement lipschitzienne par rapport à $x$ (en espace).
			\end{prp}

			\begin{proof} Soient $t \in I, x \in \Omega$. On choisit $\delta \gneqq 0$ tel que $(t-\delta, t+\delta) \subset I$ et $\varepsilon > 0$ tel que
			$B(x, \varepsilon[ \subset \Omega$. La fonction $(t, x) \mapsto \dif_xf(t, \cdot)$\footnotemark est continue sur
			$\left[t-\frac \delta2, t+\frac \delta2\right] \times B\left(x, \frac \varepsilon2\right]$ compact car $f$ est $C^1$. En particulier, elle est bornée,
			donc il existe $M \gneqq 0$ tel que~:
			\begin{equation}
				\norm {\dif_xf(t, \cdot)} \leq M.
			\end{equation}

			Soient $y_1, y_2 \in B\left(x, \frac \varepsilon2\right]$ deux valeurs en espace, et $t \in \left[t_0 \pm \frac \delta2\right]$ une valeur en temps.
			On a alors~:
			\begin{equation}
				f(t, y_2) - f(t, y_1) = (y_2-y_1)\int_{y_1}^{y_2}\pd fx(t, y)\dif y = \int_0^1\dif_{sy_2 + (1-s)y_1}f(t, \cdot)(y_2-y_1)\dif s,
			\end{equation}
			que l'on peut majorer en norme par~:
			\begin{equation}
				\norm {f(t, y_2) - f(t, y_1)} \leq \int_0^1\norm {\dif_{sy_2 + (1-s)y_1}f(t, \cdot)(y_2-y_1)\dif s} \leq \int_0^1M\norm {y_2-y_1}\dif s
				= M\norm {y_2-y_1}.
			\end{equation}
			\end{proof}		\footnotetext{La notation $\dif_xf(t, \cdot)$ correspond à la dérivée partielle de $f$ par rapport à sa variable d'espace, évaluée
			en $x$. Donc $\dif_{x_0}f(t, \cdot) = \pd fx(t, x_0)$.}

			\begin{déf} Pour tout $t_0 \in \R, y_0 \in \R^d$, $\ell, r \gneqq 0$, on appelle \textit{cylindre (en espace-temps)} centré en $(t_0, y_0)$ de rayon
			$r$ et de demi-axe $\ell$ l'ensemble~:
			\begin{equation}
				S(t_0, y_0, \ell, r) \coloneqq \left\{(t, y) \in \R \times \R^d \tq \abs {t-t_0} \leq r, \norm {y-y_0} \leq \ell\right\}.
			\end{equation}
			\end{déf}

			\begin{rmq} $S(t_0, y_0, \ell, r)$ est un compact convexe de $\R \times \R^d$.
			\end{rmq}

			\begin{prp} Soit $f : I \times \Omega \to \R^d$ localement lipschitzienne en espace. Soient $(t_0, y_0) \in I \times \Omega, \ell, r \gneqq 0 \tq$~:
			\begin{equation}
				S(t_0, y_0, \ell, r) \subset I \times \Omega.
			\end{equation}
			Alors $f$ est localement lipschitzienne sur $S(t_0, y_0, \ell, r)$.
			\end{prp}

		\subsection{Théorème d'existence et d'unicité locales}
			\begin{prp} Soient $f : J \times \Omega \to \R^d$ localement lipschitzienne en espace, $(t_0, y_0) \in J \times \Omega$. Il existe $\ell, r \gneqq 0$
			tels que~:
			\begin{itemize}
				\item[$(i)$]  $S \coloneqq S(t_0, y_0, \ell, r) \subset J \times \Omega$~;
				\item[$(ii)$] $\ell\norm f_{\infty, S} \leq r$.
			\end{itemize}
			\end{prp}

			\begin{proof} Puisque $J \times \Omega$ est ouvert, il existe $\delta, \varepsilon \gneqq 0$ tels que
			$S(t_0, y_0, \delta, \varepsilon) \subset J \times \Omega$. Posons alors $\varepsilon \eqqcolon r$, et~:
			\begin{equation}
				\ell \coloneqq \min\left(\delta, \frac \varepsilon{\norm f_{\infty, S(t_0, y_0, \delta, \varepsilon)}}\right).
			\end{equation}

			Alors $\ell > 0$, et on a donc~:
			\begin{equation}
				\ell \leq \frac \varepsilon{\norm f_{\infty, S(t_0, y_0, \delta, \varepsilon)}} \leq \frac \varepsilon{\norm f_{\infty, S(t_0, y_0, \ell, r)}},
			\end{equation}
			car $\ell \leq \delta$.
			\end{proof}

			\begin{thm}[de Cauchy-Lipschitz local (TCL local)]Soient $J \subset \R$ un intervalle non-vide et $\Omega \subset \R^d$ ouvert non-vide,
			$f : J \times \Omega \tocont \R^d$ localement lipschitzienne en espace. Soit $(t_0, x_0) \in J \times \Omega$ et $\ell, r \gneqq 0$ tels que~:
			\begin{equation}
				S \coloneqq S(t_0, y_0, \ell, r) \subset J \times \Omega\qquad\qquad\text{ et }\qquad\qquad0 < \ell < \frac r{\norm f_{\infty, S}}.
			\end{equation}

			Alors~:
			\begin{itemize}
				\item il existe $y \in C^1\left([t_0-\ell, t_0+\ell], \Omega\right)$ solution de~\eqref{eq:equadiffPC}~;
				\item pour tout solution $\widetilde y$ de~\eqref{eq:equadiffPC} définie sur $\widetilde I$, intervalle tel que $t_0 \in \widetilde I$~:
				\begin{equation}
					\forall t \in \widetilde I \cap [t_0 \pm \ell] : y(t) = \widetilde y(t).
				\end{equation}
			\end{itemize}
			\end{thm}

			\begin{rmq} Ce théorème affirme l'unicité locale de la solution au sein d'un cylindre de sécurité centré en $t_0$.
			\end{rmq}

			\begin{proof} Construisons une suite $(y_n)_n \subset C^1([t_0 \pm \ell], \Omega)$ qui converge uniformément sur $[t_0 \pm \ell]$ vers une solution
			de~\eqref{eq:equadiffPC}.

			Notons $I = [t_0 \pm \ell]$ et~:
			\begin{equation}
				A(I) \coloneqq \left\{y \in C^1(I, \Omega) \tq \forall t \in I : \norm {y(t) - y_0} \leq r\right\}.
			\end{equation}

			On remarque $A(I) \subset C^0(I, \Omega)$ et $A(I)$ est fermé dans $\evnC0I{\R^d}$ (et donc complet car fermé dans un complet). Posons
			$y_0 \equiv y_0$\footnotemark. On en déduit donc
			$A(I) \neq \emptyset$. Soit $y \in A(I)$. On pose~:
			\begin{equation}
				\forall t \in I : Ty(t) \coloneqq T(y)(t) \coloneqq y_0 + \int_{t_0}^tf(s, y(s))\dif s.
			\end{equation}

			On remarque $T(A(I)) \subset A(I)$. En effet~: $Ty$ est continue sur $I$ car de classe $C^1$. De plus~:
			\begin{equation}
				\forall t \in I : \norm {Ty(t) - y_0} \leq \int_{\min(t_0, t)}^{\max(t_0, t)}\norm {f(s, y(s))}\dif s \leq \abs {t-t_0}\norm f_{\infty, S}
				\leq \ell\frac r\ell = r.
			\end{equation}

			Soient $y_1, y_2 \in A(I)$. Calculons pour $t \in I$~:
			\begin{equation}
				T(y_1)(t) - T(y_2)(t) = \int_{t_0}^t\left(f(s, y_1(s)) - f(s, y_2(s))\right)\dif s.
			\end{equation}

			Soit $L$ une constante de Lipschitz pour $f$ sur $S$. On trouve alors~:
			\begin{equation}
				\norm {T(y_1)(t) - T(y_2)(t)} \leq \int_{\min(t_0, t)}^{\max(t_0, t)}L\abs {y_1(s)-y_2(s)}\dif s \leq L\ell\norm {y_2-y_1}_\infty.
			\end{equation}

			Montrons alors par récurrence sur $p \in \Ns$ que~:
			\begin{align}\label{eq:recTCllocal}
				\forall t \in I : \norm {T^p(y_1)(t) - T^p(y_2)(t)} \leq \frac {L^p\abs {t-t_0}^p}{p!}\norm {y_1-y_2}_{\infty, I}.
			\end{align}

			Pour le cas initial $p=1$, on vient en effet d'obtenir le résultat. Pour le pas de récurrence, supposons que \eqref{eq:recTCllocal} est vrai pour un
			certain $p \geq 2$, et estimons~:
			\begin{align}
				\forall t \in I : \norm {T^{p+1}(y_1)(t) - T^{p+1}(y_2)(t)} &\leq \int_{\min(t_0, t)}^{\max(t_0, t)}L\norm {T^p(y_1)(s) - T^p(y_2)(s)}\dif s \\
				&\leq \int_{\min(t_0, t)}^{\max(t_0, t)}L\frac {L^p\abs {s-t_0}^p}{p!}\dif s\norm {y_1-y_2}_\infty \\
				&= \frac {L^{p+1}\abs {t-t_0}^{p+1}}{(p+1)!}\norm {y_1-y_2}_\infty.
			\end{align}

			Choisissons $p_0 \in \Ns$ tel que $L^{p_0}\ell^{p_0} < p_0!$. L'application $T^{p_0}$ est donc une contraction de $A(I)$ dans elle-même. Par le
			théorème de Banach, elle admet un unique point fixe $y \in A(I)$. On a alors $T(y) = y$, et en particulier~:
			\begin{align}\label{eq:TCLlocalPCintégrale}
				\forall t \in I : y(t) = y_0 + \int_{t_0}^tf(s, y(s))\dif s,
			\end{align}
			donc $y \in C^1(I, \Omega)$ est solution de \eqref{eq:equadiffPC} par la Proposition~\ref{prp:PCintégrale}.

			Soit $\widetilde I$ un intervalle tel que $t_0 \in \widetilde I$, et soit $\widetilde y : \widetilde I \to \Omega$ solution de \eqref{eq:equadiffPC}.
			Notons $\overline I = \widetilde I \cap I$, un intervalle contenant $t_0$. Les fonctions $\restr yi{\overline I}$ et $\restr {\widetilde y}{\overline I}$
			sont des solutions de \eqref{eq:equadiffPC}, et donc de sa forme intégrale \eqref{eq:TCLlocalPCintégrale}. En particulier, elles sont fixes par
			$T : A(\overline I) \to A(\overline I)$, donc elles coïncident sur $\overline I$ (unicité par Banach).
			\end{proof}		\footnotetext{C'est-à-dire que le premier élément de la suite $(y_n)_n$ est la fonction constante $t \mapsto y_0$.}

			\begin{prp} Si $f \in C^k(J \times \Omega)$ et $y$ est une sol de \eqref{eq:equadiffPC} sur $I \subset J$ tel que $t_0 \in I$, alors $y \in C^{k+1}(I, \Omega)$.
			\end{prp}

			\begin{proof} On sait que $y$ est de classe $C^1$ sur $I$, et donc $y'$ est de classe $C^1$ par composition de fonctions $C^1$. Similairement, on
			trouve $y \in C^2(I)$, etc. jusque $y \in C^k$. À nouveau, par composition de fonctions $C^k$, on trouve $y \in C^{k+1}(I)$.
			\end{proof}

			\begin{rmq} On a donc montré que la suite $(y_n)_n$ de fonctions définie par~:
			\begin{equation}
				\begin{cases}
				y_0 &\equiv y_0 \\
				y_{n+1}(t) &= y_0 + \int_{t_0}^tf(s, y_n(s))\dif s
			\end{cases}
			\end{equation}
			vérifie~:
			\begin{equation}
				\forall n \geq 1 : \forall t \in [t_0 \pm \ell] : \norm {y_n(t) - y(t)} \leq \frac {L^n\abs {t-t_0}^n}{n!}2r.
			\end{equation}

			Cette reformulation de la convergence des $y_n$  vers une solution unique au sein du cylindre permet de déterminer numériquement des solutions au
			problème de Cauchy\footnote{Bien que ce ne soit pas le plus efficace.}.
			\end{rmq}

	\section{Existence et unicité locale}
		\subsection{Motivation}
			Soit $f \in C^0(J \times \Omega, \R^d$, localement lipschitzienne en espace. Soit $(t_0, y_0)\in J \times \Omega$. Appliquons le théorème de
			Cauchy-Lipschitz (TCL) local à \eqref{eq:equadiffPC}. On obtient $Y$ une solution de \eqref{eq:equadiffPC} sur un certain segment $[t_0 \pm \ell]$.
			On a $t_0 + \ell \in J$ et $Y(t_0 + \ell) \in \Omega$. On y applique le TCL local~:

			Il existe $\delta > 0$ et $Z$ une solution du problème de Cauchy $(t_0+\ell, Y(t_0+\ell)) \in J \times \Omega$ sur $[(t_0+\ell) \pm \delta]$.
			Les fonctions $Y$ et $Z$ coïncident sur $[(t_0 + \ell) \pm \delta] \cap [t_0 \pm \ell]$.

			La fonction définie par~:
			\begin{equation}
				W : [t_0-\ell, t_0+\ell+\delta] \to \Omega : t \mapsto \begin{cases}Y(t) &\text{ si } t \leq t_0+\ell \\Z(t) &\text{ sinon }\end{cases}
			\end{equation}
			est de classe $C^1$ sur $[t_0-\ell, t_0+\ell+\delta]$.

			De plus, on remarque~:
			\begin{equation}
				Y'(t) \xrightarrow[t \mapsto (t_0+\ell)^-]{} f(t_0+\ell, Y(t_0+\ell)),
			\end{equation}
			où~:
			\begin{equation}
				f(t_0+\ell, Z(t_0+\ell) = \lim_{t \mapsto (t_0+\ell)^+}f(t, Z(t)).
			\end{equation}

			La solution initiale $Y$ a donc été \textit{prolongée} de $[t_0 \pm \ell]$ à $[t_0 \pm \ell] \cup [(t_0+\ell) \pm \delta]$.
			Il vient cependant certaines questions~:
			\begin{itemize}
				\item La solution $y$ peut-elle être prolongée indéfiniment~?
				\item Peut-elle être étendue à $J$ tout entier~?
				\item Existe-t-il un comportement «~\textit{maximal}~» que l'on ne pourrait plus prolonger~?
			\end{itemize}

		\subsection{Exemples}
			\begin{enumerate}
				\item Prenons $J = \R, \Omega = \Rp_0$ et intéressons-nous au problème~:
				\begin{equation}
					y'(t) = -3\sin(y)y(t)^{\frac 43}.
				\end{equation}
				Pour pouvoir appliquer le TCL local, il faut que $f : \R \times \Rp_0 : (t, y) \mapsto -3\sin(t)y^{\frac 43}$ soit localement lipschitzienne et
				continue. On sait $f \in C^1(J \times \Omega)$, et donc par la Proposition~\ref{prp:C1=>loclipschitz}, on sait $f$ localement lipschitzienne en
				espace, et est continue.

				Par séparation des variables, on résout~:
				\begin{equation}
					-3\left[y^{-\frac 13}\right]_{y_0}^{y(t)} = -3\left[-\cos(t)\right]_{t_0}^t,
				\end{equation}
				ou encore~:
				\begin{equation}
					y(t) = \frac 1{\left(y_0^{-\frac 13} - \cos(t) + \cos(t_0)\right)^3}.
				\end{equation}

				\begin{itemize}
					\item Prenons $J \times \Omega \ni (t_0, y_0) = \left(\frac \pi2, \frac 18\right)$. On a alors~:
					\begin{equation}
						y(t) = \frac 1{\left(2-\cos(t)\right)^3},
					\end{equation}
					où $y$ est une solution du problème de Cauchy sur $\R$.

					\item Prenons $J \times \Omega \ni (t_0, y_0) = \left(\frac \pi2, 8\right)$. On a alors~:
					\begin{equation}
						y(t) = \frac 1{\left(\frac 12 - \cos(t)\right)^3},
					\end{equation}
					où $y$ est une solution du problème de Cauchy sur $\left(\frac \pi3, \frac {5\pi}3\right)$.

					En effet, on observe~:
					\begin{equation}
						y(t) \xrightarrow[t \to {\frac \pi3}^+]{} \pinfty\qquad\qquad\text{ et }\qquad\qquad y(t) \xrightarrow[t \to {\frac {5\pi}3}^-]{} \pinfty.
					\end{equation}
				\end{itemize}

				\item Prenons $J = \Rp_0, \Omega = \R$ et intéressons-nous au problème~:
				\begin{equation}
					y'(t) = \frac 1{t^2}\sin\left(\frac 1t\right).
				\end{equation}

				Posons $f : \Rp_0 \times \R \to \R : (t, x) \mapsto t^{-2}\sin(t^{-1})$. $f \in C^0(\Rp_0 \times \R, \R)$ donc le TCL local s'applique.

				\underline{Note~:} On remarque également que $f$ ne dépend pas de $y$. On observe donc que la théorie des équations différentielles et des
				problèmes de Cauchy comprennent entre autres la théorie des primitives.

				Prenons $J \times \Omega \ni (t_0, y_0) = \left(\frac 1{2\pi}, 1\right)$. On a alors $y(t) = \cos(t^{-1})$ est solution du problème de Cauchy
				sur $\Rp_0$. On observe également que $y$ n'admet pas de limite en $t \to 0$, et que~:
				\begin{equation}
					y(t) \xrightarrow[t \to \pinfty]{} 1.
				\end{equation}
			\end{enumerate}

		\subsection{Bouts droites et bouts gauches}
			\begin{déf} Soient $(X, d)$ un espace métrique et $A \subset X$ non-vide.

			\begin{itemize}
				\item $x \in X$ est dit \textit{adhérent} à $A$ lorsque~:
				\begin{equation}
					\forall \varepsilon > 0 : B(x, \varepsilon[ \cap A \neq \emptyset.
				\end{equation}
				\item $x \in X$ est appelé \textit{point d'accumulation} de $A$ lorsque~:
				\begin{equation}
					\forall \varepsilon > 0 : \left(B(x, \varepsilon[ \setminus \{x\}\right) \cap A \neq \emptyset.
				\end{equation}
			\end{itemize}
			\end{déf}

			\begin{déf} Soient $X, Y$ deux ensembles. Soit $f : X \to Y$. On appelle \textit{graphe de $f$} l'ensemble~:
			\begin{equation}
				\Gamma_f \coloneqq \left\{(x, f(x) \tq x \in X\right\} \subset X \times Y.
			\end{equation}
			\end{déf}

			\begin{déf} Soit $y : (\alpha, \beta) \to \Rp$ avec~:
			\begin{equation}
				\minfty \leq \alpha \leq \beta \leq \pinfty.
			\end{equation}

			\begin{itemize}
				\item On appelle \textit{bout droit}  de $y$ tout point d'accumulation de son graphe de la forme $(\beta, z)$, avec $z \in \R^d$~;
				\item on appelle \textit{bout gauche} de $y$ tout point d'accumulation de son graphe de la forme $(\alpha, z)$, avec $z \in \R^d$.
			\end{itemize}
			\end{déf}

			\begin{rmq} Pour les exemples précédents, on remarque~:

			\begin{center}
				\begin{tabular}{|c|c|c|c|}
				\hline
				$I$ & $y(t)$ & bouts gauches & bouts droits \\ \hline  \hline
				$I = \R$ & $y(t) = (2-\cos(t))^{-3}$ & $\emptyset$ & $\emptyset$ \\ \hline
				$I = \left(\frac \pi3, \frac {5\pi}3\right)$ & $y(t) = \left(\frac 12 \cos(t)\right)^{-3}$ & $\emptyset$ & $\emptyset$ \\ \hline
				$I = \Rp_0$ & $y(t) = \cos(t^{-1})$ & $\{0\} \times [-1, 1]$ & $\emptyset$ \\
				\hline
				\end{tabular}
			\end{center}
			\end{rmq}

		\subsection{Solutions maximales}
			Soit $f : J \times \Omega \to \R^d$, avec $\Omega \subset \R^d$ ouvert et $I \subset \R$ un intervalle ouvert.

			\begin{déf} Soient $y_i : I_i \to \Omega, i=1, 2$, solutions de l'équation différentielle $y'(t) = f(t, y(t))$. Soit $(t_0, y_0) \in J \times \Omega$
			tels que $y_1, y_2$ soient solutions du problème de Cauchy $y(t_0) = y_0$.

			Sur $I_1$, on dit que $y_2$ est une sur-solution de $y_1$ lorsque~:
			\begin{itemize}
				\item $I_1 \subseteq I_2$~;
				\item $y_1 = y_2$ sur $I_1$.
			\end{itemize}
			\end{déf}

			\begin{rmq} On remarque qu'une solution est toujours sur-solution d'elle-même.
			\end{rmq}

			\begin{déf} une solution $y$ de \eqref{eq:equadiffPC} définie sur $I$ est dite \textit{maximale} lorsque pour toute sur-solution $\widetilde y$ de
			$y$ définie sur $\widetilde I$, on a $\widetilde I \subseteq I$.
			\end{déf}

		\subsection{Théorème de Cauchy-Lipschitz global}
			\begin{thm}[Théorème de Cauchy-Lipschitz global] Soient $J \subset \R$ un intervalle ouvert non-vide, $\Omega \subset \R^d$ un ouvert non-vide, et
			$f \in C^0(J \times \Omega, \R^d)$ localement lipschitzienne en espace.

			Alors pour tout couple $(t_0, y_0) \in J \times \Omega$, il existe une unique solution maximale au problème de Cauchy.

			De plus, les bouts gauches et droits de la solution sont des éléments de $\partial(J \times \Omega)$\footnotemark.
			\end{thm}\footnotetext{Où $\partial X$ représente le \textit{bord} de l'ensemble $X$.}

			\begin{proof}~

			\textbf{1. Existence et unicité}

				Notons~:
				\begin{align}
					\mathcal I &\coloneqq \left\{I \subset J \tq \text{ \eqref{eq:equadiffPC} admet une solution sur } I\right\}, \\
					N^- &\coloneqq \left\{\text{ extrémités gauches des éléments de } \mathcal I\right\}, \\
					N^+ &\coloneqq \left\{\text{ extrémités droites des éléments de } \mathcal I\right\}.
				\end{align}

				$f$ est continue et localement lipschitzienne par hypothèse, on peut alors appliquer le TCL local, par lequel on sait qu'il existe $\delta > 0$
				tel que $[t_0 \pm \delta] \in \mathcal I$. Dès lors, $(t_0-\delta, t_0+\delta) \in N^- \times N^+$. Posons~:
				\begin{equation}
					\alpha \coloneqq \inf N^- \qquad\qquad \text{ et } \qquad\qquad \beta \coloneqq \sup N^+.
				\end{equation}

				On a alors~:
				\begin{equation}
					\minfty \leq \alpha \leq t_0-\delta \leq t_0+\delta \leq \beta \leq \pinfty.
				\end{equation}

				\underline{1.1. Existence}

					On va définir $y$ une solution de \eqref{eq:equadiffPC} sur $(\alpha, \beta)$ et ensuite montrer qu'elle est maximale. Procédons sur
					$[t_0, \beta)$, le cas $(\alpha, t_0]$ se déduit similairement.

					Soit $t \in (\alpha, \beta)$ et soient $y_1, y_2$ deux solutions de \eqref{eq:equadiffPC} définies respectivement sur $I_1$ et $I_2$ telles
					que $t_0 \in I_1 \cap I_2$.

					Posons~:
					\begin{equation}
						A \coloneqq \left\{s \in [t_0, t] \tq y_1 = y_2 \text { sur } [t_0, s]\right\}.
					\end{equation}

					Par le TCL local, on sait que $t_0 + \delta \in A$, quitte à «~réduire~» $\delta$ afin que $t_0+\delta \leq t$. Posons ensuite~:
					\begin{equation}
						\tau \coloneqq \sup A \in [t_0+\delta, t].
					\end{equation}

					On observe alors que $\tau \in A$. En effet, si $\varepsilon > 0$ est fixé, $\tau-\varepsilon$ n'est plus un majorant de $A$. Donc il existe
					$t_\varepsilon \in [\tau-\varepsilon, \tau]$ tel que $t_\varepsilon \in A$. Donc $y_1(t_\varepsilon)=y_2(t_\varepsilon)$. De plus, on sait que~:
					\begin{equation}
						t_\varepsilon \xrightarrow[\varepsilon \to 0]{} \tau.
					\end{equation}

					Dès lors, il vient par continuité de $y_1$ et $y_2$ en $\tau \in I_1 \cap I_2$ que $y_1(\tau) = y_2(\tau)$, et donc $\tau \in A$.

					Supposons maintenant par l'absurde $\tau \lneqq t$.  On sait $y_1(\tau) = y_2(\tau)$. Appliquons alors le TCL local au problème de Cauchy suivant~:
					\begin{equation}
						\begin{cases}
						y'(t) &= f(t, y(t)) \\
						y(\tau) &= y_1(\tau) = y_2(\tau).
					\end{cases}
					\end{equation}

					On obtient $y_1$ et $y_2$ coïncident sur $(\tau \pm \ell)$ pour un certain $\ell > 0$. Par définition de $\tau$, cela implique que $y_1$ et
					$y_2$ coïncident sur $[t_0, \tau+\ell)$. Or $\tau = \sup A$. Il y a donc une contradiction. On en déduit $\tau = t$, et dès lors
					$y_1(t) = y_2(t)$.

					On peut également définir~:
					\begin{equation}
						Y : (\alpha, \beta) \to \Omega : \hat t \mapsto Y(\hat t),
					\end{equation}
					où $Y(\hat t)$ est la valeur de n'importe quelle solution du problème de Cauchy en $\hat t \in [t_0, t]$. La fonction $Y$ est bien définie
					et est de plus de classe $C^1$ sur $(\alpha, \beta)$ (ainsi que solution du problème de Cauchy).

					Soit $\widetilde Y$ une sur-solution de $Y$ sur un intervalle défini par $\widetilde \alpha$ et $\widetilde \beta$ (ouvert ou fermé) avec~:
					\begin{equation}
						\widetilde \alpha \leq \alpha \leq \beta \leq \widetilde \beta.
					\end{equation}

					Par continuité de $Y$, on a $\alpha = \inf N^-$ et $\beta = \sup N^+$. Or $\widetilde \alpha \in N^-$ et $\widetilde \beta \in N^+$. Il en
					découle que $\alpha = \widetilde \alpha$ et $\widetilde \beta = \beta$. Les seules possibilités pour $\widetilde I$ sont alors~:
					\begin{itemize}
						\item $\widetilde I = (\widetilde \alpha, \widetilde \beta)$~;
						\item $\widetilde I = [\widetilde \alpha, \widetilde \beta)$~;
						\item $\widetilde I = (\widetilde \alpha, \widetilde \beta]$~;
						\item $\widetilde I = [\widetilde \alpha, \widetilde \beta]$.
					\end{itemize}

					Si $\widetilde \beta \in \widetilde I$, le TCL local permet de prolonger $\widetilde Y$ (et donc $Y$ par continuité) en une solution
					au-delà strictement de $\beta = \widetilde \beta$. Il y a donc contradiction avec la définition même de $\beta$. Idem pour
					$\widetilde \alpha \in \widetilde I$.

					Donc $\widetilde I = (\widetilde \alpha, \widetilde \beta)$, et $Y = \widetilde Y$. $Y$ est donc maximale.

				\underline{1.2. Unicité}

					Idem, $Y$ est la solution maximale ci-dessus et $\widetilde Y$ est une solution maximale de \eqref{eq:equadiffPC}.  % Revoir...

			\textbf{2. Appartenance des bouts au bord}

				Soit $y$ une solution maximale de \eqref{eq:equadiffPC} sur $(\alpha, \beta)$ et soit $(\beta, z)$ un bout droit de $y$ (la démonstration pour
				les bouts gauches se déduit similairement). On suppose par l'absurde $(\beta, z) \not \in \partial(J \times \Omega)$.

				Il existe $\ell, r \gneqq 0$ tels que $S(\beta, z, \ell, r) \subset J \times \Omega$ est de sécurité pour $f$. Par définition de $(\beta, z)$,
				il existe $(t_n)_n \in (\alpha, \beta)^\N$ injective telle que~:
				\begin{equation}
					t_n \xrightarrow[n \to \pinfty]{} \beta \qquad\qquad \text{ et } \qquad\qquad y(t_n) \xrightarrow[n \to \pinfty]{} z.
				\end{equation}

				Pour $n$ suffisamment grand, on trouve alors $(t_n, y(t_n)) \in S\left(\beta, z, \frac \ell4, \frac r4\right)$. Ainsi~:
				\begin{equation}
					S' \coloneqq S\left(\beta, z, \frac \ell2, \frac r2\right) \subset S(\beta, z, \ell, r) \eqqcolon S,
				\end{equation}
				et donc~:
				\begin{equation}
					\norm f_{\infty, S'} \leq \norm f_{\infty, S},
				\end{equation}
				et on a $(\beta, z) \in \intr S'$. Puisque $0 < \ell < r{\norm f_{\infty, S}^{-1}}$ (car $S$ est de sécurité pour $f$), on sait~:
				\begin{equation}
					0 < \frac \ell2 < \frac r{2\norm f_{\infty, S}} \leq \frac r{2\norm f_{\infty, S'}}.
				\end{equation}

				On en déduit que $S'$ est également de sécurité pour $f$. Le TCL local appliqué à~:
				\begin{align}\label{eq:TCLglobalPCZ}
					\begin{cases}
						Z'(t) &= f(t, Z(t)) \\
						Z(t_n) &= y(t_n)
					\end{cases}
				\end{align}
				permet de s'assurer que la solution maximale de \eqref{eq:TCLglobalPCZ} vit encore en $t_n + \frac \ell2$. Or $t_n + \frac \ell2 \gneqq \beta$,
				ce qui contredit le caractère maximal de $y$. Dès lors, $(\beta, z) \in \partial(J \times \Omega)$.
			\end{proof}

			\begin{rmq} Soit $\Omega \subset \R^d$. Notons~:
			\begin{equation}
				K \subscpct \Omega
			\end{equation}
			lorsque $K$ est un compact inclus dans $\Omega$.
			\end{rmq}

			\begin{cor}[Théorème de sortie de tout compact] Soient $J = (a, b)$ un intervalle réel non-vide, $\Omega \subset \R^d$ ouvert non-vide, et soit~:
			\begin{equation}
				f : J \times \Omega \toC0 \R^d,
			\end{equation}
			localement lipschitzienne en espace sur $J \times \Omega$. Soit $(t_0, y_0) \in J \times \Omega$ et soit $y$, la solution maximale de~:
			\begin{equation}
				\begin{cases}
				y'(t) &= f(t, y(t)), \\
				y(t_0) &= y_0.
			\end{cases}
			\end{equation}

			Notons $(\alpha, \beta) \subset (a, b)$ l'intervalle de définition de $y$.

			\begin{itemize}
				\item Si $a \lneqq \alpha$, alors pour tout suite $(t_n)_n \subset (\alpha, \beta)$ telle que $t_n \xrightarrow[n \to \pinfty]{} \alpha^+$~:
				\begin{equation}
					\forall K \subscpct \Omega : \abs {\left\{n \in \N \tq y(t_n) \in K\right\}} < \pinfty~;
				\end{equation}
				\item si $\beta \lneqq b$, alors pour toute suite $(t_n)_n \subset (\alpha, \beta)$ telle que $t_n \xrightarrow[n \to \pinfty]{} \beta^-$~:
				\begin{equation}
					\forall K \subscpct \Omega : \abs {\left\{n \in \N \tq y(t_n) \in K\right\}} < \pinfty.
				\end{equation}
			\end{itemize}
			\end{cor}

			\begin{proof} Notons tout d'abord que, par ouverture de $J \times \Omega$, on sait~:
			\begin{equation}
				\partial(J \times \Omega) = \adh(J \times \Omega) \setminus (J \times \Omega).
			\end{equation}

			Montrons ici le cas $\beta \lneqq b$ (le cas $a \lneqq \alpha$ se déduit similairement). Supposons par l'absurde qu'il existe $(t_n)_n$, une suite dans
			$(\alpha, \beta)$ qui tend vers $\beta^-$ pour $n \to \pinfty$ et $K \subscpct \Omega$ tels que~:
			\begin{equation}
				\left\{n \in \N \tq y(t_n) \in K\right\}
			\end{equation}
			est infini dénombrable. Par compacité de $K$, il existe $\varphi : \N \to \N$ strictement croissante et $y^* \in K$ tels que~:
			\begin{equation}
				y(t_{\varphi(n)}) \xrightarrow[n \to \pinfty]{} y^* \in K \subscpct \Omega.
			\end{equation}

			Ainsi, $(\beta, y^*)$ est un bout droit de $y$, qui est solution maximale. Par le théorème des bouts, on sait $(\beta, y^*) \in \partial (J \times \Omega)$.
			Or $\beta \in (a, b)$ et $y^* \in K$, donc $(\beta, y^*) \in J \times \Omega$. Il y a donc une contradiction car le bord n'est pas contenu dans
			$J \times \Omega$.
			\end{proof}

	\section{Flot d'une équation différentielle et intégrale première}
		\subsection{Flot associé à une équation différentielle}
			Soient $J \subset \R$ un intervalle non-vide, $\Omega \subset \R^d$ ouvert non-vide et $f \in C^0(J \times \Omega, \R^d)$ localement lipschitzienne
			en espace. L'existence et l'unicité de solution maximale au problème de Cauchy~\eqref{eq:equadiffPC} pour $(t_0, y_0) \in J \times \Omega$ permet
			de «~définir une fonction~» $\Phi$ associée à l'équation différentielle $y(t) = f(t, y(t))$ en posant~:
			\begin{equation}
				\Phi(t_0, y_0, t) = \hat y(t),
			\end{equation}
			où $\hat y$ est la solution maximale de~\eqref{eq:equadiffPC}.

			$\Phi : J \times \Omega \times J \to \Omega \subset \R^d$ n'est pas définie pour tous les triplets $(t_0, y_0, t) \in J \times \Omega \times J$.
			En revanche, lorsque $(t_0, y_0) \in J \times \Omega$ est fixé, $t \mapsto \Phi(t_0, y_0, t)$ est définie sur un voisinage de $t_0$ dans $J$
			par le TCL global.

			Il y a même une certaine uniformité (par rapport à $y_0$).

			\begin{prp} Soit $(t_0, y_0) \in J \times \Omega$ et soit $\varepsilon > 0$ tel que $B(y_0, \varepsilon] \subset \Omega$. Alors~:
			\begin{equation}
				\exists \ell > 0 \tq \forall y \in B(y_0, \varepsilon] : t \mapsto \Phi(t_0, y, t) \text{ est définie sur } [t_0 \pm \ell].
			\end{equation}
			\end{prp}

			\begin{rmq} Cela veut dire que l'intersection des intervalles de définition des $\Phi(t_0, y, t)$ pour $y$ arbitrairement proche de $y_0$ (au sens
			d'inclusion dans la boule de centre $y_0$ et de rayon $\varepsilon$) n'est pas le singleton $\{t_0\}$ mais bien un intervalle de la forme $[t_0+\ell]$
			pour un certain $\ell > 0$.
			\end{rmq}

			\begin{proof} $\R^d \setminus \Omega$ est un fermé, $B(y_0, \varepsilon]$ est un fermé de $\Omega$, et est borné, donc est compact, et
			$\R^d \setminus \Omega \cap B(y_0, \varepsilon] = \emptyset$. Ainsi~:
			\begin{equation}
				\delta \coloneqq \inf_{(x, y) \in (\R^d \setminus \Omega) \times B(y_0, \varepsilon]}\norm {x-y} > 0.
			\end{equation}

			En effet, si $(x_n)_n$ est un suite d'éléments de $\R^d \setminus \Omega$, et si $(y_n)_n$ est une suite d'éléments de $B(y_0, \varepsilon]$ telles
			que ces suites minimisent $\norm {x-y}$, supposons par l'absurde que $\norm {x_n-y_n}_n$ tende vers 0. On a donc~:
			\begin{equation}
				\lim_{n \to \pinfty}x_n = X = \lim_{n \to \pinfty}y_n.
			\end{equation}

			Par fermeture de $B(y, \varepsilon]$ et $\R^d \setminus \Omega$, on détermine $X \in B(y, \varepsilon]$ et $X \in \R^d \setminus \Omega$.
			Et donc $B(y, \varepsilon] \cap \R^d \cap \Omega \ni X$, ce qui est une contradiction. Donc $\delta \gneqq 0$.

			Notons alors~:
			\begin{equation}
				A \coloneqq \norm f_{\infty, [t_0 \pm \ell_0] \times B\left(y_0, \varepsilon+\frac \delta2\right]},
			\end{equation}
			où $\ell_0 > 0$ est fixé tel que $[t_0 \pm \ell_0] \subset J$. On peut alors choisir $\ell > 0$ tel que~:
			\begin{equation}
				0 < \ell < \min\left(\ell_0, \frac \delta{4A}\right).
			\end{equation}

			Alors pour tout $y \in B(y,_0, \varepsilon]$, le cylindre $S(t_0, y_0, \ell, \frac \delta4)$ est de sécurité pour $f$. En effet,
			$S(t_0, y_0, \ell, \frac \delta 4) \subset J \times \Omega$ et~:
			\begin{equation}
				0 < \ell < \frac \delta{4A} \leq \frac \delta{4\norm f_{\infty, S}}.
			\end{equation}

			Par le TCL local, on sait que la solution maximale de~:
			\begin{equation}
				\begin{cases}
				Z'(t) &= f(t, Z(t)) \\
				Z(t_0) &= Z_0
			\end{cases}
			\end{equation}
			existe au moins sur $[t_0 \pm \ell]$, avec $\ell$ qui dépend de $\varepsilon$ mais qui ne dépend pas de $y$.
			\end{proof}

			\begin{rmq} Une conséquence de cela est qu'au voisinage de tout point $y_0$ en espace, les solutions maximales aux problèmes de Cauchy sont
			paramétrées par leur valeur en $t_0$.
			\end{rmq}

			\begin{prp}  Pour $y \in B(y_0, \varepsilon]$ et $t \in [t_0 \pm \ell]$ sous les hypothèses précédentes, on a~:
			\begin{equation}
				\Phi\left(t_0, \Phi(t, y, t_0), t\right) = y.
			\end{equation}
			\end{prp}

		\subsection{Intégrales premières}
			\begin{déf} Soient $J \subset \R$ un intervalle non-vide, $\Omega \subset \R^d$ ouvert non-vide, et $f \in C^0(J \times \Omega, \R^d)$ localement
			lipschitzienne en espace sur $J \times \Omega$. On appelle \textit{intégrale première de l'intégrale~\eqref{eq:equadiff}}~:
			\begin{align}\label{eq:equadiff}
				y'(t) = f(t, y(t))
			\end{align}
			toute fonction $H : J \times \Omega \to \R$ constante le long des solutions de l'équation différentielle, c-à-d~:
			\begin{equation}
				t \mapsto H(t, y(t))
			\end{equation}
			est constante pour toute fonction $y$ solution de~\eqref{eq:equadiff}.
			\end{déf}

			\begin{prp} $H \in C^1(J \times \Omega, \R)$ est une intégrale première de~\eqref{eq:equadiff} si et seulement si~:
			\begin{equation}
				\forall (t, y) \in J \times \Omega : \pd Ht(t, y(t)) + \scpr {f(t, y)}{\nabla_yH(t, y(t))} = 0.\footnotemark
			\end{equation}
			\end{prp}		\footnotetext{La notation $\nabla_yH(t, y(t))$ représente le gradient en espace de $H$, et donc
			$\nabla_yH(t, y(t)) = \pd Hy(t, y(t))$ mais pour $y$ à plusieurs dimensions. i.e.~: $\nabla_y H = [\pd H{y_i}]_i$.}

			\begin{proof} Soit $H \in C^1(J \times \Omega)$ une intégrale première de~\eqref{eq:equadiff}. Soit $(t_0, y_0) \in J \times \Omega$, et soit $y$
			la solution maximale du problème de Cauchy $(t_0, y_0)$. On a $t \mapsto H(t, y(t))$ est de classe $C^1$ sur un intervalle $I \subset J$ et est
			constante. On en déduit, sur cet intervalle~:
			\begin{equation}
				\od Ht(t, y(t)) = 0,\footnotemark
			\end{equation}
			Ou encore~:
			\begin{equation}
				\pd Ht(t, y(t)) + \scpr {\od yt(t)}{\pd H(t, y(t))} = 0.
			\end{equation}

			En particulier, en $t=t_0$, on trouve~:
			\begin{equation}
				\pd Ht(t_0, y(t_0)) + \scpr {f(t_0, y(t_0))}{\pd Ht(t_0, y(t_0))} = 0.
			\end{equation}

			Supposons maintenant $H \in C(J \times \Omega)$ telle que pour toute solution $y$ de~\eqref{eq:equadiff}~:
			\begin{equation}
				t \mapsto H(t, y(t)) \in C^1(I \subset J, \R) \tq \pd Ht(t, y(t)) + \scpr {f(t, y(t))}{\pd Ht(t, y(t))} = 0.
			\end{equation}

			On en déduit que $t \mapsto H(t, y(t))$ est constante sur $I$, son intervalle de définition.
			\end{proof}  \footnotetext{La notation $\od Ht(t, y(t))$ représente la dérivée ordinaire (ou totale) de $H$ par rapport à $t$. On peut établir~:
			\begin{equation}
				\od Ht(t, y(t)) = \od \eta t(t),
			\end{equation}
			où $\eta : J \to \R : t \mapsto H(t, y(t))$.}

		\subsection{Exemple de système Hamiltonien}
			TODO. %TODO

	\section{Condition suffisante d'existence locale}
		\begin{thm}[Théorème de Cauchy-Peano-Arzela] Soient $J \subset \R$ un intervalle et $\Omega \subset \R^d$ ouverts. Soient $f \in C^0(J \times \Omega, \R^d)$ et
		$(t_0, y_0) \in J \times \Omega$. Alors~\eqref{eq:equadiffPC} admet une solution sur un intervalle $[t_0 \pm \delta]$ pour $\delta > 0$.
		\end{thm}

		\begin{rmq}Par rapport au théorème de Cauchy-Lipschitz, en hypothèses, on a perdu le caractère lipschitzien en espace de $f$ (même localement), et on
		a perdu l'unicité de la solution dans les conséquences.
		\end{rmq}

		\begin{proof} On prolonge $f$ par la valeur nulle de $\R^d$ à $J \times \Omega$. Soit une suite $\rho_k$ telle que $\rho_k : \R^d \to \R^+$ est
		régularisante (donc pour tout $k \geq 1$, on a $\rho_k \in C^\infty(\R^d, \R^+)$, $\int_{\R^d}\rho_k(x)\dif x = 1$, et $\supp \rho_k \subset B(0, \frac 1k[$).

		Pour $(t, x) \in J \times \R^d$ et $k \in \Ns$, on pose~:
		\begin{equation}
			f_k(t, x) = \left(f_k(t, \cdot) * \rho_k\right)(x).
		\end{equation}

		Alors $\forall k \in \Ns$, on a (EXERCICE)~:
		\begin{equation}
			\begin{cases}
			f_k &\in C^0(J \times \R^d, \R^d), \\
			f_k(t, \cdot) &\CVUc {J \times \Omega}k\pinfty f(t, \cdot), \\
			f_k &\CVUc {J \times \Omega}k\pinfty f.
		\end{cases}
		\end{equation}

		On choisit $r > 0$ et $\ell > 0$ tels que~:
		\begin{equation}
			S \coloneqq S(t_0, y_0, \ell, r) \subset J \times \Omega
		\end{equation}

		$S$ est un compact de $J \times \Omega$, on donc~:
		\begin{equation}
			f_k \CVUc Sk\pinfty f.
		\end{equation}

		Par inégalité triangulaire, on sait $\norm {f_k}_{\infty, S} \leq \norm {f_k - f}_{\infty, S} + \norm f_{\infty, S}$, avec
		$\norm {f_k - f}_{\infty, S} \xrightarrow[k \to \pinfty]{} 0$ par convergence uniforme.

		On sait qu'il existe $A \geq 0$ tel que~:
		\begin{equation}
			\forall k \geq 1 : \norm {f-f_k}_{\infty, S} \leq A.
		\end{equation}
		Quitte à diminuer $S$ en diminuant $\ell$, on peut supposer~:
		\begin{equation}
			0 < \ell < \frac rA,
		\end{equation}
		et donc $S$ est un cylindre de sécurité pour $f_k$ quel que soit $k \geq 1$.

		Remarquons ensuite (EXERCICE) que pour tout $k \geq 1$, on a~:
		\begin{equation}
			\begin{cases}
			f_k &\in C^0(J \times \Omega, \R^d), \\
			\forall t \in J &: f_k(t, \cdot) \in C^\infty(\Omega, \R^d),
		\end{cases}
		\end{equation}
		et $(t, y) \mapsto d_yf(t, \cdot)$ est $C^0$ sur $J \times \Omega$.

		On en déduit que $f_k$ vérifie les hypothèses du TCL. Fixons $k \geq 1$ et appliquons le TCL local pour obtenir une (unique) solution $y_k$
		de~\eqref{eq:equadiffPC} sur $[t_0 \pm \ell]$ (car $S$ est de sécurité pour les $f_k$).

		Montrons maintenant que la suite $(y_k)_k \subset \left(C^0\left([t_0 \pm \ell]\right), \R^d\right)$ vérifie les hypothèses du théorème d'Arzela-Ascoli
		(Théorème~\ref{thm:Arzela-Ascoli}). En effet~:
		\begin{itemize}
			\item
				\begin{equation}
					\forall t \in [t_0 \pm \ell] : \forall k \geq 1 : \left(t, y_k(t)\right) \in S,
				\end{equation}
			par construction de $S$, et donc $\norm {y_k(t) - y_0} \leq r$~;

			\item $B = \left\{y_k \tq k \in \Ns\right\}$ est équicontinue sur $[t_0 \pm \ell]$ car les $y_k$ sont équi-lipschitziennes.

			En effet, prenons $t, s \in [t_0 \pm \ell]$, on trouve~:
			\begin{equation}
				\norm {y_k(t) - y_0} \leq \int_{\min(\ell, t)}^{\max(\ell, t)}\norm {f_k(\sigma, y_k(\sigma))}\dif \sigma \leq \abs {s-t}\norm {f_k}_{\infty, S}
				\leq A\abs {s-t}.
			\end{equation}
		\end{itemize}

		Dès lors, par Arzela-Ascoli, il existe $y^* \in C^0([t_0 \pm \ell], \R^d)$ et $\varphi : \N \to \N$ strictement croissante tels que~:
		\begin{equation}
			y_{\varphi(k)} \CVU {[t_0 \pm \ell]}k\pinfty y^*.
		\end{equation}

		Montrons alors maintenant que~:
		\begin{equation}
			f_k\left(t, y_k(t)\right) \CVU {[t_0 \pm \ell]}k\pinfty f\left(t, y^*(t)\right).
		\end{equation}

		En effet, pour $t \in I = [t_0 \pm \ell]$ et $k \geq 1$, on trouve~:
		\begin{align}
			\norm {f_k\left(t, y_{\varphi(k)}(t)\right) - f\left(t, y^*(t)\right)}
				&\leq \norm {f_k\left(t, y_{\varphi(k)}(t)\right) - f\left(t, y_{\varphi(k)}(t)\right)} + \norm {f\left(t, y_{\varphi(k)}(t)\right) - f(t, y^*(t))} \\
			&\leq \norm {f-f_k}_{\infty, S} + \sup_{t \in I}\norm {f\left(t, y_{\varphi(k)}(t)\right) - f(t, y^*(t))},
		\end{align}
		qui tend vers $0$ pour $k \to \pinfty$ par CVU de $f_k$ vers $f$ et de $y_{\varphi(k)}$ vers $y^*$.

		Finalement, par le TCL local, on observe~:
		\begin{equation}
			\forall k \geq 1 : \forall t \in I : y_{\varphi(k)} = y_0 + \int_{t_0}^tf(\sigma, y_{\varphi(k)}(\sigma))\dif \sigma,
		\end{equation}
		et donc en passant à la limite $k \to \pinfty$, on obtient~:
		\begin{equation}
			\forall t \in I : y^*(t) = y_0 + \int_{t_0}^tf(\sigma, y^*(\sigma))\dif \sigma.
		\end{equation}

		Ainsi, il existe au moins une solution de~\eqref{eq:equadiffPC}, à savoir $y^*$.
		\end{proof}

		\begin{ex} Considérons le problème de Cauchy suivant~:
		\begin{equation}
			\begin{cases}
			y'(t) &= \sqrt {\abs {y(t)}} \\
			y(0) &= 0
		\end{cases}
		\end{equation}

		On a $f : \R \times \R \to \R : (t, y) \mapsto \sqrt {\abs y}.$ $f$ est $C^0$ sur $\R^2$ donc le théorème de Cauchy-Peano s'applique. On sait alors
		qu'il existe une solution $y$ définie sur $[-\delta, \delta]$.

		On peut en effet trouver~:
		\begin{enumerate}
			\item $y \equiv 0$ est une sol sur $\R$~;
			\item $y : \R \to \R : t \mapsto \begin{cases}0 &\text{ si } t \leq 0 \\\frac {t^2}4 &\text{ si } t > 0\end{cases}$ est également solution sur $\R$.
			\item on peut généraliser cette dernière par~:
			\begin{equation}
				y : \R \to \R : t \mapsto \begin{cases}0 &\text{ si } t \leq \alpha \\ \frac {(t-\alpha)^2}4 &\text{ si } t > \alpha,\end{cases}
			\end{equation}
			pour $\alpha \geq 0$.
		\end{enumerate}
		\end{ex}

\chapter{Équations différentielles linéaires}
	\section{Existence et unicité des solutions globales}
		\subsection{Notations}
			On se donne $J \subset \R$ un intervalle non-vide et $\Omega \subset \R^d$ ouvert non-vide et $d(d+1)$ fonctions~:
			\begin{equation}
				[a_{ij}]_{1 \leq i, j \leq d} \qquad\qquad \text{ et } \qquad\qquad [b_i]_{1 \leq i \leq g}
			\end{equation}
			sur $J$ et à valeurs dans $\K \in \{\R, \C\}$. On souhaite résoudre le système~:
			\begin{equation}
				y_i'(t) = \sum_{j=1}^da_{ij}(t)y_j(t) + b_i(t)
			\end{equation}
			pour $i \in \intint 1d$.

			Posons~:
			\begin{equation}
				Y(t) = [y_i(t)]_{1 \leq i \leq d} \qquad A(t) = [a_{ij}(t)]_{1 \leq i, j \leq d} \quad B(t) = [b_i(t)]_{1 \leq i \leq d}.
			\end{equation}
			Le système est équivalent à~:
			\begin{align}\label{eq:PCL}\tag{PCL}
				Y'(t) = A(t)Y(t) + B(t).
			\end{align}

		\subsection{Théorie de Cauchy}
			\begin{lem}[Lemme de Gronwall] Soient $[a, b]$ un segment de $\R$ et $\alpha, \beta > 0$. Soit~:
			\begin{equation}
				u : [a, b] \toC0 \R^+.
			\end{equation}
			Si~:
			\begin{equation}
				\forall t \in [a, b] : u(t) \leq \alpha + \beta\int_a^tu(s)\dif s,
			\end{equation}
			alors~:
			\begin{equation}
				u(t) \leq \alpha\exp\left(\beta(t-a)\right).
			\end{equation}
			\end{lem}

			\begin{proof} Si $\beta = 0$, alors le lemme est trivial. Sinon, posons~:
			\begin{equation}
				f : [a, b] \to \R : t \mapsto \exp\left(-\beta(t-a)\right)\left[\frac \alpha\beta + \int_a^tu(s)\dif s\right].
			\end{equation}

			On observe alors~:
			\begin{align}
				\forall t \in [a, b] : f'(t) &= u(t)\exp\left(-\beta(t-a)\right) + \left[\frac \alpha\beta + \int_a^tu(s)\dif s\right]\exp\left(-\beta(t-a)\right) \\
				&= \exp\left(-\beta(t-a)\right)\left(u(t) - \left(\alpha + \beta\int_a^tu(s)\dif s\right)\right) \leq 0.
			\end{align}

			Ainsi~:
			\begin{equation}
				\forall t \in [a, b] : f(t) \leq f(a),
			\end{equation}
			et donc~:
			\begin{align}
				\left(\frac \alpha\beta + \int_a^tu(s)\dif s\right)\exp\left(-\beta(t-a)\right) &\leq \frac \alpha\beta \\
				\frac \alpha\beta + \int_a^tu(s)\dif s &\leq \frac \alpha\beta \left(\beta(t-a)\right) \\
				u(t) &\leq \alpha\exp\left(\beta(t-a)\right).
			\end{align}
			\end{proof}


			\begin{thm}[Théorème de Cauchy linéaire] Si les fonctions $a_{ij}$ et $b_i$ sont continues, alors pour tout $(t_0, Y_0) \in J \times \K^d$,
			le problème de Cauchy~\eqref{eq:PCL} admet une unique solution définie sur $J$.
			\end{thm}

			\begin{proof} $J$ est un intervalle d'extrémités $a < b$ (ouvert ou fermé). Si $J$ est fermé en $a$, alors on prolonge $A$ et $B$ à gauche par $A(a)$
			et $B(a)$. De même, si $J$ est fermé en $b$, on prolonge $A$ et $B$ à droite par $A(b)$ et $B(b)$. On a alors des fonctions continues sur $\R$.

			On peut alors considérer $J$ ouvert sans perte de généralité. Montrons alors que l'on peut appliquer le TCL global à~\eqref{eq:PCL}.
			Posons donc~:
			\begin{equation}
				f(t, Y) = A(t)Y + B(t).
			\end{equation}
			On remarque aisément que $f \in C^0(J \times \K^d, \R^d)$. Soit $K \subset J$ compact. Il existe $M_A \gneqq 0$ tel que~:
			\begin{equation}
				\forall t \in K : \norm {A(t)} \leq M_A.
			\end{equation}
			Ainsi pour tout $t \in K$~:
			\begin{align}
				\forall Y, Z \in \R^d : \norm {f(t, Y) - f(t, Z)} &\leq \norm {A(t)(Y-Z)} \leq \norm {A(t)}\norm {Y-Z} \leq M_A\norm{Y-Z},
			\end{align}
			car en posant la norme~:
			\begin{equation}
				\norm A \coloneqq \sup_{X \in \K^d \tq \norm X = 1}\norm {AX},
			\end{equation}
			on a bien~:
			\begin{equation}
				\forall X \in \K^d : \norm {AX} \leq \norm A \norm X.
			\end{equation}

			La fonction $f$ est donc localement en temps globalement lipschitzienne en espace, et donc localement lipschitzienne en espace.

			Par le TCL global, il existe une unique solution maximale $Y$ de classe $C^1$ sur un intervalle de définition $(\alpha, \beta) \subset J$.

			Supposons par l'absurde que $\beta$ n'est pas l'extrémité droite de $J$. Il existe alors $t_1 > \beta \tq t_1 \in J$. Puisque $[t_0, t_1]$ est un
			segment de $J$, il existe $\mu, \gamma > 0$ tels que~:
			\begin{equation}
				\forall t \in [t_0, t_1] : \begin{cases}\norm {A(t)} &\leq \mu, \\\norm {B(t)} &\leq \gamma.\end{cases}
			\end{equation}

			Ainsi, pour tout $t \in [t_0, \beta)$~:
			\begin{equation}
				Y(t) = Y_0 + \int_{t_0}^tf(s, Y(s))\dif s.
			\end{equation}
			Et donc~:
			\begin{equation}
				\norm {Y(t)} \leq \norm {Y_0} + \int_{t_0}^t\norm {f(s, Y(s))}\dif s \leq \norm {Y_0} + \int_{t_0}^t\left(\norm {A(s)}\norm {Y(s)} + \norm {B(s)}\right)\dif s
				\leq \norm {Y_0} + \gamma(t-t_0) + \mu\int_{t_0}^t\norm {Y(s)}\dif s.
			\end{equation}

			Par le lemme de Gronwall, on peut écrire~:
			\begin{equation}
				\norm {Y(t)} \leq \left(\norm {Y_0} + \gamma(t-t_0)\right)\exp\left(\mu(t-t_0)\right)
				\leq \left(\norm {Y_0} + \gamma(t-t_0)\right)\exp\left(\mu(\beta-t_0)\right).
			\end{equation}

			Donc $Y$ est bornée sur $[t_0, \beta)$ et est solution maximale de~\eqref{eq:PCL} et $\beta$ n'est pas l'extrémité droite de $J$. Cela contredit le
			théorème de sortie de tout compact. On en déduit que $\beta$ est l'extrémité droite de $J$. Par un argument similaire, on trouve $\alpha$ est
			l'extrémité gauche de $J$. La solution $Y$ est donc l'unique solution maximale et est définie sur $(\alpha, \beta) = J$.
			\end{proof}

			\begin{rmq} L'unicité vient du théorème de Cauchy-Lipschitz~: la preuve fait appel au TCL global afin d'obtenir une solution $Y$ maximale du problème
			de Cauchy et définie sur un intervalle inclus dans $J$. La seconde partie de la preuve est de montrer que ce sous-intervalle est $J$.
			\end{rmq}

	\section{Systèmes différentiels linéaires homogènes --- matrice résolvante}
		\begin{déf} Avec les notations précédentes, un système différentiel $Y'(t) = A(t)Y(t) + B(t)$ est dit \textit{homogène} lorsque $B \equiv 0$.
		\end{déf}

		\begin{rmq} Lorsque $A \in C^0(I, \Mat_d(\K))$, on note~:
		\begin{equation}
			\Phi(t, t_0, y_0) = y(t)
		\end{equation}
		pour $t, t_0 \in I$ et $y_0 \in \K^d$ la valeur de la solution globale du problème de Cauchy à l'instant $t$~:
		\begin{equation}
			\begin{cases}
			Y'(s) &= A(s)Y(s) \\
			Y(t_0) &= y_0.
		\end{cases}
		\end{equation}
		\end{rmq}

		\begin{déf} Notons $S_H$ l'ensemble des fonctions dérivables sur $I$, à valeurs dans $\K^d$, solutions de $Y'(t) = A(t)Y(t)$.
		\end{déf}

		\begin{prp} $S_H$ est un espace vectoriel.
		\end{prp}

		\begin{proof} EXERCICE.
		\end{proof}

		\begin{prp} Pour $t \in I$, l'application~:
		\begin{equation}
			\varphi_t : S_H \to \K^d : y \mapsto y(t)
		\end{equation}
		est un isomorphisme d'espace vectoriel.
		\end{prp}

		\begin{proof} La linéarité est triviale par opérations sur les fonctions, et la bijection vient directement du théorème de Cauchy linéaire.
		\end{proof}

		\begin{cor} $\dim S_H = d$.
		\end{cor}

		\begin{déf} Pour $t, t_0 \in I$, on appelle \textit{matrice résolvante} $R(t, t_0)$ la matrice de $(\varphi_t \circ \varphi_{t_0}^{-1})$ dans la base
		canonique de $\K^d$.
		\end{déf}

		\begin{rmq} Pour les notations précédentes, on a~:
		\begin{equation}
			(\varphi_t \circ \varphi_{t_0}^{-1})(y_0) = \Phi(t, t_0, y_0) = R(t, t_0)y_0.
		\end{equation}
		\end{rmq}

		\begin{prp} Soient $s, t, t_0 \in I$. Alors~:
		\begin{enumerate}
			\item $R(t, t_0) \in \GL_d(\K)$ et $R(t_0, t_0) = \Id$~;
			\item $R(t, s)R(s, t_0) = R(t, t_0)$~;
			\item $R(t, t_0)^{-1} = R(t_0, t)$.
		\end{enumerate}
		\end{prp}

		\begin{proof}~
		\begin{enumerate}
			\item $(\varphi_t \circ \varphi_{t_0}^{-1})$ est une application linéaire bijective de $\K^d \to \K^d$ donc $R(t, t_0) \in \GL_d(\K)$.
			De plus, $R(t_0, t_0) = (\varphi_{t_0} \circ \varphi_{t_0}^{-1}) = \Id$.
			\item Soit $y \in \K^d$. Calculons~:
			\begin{equation}
				R(t, s)R(s, t_0)y = \left(\left(\varphi_t \circ \varphi_s^{-1}\right) \circ \left(\varphi_s \circ \varphi_{t_0}^{-1}\right)\right)(y)
				= \left(\varphi_t \circ \varphi_{t_0}^{-1}\right)(y) = R(t, t_0)y.
			\end{equation}

			Et donc $R(t, s)R(s, t_0) = R(t, t_0)$.

			\item On observe~:
			\begin{equation}
				R(t, t_0)^{-1} = \MatBC\left(\varphi_t \circ \varphi_{t_0}^{-1}\right)^{-1} = \Mat_{B_C}\left(\varphi_{t_0} \circ \varphi_t^{-1}\right) = R(t_0, t),
			\end{equation}
			où $\MatBC(f)$ représente la matrice de l'application $f$ dans la base canonique de son ensemble d'arrivée.
		\end{enumerate}
		\end{proof}

		\begin{déf} On appelle \textit{système fondamental de solutions} (abrégé \textit{SFS}) de $Y( = AY)$ toute base de l'espace $S_H$.
		\end{déf}

		\begin{prp} Soient $v_1, \ldots, v_k \in S_H$. Les trois affirmations suivantes sont équivalentes~:
		\begin{itemize}
			\item $\{v_1, \ldots v_k\}$ forment un SFS~;
			\item $\forall t \in I : \det(v_1(t), \ldots, v_k(t)) \neq 0$~;
			\item $\exists t \in I : \det(v_1(t), \ldots, v_k(t)) \neq 0$.
		\end{itemize}
		\end{prp}

		\begin{proof} Montrons d'abord que $(1) \Rightarrow (2)$. L'application $\varphi_t$ envoie $(v_i)_i$ sur une base de $\K^k$ car c'est un isomorphisme.
		On en déduit que $(v_i(t))_i$ est une base de $\K^k$. Le déterminant ne peut donc pas être nul.

		Il est trivial que $(2) \Rightarrow (3)$ car $I \neq \emptyset$.

		Pour montrer que $(3) \Rightarrow (1)$, on sait que $\varphi_t^{-1}$ envoie $(v_i(t))_i$ sur une base de $S_H$, et donc $(v_i)_i$ est une base de $S_H$, ou
		encore un système fondamental de solutions.
		\end{proof}

		\begin{déf} On appelle \textit{matrice fondamentale} de l'équation $Y' = AY$ toute «~matrice~» $t \mapsto C(t)$ dont les colonnes $t \mapsto C_i(t)$
		forment une base de $S_H$.
		\end{déf}

		\begin{prp} Si $t \mapsto C(t)$ est une matrice fondamentale de $Y' = AY$ et $R(t, t_0)$ en est la matrice résolvante, alors~:
		\begin{equation}
			\forall (s, t) \in I^2 : R(t, s)C(s) = C(t),
		\end{equation}
		et par suite~:
		\begin{equation}
			\forall (t, s) \in I^2 : R(t, s) = C(t)C(s)^{-1}.
		\end{equation}
		\end{prp}

		\begin{proof} On prend $t \mapsto C_i(t)$ pour $i \in \intint 1d$ les colonnes de la matrice $t \mapsto C(t)$. On pose $D_i$ définie par~:
		\begin{equation}
			t \mapsto D_i(t) \coloneqq \left[R(t, s)C(s)\right]_i.
		\end{equation}

		On sait que $C_i$ est solution de $Y' = AY$ sur $I$ avec $C_i(s) = [C(s)]_i$.

		$D_i$ est également solution de $Y' = AY$ sur $I$ avec $D_i(s) = [R(t, s)C(s)]_i$.

		Par le théorème de Cauchy linéaire, on a unicité de la solution, et donc~:
		\begin{equation}
			\forall i \in \intint 1d : C_i = D_i,
		\end{equation}
		d'où la formule suivante~:
		\begin{equation}
			R(t, s) = C(t)C(s)^{-1}.
		\end{equation}
		\end{proof}

		\begin{rmq} Chaque colonne de $t \mapsto R(t, t_0)$ est dérivable sur $I$, solution de $Y'=AY$. On en déduit que $R(t, t_0)$ est dérivable sur $I$
		et vérifie le problème de Cauchy suivant~:
		\begin{equation}
			\begin{cases}
			\forall t \in I : &Z'(t) = A(t)Z(t) \\
			&Z(t_0) = \Id,
		\end{cases}
		\end{equation}
		avec $Z : I \to \Mat_d(\K)$.

		Ainsi $t \mapsto R(t, t_0)$ est solution d'un problème de Cauchy linéaire d'ordre 1 dont la matrice à l'instant $t$ n'est (en général) par $A(t)$.
		\end{rmq}

		\begin{prp} Soit $A \in \Mat_d(\C)$. Alors~:
		\begin{equation}
			\det(I + hA) \underset{h \to 0}= 1 + h\Tr(A) + O(h^2).
		\end{equation}
		\end{prp}

		\begin{proof} Il existe $P \in \GL_d(\C)$ tel que~:
		\begin{equation}
			P^{-1}AP = \begin{bmatrix}\lambda_1 & & ? \\& \ddots & \\0 & & \lambda_d\end{bmatrix}.
		\end{equation}

		On en déduit~:
		\begin{align}
			\det(I + hA) &= \det(I + hP(P^{-1}AP)P^{-1}) = \det\left(P(I + hP^{-1}AP)P^{-1}\right) = \det(I + hP^{-1}AP) \\
			&=\prod_{i=1}^d\left(1 + h\lambda_i\right) = 1 + \sum_{i=1}^d\lambda_ih  + O(h^2).
		\end{align}

		De plus~:
		\begin{align}
			\Tr(A) &= \Tr\left(P\begin{bmatrix}\lambda_1 & & ?\\& \ddots & \\0 & & \lambda_d\end{bmatrix}P^{-1}\right) =
				\Tr\left(\begin{bmatrix}\lambda_1 & & ? \\& \ddots & \\0 & & \lambda_d\end{bmatrix}P^{-1}P\right) \\
			&= \Tr\left(\begin{bmatrix}\lambda_1 & & ? \\ & \ddots & \\0 & & \lambda_d\end{bmatrix}\right) = \sum_{i=1}^d\lambda_i.
		\end{align}

		On a donc bien~:
		\begin{equation}
			\det(I + hA) = 1 + h\Tr(A) + O(h^2).
		\end{equation}
		\end{proof}

		\begin{prp} La fonction $t \mapsto \Delta(t) \coloneqq \det(R(t, t_0))$ est dérivable (en réalité de classe $C^1$) sur $I$ telle que~:
		\begin{align}\tag{Équation de Jacobi}
			\Delta'(t) = \Tr(A(t))\Delta(t),
		\end{align}
		et on en déduit~:
		\begin{align}\tag{Équation de Liouville}
			\Delta(t) = \exp\left(\int_{t_0}^t\Tr(A(s))\dif s\right)\Delta(t_0).
		\end{align}
		\end{prp}

		\begin{proof} Prenons $t, t_0 \in I$, et $h \in \R$ tel que $h + t_0 \in I$. Calculons~:
		\begin{align}
			\Delta(t+h) &= \det(R(t+h, t_0)) = \det(R(t+h, t_0)R(t_0, t)R(t, t_0)) = \det(R(t+h, t))\det(R(t, t_0)).
		\end{align}

		Or, puisque $Z : t \mapsto R(t, t_0)$ satisfait $Z'(t) = A(t)Z(t)$, en développement d'ordre 1, on a~:
		\begin{equation}
			R(t+h, t) \underset{h \to 0}= R(t, t) + hA(t)R(t, t) + o(h).
		\end{equation}

		D'où l'on déduit~:
		\begin{align}
			\Delta(t+h) &\underset{h \to 0}= \Delta(t)\det(I + hA(t) + o(h)) \\
			&\underset{h \to 0}= \Delta(t)\det(I + hA(t)) + o(h) \\
			&\underset{h \to 0}= \Delta(t)\left(1 + h\Tr(A(t))\right) + O(h^2) + o(h).
		\end{align}

		Puisque $f = O(h^2) \Rightarrow f = o(h)$, on trouve~:
		\begin{equation}
			\Delta(t+h) \underset{h \to 0}= \Delta(t) + h\Delta(t)\Tr(A(t)) + o(h).
		\end{equation}

		Pour $h \neq 0$, on sait donc~:
		\begin{equation}
			\frac {\Delta(t+h)-\Delta(t)}h = \Tr(A(t))\Delta(t) + o(1).
		\end{equation}

		Dès lors, $\Delta(t)$ est dérivable en $t$ et $\Delta'(t) = \Tr(A(t))\Delta(t)$.

		De plus, les fonctions $t \mapsto \Delta(t)$ et $t \mapsto \Delta(t_0)\exp\left(\int_{t_0}^t\Tr(A(s))\dif s\right)$ sont solutions, sur $I$, de~:
		\begin{equation}
			\begin{cases}y'(t) &= \Tr(A(t))y(t) \\y(t_0) &= \Delta(t_0)\end{cases}.
		\end{equation}

		Par le théorème de Cauchy linéaire, les équations de Jacobi et de Liouville coïncident sur $I$.
		\end{proof}

	\section{Systèmes linéaires linéaires inhomogènes --- variation des constantes}
		\subsection{Structure de l'espace des solutions}
			On revient au cas où $B \not \equiv 0$, et on considère~:
			\begin{equation}
				Y'(t) = A(t)Y(t) + B(t),
			\end{equation}
			avec $A \in C^0(I, \Mat_d(\K))$ et $B \in C^0(I, \K^d)$ données et $Y : I \to \K^d$ dérivable et inconnue.

			\begin{thm} Si $Y_p$ est une solution de $Y' = AY + B$ (que l'on appelle \textit{solution particulière}), alors~:
			\begin{equation}
				S = Y_p + S_H,
			\end{equation}
			où $S$ est l'ensemble des fonctions dérivables solutions de $Y' = AY + B$, que l'on appelle également \textit{espace affine par $Y_p$ de
			direction $S_H$}
			\end{thm}

			\begin{proof} Montrons que $S \subseteq S_H + Y_p$. Soit $Y \in S$. Alors, on sait que $Z \coloneqq Y-Y_p$ est dérivable sur $I$ tel que~:
			\begin{equation}
				Z'(t) = Y'(t) - Y_p'(t) = \left(A(t)Y(t) + B(t)\right) - \left(A(t)Y_p(t) + B(t)\right) = A(t)\left(Y(t)-Y_p(t)\right) = A(t)Z(t).
			\end{equation}

			Donc on sait que $Z = Y-Y_p \in S_H$, ou encore $Y \in S_H + Y_p$.

			Montrons ensuite que $S_H + Y_p \subseteq S$. Soit $Y \in S_H$. La fonction $Z \coloneqq Y_p + Y$ est dérivable sur $I$, et on a~:
			\begin{equation}
				Z'(t) = Y_p'(t) + Y'(t) = A(t)\left(Y(t) + Y_p(t)\right) + B(t).
			\end{equation}

			On en déduit que la fonction $Z = Y + Y_p \in S$. Dès lors, on a bien~:
			\begin{equation}
				S_H + Y_p \subseteq S \subseteq S_H + Y_p,
			\end{equation}
			ou encore $S_H + Y_p = S$.
			\end{proof}

		\subsection{Recherche d'une solution particulière --- variation de la constante}
			Connaissant $R(t, t_0)$, soit un système fondamental de solutions où $R(t, t_0)$ est la matrice résolvante de $Y'= AY$. On cherche une solution
			$t \mapsto y(t)$ dérivable sur $I$ de~:
			\begin{equation}
				Y'(t) = A(t)Y(t) + B(t)
			\end{equation}
			sous la forme $y(t) = R(t, t_0)x(t)$.

			Si $y$ est solution de $Y' = AY + B$, alors~:
			\begin{equation}
				y'(t) = A(t)y(t) + B(t) = \od{}t \left(R(t, t_0)x(t)\right) = \left(\od{}t R(t, t_0)\right)x(t) + R(t, t_0)x'(t).
			\end{equation}

			Or $\od{}t R(t, t_0) = A(t)R(t, t_0)$. Ainsi~:
			\begin{equation}
				y'(t) = A(t)R(t, t_0)x(t) + R(t, t_0)x'(t) = A(t)y(t) + B(t).
			\end{equation}

			D'où $R(t, t_0)x'(t) = B(t)$ par identité.

			Soit $x'(t) = R(t_0, t)B(t)$. Ainsi~:
			\begin{equation}
				x(t) = x(t_0) + \int_{t_0}^tR(t_0, s)B(s)\dif s,
			\end{equation}
			ou encore~:
			\begin{equation}
				R(t_0, t)y(t) = y(t_0) + \int_{t_0}^tR(t_0, s)B(s)\dif s.
			\end{equation}

			On trouve finalement pour solution particulière~:
			\begin{align}\tag{Formule de Duhamel}
				y(t) = R(t, t_0)y(t_0) + \int_{t_0}^tR(t, s)B(s)\dif s.
			\end{align}

		\subsection{Équations scalaires --- Wronskien}
			Considérons l'équation différentielle suivante~:
			\begin{equation}
				y^{(d)}(t) + u_{d-1}(t)y^{(d-1)}(t) + u_{d-2}(t)y^{(d-2)}(t) + \ldots + u_1(t)y'(t) + u_0(t)y(t) = b(t),
			\end{equation}
			où $u_0, \ldots, u_{d-1} \in C^0(I, \K)$, $I$ un intervalle de $\R$ non-vide, $b \in C^0(I, \K)$ connus, et $y : I \to \K$ $d$ fois dérivable, inconnue.

			Posons~:
			\begin{equation}
				Y(t)  = \begin{bmatrix}y(t) \\ \vdots \\ y^{(d-1)}(t)\end{bmatrix} \in \K^d.
			\end{equation}

			Pour $A$ définie par~:
			\begin{equation}
				\begin{bmatrix}
					0 & 1 & 0 & \ldots & 0 \\
					0 & 0 & 1 & \ldots & 0 \\
					  &  \ddots & & \ddots \\
					0 & 0 & \ldots & 0 & 1 \\
					-u_0(t) & -u_1(t) & \ldots & -u_{d-2}(t) & -u_{d-1}(t)
				\end{bmatrix},
			\end{equation}
			et $B(t)$ défini par~:
			\begin{equation}
				B(t) = \begin{bmatrix}0 \\ 0 \\ \vdots \\ b(t)\end{bmatrix},
			\end{equation}
			on a~:
			\begin{equation}
				Y'(t) = A(t)Y(t) + B(t).
			\end{equation}

			\begin{déf} Lorsque $b \equiv 0$, on appelle \textit{Wronskien de $y_1, \ldots, y_d$} la fonction~:
			\begin{equation}
				W : I \to \K : t \mapsto \det\left(y_1(t), \ldots, y_d(t)\right).
			\end{equation}
			\end{déf}

			\begin{prp} Les trois affirmations suivantes sont équivalentes~:
			\begin{enumerate}
				\item $y_1, \ldots, y_d$ forme un système fondamental de solutions~;
				\item $\forall t \in I : W(t) \neq 0$~;
				\item $\exists t \in I \tq W(t) \neq 0$.
			\end{enumerate}
			\end{prp}

			\begin{proof} EXERCICE.
			\end{proof}

			\begin{prp}~
			\begin{align}\tag{Formule de Liouville pour le Wronskien}
				\forall (t, t_0) \in I^2 : W(t) = \exp\left[-\int_{t_0}^tu_{d-1}(s)\dif s\right]W(t_0)
			\end{align}
			\end{prp}

	\section{Systèmes différentiels à coefficients constants}
		\subsection{Rappels d'algèbre linéaire --- forme normale de Jordan}
			\begin{prp}[Réduction des matrices nilpotentes]Soit $M \in \Mat_d(\C)$ une matrice nilpotente. Il existe $(\epsilon_1, \ldots, \epsilon_{d-1}) \in
			\{0, 1\}^d$ et $P \in \GL_d(\C)$ tels que~:
			\begin{equation}
				P^{-1}MP =
			\begin{bmatrix}
				0 & \epsilon_1 & 0 & \ldots & 0 \\
				0 & 0 & \epsilon_2 & \ldots & 0 \\
				0 & 0 & 0		  & \ddots & 0 \\
				  & &   &  \ddots   & \epsilon_{d-1} \\
				0 & 0 & \ldots & \ldots & 0
			\end{bmatrix}
			\end{equation}
			\end{prp}

			\grantedproof

			\begin{déf} Soit $P \in \K[x]$ un polynôme. On dit que $P$ est \textit{scindé} lorsque $P$ est le produit de polynômes de degré 1.
			\end{déf}

			\begin{prp}[Décomposition de Dunford] Soit $M \in \Mat_d(\C)$. Il existe $D, N \in \Mat_d(\C)$ uniques telles que~:
			\begin{equation}
				\begin{cases}M &=D+N \\DN &= ND\end{cases},
			\end{equation}
			et $D$ est diagonalisable et $N$, nilpotente.
			\end{prp}

			\begin{rmq} Les matrices $D$ et $N$ sont des polynômes en $M$.
			\end{rmq}

			\begin{proof} Pour prouver cela, trouvons d'abord une expression pour les matrices $D$ et $N$, et ensuite montrons qu'elles sont bien respectivement
			diagonalisable et nilpotente. Il ne restera plus alors qu'à prouver leur unicité.

			Prenons $\pi(M)$ le polynôme caractéristique de la matrice $M$\footnote{Que l'on peut également noter $\chi_M$.}. Le degré de $\pi(M)$ est
			$d$. Par le théorème fondamental de l'algèbre (théorème de D'alembert-Gauß), $\pi(M)$ est scindé. En particulier, il existe $\lambda_1, \ldots, \lambda_p$
			dans $\C$ deux à deux distincts et $n_1, \ldots, n_p \in \Ns$ tels que $\sum_{i=1}^pn_i = d$ et~:
			\begin{equation}
				P = \prod_{j=1}^p\left(x- \lambda_j\right)^{n_j}.
			\end{equation}

			Par le théorème de Cayley-Hamilton, on sait que~:
			\begin{equation}
				\pi(M)(M) = 0.
			\end{equation}

			Par le théorème de décomposition du noyau, on a~:
			\begin{equation}
				\C^d = \Ker\left(\pi(M)(M)\right) = \bigoplus_{j=1}^pC_j,
			\end{equation}
			pour $C_j \coloneqq \Ker\left(\left(M - \lambda_jI\right)^{n_j}\right)$.

			Pour $i \in \intint 1d$, on pose~:
			\begin{equation}
				Q_i \coloneqq \prod_{j \neq i} \left(\left(x-\lambda_j\right)^{n_j}\right).
			\end{equation}

			Les $Q_i$ sont premiers entre eux dans leur ensemble. On peut donc y appliquer la relation de Bezout. Il existe $U_1, \ldots U_p \in \C[x]$ tels que~:
			\begin{equation}
				\sum_{i=1}^pU_iQ_i = 1.
			\end{equation}

			Posons donc $P_i \coloneqq U_iQ_i$. On peut donc écrire~:
			\begin{align}\label{eq:Dunford sum_Pi = Id}
				\sum_{i=1}^pP_i(M) = \Id.
			\end{align}

			Pour $i \neq j$, calculons~:
			\begin{equation}
				P_i(M)P_j(M) = U_i(M)P_i(M)U_j(M)P_j(M) = 0,
			\end{equation}
			car $\pi(M) | Q_iQ_j$ et donc $Q_i(M)Q_j(M) = 0$.

			En multipliant~\eqref{eq:Dunford sum_Pi = Id} par $P_j(M)$, on a~:
			\begin{equation}
				P_j(M)^2 = P_j(M).
			\end{equation}

			$P_j(M)$ est donc un projecteur (une application égale à son carré).

			On sait que~:
			\begin{equation}
				\begin{cases}
				\Imf P_i(M) &\subset C_i \\\Ker P_i(M) &\subset \displaystyle \bigoplus_{j \neq i}C_j
			\end{cases}.
			\end{equation}

			Or, par le théorème du rang, on sait que $d = \dim \Ker P_i(M) + \dim \Imf P_i(M)$, avec~:
			\begin{equation}
				\begin{cases}
				\dim \Ker P_i(M) &\leq \sum_{j \neq i} \dim C_j, \\
				\dim \Imf P_i(M) &\leq \dim C_i.
			\end{cases}
			\end{equation}

			Cela force donc $\sum_{j=1}^p\dim C_j = d$, et donc cela force les inégalités en égalités~:
			\begin{equation}
				\begin{cases}
				\Ker P_i(M) &= \displaystyle \bigoplus_{j \neq i}C_j, \\
				\Imf P_i(M) &= C_i.
			\end{cases}
			\end{equation}

			$P_i$ est donc plus précisément le projecteur sur $C_i$ parallèlement à $\bigoplus_{j \neq i}C_j$.\footnote{Notons tout de même que les $C_i$ sont des
			noyaux de polynômes en $M$. Ils sont donc $M$-stables, ou encore stables par la transformation linéaire associée à $M$.}

			Posons~:
			\begin{equation}
				D \coloneqq \sum_{i=1}^p\lambda_iP_i(M).
			\end{equation}

			Alors $D \in \K[M]$ et $\exists N \in \K[M] \tq M = D+N$ (prenons $N \coloneqq M-D$).

			Il reste à voir que $D$ est diagonalisable et $N$ est nilpotente.

			Soient $i \in \intint 1p$ et $x \in C_i$. Calculons~:
			\begin{equation}
				Dx = \sum_{j=1}^p\lambda_jP_j(M)x = \lambda_ix.
			\end{equation}

			Puisque $\K^d = \bigoplus_{j = 1}^pC_i$, il vient $D$ diagonalisable avec $D = \diag(\lambda_1, \ldots, \lambda_p)$.

			En posant $n \coloneqq \max\left(n_1, \ldots, n_p\right)$, on a $n \geq 1$, et en prenant $i \in \intint ip$ et $x \in C_i$, on trouve~:
			\begin{equation}
				N^nx = (M-D)^nx = (M-D)^{n-n_i}(M-D)^{n_i}x = (M-D)^{n-n_i}\left(M-\lambda_iI\right)^{n_i}x,
			\end{equation}
			car $x \in C_i$. On a donc $N^nx = N^{n-n_i} \cdot 0 = 0$. La matrice $n$ est donc nilpotente d'ordre $\leq n$. Nous avons donc montré l'existence
			d'un tel couple $(D, N)$ de matrices.

			Prouvons maintenant l'unicité de ces matrices.

			Supposons qu'il existe $N_1, D_1$ respectivement nilpotente et diagonalisable telles que~:
			\begin{equation}
				\begin{cases}
				M &= D_1+N_1 \\
				N_1D_1 &= D_1N_1.
			\end{cases}
			\end{equation}

			Alors ces matrices commutent à $M$, et donc à tout polynôme en $M$. Or $D$ et $N$ sont des polynômes en $M$. Donc $N_1$ et $D_1$ commutent à $N$ et $D$.

			$D$ et $D_1$ commutent et sont diagonalisables. Elles sont donc diagonalisables dans la même base, et donc $D-D_1$ est diagonalisable également.
			Or puisque $N+D = N_1+D_1$, on a $D-D_1 = N-N_1$. De plus, $N$ et $N_1$ commutent et sont nilpotentes. Donc la matrice $N-N_1$ est nilpotente.

			On en déduit que la seule valeur propre admise par $D-D_1$ est 0, et donc $D-D_1 = 0$ car elle est diagonalisable. Dès lors, on a $D=D_1$ et $N=N_1$.
			\end{proof}

			\begin{déf} On appelle \textit{bloc de Jordan} pour $\lambda \in \C$ et $s \in \Ns$ les matrices de la forme~:
			\begin{equation}
				J_s(\lambda) =
				\begin{bmatrix}
						\lambda & 1 & \ldots & 0 \\
					0 & \lambda & 1 & \vdots \\
					0 & \ddots & \ddots & 1 \\
					0 & \ldots & 0 & \lambda
				\end{bmatrix}
			\end{equation}
			\end{déf}

			\begin{thm}[Décomposition de Jordan] Soit $M \in \Mat_d(\C)$. Il existe $s_1, \ldots, s_p \in \Ns$ et $\lambda_1, \ldots, \lambda_p \in \C$
			et $P \in \GL_d(\C)$ tels que~:
			\begin{equation}
				M = PJP^{-1},
			\end{equation}
			avec~:
			\begin{equation}
				J =
				\begin{bmatrix}
					J_{s_1}(\lambda_1) & & 0 \\
					& \ddots & \\
					0 & & J_{s_p}(\lambda_p)
				\end{bmatrix}.
			\end{equation}
			\end{thm}

			\begin{rmq} La forme de la matrice $J$ est donc une matrice diagonale avec les valeurs propres et certaines valeurs 1 sur la diagonale supérieure.
			C'est une matrice contenant des blocs de Jordan sur sa diagonale.
			\end{rmq}

			\begin{proof} Par Dunford et par réduction des matrices nilpotentes.
			\end{proof}

		\subsection{Exponentielle de matrice}
			\begin{prp} Soit $A \in \Mat_d(\C)$. La série de puissances de terme général $\frac {t^kA^k}{k!}$ a un rayon de convergence infini.
			\end{prp}

			\begin{déf} On appelle cette série \textit{l'exponentielle de $A$}, et on la note~:
			\begin{equation}
				\exp\left(tA\right) = \sum_{k \geq 0}\frac {(tA)^k}{k!}.
			\end{equation}
			\end{déf}

			\begin{proof} Soit $\norm \cdot$ une norme d'algèbre sur $\Mat_d(\C)$ (par exemple une norme subordonnée à une norme sur $\C^d$), en particulier telle que~:
			\begin{equation}
				\forall M, N \in \Mat_d(\C) : \norm {MN} \leq \norm M\norm N.
			\end{equation}

			Ainsi~:
			\begin{equation}
				\forall k \in N : \norm {A^k} \leq \norm A^k,
			\end{equation}
			et donc~:
			\begin{equation}
				\forall k \in \N : \norm {\frac {(tA)^k}{k!}} \leq \frac {\abs t^k}{k!}\norm A^k,
			\end{equation}
			qui est le terme général d'une série qui converge.

			On en déduit que $\sum_{k \geq 0}\frac {(tA)^k}{k!}$ converge absolument quel que soit $t \in \C$. Dès lors, $R = \pinfty$.
			\end{proof}

			\begin{prp} La fonction $t \mapsto \exp(tA)$ est de classe $C^\infty$ sur $\R$ et on a~:
			\begin{equation}
				\forall t \in \R : \forall p \in \N : \od[p]{}t \exp(tA) = A^p\exp(tA) = \exp(tA)A^p.
			\end{equation}
			\end{prp}

			\begin{proof} La fonction $t \mapsto \frac {(tA)^p}{p!}$ est de classe $C^\infty$ sur $\R$ et la série converge normalement, et donc converge
			uniformément  sur les segments de $\R$, de même que ses dérivées à tout ordre (cf Section~\ref{sec:séries de puissances}), et il suffit de dériver
			terme à terme.
			\end{proof}

			\begin{prp} Soient $A, B \in \Mat_d(\C)$. On a~:
			\begin{equation}
				\left(\forall t \in \R : \exp\left((A+B)\right) = \exp(tA)\exp(tB)\right) \iff AB = BA.
			\end{equation}
			\end{prp}

			\begin{proof} Montrons d'abord que cette égalité implique que $A$ et $B$ commutent.

			Par la proposition précédente, dérivons. On observe~:
			\begin{equation}
				\od {}t\exp\left(t(A+B)\right) = (A+B)\exp\left(t(A+B)\right) = A\exp(tA)\exp(tB) + B\exp(tA)\exp(tB).
			\end{equation}

			De même, pour dérivée seconde, on a~:
			\begin{equation}
				\od[2]{}t \exp\left(t(A+B)\right) = (A+B)^2\exp\left(t(A+B)\right),
			\end{equation}
			et également~:
			\begin{equation}
				\od[2]{}t \exp\left(t(A+B)\right) = A^2\exp(tA)\exp(tB) + A\exp(tA)B\exp(tB) + A\exp(tA)B\exp(tB) + \exp(tA)\exp(tB)B^2.
			\end{equation}

			En évaluant ces deux dernières formules en $t=0$ et en les égalisant, on obtient~:
			\begin{equation}
				(A+B)^2I = A^2II + AIBI + AIBI + IIB^2.
			\end{equation}
			On sait $(A+B)^2 = A^2 + AB + BA + B^2$. En simplifiant, on trouve alors~:
			\begin{equation}
				BA = AB.
			\end{equation}

			Montrons alors que si $AB=BA$, alors l'égalité est vérifiée.

			\begin{equation}
				\exp\left(t(A+B)\right) = \sum_{k \geq 0}\frac {t^k}{k!}(A+B)^k = \sum_{k \geq 0}\frac {t^k}{k!}\sum_{p=0}^k\frac {k!}{p!(k-p)!}A^pB^{k-p}.
			\end{equation}
			En effet, la formule du binôme de Newton est ici applicable car $AB = BA$.

			En simplifiant les $k!$, et en «~rentrant~» $t^k$ dans la seconde sommation, on trouve~:
			\begin{equation}
				\sum_{k \geq 0}\sum_{k=0}^p\frac {t^k}{p!(k-p)!}A^pB^{k-p} = \sum_{k \geq 0}\sum_{k=0}^p\frac {t^p}{p!}A^p\frac {t^{k-p}}{(k-p)!}B^{k-p}.
			\end{equation}

			On y reconnait le produit de Cauchy de~:
			\begin{equation}
				\exp(tA)\exp(tB).
			\end{equation}
			\end{proof}

			\begin{cor} Pour $s, t \in I$, et $A \in \Mat_d(\C)$, on a~:
			\begin{equation}
				\exp\left((t+s)A\right) = \exp(tA)\exp(sA).
			\end{equation}
			\end{cor}

		\subsection{Systèmes différentiels à coefficients constants}
			Fixons $A \in \Mat_d(\C)$ et considérons un système différentiel $Y' = AY$. On a vu que $t \mapsto R(t, t_0)$ est solution de~:
			\begin{equation}
				\begin{cases}
				Z'(t) &= A(t)Z(t) \\
				Z(t_0) &= \Id.
			\end{cases}
			\end{equation}

			Or la fonction $t \mapsto \exp\left(t-t_0)A\right)$ est solution du même problème de Cauchy sur $\R$. On en déduit~:
			\begin{equation}
				\forall (t, t_0) \in \R^2 : R(t, t_0) = \exp\left((t-t_0)A\right).
			\end{equation}

			\begin{rmq} Pour les équations différentielles linéaires homogènes à coefficient constant, calculer un système fondamental de solutions (ou la matrice
			résolvante) revient à calculer $t \mapsto \exp(tA)$.
			\end{rmq}

			\begin{cor} $\det(\exp(A)) = \exp(\Tr(A))$
			\end{cor}

			\begin{proof} Prenons $\Delta(t) = \det(R(t, t_0))$. On sait également, par la formule de Liouville~:
			\begin{equation}
				\Delta(t) = \Delta(0)\exp\left(\int_{t_0}^t\Tr(A(s))\dif s\right).
			\end{equation}

			Or on a $\Delta(0) = \det(\Id) = 1$.

			En $t=1$, on a~:
			\begin{equation}
				\det(R(1, 0)) = \det\left(\exp\left((1-0)A\right)\right) = \det\left(\exp(A)\right) = \exp\left(\int_0^1\Tr(A(s))\dif s\right).
			\end{equation}

			Puisque $A$ ne dépend pas de $t$, on a~:
			\begin{equation}
				\det(R(1, 0)) = \exp\left(\Tr(A)\right).
			\end{equation}
			\end{proof}

			\begin{prp} Si $A$ et $J$ sont semblables (c-à-d qu'il existe $P \in \GL_d(\C) \tq A = PJP^{-1}$), alors~:
			\begin{equation}
				\forall s \in \R : \exp(sA) = P\exp(sJ)P^{-1}.
			\end{equation}
			\end{prp}

			\begin{proof} Passer à la limite dans~:
			\begin{equation}
				\sum_{k=0}^n\frac {s^kA^k}{k!} = \sum_{k=0}^n\frac {s^k\left(PJP^{-1}\right)^k}{k!} = P\left(\sum_{k=0}^n\frac {s^kJ^k}{k!}\right)P^{-1},
			\end{equation}
			car $(PJP^{-1})^2 = (PJP^{-1})(PJP^{-1}) = PJ(P^{-1}P)JP^{-1} = PJP^{-1}$. Il reste à appliquer ce résultat par récurrence pour $k$.
			\end{proof}

			\begin{cor} Si l'on sait calculer~:
			\begin{itemize}
				\item une décomposition de Jordan de $A$ (c-à-d $A = PJP^{-1}$),
				\item ou une exponentielle d'une matrice sous forme blocs de Jordan,
			\end{itemize}
			alors on sait résoudre $Y' = AY$ où $A \in \Mat_d(\C)$ est fixé.
			\end{cor}

			\begin{prp} Le flot $\Phi$ de l'équation différentielle linéaire homogène à coefficients constants $Y'(t) =AY(t)$ est donné par~:
			\begin{equation}
				\forall (t, t_0) \in \R^2 : Y(t) = \Phi(t, t_0, y_0) = \exp\left((t-t_0)A\right)Y_0.
			\end{equation}

			$\exp\left((t-t_0)A\right) \in \GL_d(\K)$ et $Y_0 \in \K^d$ et donc $Y(t) \in \K^d$.
			\end{prp}

		\subsection{Exponentielle d'une matrice de Jordan}
			Soit $J$, une réduite de Jordan de $A \in \Mat_d(\C)$. On écrit $A = PJP^{-1}$, avec~:
			\begin{equation}
				J =
			\begin{bmatrix}
				J_1(\lambda_1) & & 0 \\
				 & \ddots & \\
				 0 &  & J_k(\lambda_p)
			\end{bmatrix},\qquad\text{ et }\qquad J_i(\lambda_j) =
			\begin{bmatrix}
				\lambda_j & 1 & & 0 \\
				 & \ddots & \ddots & \\
				 & & \ddots & 1 \\
				 0 & & & \lambda_j
			\end{bmatrix}.
			\end{equation}

			Pour $(t, \ell) \in \R \times \Ns$, on calcule~:
			\begin{equation}
				J^\ell =
			\begin{bmatrix}
				J_1^\ell(\lambda_1) & & 0 \\
				& \ddots & &
				0 & & J_k^\ell(\lambda_p)
			\end{bmatrix},
			\end{equation}
			car $J$ est diagonale par morceaux. On en déduit alors~:
			\begin{equation}
				\exp\left(tJ\right) =
			\begin{bmatrix}
				\exp\left(tJ_1(\lambda_1)\right) & & 0 \\
				& \ddots & \\
				0 & & \exp\left(tJ_k(\lambda_p)\right)
			\end{bmatrix},
			\end{equation}
			et puisque~:
			\begin{equation}
				\exp\left(tA\right) = P\exp\left(tJ\right)P^{-1},
			\end{equation}
			il faut savoir calculer l'exponentielle d'un bloc de Jordan. On calcule~:
			\begin{align}
				\exp\left(tJ_i(\lambda_j)\right) &= \sum_{\ell \geq 0}\frac {t^\ell}{\ell!}J_i^\ell(\lambda_j)
					= \sum_{\ell \geq 0}\frac {t^\ell}{\ell!}\left(\lambda_jI_{n_i} +
					\begin{bmatrix}
						0 & 1 & & 0 \\
						  & \ddots  & \ddots & \\
						  &  & \ddots & 1 \\
						0 & & & 0
					\end{bmatrix}\right)^\ell \\
				&= \sum_{\ell \geq 0}\frac {t^\ell}{\ell!}\left(\sum_{m = 0}^\ell\frac {\ell!}{m!(\ell-m)!}\lambda_j^mI_{n_i}^m
				{\underbrace{\begin{bmatrix}
					0 & 1 &  & 0 \\
					  & \ddots & \ddots & \\
					  & & \ddots & 1 \\
					0 & & 0
				\end{bmatrix}}_{\coloneqq D}}^{\ell-m}\right) \\
				&= \sum_{\ell \geq 0}\frac {\ell!}{\ell!}\sum_{m=0}^\ell\frac {t^mt^{\ell-m}\lambda_j^m}{m!(\ell-m)!}D^{\ell-m} \\
				&= \sum_{\ell \geq 0}\sum_{m=0}^\ell\frac {\left(t\lambda_j\right)^m}{m!(\ell-m)!}
				{\underbrace{\begin{bmatrix}
					0 & t & & 0 \\
					  &  \ddots & \ddots & \\
					  & & \ddots & t \\
					0 & & & 0
				\end{bmatrix}}_{\coloneqq D_t}}^{\ell-m}.
			\end{align}

			On observe~:
			\begin{equation}
				{D_t}^2 =
			\begin{bmatrix}
				0 & t & & 0 \\
				  & \ddots & \ddots & \\
				  & & \ddots & t \\
				0 & & & 0
			\end{bmatrix}^2 =
			\begin{bmatrix}
				0 & 0 & t^2 & & 0 \\
				& \ddots & \ddots & \ddots & \\
				& & \ddots & \ddots & t^2 \\
				& & & \ddots & 0 \\
				0 & & & & 0
			\end{bmatrix}.
			\end{equation}

			De même, on observe~:
			\begin{equation}
				{D_t}^3 =
			\begin{bmatrix}
				0 & t & & 0 \\
				  & \ddots & \ddots & \\
				  & & \ddots & t \\
				0 & & & 0
			\end{bmatrix}^3 =
			\begin{bmatrix}
				0 & 0 & 0 & t^3 & & 0 \\
				& \ddots & \ddots & \ddots & \ddots & \\
				& & \ddots & \ddots & \ddots & t^3 \\
				& & & \ddots & \ddots & 0 \\
				& & & & \ddots & 0 \\
				0 & & & & & 0
			\end{bmatrix}.
			\end{equation}

			On généralise pour $K \in \Ns$~:
			\begin{equation}
				{D_t}^K =
			\begin{bmatrix}
				0 & t & & 0 \\
				  & \ddots & \ddots & \\
				  & & \ddots & t \\
				0 & & & 0
			\end{bmatrix}^K =
			\begin{bmatrix}
				0 & \ldots & 0 & t^K & 0 & \ldots & 0 \\
				& \ddots & \ddots & \ddots & \ddots & \ddots & \vdots \\
				& & \ddots & \ddots & \ddots & \ddots & 0 \\
				& & & \ddots & \ddots & \ddots & t^K \\
				& & & & \ddots & \ddots & 0 \\
				 & & & & & \ddots & \vdots \\
				0 & & & & & & 0
			\end{bmatrix}.
			\end{equation}  % We like alignment in matrices so much...

			On déduit l'expression suivante pour $\exp\left(tJ_i(\lambda_j)\right)$~:
			\begin{align}\label{eq:exponentielle bloc Jordan}
				\exp\left(tJ_i(\lambda_j)\right) = \exp(t\lambda_j) \cdot
				\begin{bmatrix}
					1 & t & \frac {t^2}2 & \ldots & \frac {t^{n_i-1}}{(n_i-1)!} \\
					 & \ddots & \ddots & \ddots & \vdots \\
					 & & \ddots & \ddots & \frac {t^2}2 \\
					 & & & \ddots & t \\
					0 & & & & 1
				\end{bmatrix}
			\end{align}


		%%%%%%%%%% 2nd quadrimestre %%%%%%%%%%


\part{Séries de Fourier}
\chapter{Séries de Fourier}
	\section{Motivation --- Lien entre signaux périodiques}
		Pour $T > 0$, on définit l'ensemble $L^2_T$ des fonctions dont le carré est abs-int sur $[0, T]$ et $T$-périodiques. On définit également $\ell^2(\Z)$,
		l'ensemble des suites complexes $(u_n)_{n \in \Z}$ telles que~:
		\begin{equation}
			\sum_{n \in \Z}\abs {u_n}^2 \lneqq \pinfty.
		\end{equation}

		Les séries de Fourier servent par exemple à résoudre des équations aux dérivées partielles (EDP) telles que l'équation de la chaleur donnée par~:
		\begin{equation}\label{eq:chaleur}
			\partial_tu(t, x) = \partial_x^2u(t, x).
		\end{equation}

		En effet, en cherchant $u(t, x)$ sous la forme~:
		\begin{equation}
			\sum_{n \geq 1}b_n(t)\sin\left(\frac nL\pi x\right),
		\end{equation}
		on trouve, en dérivant formellement~:
		\begin{align}
			\partial_tu(t, x) &= \sum_{n \geq 1}b_n'(t)\sin\left(\frac nL\pi x\right), \\
			\partial_x^2u(t, x) &= -\frac {\pi^2}{L^2}\sum_{n \geq 1}n^2b_n(t)\sin\left(\frac nL\pi x\right).
		\end{align}

		Si on peut identifier les coefficients, on a~:
		\begin{equation}
			b_n'(t) = -\frac {\pi^2}{L^2}n^2b_n(t),
		\end{equation}
		et donc~:
		\begin{equation}
			b_n(t) = b(0)\exp\left(-\frac {(n\pi)^2}{L^2}t\right),
		\end{equation}
		ce qui donne~:
		\begin{equation}
			u(t, x) = \sum_{n \geq 1}b_n(0)\sin\left(n\frac \pi Lx\right)\exp\left(-\frac {(n\pi)^2}{L^2}t\right).
		\end{equation}

		Il ne reste qu'à imposer des conditions initiales pour $b_n(0)$ afin de trouver une solution à~\eqref{eq:chaleur}.

	\section{Séries trigonométriques}
		\subsection{«~Rappels~» d'intégration}
			\begin{lem} Soit $f : \R \to \C$, R-int sur tout segment de $\R$ et $T$ périodique pour $T > 0$. Alors~:
			\begin{equation}
				\forall a \in \R : \int_a^{T+a}f(t)\dif t = \int_0^Tf(t)\dif t.
			\end{equation}
			\end{lem}

			\begin{lem} Soit $n \in \Z$. Alors~:
			\begin{equation}\label{eq:int exp(int) = delta n0}
				\frac 1T\int_0^T\exp\left(i\frac {2\pi}Tnt\right) = \delta_{0n}.
			\end{equation}
			\end{lem}

			\begin{proof} Pour $n = 0$, on a~:
			\begin{equation}
				\frac 1T\int_0^Te^0\dif t = \frac TT = 1.
			\end{equation}

			Pour $n \neq 0$, on a~:
			\begin{equation}
			\frac 1T\int_0^T\exp\left(i\frac {2\pi}Tnt\right)\dif t = \left[\frac {-i}{2\pi n}\exp\left(i\frac {2\pi}Tnt\right)\right]_0^T = 0.
			\end{equation}
			\end{proof}

			\begin{déf} Pour $T > 0$, on définit $C^0_T(\R, \C) \subset C^0(\R, \C)$, l'ensemble des fonctions continues $T$-périodiques de $\R$ dans $\C$.
			\end{déf}

			\begin{prp} $C^0_T(\R, \C)$ est complet.
			\end{prp}

			\begin{proof} Par le théorème des bornes atteintes, on a $C^0_T(\R, \C) \subset B(\R, \C)$. De plus, $C^0_T(\R, \C)$ est fermé, et est donc complet
			car fermé dans un complet.
			\end{proof}

		\subsection{Généralités sur les séries trigonométriques}
			À partir d'ici, nous posons $T = 2\pi$. Tous les résultats vus peuvent être appliqués par changement de période. Si $f : \R \to \C$ est $T$-périodique,
			alors la fonction $\widetilde f$ définie par $\widetilde f : t \mapsto f(t\frac {2\pi}T)$ est $2\pi$-périodique. Il faut alors appliquer les
			résultats sur $\widetilde f$, puis repasser à $f$ par changement de variable.

			\begin{déf} On définit $T_n$ par~:
			\begin{equation}
				T_n \coloneqq \Vect_\C\left(\left(x \mapsto \exp(ikx)\right)_{\abs k \leq n}\right) = \Vect_\C\left(\left(x \mapsto \sin(kx)\right)_{1 \leq k \leq n}, 1, \left(x \mapsto\cos(nx)\right)_{1 \leq k \leq n}\right).
			\end{equation}
			\end{déf}

			\begin{rmq} On observe que $T_n \subset C^0_{2\pi}(\R, \C)$. De plus, ces familles sont libres, ce sont donc des bases de $T_n$.
			\end{rmq}

			\begin{déf} On appelle \textit{série trigonométrique} toute série de fonctions de terme général $u_n : \R \to \C$ telle que~:
			\begin{equation}
				\forall n \in \N : \exists c_n, c_{-n} \in \C \tq \forall x \in \R : u_n(x) = c_{-n}e^{-inx} + c_ne^{inx},
			\end{equation}
			ou de manière équivalente~:
			\begin{equation}
				\forall n \in \N : \exists a_n, b_n \in \C \tq \forall x \in \R : u_n(x) = a_n\cos(nx) + b_n\sin(nx).
			\end{equation}
			\end{déf}

			\begin{rmq} On a alors~:
			\begin{equation}
				\forall n \in \N : \sum_{k=0}^nu_k \in T_n.
			\end{equation}

			Les deux définitions ci-dessus étant équivalentes, il y a une relation bijective entre les $(a_n, b_n)$ et les $(c_n, c_{-n})$~:
			\begin{align}
				c_n &= \frac 12(a_n - b_n) \\
				c_{-n} &= \frac 12(a_n + b_n),
			\end{align}
			qui peut se réécrire en fonction des $c_n$, $n \in \Z$ par~:
			\begin{align}\label{eq:Fourier exp -> trigo}
				a_n &= c_n + c_{-n} \\
				b_n &= i(c_n - c_{-n}).
			\end{align}
			\end{rmq}

			\begin{proof} En réécrivant l'exponentielle complexe à l'aide de $\cos$ et $\sin$, on trouve~:
			\begin{align}
				c_ne^{inx} + c_{-n}e^{-inx} &= c_{-n}\left(\cos(nx) - i\sin(nx)\right) + c_n\left(\cos(nx) + i\sin(nx)\right) \\
					&= \left(c_n + c_{-n}\right)\cos(nx) + i\left(c_n - c_{-n}\right)\sin(nx).
			\end{align}

			Puisque la famille est libre, la représentation est unique, et on a bien l'identité~\eqref{eq:Fourier exp -> trigo}.
			\end{proof}

		\subsection{Cas de la convergence normale dans $C^0_{2\pi}(\R, \C)$}
			\begin{prp} Avec les notations précédentes~:
			\begin{equation}
				\forall x \in \R : \forall n \geq 1 : u_n(x) = a_n\cos(nx) + b_n\sin(nx) = c_{-n}e^{-inx} + c_ne^{inx},
			\end{equation}
			la série de terme général $(c_n)_{n \in \Z}$ converge absolument si et seulement si les séries de terme général $a_n$ et $b_n$ convergent
			absolument.

			Dans ce cas, la série de fonctions de terme général $u_n$ converge normalement dans $C^0_{2\pi}(\R, \C)$, et la fonction somme est dans
			$C^0_{2\pi}(\R, \C)$.
			\end{prp}

			\begin{proof} Avec les formules précédentes, on sait que $b_0$ est arbitraire car $\forall x \in \R : b_0\sin(nx) = 0$.

			Supposons d'abord que la série numérique de terme général $c_n$ converge absolument. Le cas symétrique est laissé en exercice. Par les identités
			ci-dessus, on trouve~:
			\begin{equation}
				\abs {a_n} \leq \abs {c_n} + \abs {c_{-n}},
			\end{equation}
			et donc la série de terme général $\abs {a_n}$ converge, ou encore la série de terme général $a_n$ converge absolument. Idem pour $b_n$ car~:
			\begin{equation}
				\abs {b_n} \leq \abs {c_n} + \abs {c_{-n}}.
			\end{equation}

			Pour démontrer la convergence normale, on observe que~:
			\begin{equation}
				\forall n \geq 1 : \forall x \in \R : \abs {u_n(x)} \leq \abs {a_n} + \abs {b_n},
			\end{equation}
			or $a_n$ et $b_n$ sont des termes généraux de séries numériques convergentes. La série numérique de terme général $\norm {u_n}_\infty$ converge alors, et
			donc la série de terme général $u_n$ converge normalement avec $\sum_{n \in \Z}u_n$ dans $C^0_{2\pi}(\R, \C)$.
			\end{proof}

			\begin{prp} Soit $f \in C^0_{2\pi}$ avec $f = \sum_{n \in \Z}u_n$. Alors~:
			\begin{subnumcases}{}
				\forall n \geq 0 : a_n = \frac 1\pi \int_0^{2\pi}f(t)\cos(nt)\dif t \\
				\forall n \geq 1 : b_n = \frac 1\pi \int_0^{2\pi}f(t)\sin(nt)\dif t \\
				\forall n \in \Z : c_n = \frac 1{2\pi}\int_0^{2\pi}f(t)e^{-int}\dif t\label{eq:Fourier c_n}
			\end{subnumcases}
			\end{prp}

			\begin{proof} Montrons~\eqref{eq:Fourier c_n}, $a_n$ et $b_n$ se déduisent des identités ci-dessus.

			Pour $p \in \Z$ fixé, on sait que~:
			\begin{equation}
				\forall n \geq 1 : \forall x \in \R : \abs {u_n(x)e^{ipx}} = \abs {u_n(x)} \leq \norm {u_n}_\infty.
			\end{equation}

			Donc $x \mapsto \sum_{n \in \Z}u_n(x)e^{ipx}$ est le terme général d'une série convergeant normalement sur $\R$. Ainsi~:
			\begin{align}
				\frac 1{2\pi} \int_0^{2\pi}f(t)e^{-ipt}\dif t &= \frac 1{2\pi}\int_0^{2\pi}\left(c_0 + \sum_{n \geq 1}\left(c_ne^{int} + c_{-n}e^{-int}\right)\right)e^{-ipt}\dif t \\
					&= \underbrace {c_0\frac 1{2\pi}\int_0^{2\pi}e^{-ipt}\dif t}_{\eqqcolon \alpha_p}
						+ \sum_{n \geq 1}\left[\underbrace {\frac {c_n}{2\pi}\int_0^{2\pi}e^{i(n-p)t}\dif t}_{\eqqcolon \beta_{np}}
							+ \underbrace {\frac {c_{-n}}{2\pi}\int_0^{2\pi}e^{i(p-n)t}\dif t}_{\eqqcolon \gamma_{np}}\right]
			\end{align}

			On observe que $\alpha_p = c_0\delta_{p0}$ par~\eqref{eq:int exp(int) = delta n0}. De même, on a $\beta_{np} = c_n\delta_{np}$, et $\gamma_{np}
				= c_{-n}\delta_{(-n)p}$.
			On trouve finalement~:
			\begin{equation}
				\frac 1{2\pi}\int_0^{2\pi}f(t)e^{-ipt}\dif t = \alpha_0\delta_{p0} + \sum_{n \geq 1}\left(c_n\delta_{np} + c_{-n}\delta_{(-n)p}\right)
					= \sum_{n \in \Z}c_n\delta_{np} = c_p.
			\end{equation}
			\end{proof}

	\section{Série de Fourier d'une fonction périodique}
		\subsection{Espaces fonctionnels}
			\begin{déf} On définit l'ensemble $\Czm(\R, \C) \subset C^0_{2\pi}(\R, \C)$ des fonctions $2\pi$-périodiques et continues par morceaux sur $\R$ et
			à valeurs dans $\C$.

			On définit alors l'ensemble $\mathcal D \subset \Czm(\R, \C)$ des fonctions $f$ telles que $f \in \Czm(\R, \C)$ et~:
			\begin{equation}
				\forall x \in \R : f(x) = \frac 12(f(x^+)+f(x^-)),
			\end{equation}
			i.e. si $f \in \mathcal D \iff \forall p \in \R : f$ est discontinue en $p \Rightarrow f(p)$ est la moyenne arithmétique de ses limites à gauche et à droite.
			\end{déf}

			\begin{déf}[Produit scalaire dans $\Czm(\R, \C)$] Soient $f, g \in \Czm(\R, \C)$. On définit~:
			\begin{subnumcases}{}
				\scpr fg \coloneqq \frac 1{2\pi}\int_0^{2\pi}f(t)\overline {g(t)}\dif t, \\
				\norm f_2 \coloneqq \sqrt {\scpr ff}.
			\end{subnumcases}
			\end{déf}

			\begin{déf} Une forme $\scpr \cdot\cdot : \left(\Omega \subset \C^\R\right)^2 \to \C$ est dite \textit{hermitienne} lorsque
			$\forall \lambda_1, \lambda_2, \mu_1, \mu_2 \in \C$ et $\forall f_1, f_2, g_1, g_2 \in \Omega$~:
			\begin{equation}
					\scpr {\lambda_1 f_1 + \lambda_2 f_2}{\mu_1 g_1 + \mu_2 g_2}
						= \lambda_1\overline {\mu_1}\scpr {f_1}{g_1}
						+ \lambda_1\overline {\mu_2}\scpr {f_1}{g_2}
						+ \lambda_2\overline {\mu_1}\scpr {f_2}{g_1}
						+ \lambda_2\overline {\mu_2}\scpr {f_2}{g_2}.
			\end{equation}
			\end{déf}

			\begin{prp}\label{prp:prop scpr Czm}~
				\begin{enumerate}
					\item $\Czm(\R, \C)^2 \to \C : (f, g) \mapsto \scpr fg$ est une forme hermitienne positive (i.e. $\scpr ff \geq 0$)~;
					\item $\mathcal D^2 \to \C : (f, g) \mapsto \scpr fg$ est une forme hermitienne définie positive (i.e. $\scpr ff = 0 \iff f \equiv 0$).
				\end{enumerate}
			\end{prp}

			\begin{déf} Pour $n \in \Z$, on note $e_n : \R \to \C : t \mapsto e^{int}$.
			\end{déf}

			\begin{rmq} La famille $\{e_n\}_{n \in \Z}$ libre est orthonormée dans $\Czm(\R, \C)$, i.e. $\forall m, n \in \Z : \scpr {e_n}{e_m} = \delta_{mn}$.
			\end{rmq}

			\begin{proof}[Preuve de la Proposition~\ref{prp:prop scpr Czm}] Si $f, g \in \Czm(\R, \C)$, alors $t \mapsto f(t)\overline {g(t)} \in \Czm(\R, \C)$,
			et est donc R-int sur $[0, 2\pi]$. La quantité $\scpr fg$ est alors bien définie. On montre alors que c'est une forme hermitienne par calcul (EXERCICE).

			De plus, pour $f \in \Czm(\R, \C)$, on a $\forall x \in \R : \abs {f(x)}^2 \geq 0$. Donc la quantité $\scpr ff \geq 0$.

			Montrons maintenant le point 2 de la proposition, i.e. montrons que $\scpr \cdot\cdot$ est défini positif sur $\mathcal D$. Soit $f \in \mathcal D \tq$~:
			\begin{equation}
				\frac 1{2\pi}\int_0^{2\pi}\abs {f(x)}^2\dif x = 0.
			\end{equation}
			Soit $t \in [0, 2\pi]$. Distinguons deux cas~:
			\begin{itemize}
				\item soit $t$ est un point de continuité de $f$, et $f(t) = 0$ (en effet, sinon il existe $\eta > 0 \tq f > \frac 12f(t)$ sur $[t \pm \eta]$)~;
				\item soit $t$ est un point de discontinuité de $f$, et donc $t$ est isolé. Il existe alors $\delta > 0$ tel que $[t \pm \delta]$ ne contient que des
				points de continuité de $f$. $f$ admet donc une limite à gauche et à droite de $t$ tel que $f(t^+) = f(t^-) = 0$. On a donc bien $f(t) = 0$.
			\end{itemize}
			\end{proof}

			\begin{rmq} Pour $\xi \in (0, 2\pi)$, on a $f : \R \to \C : t \mapsto I_{[x \in \xi + 2\pi\Z]} \in \Czm(\R, \C)$ et $\scpr ff = 0$. Or, $f \not \equiv 0$.
			En effet, dans $\Czm(\R, \C)$, on a $\scpr ff = 0 \Rightarrow \abs {\{x \in \R \tq f(x) \neq 0\}}$ est fini~; et pas $\scpr ff = 0 \Rightarrow f \equiv 0$.
			\end{rmq}

		\subsection{Coefficients de Fourier d'une fonction périodique}
			\begin{déf} Soit $f \in \Czm(\R, \C)$. On appelle \textit{coefficients de Fourier (exponentiels) de $f$} les nombres $(c_n)_{n \in \Z}$ définis par~:
			\begin{equation}
				c_n(f) = \frac 1{2\pi}\int_0^{2\pi}f(t)\overline {e_n(t)}\dif t = \scpr f{e_n}.
			\end{equation}

			On appelle \textit{coefficients de Fourier (trigonométriques) de $f$} les nombres $(a_n)_{n \geq 0}$ et $(b_n)_{n \geq 0}$ définis par~:
			\begin{subnumcases}{}
				a_n(f) = \frac 1\pi\int_0^{2\pi}f(t)\cos(nt)\dif t, \\
				b_n(f) = \frac 1\pi\int_0^{2\pi}f(t)\sin(nt)\dif t.
			\end{subnumcases}
			\end{déf}

			\begin{prp} Soit $f \in \Czm(\R, \C)$.
			\begin{itemize}
				\item si $f(\R) \subset \R$, alors $\forall n \geq 1 : (a_n, b_n) \in \R^2$ et $a_0 \in \R$~;
				\item si $f$ est paire, alors $\forall n \geq 1 : b_n \equiv 0$~;
				\item si $f$ est impaire, alors $\forall n \geq 1 : a_n \equiv 0$.
			\end{itemize}
			\end{prp}

			\begin{proof} EXERCICE.
			\end{proof}

			\begin{déf} Soit $f \in \Czm(\R, \C)$. On appelle \textit{série de Fourier associée à $f$} la série de terme général~:
			\begin{equation}
				u_n(f) = c_n(f)e_n + c_{-n}(f)e_{-n} = a_n(f)\cos(n\cdot) + b_n(f)\sin(n\cdot).
			\end{equation}

			On note les sommes partielles par $S_n(f) \coloneqq \sum_{k=-n}^nc_k(f)e_k$.
			\end{déf}

		\subsection{Inégalité de Bessel et lemme de Riemann-Lebesgue}
			\begin{prp} Soit $f \in \Czm(\R, \C)$. Pour $n \geq 1$, on a~:
			\begin{equation}
				\norm {S_n(f)}_2^2 = \sum_{\abs k \leq n} \abs {c_k(f)}^2 = \frac {\abs {a_0}^2}4 + \frac 12\sum_{k=1}^n\left(\abs {a_k(f)}^2 + \abs {b_k(f)}^2\right).
			\end{equation}
			\end{prp}

			\begin{proof} Les fonctions $(e_k)_{\abs k \leq n}$ forment une base orthonormale de $T_n$. Pour $n \geq 1$, on a~:
			\begin{equation}
				S_n(f) = \sum_{\abs k \leq n}c_k(f)e_k.
			\end{equation}

			Donc $(c_k(f))_{\abs k \leq n}$ sont les coordonnées de $S_n(f)$ dans une base orthonormée.
			\end{proof}

			\begin{rmq} Vérifions quand même~:
			\begin{equation}
				\norm {S_n(f)}_2^2 = \scpr {S_n(f)}{S_n(f)} = \sum_{\abs k \leq n}\sum_{\abs p \leq n}c_k(f)c_p(f)\scpr {e_k}{e_p}
					= \sum_{\abs k \leq n}\abs {c_k(f)}^2.
			\end{equation}
			\end{rmq}

			\begin{prp} Soit $f \in \Czm(\R, \C)$ et $n \in \N$. Il existe une unique fonction $p_n \in T_n$ telle que~:
			\begin{equation}
				\norm {f-p_n}_2^2 = \inf_{p \in T_n}\norm {f-p}_2^2.
			\end{equation}

			Cette fonction est de plus caractérisée par~:
			\begin{subnumcases}{}
				p_n \in T_n \\
				\forall p \in T_n : f-p_n \perp p \text{ ou } \forall p \in T_n : \scpr {f-p_n}p = 0.
			\end{subnumcases}
			\end{prp}

			\begin{rmq} La norme $\norm \cdot_2$ est appelée \textit{norme en moyenne quadratique}. Donc le théorème donne l'existence et l'unicité d'un
			polynôme trigonométrique de degré $\leq n$ approchant $f$ au mieux en moyenne quadratique.
			\end{rmq}

			\begin{rmq} Si $f \in \mathcal D$, alors $p_n$ est le projeté orthogonal défini sur $T_n$.
			\end{rmq}

			\begin{prp} Soient $f \in \Czm(\R, \C)$ et $n \in \N$. On a ~:
			\begin{equation}
				p_n = S_n(f).
			\end{equation}

			i.e. $S_n(f)$ est la meilleure approximation en moyenne quadratique de $f$ par un polynôme trigonométrique de degré $\leq n$.
			\end{prp}

			\begin{proof} Montrons que i) $S_n(f) \in T_n$ (par construction) et ii) $\forall p \in T_n : f-S_n(f) \perp p$. Soit $k \in \Z$ tel que
			$\abs k \leq n$. On a~:
			\begin{align}
				\scpr {f-S_n(f)}{e_k} &= \frac 1{2\pi}\int_0^{2\pi}\left(f(t) - \sum_{\abs p \leq n}c_p(f)e^{ipt}\right)e^{-ikt}\dif t
						= \frac 1{2\pi}\int_0^{2\pi}f(t)e^{-ikt}\dif t - \frac 1{2\pi}\sum_{\abs p \leq n}\int_0^{2\pi}e^{i(p-k)t}\dif t \\
					&= c_k(f) - \sum_{\abs p \leq n}c_p(f)\frac 1{2\pi}\int_0^{2\pi}e^{i(p-k)t}\dif t = c_k(f) - c_k(f) = 0.
			\end{align}

			Par anti-linéarité, on a donc~:
			\begin{equation}
				\forall p \in T_n : \scpr {f-S_n(f)}p = 0,
			\end{equation}
			et donc $p_n = S_n(f)$ satisfait la caractérisation précédente.
			\end{proof}

			\begin{prp}[Inégalité de Bessel] Soit $f \in \Czm(\R, \C)$. Les séries numériques de termes généraux $\abs {c_k(f)}^2 + \abs {c_{-k}(f)}^2$
			et $\abs {a_k(f)}^2 + \abs {b_k(f)}^2$ sont convergentes et on a~:
			\begin{equation}
				\forall n \geq 1 : \sum_{\abs k \leq n}\abs {c_k}^2 = \frac {\abs {a_0(f)}^2}4 + \frac 12\sum_{k=1}^n\left(\abs {a_k(f)}^2 + \abs {b_k(f)}^2\right) \leq \norm f_2^2.
			\end{equation}
			\end{prp}

			\begin{rmq}~
				\begin{itemize}
					\item En passant à la limite $n \to \pinfty$, on a~:
					\begin{equation}
						\sum_{k \in \Z}\abs {c_k(f)}^2 \leq \norm f_2^2.
					\end{equation}
					\item L'identité de Parseval transforme l'inégalité de Bessel en égalité~:
					\begin{equation}
						\norm f_2^2 = \sum_{k \in \Z}\abs {c_k(f)}^2.
					\end{equation}
				\end{itemize}
			\end{rmq}

			\begin{proof} Soit $n \geq 1$. Observons que~:
			\begin{equation}
				\sum_{\abs k \leq n}\abs {c_k(f)}^2 = \norm {S_n(f)}^2.
			\end{equation}

			De plus~:
			\begin{equation}
				f = \underbrace {f - S_n(f)}_{\in T_n^\perp} + \underbrace {S_n(f)}_{\in T_n}.
			\end{equation}
			Donc $\norm f_2^2 = \norm {f-S_n(f)}_2^2 + \norm {S_n(f)}^2$, avec $\norm {f - S_n(f)}_2^2 \geq 0$, et donc $\norm f_2^2 \geq \norm {S_n(f)}_2^2$.
			\end{proof}

			\begin{cor}[Lemme de Riemann-Lebesgue] Soit $f \in \Czm(\R, \C)$. On a~:
			\begin{itemize}
				\item $c_n(f) \mconv {\abs n}\pinfty{} 0$~;
				\item $a_n(f) \mconv n\pinfty{} 0$~;
				\item $b_n(f) \mconv n\pinfty{} 0$.
			\end{itemize}
			\end{cor}

			\begin{proof} Ces suites en carré du module sont les termes d'une série numérique qui convergent, donc tendent vers $0$.
			\end{proof}

		\subsection{Les théorèmes de Dirichlet}
			\begin{déf} Soit $N \in \N$. On appelle \textit{noyau de Dirichlet d'ordre $N$} la fonction~:
			\begin{equation}
				T_n \ni D_N : \R \to \C : t \mapsto \sum_{\abs k \leq N}e^{ikt}.
			\end{equation}
			\end{déf}

			\begin{prp}\label{prp:noyau de Dirichlet sin} Soient $N \in \N$ et $t \in \R \setminus 2\pi\Z$. On a~:
			\begin{equation}
				D_N(t) = \frac {\sin\left((2N+1)\frac t2\right)}{\sin\left(\frac t2\right)}.
			\end{equation}
			\end{prp}

			\begin{proof} Observons que, pour $N \in \N$ et $t \in \R \setminus 2\pi\Z$, on a $e^{it} \neq 1$. $D_N(t)$ est donc la somme d'une suite géométrique
			de raison $\neq 1$. On applique alors la formule~:
			\begin{equation}
				D_N(t) = e^{-iNt}\frac {1 - e^{i(2N+1)t}}{1 - e^{it}} =
					\underbrace {e^{-iNt}\frac {e^{i\frac {2N+1}2t}}{e^{\frac {it}2}}}_{=1}
					\underbrace {\frac {e^{-i(2N+1)\frac t2} - e^{i(2N+1)\frac t2}}{e^{-i\frac t2} - e^{i\frac t2}}}_{\text{ à transformer en $\cos$ et $\sin$}} = \frac {\sin\left((2N+1)\frac t2\right)}{\sin\left(\frac t2\right)}.
			\end{equation}
			\end{proof}

			\begin{thm}[Théorème de Dirichlet local]\label{thm:Dirichlet local} Soit $f \in C^{1,m}_{2\pi}(\R, \C)$ et $x \in \R$. La série de Fourier de $f$ en $x$
			converge vers $\frac 12(f(x^+) + f(x^-))$. i.e.~:
			\begin{equation}
				\frac {f(x^+) + f(x^-)}2 = \sum_{k \in \Z}c_k(f)e^{ikx}.
			\end{equation}
			\end{thm}

			\begin{proof} Écrivons pour $(x, N) \in \R \times \N$ et $f \in C^{1,m}_{2\pi}(\R, \C)$.
			\begin{align}
				S_n(f)(x) &= \sum_{k=-N}^Nc_k(f)e^{ikx} = \sum_{k=-N}^N\frac 1{2\pi}\int_0^{2\pi}f(t)e^{-ikt}\dif te^{ikx} = \frac 1{2\pi}\int_0^{2\pi}f(t)\sum_{k=-N}^Ne^{ik(x-t)}\dif t \\
				&= \frac 1{2\pi}\int_0^{2\pi}f(t)D_N(x-t)\dif t = \frac 1{2\pi}\int_{-\pi}^{\pi}f(t)D_N(x-t)\dif t.
			\end{align}
			En posant $t \coloneqq x+u$, on trouve~:
			\begin{align}
				S_n(f)(x) &= \frac 1{2\pi}\int_{-\pi-x}^{\pi-x}f(x+u)D_N(u)\dif u  \\
					&= \frac 1{2\pi}\int_{-\pi}^{\pi}f(x+u)D_N(u)\dif u \label{eq:S_n fD_N}\\
					&= \frac 1{2\pi}\int_{-\pi}^0 f(x+u)D_N(u)\dif u + \frac 1{2\pi}\int_0^\pi f(x+u)D_N(u)\dif u.
			\end{align}

			On remarque qu'en appliquant\eqref{eq:S_n fD_N} à ${\bf 1} : \R \to \C : t \mapsto 1$, on trouve~:
			\begin{equation}
				1 = S_n({\bf 1})(x) = \frac 1{2\pi}\int_{-\pi}^\pi D_N(u)\dif u.
			\end{equation}

			Ainsi~:
			\begin{equation}
				\frac 12(f(x^+) + f(x^-)) = \frac 1{2\pi}\int_{-\pi}^\pi\frac 12(f(x^+) + f(x^-))D_N(u)\dif u,
			\end{equation}
			d'où, par différence~:
			\begin{align}
				&S_n(f)(x) - \frac 12(f(x^+) + f(x^-)) = \frac 1{2\pi}\int_{-\pi}^\pi\left(f(x+u) - \frac 12(f(x^+)+f(x^-))\right)D_N(u)\dif u \\
				&= \frac 1{2\pi}\int_{-\pi}^0f(x+u)D_N(u)\dif u - \frac 1{4\pi}\int_{-\pi}^\pi f(x^-)D_N(u)\dif u + \frac 1{2\pi}\int_0^\pi f(x+u)D_N(u)\dif u - \frac 1{4\pi}\int_{-\pi}^\pi f(x^+)D_N(u)\dif u \\
				&= \frac 1{2\pi}\int_{-\pi}^0f(x+u)D_N(u)\dif u - \frac 1{2\pi}\int_{-\pi}^0 f(x^-)D_N(u)\dif u + \frac 1{2\pi}\int_0^\pi f(x+u)D_N(u)\dif u - \frac 1{2\pi}\int_0^\pi f(x^+)D_N(u)\dif u
			\end{align}
			par parité de $D_N$. On trouve alors~:
			\begin{equation}
				S_n(f) - \frac 12(f(x^+)+f(x^-)) = \frac 1{2\pi}\int_{-\pi}^0\left(f(x+u) - f(x^-)\right)D_N(u)\dif u + \int_0^\pi\left(f(x+u) - f(x^+)\right)D_N(u)\dif u.
			\end{equation}
			En posant $v \coloneqq \frac u2$, on trouve~:
			\begin{equation}
				S_n(f) - \frac 12(f(x^+) - f(x^-)) = \frac 1\pi\int_{-\frac \pi2}^0\left(f(x+2v)-f(x^-)\right)D_N(2v)\dif v + \frac 1\pi\int_0^{\frac \pi2}\left(f(x+2v)-f(x^+)\right)D_N(2v)\dif v.
			\end{equation}

			Considérons les fonctions $2\pi$-périodiques dont la restriction à $[\pm \pi]$ est définie par~:
			\begin{subnumcases}
				{g : [\pm\pi] \to \C : t \mapsto}
					\frac {f(x+2t)-f(x^-)}{\sin t} & si $\frac {-\pi}2 < t < 0$ \\
					0 & sinon
			\end{subnumcases}
			et~:
			\begin{subnumcases}
				{h : [\pm\pi] \to \C : t \mapsto}
					\frac {f(x+2t)-f(x^+)}{\sin t} & si $0 < t < \frac \pi2$ \\
					0 & sinon
			\end{subnumcases}

			Pour $t \in [-\pi, 0]$, on a~:
			\begin{equation}
				f(x+2t) - f(x^-) = 2tf'(x^-) + o(t),
			\end{equation}
			et donc~:
			\begin{equation}
				\frac {f(x+2t) - f(x^-)}{\sin t} \underset {t \to 0}= 2f'(x^-) + o(1).
			\end{equation}

			Donc $g$ admet une limite finie en $0^-$ (et en $0^+$ par définition), donc en $0$. Par un argument similaire, on trouve~:
			\begin{equation}
				\frac {f(x+2t)-f(x^+)}{\sin t} \underset {t \to 0}= 2f'(x^+) + o(1).
			\end{equation}
			On en déduit donc que $h$ admet également une limite en $0$. $g$ et $h$ sont donc dans $\Czm(\R, \C)$. On trouve alors finalement, par la
			Proposition~\ref{prp:noyau de Dirichlet sin}~:
			\begin{align}
				S_n(f)(x) - \frac 12(f(x^+)+f(x^-)) &= \frac 1\pi\int_{-\frac \pi2}^0g(v)\sin((2N+1)v)\dif v + \frac 1\pi\int_0^{\frac \pi2}h(v)\sin((2N+1)v)\dif v \\.
				&\frac 1\pi\int_{-\pi}^\pi g(v)\sin((2N+1)v)\dif v + \frac 1\pi\int_{-\pi}^\pi h(v)\sin((2N+1)v)\dif v \\
				&= 2b_{2N+1}(g) + 2b_{2N+1}(h).
			\end{align}

			Puisque $g, h \in \Czm(\R, \C)$, en appliquant le lemme de Riemann-Lebesgue, on a~:
			\begin{equation}
				S_n(f)(x) \xrightarrow[n \to \pinfty]{} \frac {f(x^+)+f(x^-)}2.
			\end{equation}
			\end{proof}

			\begin{rmq} Puisque $\Czm(\R, \C) \ni f \xrightarrow{\text{Bessel}} c_n(f) \in \ell^2(\Z)$ et $c_n(f) \xrightarrow {\text{Dirichlet}} f$, la régularité
			de $f$ se traduit en décroissance de $\abs {c_n(f)}$ et vice-versa.  % et je dis, et tu dis... Que le bonheur est irréductible !
			\end{rmq}

			\begin{lem}\label{lem:c_n(f')=inc_n(f)} Soit $f \in C^0_{2\pi}(\R, \C) \cap \Com(\R, \C)$. Pour tout $n \in \Z$, on a $c_n(f') = inc_n(f)$, où~:
			\begin{subnumcases}
				{f' : \R \to \C : x \mapsto}
				f'(x) & si $f$ est dérivable en $x$ \\
				\frac {f'(x^+)+f'(x^-)}2 & sinon.
			\end{subnumcases}
			\end{lem}

			\begin{proof} Soit $0 = a_0 < a_1 < \ldots < a_p = 2\pi$, une subdivision de $[0, 2\pi]$, i.e. $f$ se prolonge en une fonction de classe $C^1$ sur les
			segments $[a_k, a_{k+1}]$ pour $k \in \intint 0{p-1}$.

			Fixons alors $k \in \intint 0{p-1}$, et $[a, b] \subset (a_k, a_{k+1})$. Par dérivabilité continue de $f$ sur $[a, b]$, on a pour $n \in \Z$~:
			\begin{equation}
				\int_a^b f(x)e^{-inx}\dif x = \left[f(x)\frac {e^{-inx}}{-in}\right]_a^b - \int_a^b f'(x)e^{-inx}\dif x.
			\end{equation}

			Faisons alors tendre $a \to a_k$ et $b \to a_{k+1}$. Il vient~:
			\begin{equation}
				\int_{a_k}^{a_{k+1}} f(x)e^{-inx}\dif x = \left[\frac in f(x)e^{-inx}\right]_{a_k}^{a_{k+1}} - \frac in\int_{a_k}^{a_{k+1}}f'(x)e^{-inx}\dif x.
			\end{equation}

			Sommons sur $k \in \intint 0{p-1}$, on obtient~:
			\begin{equation}
				\int_{a_0=0}^{a_p=2\pi}f(x)e^{-inx}\dif x = \frac in\left(f(2\pi)e^{-in2\pi} - f(0)e^{-in0}\right) - \frac in \int_0^{2\pi}f'(x)e^{-inx}\dif x.
			\end{equation}

			Dès lors, on a~:
			\begin{equation}
				c_n(f) = 0 - \frac inc_n(f') \iff c_n(f') = inc_n(f).
			\end{equation}

			Pour $n = 0$, on sait $f$ de classe $C^1$ sur $[a_k, a_{k+1}]$, donc~:
			\begin{equation}
				\int_{a_k}^{a_{k+1}}f'(x)e^{-inx}\dif x = \int_{a_k}^{a_{k+1}}f'(x)\dif x = f(a_{k+1}) - f(a_k).
			\end{equation}

			À nouveau, en sommant sur $k \in \intint 0{p-1}$, on trouve~:
			\begin{equation}
				c_0(f') = \frac 1{2\pi}\int_0^{2\pi}f'(x)\dif x = \frac 1{2\pi} \cdot 0 = 0 = i \cdot 0 \cdot c_n(f).
			\end{equation}
			\end{proof}

			\begin{rmq} Toute autre valeur en les points non dérivables ne changerait pas $c_n(f')$ car c'est une valeur intégrale. Cependant, ici cela nous permet de
			dire que $f' \in \mathcal D$.

			Notons également que ce lemme peut se retrouver \textit{naïvement} en appliquant le théorème de Dirichlet qui dit que~:
			\begin{equation}
				f(x) = \sum_{n \in \Z}c_n(f)e^{inx},
			\end{equation}
			et en dérivant formellement puis en identifiant les termes en $e^{ikx}$. Cependant, cela requiert $f \in C^1_{2\pi}(\R, \C)$, ce qui est une hypothèse
			plus restrictive que celle du lemme.
			\end{rmq}

			\begin{thm}[Théorème de Young] Soient $p, q > 1 \tq \frac 1p + \frac 1q = 1$. Alors~:
			\begin{equation}
				\forall u, v > 0 : uv \leq \frac {u^p}p + \frac {v^q}q.
			\end{equation}
			\end{thm}

			\grantedproof

			\begin{thm}[Dirichlet \textit{global}] Si $f \in C^0(\R, \C) \cap \Com(\R, \C)$, alors~:
			\begin{enumerate}
				\item la série de terme général $\abs {c_n(f)} + \abs {c_{-n}(f)}$ converge~;
				\item la série de Fourier de $f$ converge normalement sur $\R$~;
				\item la série de Fourier de $f$ converge uniformément vers $f$ sur $\R$.
			\end{enumerate}
			\end{thm}

			\begin{proof} Montrons le premier point. Soit $n \in \Z_0$. Observons~:
			\begin{equation}
				c_n(f) = \frac {-i}nc_n(f').
			\end{equation}

			On en déduit, par Young pour $p=q=2$~:
			\begin{equation}
				\abs {c_n(f)} \leq \frac 1n\abs {c_n(f')} \leq \frac 12\frac 1{n^2} + \frac 12\abs {c_n(f')}^2.
			\end{equation}

			La série de terme général $\frac 1{n^2}$ converge, et l'inégalité de Bessel assure que la série de terme général $\abs {c_n(f')}$ converge car $f'$ est
			de classe $\Czm$ (et pour $f'$ défini au Lemme~\ref{lem:c_n(f')=inc_n(f)}). Ainsi, la série de terme général $\abs {c_n(f)} + \abs {c_{-n}(f)}$ converge.

			Pour le second point, observons que pour $n \in \Z$ et $x \in \R$, on a~:
			\begin{equation}
				\abs {c_n(f)e^{inx} + c_{-n}(f)e^{-inx}} \leq \abs {c_n(f)} + \abs {c_{-n}(f)}.
			\end{equation}
			Par le premier point, on en déduit que $S_n(f)$ converge normalement sur $\R$.

			Pour le dernier point, en appliquant Dirichlet local (Théorème~\ref{thm:Dirichlet local}), on trouve que $S_n(f)$ converge simplement en $x \in \R$
			vers $\frac 12(f(x^+)+f(x^-))$. De plus, cette série converge normalement sur $\R$ qui est complet, la convergence est donc uniforme. Par continuité de $f$,
			on sait que $\frac 12(f(x^+)+f(x^-)) = f(x)$. Donc~:
			\begin{equation}
				S_n \CVU \R n\pinfty f.
			\end{equation}
			\end{proof}

			\begin{prp} Soient $k \in \N$ et $f \in C^k_{2\pi}(\R, \C)$. Alors~:
			\begin{equation}
				c_n(f) \underset {\abs n \to \pinfty}= o\left(\frac 1{n^k}\right).
			\end{equation}
			\end{prp}

			\begin{proof} On sait par récurrence que $c_n(f^{(k)}) = (in)^kc_n(f)$. Si $n \neq 0$, on a~:
			\begin{equation}
				\abs {(in)^k}\abs {c_n(f)} = \abs {c_n(f^{(k)})} \xrightarrow[\abs n \to \pinfty]{} 0,
			\end{equation}
			et donc $c_n(f) = o\left(\frac 1{n^k}\right)$.
			\end{proof}

		\subsection{Théorème de Parseval}  % C'est pas faux...
			\begin{lem} Soient $f \in \Czm(\R, \C)$ et $\varepsilon > 0$ fixé. Il existe $g \in C^0_{2\pi}(\R, \C) \cap C^{1,m}(\R, \C)$ telle que~:
			\begin{equation}
				\norm {f-g}_2 < \varepsilon.
			\end{equation}
			\end{lem}

			\begin{proof} Supposons d'abord que $f$ est élémentaire (\textit{en escaliers}). Notons $\lambda_j$ la valeur de $f$ sur $[a_j, a_{j+1}]$ pour
			$0 = a_0 < a_1 < \ldots < a_p = 2\pi$ ($p \geq 1$) une subdivision adaptée à $f$. Posons~:
			\begin{equation}
				\eta \coloneqq \min_{0 \leq k < p}\abs {a_{k+1}-a_k}.
			\end{equation}

			On sait que $\eta > 0$. Choisissons $n \in \Ns$ tel que $\frac 1n < \frac \eta2$.

			Posons $g_n$ la fonction continue et affine par morceaux définie par~:
			\begin{itemize}
				\item $\forall i \in \intint 0p : g_n(a_i) = 0$~;
				\item $\forall i \in \intint 0{p-1} : g_n\left([a_i + \frac 1n, a_{i+1} - \frac 1n]\right) \equiv \lambda_i$~;
				\item $\forall i \in \intint 0{p-1} : g_n$ est affine sur $(a_i, a_i+\frac 1n)$ et sur $(a_{i+1}-\frac 1n, a_{i+1})$.
			\end{itemize}

			Observons que~:
			\begin{align}
				\norm {f-g}_2 &= \frac 1{2\pi}\int_0^{2\pi}\abs {f(x)-g(x)}^2\dif x = \frac 1{2\pi}\sum_{k=0}^{p-1}\int_{a_k}^{a_{k+1}}\abs {f(x)-g(x)}^2\dif x \\
				&= \frac 1{2\pi}\sum_{k=0}^{p-1}\int_{a_k}^{a_k+\frac 1n}\abs {f-g}^2\dif x + \int_{a_{k+1}-\frac 1n}^{a_{k+1}}\abs {f-g}^2\dif x \\
				&\leq \frac 1{2\pi}\sum_{k=0}^{p-1}\int_{a_k}^{a_k+\frac 1n}\abs {\lambda_k}^2\dif x + \int_{a_{k+1}-\frac 1n}^{a_{k+1}}\abs {\lambda_k}^2\dif x \\
				&\leq \frac 1{2\pi}\sum_{k=0}^{p-1}\abs {\lambda_k}^2\frac 2n \leq \frac 1{n\pi}\sum_{k=0}^{p-1}\abs {\lambda_k}^2 \xrightarrow[n \to \pinfty]{} 0.
			\end{align}

			Donc si $\int \abs f^2 \neq 0$, et si $\varepsilon > 0$ est fixé, on peut choisir $n \in \Ns$ tel que~:
			\begin{equation}
				n > \max\left(\frac 2\eta, \frac {\pi\varepsilon^2}{\sum_{k=0}^{p-1}\abs {\lambda_k}^2}\right),
			\end{equation}
			pour avoir $\norm {f-g}_2^2 < \varepsilon^2$, et donc $\norm {f-g}_2 < \varepsilon$.

			Si $\int \abs f^2 = 0$, alors $g \equiv 0$ convient.
			Supposons maintenant que $f \in \Czm(\R, \C)$. Soit $\varepsilon > 0$. Il existe $g \in \El \R\C$ telle que $\norm {f-g}_\infty < \frac \varepsilon2$.

			Alors il existe $h \in C^0_{2\pi}(\R, \C) \cap C^{1,m}_{2\pi}(\R, \C)$ telle que $\norm {h-g}_2 < \frac \varepsilon2$. On trouve donc~:
			\begin{equation}
				\norm {f-h}_2 \leq \norm {f-g}_2 + \norm {g-h}_2 \leq \norm {f-g}_\infty + \norm {g-h}_2 < 2\frac \varepsilon2 = \varepsilon2.
			\end{equation}
			\end{proof}

			\begin{rmq} pour $\norm \cdot_2 \leq \norm \cdot_\infty$, il faut remarquer que~:
			\begin{equation}
				\norm {f-g}_2^2 = \frac 1{2\pi}\int_0^{2\pi}\abs {f(x)-g(x)}^2\dif x \leq \frac 1{2\pi}\int_0^{2\pi}\norm {f-g}_\infty^2\dif x = \norm {f-g}_\infty.
			\end{equation}
			\end{rmq}

			\begin{thm}[Théorème de Parseval]\label{thm:Parseval} Soit $f \in \Czm(\R, \C)$. On a~:
			\begin{align}
				\frac 1{2\pi}\int_0^{2\pi}\abs f^2 &= \sum_{n \in \Z}\abs {c_n(f)}^2\label{eq:Parseval exp} \\
				&= \frac {\abs {a_0(f)}^2}4 + \frac 12\sum_{n=1}^\pinfty\abs {a_n(f)}^2 + \abs {b_n(f)}^2.
			\end{align}
			\end{thm}

			\begin{proof} Nous ne montrons ici que~\eqref{eq:Parseval exp}, la seconde égalité ne se déduit que des identités entre coefficients trigonométriques
			et exponentiels.

			On sait pour $n \geq 1$~:
			\begin{equation}
				f = \underbrace {f-S_n(f)}_{\in T_n^\perp} + \underbrace {S_n(f)}_{\in T_n}.
			\end{equation}

			Donc par Pythagore, on sait $\norm f_2^2 = \norm {f-S_n(f)}_2^2 + \norm {S_n(f)}_2^2$. En particulier, par Bessel, on sait~:
			\begin{equation}
				\norm {S_n(f)}_2^2 = \sum_{\abs k \leq n}\abs {c_k(f)}^2 + \abs {c_{-k}(f)}^2 \leq \norm f_2^2.
			\end{equation}

			Remarquons donc que si $f \in C^1_{2\pi}(\R, \C)$, alors $S_n(f) \CVU \R n\pinfty f$ par Dirichlet global. Cela implique~:
			\begin{equation}
				S_n(f) \xrightarrow[n \to \pinfty]{\norm \cdot_2 \text{ sur } \R} f,
			\end{equation}
			d'où la convergence~:
			\begin{equation}
				\sum_{\abs k \leq n}\abs {c_k(f)}^2 \xrightarrow[n \to \pinfty]{} \norm f_2^2.
			\end{equation}

			Si $f \in \Czm(\R, \C)$ et $\varepsilon > 0$ est fixé, alors il existe $g \in C^0_{2\pi}(\R, \C) \cap C^{1,m}(\R, \C)$ telle que $\norm {f-g}_2 < \varepsilon$
			par le lemme précédent. Ainsi, puisque $S_n(f)$ est la meilleure approximation de $f$ en moyenne quadratique dans $T_n$~:
			\begin{equation}
				\norm {f-S_n(f)}_2 \leq \norm {f-S_n(g)}_2 \leq \norm {f-g}_2 + \norm {g-S_n(g)}_2.
			\end{equation}
			par la remarque ci-dessus, on sait que $\norm {g-S_n(g)}_2 \xrightarrow[n \to \pinfty]{} 0$. Donc à partir d'un certain rang $N$, on a
			$\norm {f-S_n(f)}_2 \leq 2\varepsilon$. On a donc bien~:
			\begin{equation}
				\norm {S_n(f)}_2 \xrightarrow[n \to \pinfty]{} \norm f_2.
			\end{equation}
			\end{proof}

			\begin{cor} Soient $f, g \in \mathcal D$ telles que $\forall n \in \Z : c_n(f) = c_n(g)$. Alors $f=g$.
			\end{cor}

			\begin{proof} On sait que $f-g \in \mathcal D \subset \Czm(\R, \C)$. On a donc~:
			\begin{equation}
				\norm {f-g}_2^2 = \sum_{n \in \Z}\abs {c_n(f-g)}^2 = \sum_{n \in \Z}\abs {c_n(f) - c_n(g)}^2 = 0.
			\end{equation}

			Puisque $\norm {f-g}_2 = 0$ et que $\norm \cdot_2$ est une norme sur $\mathcal D$ (en particulier, elle satisfait la séparation des points), on sait que
			$f = g$.
			\end{proof}

\part{Fonctions d'une variable complexe}
\chapter{Fonctions d'une variable complexe}
	\section{Isomorphisme entre $\C$  et $\R^2$ --- Différentiabilité et $\C-$dérivabilité}
		\subsection{Ouverts de $\C$ et de $\R^2$}
			Notons $I : \R^2 \to \C : (x, y) \mapsto x + iy$.

			\begin{prp} $I$ est un isomorphisme $\R$-linéaire et une isométrie. En particulier, $I$ induit une bijection entre les ouverts de $\C$ et ceux de $\R^2$.
			\end{prp}

			\begin{proof} $I$ est une isométrie car~:
			\begin{equation}
				\forall x, y \in \R : \norm {(x, y)} = \sqrt {x^2 + y^2} = \abs {x+iy}
			\end{equation}

			$I$ est trivialement continue, idem pour $I^{-1}$. Si $\Omega_{\R^2}$ est un ouvert de $\R^2$, posons~:
			\begin{equation}
				\Omega_\C \coloneqq I(\Omega_{\R^2}).
			\end{equation}

			Par continuité de $I^{-1}$, $\Omega_\C$ est un ouvert de $\C$. Réciproquement, par continuité de $I$, pour tout $\Omega_\C$ ouvert de $\C$,
			$\Omega_{\R^2} \coloneqq I^{-1}(\Omega_\C)$ est un ouvert de $\R^2$.
			\end{proof}

			\begin{prp} Soit $\Omega_\C \subset \C$, un ouvert de $\C$, et $f : \Omega_\C \to \C$. On associe à $f$ la fonction~:
			\begin{equation}
				F : \Omega_{\R^2} \to \R^2 : (x, y) \mapsto (I^{-1} \circ f \circ I)(x, y).
			\end{equation}
			Dès lors, la fonction $f$ est continue en $\C \ni z_0 = x_0 + iy_0 \iff F$ est continue en $I^{-1}(z_0) = (x_0, y_0)$.
			\end{prp}

			\begin{rmq}~
				\begin{enumerate}
					\item Pour $F : \Omega_{\R^2} \to \R^2$, on pose \textit{symétriquement} $f : \Omega_\C \to \C : x+iy \mapsto (I \circ F \circ I^{-1})(x+iy)$. On
					a toujours la proposition précédente.
					\item On note~:
					\begin{equation}
						F(x, y) = \begin{bmatrix}u(x, y) \\v(x, y)\end{bmatrix}.
					\end{equation}
					On peut alors noter $f(x+iy) = u(x, y) + iv(x, y)$.
				\end{enumerate}
			\end{rmq}

			\begin{prp} Soit $F : \R^2 \to \R^2$, $\R$-linéaire. L'application associée $f = I \circ F \circ I^{-1}$ est $\C$-linéaire si et seulement si la matrice
			de $F$ dans la base canonique de $\R^2$ est de la forme~:
			\begin{equation}\label{eq:C-linéaire -> matrice rotation}
				\MatBC(F) = \begin{bmatrix}\alpha & -\beta \\\beta & \alpha\end{bmatrix},
			\end{equation}
			avec $(\alpha, \beta) \in \R^2$.
			\end{prp}

			\begin{proof} Supposons $f$ $\C$-linéaire. On sait donc qu'il existe $a \in \C \tq \forall z \in \C : f(z) = az$. Prenons $\alpha, \beta \in \R$ tels que
			$a = \alpha + i\beta$. On trouve alors pour $x, y \in \R$~:
			\begin{align}
				F(x, y) &= (I^{-1} \circ f \circ I)(x, y) = (I^{-1} \circ f)(x+iy) = I^{-1}(a(x+iy)) = I^{-1}((\alpha + i\beta)(x+iy)) \\
				&= \left(\alpha x - \beta y, \alpha y + \beta x\right) = \begin{bmatrix}\alpha & -\beta \\\beta & \alpha\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix}.
			\end{align}

			Supposons alors que la matrice est sous la forme~\eqref{eq:C-linéaire -> matrice rotation}, pour tout $z \in \C$, on a alors $f(z) = (\alpha + i\beta)z$,
			et donc $f$ est $\C$-linéaire.
			\end{proof}

			\begin{rmq} Rappelons que pour $\Omega_{\R^2} \subset \R^2$, un ouvert et $F : \Omega_{\R^2} \to \R^2$, on dit que $F$ est \textit{différentiable} en
			$(x_0, y_0) \in \Omega_{\R^2}$ lorsqu'il existe $L : \R^2 \to \R^2$, une approximation $\R$-linéaire telle que~:
			\begin{equation}
				F(x_0+h, y_0+k) = F(x_0, y_0) + L(h, k) + o((h, k))
			\end{equation}
			pour $(h, k)$ au voisinage de $(0, 0)$. $L$ est alors unique et est appelée la \textit{différentielle de $F$ en $(x_0, y_0)$} notée $\dif_{(x_0, y_0)}F$.

			Si $F$ est de classe $C^1$ sur $\Omega_{\R^2}$, alors $\dif F : \Omega_{\R^2} \to L(\R^2, \R^2) : (x_0, y_0) \mapsto \dif_{(x_0, y_0)}F$ est continue.

			Si $F$ est différentiable en $(x_0, y_0) \in \Omega_{\R^2}$, alors $F$ admet une dérivée partielle d'ordre 1 en $x$ et en $y$, et on a~:
			\begin{equation}
				\MatBC(\dif_{(x_0, y_0)}F) = \begin{bmatrix}\pd {F_1}x(x_0, y_0) & \pd {F_1}y(x_0, y_0) \\ \pd {F_2}x(x_0, y_0) & \pd {F_2}y(x_0, y_0)\end{bmatrix}.
			\end{equation}
			\end{rmq}

		\subsection{Application $\C$-dérivable sur un ouvert de $\C$}
			\begin{déf} Soient $\Omega_\C$ ouvert de $\C$ et $f : \Omega_\C \to \C$. Soit $z_0 \in \C$. On dit que $f$ est $\C$-dérivable (ou dérivable) en $z_0$
			lorsqu'il existe $a \in \C$ tel que~:
			\begin{equation}
				f(z_0+h) = f(z_0) + ah + o(h)
			\end{equation}
			au voisinage de $0_\C$, ou de manière équivalente lorsque~:
			\begin{equation}
				\frac {f(z_0+h)-f(z_0)}h \xrightarrow[h \underset {\neq}{\to} 0]{} a.
			\end{equation}
			\end{déf}

			\begin{prp} $f : \Omega \to \C$ est $\C$-dérivable en $x_0+iy_0 \in \Omega$ si et seulement si l'application $F$ canoniquement associée est différentiable en
			$(x_0, y_0)$, et l'on a~:
			\begin{equation}\label{eq:Cauchy-Riemann}\tag{CR}
				\pd ux(x_0, y_0) = \pd vy(x_0, y_0) \quad \text{ et } \quad \pd uy(x_0, y_0) = -\pd vx(x_0, y_0),
			\end{equation}
			que l'on appelle \textit{relation de Cauchy-Riemann}.
			\end{prp}

			\begin{proof} Si $f$ est $\C$-dérivable en $z_0 = x_0 + iy_0$, alors~:
			\begin{equation}
				f(x_0 + iy_0 + (h + ik)) = f(x_0 + iy_0) + a(h+ik) + o(h+ik),
			\end{equation}
			pour $\C \ni a = \alpha + i\beta$ avec $\alpha, \beta \in \R$. On a donc~:
			\begin{align}
				F\left(\begin{bmatrix}x_0 \\ y_0\end{bmatrix} + \begin{bmatrix}h \\ k\end{bmatrix}\right)
					&= F\left(\begin{bmatrix}x_0 \\ y_0\end{bmatrix}\right) + I^{-1}\left((\alpha + i\beta)(h+ik)\right) + o\left(\begin{bmatrix}h\\k\end{bmatrix}\right) \\
					&= F\left(\begin{bmatrix}x_0 \\ y_0\end{bmatrix}\right) + \begin{bmatrix}\alpha h - \beta k \\\alpha k + \beta h\end{bmatrix} + o\left(\begin{bmatrix}h \\ k\end{bmatrix}\right) \\
					&= F\left(\begin{bmatrix}x_0 \\ y_0\end{bmatrix}\right) + \begin{bmatrix}\alpha & -\beta \\\beta & \alpha\end{bmatrix}\begin{bmatrix}h \\ k\end{bmatrix} + o\left(\begin{bmatrix}h \\ k\end{bmatrix}\right).
			\end{align}
			Ainsi, $F$ est différentiable en $(x_0, y_0)$, et on a bien~\eqref{eq:Cauchy-Riemann} en $(x_0, y_0)$.

			L'autre implication est laissée en exercice.
			\end{proof}

			\begin{déf} Si $f : \Omega \to \C$ est dérivable en $z_0 \in \Omega$, le nombre unique $a \in \C$ est appelé \textit{nombre dérivé de $f$ en $z_0$}, noté $f'(z_0)$.
			\end{déf}

			\begin{déf} Une fonction $f : \Omega \to \C$ est dite dérivable sur l'ouvert $\Omega$ lorsqu'elle est dérivable en tout point $z_0 \in \Omega$.
			\end{déf}

			\begin{déf} Une fonction $f : \Omega \to \C$ est dite \textit{holomorphe} sur l'ouvert $\Omega$ lorsqu'elle est dérivable sur $\Omega$ et lorsque l'application~:
			\begin{equation}
				f' : \Omega \to \C : z \mapsto f'(z)
			\end{equation}
			est continue.
			\end{déf}

			\begin{rmq} La notion d'holomorphe sur $\C$ correspond à la notion de $\C^1$ sur $\R$. On peut donc dire $f : \Omega_\C \subset \C \to \C$ est
			holomorphe sur $\Omega$ si et seulement si $f \in C^1(\Omega_\C, \C)$.
			\end{rmq}

			\begin{prp} $f : \Omega \to \C$ est holomorphe si et seulement si~:
			\begin{enumerate}
				\item $F$ est de classe $C^1$ sur $\Omega$~;
				\item les relations de Cauchy-Riemann sont vérifiées sur $\Omega$.
			\end{enumerate}
			\end{prp}

			\begin{proof} Supposons d'abord $f$ holomorphe. $F$ est donc différentiable sur $\R$. Puisque $f'$ est continue sur $\Omega$, on sait que les applications
			$\pd fx$ et $\pd fy$ sont continues sur $\R$. De plus, elles vérifient~\eqref{eq:Cauchy-Riemann} car $f$ est $\C$-dérivable.

			Supposons maintenant $F$ différentiable en $(x_0, y_0)$ et vérifie~\eqref{eq:Cauchy-Riemann}. Alors $f$ est $\C$-dérivable en $x_0 + iy_0$. Ainsi, $f$
			est dérivable sur $\Omega$. De plus, $\pd Fx$ et $\pd Fy$ sont des applications continues, qui induisent donc la continuité de $f'$.
			\end{proof}

			\begin{prp} Soient $\Omega \subset \C$, un ouvert, $f, g : \Omega \to \C$, $z_0 \in \Omega$, et $\lambda, \mu \in \C$.
			\begin{itemize}
				\item si $f$ et $g$ sont $\C$-dérivables en $z_0$, alors $\lambda f + \mu g : \Omega \to \C$ l'est aussi, et on a~:
				\begin{equation}
					(\lambda f + \mu g)'(z_0) = \lambda f'(z_0) + \mu g'(z_0)~;
				\end{equation}
				\item si $f$ et $g$ sont $\C$-dérivables en $z_0$, et $g(z_0) \neq 0$, alors $\frac fg$ est dérivable sur un voisinage de $z_0$, et on a~:
				\begin{equation}
					\left(\frac fg\right)'(z_0) = \frac {f'(z_0)g(z_0) - f(z_0)g'(z_0)}{g(z_0)^2}
				\end{equation}
			\end{itemize}
			\end{prp}

			\begin{proof} EXERCICE.
			\end{proof}

			\begin{prp} Soient $\Omega_1, \Omega_2 \subset \C$, deux ouverts, $z_0 \in \Omega_1$, et~:
			\begin{align}
				f : \Omega_1 \to \C, \\
				g : \Omega_2 \to \C,
			\end{align}
			avec $f(z_0) \in \Omega_2$. Si $f$ est $\C$-dérivable en $z_0$ et $g$ est $\C$-dérivable en $f(z_0)$, alors $g \circ f$ est $\C$-dérivable, et on a~:
			\begin{equation}
				(g \circ f)'(z_0) = f'(z_0)(g' \circ f)(z_0).
			\end{equation}
			\end{prp}

			\begin{proof} EXERCICE.
			\end{proof}

			\begin{prp} La fonction $\Id_\C : \C \to \C : z \mapsto z$ est holomorphe sur $\C$ et sa dérivée est constante égale à $1$.
			\end{prp}

			\begin{proof} $\Id_\C$ est trivialement continue car $z \to x \in \C$ lorsque $z \to x$\ldots Et on a~:
			\begin{equation}
				\Id_\C(z+h) = z + h = z + 1h + o(h).
			\end{equation}
			\end{proof}

			\begin{cor}~
				\begin{enumerate}
					\item Les fonction polynômiales sont holomorphes sur $\C$~;
					\item Les fonctions rationnelles sont holomorphes sur $\C \setminus \poles$.
				\end{enumerate}
			\end{cor}

			\begin{prp} La fonction $\Re : \C \to \C : z \mapsto \Re(z)$ n'est $\C$-dérivable en aucun point.
			\end{prp}

			\begin{proof} En effet, la fonction $F$ canoniquement associée est bien $C^\infty$, mais ne satisfait pas le critère de Cauchy-Riemann~:
			\begin{equation}
				\forall (x_0, y_0) \in \R^2 : \pd ux(x_0, y_0) = 1 \neq 0 = \pd vy(x_0, y_0).
			\end{equation}
			\end{proof}

			\begin{rmq} On remarque que $\Im : z \mapsto \Im(z)$ n'est $\C$-dérivable en aucun point par un argument similaire.
			\end{rmq}

			\begin{prp} La fonction $\bar \cdot : \C \to \C : z \mapsto \bar z$ n'est dérivable en aucun point.
			\end{prp}

			\begin{proof} À nouveau, la fonction $F$ canoniquement associée est $C^\infty$ mais ne satisfait par Cauchy-Riemann~:
			\begin{equation}
				\forall (x_0, y_0) \in \R^2 : \pd ux(x_0, y_0) = 1 \neq -1 = \pd vy(x_0, y_0).
			\end{equation}
			\end{proof}

			\begin{prp} L'application $\abs \cdot : \C \to \R : z \mapsto \abs z$ est dérivable en l'origine et nulle part ailleurs.
			\end{prp}

			\begin{proof} À nouveau, $F$ est $C^\infty$ mais ne respecte pas Cauchy-Riemann sur $\C^*$. En effet, soit $(x_0, y_0) \in \R$~:
			\begin{equation}
				\pd ux(x_0, y_0) = \pd vy(x_0, y_0) \text{ et } \pd uy(x_0, y_0) = \pd vx(x_0, y_0) \iff x_0 = y_0 = 0.
			\end{equation}
			\end{proof}

			\begin{déf} Soient $f : \Omega \subset \C \to \C$, et $z_0 \in \Omega$ tels que $f$ est $\C$-dérivable en $z_0$. On dit que $f$ est \textit{conforme} en $z_0$
			lorsqu'elle conserve les angles, i.e.~:
			\begin{equation}
				\forall (u, v) \in \left(\C \setminus \poles\right)^2 : \Angle(f'(z_0)u, f'(z_0)v) = \Angle(u, v).
			\end{equation}
			\end{déf}

			\begin{rmq} En définissant pour tout $z \in \C^*$~: $\Arg(z) \coloneqq \{\theta \in \R \tq \abs ze^{i\theta} = z\}$, on peut exprimer~:
			\begin{equation}
				\forall (u, v) \in \left(\C \setminus \poles\right) : \Angle(u, v) = \Arg(u\bar v).
			\end{equation}

			En effet, prenons $z_1, z_2 \in \C \tq z_1 = \rho_1e^{i\theta_1}$ et $z_2 = \rho_2e^{i\theta_2}$. $\Arg(z_1\bar {z_2}) = \Arg(\rho_1\rho_2e^{i(\theta_1-\theta_2)})
			= \theta_1-\theta_2$, ce qui est précisément l'angle entre $z_1$ et $z_2$.
			\end{rmq}

			\begin{prp} Soient $f : \Omega \to \C$ et $z_0 \in \Omega$ tels que $f$ est $\C$-dérivable en $z_0$. $f$ est conforme en $z_0$ si et seulement si
			$f'(z_0) \neq 0$.
			\end{prp}

			\begin{proof} Par la remarque précédente, pour $u, v \in \C \setminus \poles$, on a~:
			\begin{equation}
				\Angle(f'(z_0)u, f'(z_0)v) = \Arg\left(f'(z_0)u\overline {f'(z_0)v}\right) = \Arg\left(\abs {f'(z_0)}^2u\overline v\right).
			\end{equation}

			Or l'argument n'est défini que pour $z \in \C^*$. Il faut et est suffisant d'avoir $f'(z_0) \neq 0$.
			\end{proof}

			\begin{déf} Soit $f : \C \to \C$ telle que la fonction $F$  canoniquement associée admet des dérivées partielles d'ordre 1. On définit les opérateur
			différentiels~:
			\begin{numcases}{}
				\pd fz : \Omega \to \C : x_0 + iy_0 \mapsto \frac 12\left(\pd fx - i\pd fy\right)(x_0 + iy_0) \\
				\pd f{\overline z} : \Omega \to \C : x_0 + iy_0 \mapsto \frac 12\left(\pd fx + i\pd fy\right)(x_0 + iy_0),
			\end{numcases}
			et les éléments différentiels~:
			\begin{numcases}{}
				\dif z \coloneqq \dif x + i\dif y \\
				\dif \bar z \coloneqq \dif x - i\dif y.
			\end{numcases}
			\end{déf}

			\begin{prp} $f : \Omega \to \C$ est holomorphe sur $\Omega$ si et seulement si~:
			\begin{enumerate}
				\item $F \in C^1\left(I^{-1}(\Omega), \R^2\right)$~;
				\item $\pd f{\bar z} \equiv 0$.
			\end{enumerate}
			\end{prp}

			\begin{proof}
			Il suffit de remarquer que~:
			\begin{equation}
				\pd f{\bar z} = \frac 12\left(\pd {(u+iv)}x + i\pd {(u+iv)}y\right) = \frac 12\left(\pd ux +i\pd vx\right) + \frac 12\left(i\pd uy - \pd vy\right)
					= \frac 12\left(\pd ux - \pd vy\right) + \frac i2\left(\pd uy + \pd vx\right).
			\end{equation}
			Ainsi, on a bien $\pd f{\overline z} \equiv 0 \iff \eqref{eq:Cauchy-Riemann}$.
			\end{proof}

			\begin{rmq} En général $\pd fz\dif z + \pd f{\bar z}\dif \bar z = \dif f$.  % (avec des lunettes réelles)
			En effet~:
			\begin{align}\displaystyle
				\pd fz\dif z + \pd f{\bar z}\dif \bar z &= \frac 12\left(\pd {(u+iv)}x - i\pd {(u+iv)}y\right)\left(\dif x + i\dif y\right) + \frac 12\left(\pd {(u+iv)}x + i\pd {(u+iv)}y\right)\left(\dif x - i\dif y\right) \\
				&= \frac 12\left[\pd ux\dif x + i\pd vx\dif x - i\pd uy\dif x + i\pd vy\dif x + i\pd ux\dif y - \pd vx\dif y + \pd uy\dif y + i\pd vy\dif y\right] \\
				&+ \frac 12\left[\pd ux\dif x + i\pd vx\dif x + i\pd uy\dif x - i\pd vy\dif x - i\pd ux\dif y + \pd vx\dif y + \pd uy\dif y + i\pd vy\dif y\right] \\
				&= \pd ux\dif x + i\pd vx\dif x + \pd uy\dif y + i\pd vy\dif y = \pd fx\dif x + \pd fy\dif y = \dif f.
			\end{align}
			\end{rmq}

			\begin{prp} Soient $f : \Omega \to \C$ et $z_0 = x_0 + iy_0 \in \Omega$ tels que $F$ admet des dérivées partielles d'ordre 1 en $(x_0, y_0)$. $f$
			est $\C$-dérivable en $z_0$ si et seulement si $\pd f{\bar z}(z_0) = 0$, et dans ce cas, $f'(z_0) = \pd fz(z_0)$.
			\end{prp}

	\subsection{Les fonctions développables en séries de puissances sont holomorphes}
		\subsubsection{Résultat principal}
			\begin{thm} Soit $(c_n)_{n \in \N} \in \C^\N \tq R\left(\sum c_nz^n\right) \gneqq 0$.
			Soit $R = \begin{cases}a > 0 &\text{ quelconque si } R\left(\sum c_nz^n\right) = \pinfty \\R\left(\sum c_nz^n\right) &\text{ sinon}.\end{cases}$

			La fonction~:
			\begin{equation}\label{eq:f DSP}
				f : D(z_0, R[ \to \C : z \mapsto \sum_{n \geq 0}c_n(z-z_0)^n
			\end{equation}
			est holomorphe sur $D(z_0, R[$ et~:
			\begin{equation}\label{eq:DSP => holomorphe}
				\forall z \in D(z_0, R[ : f'(z) = \sum_{n \geq 1}nc_n(z-z_0)^{n-1}.
			\end{equation}
			\end{thm}

			\begin{proof} Soient $z \in D(z_0, R[$ et $\rho > 0$ tel que $\abs {z-z_0} < \rho < R$. Soit $y \in D(z_0, \rho[$ tel que $y \neq z$.

			On veut montrer que~:
			\begin{equation}
				F(y, z) \coloneqq \frac {f(y)-f(z)}{y-z} - \sum_{nn \geq 1}n c_n(z-z_0)^{n-1} \xrightarrow[y \to z]{} 0.
			\end{equation}

			Les séries de puissances de terme général $c_n(z-z_0)^n$, $c_n(y-z_0)^n$, et $nc_n(z-z_0)^{n-1}$ sont absolument convergentes car
			$\rho < R \leq R\left(\sum c_nz^n\right)$. Ainsi~:
			\begin{equation}
				F(y, z) = \frac 1{y-z}\left(\sum_{n \geq 0}c_n(y-z_0)^n - \sum_{n \geq 0}c_n(z-z_0)^n\right) - \sum_{n \geq 1}nc_n(z-z_0)^{n-1}.
			\end{equation}

			Pour $n=0$, on a $c_0 \cdot 1 - c_0 \cdot 1 = 0$. Pour $n \geq 1$, on a~:
			\begin{equation}
				\frac {(y-z_0)^n - (z-z_0)^n}{(y-y_0) - (z-z_0)} = \sum_{k=0}^{n-1}(y-z_0)^{n-k-1}(z-z_0)^k.
			\end{equation}
			Ainsi~:
			\begin{align}
				&\frac {(y-z_0)^n - (z-z_0)^n}{y-z} - n(z-z_0)^{n-1} = \sum_{k=0}^{n-1}\left((y-z_0)^{n-k-1}(z-z_0)^k\right) - n(z-z_0)^{n-1} \\
				&= -n(z-z_0)^{n-1} + \sum_{k=0}^{n-1}(k+1)(y-z_0)^{n-k-1}(z-z_0)^k - \sum_{k=0}^{n-1}k(y-z_0)^{n-k-1}(z-z_0)^k \\
				&= \sum_{k=0}^{n-2}(k+1)(y-z_0)^{n-k-1}(z-z_0)^k - \sum_{k=1}^{n-1}k(y-z_0)^{n-k-1}(z-z_0)^k \\
				&= \sum_{k=1}^{n-1}(k)(y-z_0)^{n-k}(z-z_0)^{k-1} - \sum_{k=1}^{n-1}k(y-z_0)^{n-k-1}(z-z_0)^k \\
				&= \left((y-y_0) - (z-z_0)\right)\sum_{k=1}^{n-1}k(y-z_0)^{n-k-1}(z-z_0)^{k-1} = (y-z)\sum_{k=1}^{n-1}k(y-z_0)^{n-k-1}(z-z_0)^{k-1}.
			\end{align}

			Dès lors~:
			\begin{align}
				\abs {F(y, z)} &\leq \sum_{n \geq 1}\abs {c_n}\abs {y-z}\sum_{k=1}^{n-1}k\abs {y-z_0}^{n-k-1}\abs {z-z_0}^{k-1} \\
				&\leq \abs {y-z}\sum_{n \geq 1}\abs {c_n}\sum_{k=1}^{n-1}k\rho^{n-k-1}\rho^{k-1} \\
				&\leq \abs {y-z}\sum_{n \geq 1}\rho^{n-2}\abs {c_n}\frac {n(n-1)}2 < \pinfty,
			\end{align}
			car $\rho < R$. On trouve donc bien que $F(y, z) \xrightarrow[y \underset \neq\to z]{} 0$, donc $f$ est $\C$-dérivable en $z$, et on
			a~\eqref{eq:DSP => holomorphe}.

			Et par les séries de puissances, on sait que $f' \in C^0\left(D(z_0, R[, \C\right)$, donc $f$ est holomorphe.
			\end{proof}

			\begin{cor} Sous les mêmes hypothèses, pour tout $k \in \N$, $f$ est de classe $C^k$ sur $D(z_0, R[$, et on a~:
			\begin{equation}
				\forall z \in D(z_0, R[ : f^{(k)}(z) = \sum_{n \geq k}\frac {n!}{(n-k)!}c_n(z-z_0)^{n-k}
			\end{equation}
			\end{cor}

			\begin{cor} Si $(c_n)_{n \geq 0} \in \C^\N$ telle que $R\left(\sum c_nz^n\right) > 0$, alors les $c_n$ sont uniquement déterminés par~:
			\begin{equation}
				\forall n \in \N : c_n = \frac {f^{(n)}(z_0)}{n!},
			\end{equation}
			où $f$ est la fonction définie en~\eqref{eq:f DSP}.
			\end{cor}

		\subsubsection{Une condition suffisante de développement en série de puissances}
			\begin{thm}\label{thm:frac C0m DSP} Soit $[a, b]$, un segment de $\R$, et soient $\varphi, m \in C^{0,m}([a, b], \C)$. Soit $\Omega \subset \C$, un ouvert tel que
			$\Omega \cap \varphi([a, b]) = \emptyset$. La fonction $f$ définie par~:
			\begin{equation}
				f : \Omega \to \C : z \mapsto \int_a^b\frac {m(s)}{\varphi(s) - z}\dif s
			\end{equation}
			est développable en série de puissances dans $\Omega$ (et est donc holomorphe par le théorème précédent).
			\end{thm}

			\begin{proof} Soient $z_0 \in \Omega, R > 0$ tels que $D(z_0, R[ \subset \Omega$. Pour $s \in [a, b]$, on a $\varphi(s) \not \in \Omega$, et donc
			$\abs {z_0-\varphi(s)} \geq R$ par définition de $R$.

			Ainsi, pour $z \in D(z_0, R[$, on a~:
			\begin{equation}
				\abs {\frac {z-z_0}{\varphi(s)-z_0}} \leq \frac {\abs {z-z_0}}R < 1.
			\end{equation}

			On en déduit que la série (géométrique) de fonctions de terme général $s \mapsto \frac {(z-z_0)^n}{(\varphi(s) - z_0)^{n+1}}$ converge normalement
			sur $[a, b]$, et donc uniformément sur $[a, b]$ par complétude de $\C$.

			Puisque $m$ est bornée sur $[a, b]$, il en est de même pour la série de terme général $s \mapsto m(s)\frac {(z-z_0)^n}{(\varphi(s) - z_0)^{n+1}}$.
			Ce terme général est de classe $C^{0, m}$ sur $[a, b]$. De plus, la somme de cette série est, pour $s \in [a, b]$~:
			\begin{align}
				\sum_{n \geq 0}m(s)\frac {(z-z_0)^n}{(\varphi(s)-z_0)^{n+1}} &= \frac {m(s)}{\varphi(s)-z_0}\sum_{n \geq 0}\frac {(z-z_0)^n}{(\varphi()-z_0)^n}
					= \frac {m(s)}{\varphi(s)-z_0}\frac 1{1-\frac {z-z_0}{\varphi(s)-z_0}} \\
				&= \frac {m(s)}{\varphi(s)-z_0} \cdot \frac {\varphi(s)-z_0}{\varphi(s) - z_0 - (z-z_0)} = \frac {m(s)}{\varphi(s)-z}.
			\end{align}

			La somme étant $C^{0,m}$ et la convergence étant uniforme, il vient~:
			\begin{align}
				\int_a^b\frac {m(s)}{\varphi(s)-z}\dif s &= \int_a^b\sum_{n \geq 0}\frac {m(s)(z-z_0)^n}{(\varphi(s) - z_0)^{n+1}}\dif s \\
				f(z) &= \sum_{n \geq 0}\left(\int_a^b\frac {m(s)}{(\varphi(s)-z_0)^{n+1}}\right)(z-z_0)^{n+1}.
			\end{align}
			\end{proof}

	\subsection{Les fonctions holomorphes sont développables en séries de puissances}
		\subsubsection{Intégration sur des chemins}
			\begin{déf} Soit $\Omega \subset \C$, un ouvert. On appelle \textit{chemin tracé dans $\Omega$} toute application $\gamma : [a, b] \tocont \Omega$
			telle que $\gamma \in C^{1,m}([a, b])$.

			On dit que $\gamma$ est fermé lorsque $\gamma(a) = \gamma(b)$.
			\end{déf}

			\begin{déf} Lorsque $\gamma$ est un chemin fermé tracé dans $\C$, on appelle \textit{longueur} de $\gamma$, le réel positif~:
			\begin{equation}
				L(\gamma) \coloneqq \int_a^b\abs {\gamma'(t)}\dif t.
			\end{equation}
			\end{déf}

			\begin{rmq} Avec les conventions que $\gamma'$ est définie sauf en un nombre fini de points (si $\gamma$ est $C^1$ par morceaux, elle peut être mal
			définie aux points de jonctions), on peut poser $\gamma'(s_k) = z_k \in \C$ pour tous les $k$, par exemple $z_k = 0$. $\gamma'$ est alors de classe
			$C^{0,m}([a, b], \C)$, et donc R-intégrable. De plus, le choix des $z_k$ ne change pas la valeur de $L(\gamma)$.
			\end{rmq}

			\begin{déf} Soit $\Omega \subset \C$, un ouvert de $\gamma : [a, b] \to \Omega$, un chemin tracé dans $\Omega$. On appelle \textit{image de $\gamma$}
			l'ensemble~:
			\begin{equation}
				\gamma^* \coloneqq \gamma\left([a, b]\right).
			\end{equation}
			\end{déf}

			\begin{déf} Soit $\Omega \subset \C$, un ouvert et soient $\gamma : [a, b] \to \Omega$, un chemin tracé dans $\Omega$, et $f : \gamma^* \to \C$.
			On appelle \textit{intégrale de $f$ le long de $\gamma$} que l'on note~:
			\begin{equation}
				\int_\gamma f(\omega)\dif \omega
			\end{equation}
			le nombre complexe~:
			\begin{equation}
				\int_\gamma f(\omega)\dif \omega \coloneqq \int_a^b(f \circ \gamma)(s) \gamma'(s)\dif s,
			\end{equation}
			avec la convention usuelle sur $\gamma'$.
			\end{déf}

			\begin{rmq} $\int_\gamma f(\omega)\dif \omega$ est bien défini car $f \in C^0(\gamma^*, \C)$ et $\gamma \in C^0([a, b], \gamma^*)$, et donc
			$(f \circ \gamma) \in C^0([a, b], \C)$ et $\gamma' \in C^{0,m}([a, b], \C)$. Donc~:
			\begin{equation}
				\left(f \circ \gamma\right) \gamma' \in C^{0,m}\left([a, b], \C\right).
			\end{equation}
			\end{rmq}

			\begin{prp} Soit $\gamma : [a, b] \to \Omega \subset \C$, un chemin tracé dans un ouvert $\Omega$. Alors $\gamma^*$ est un compact inclus dans $\Omega$.
			\end{prp}

			\begin{proof} Par continuité de $\gamma$ et compacité de $[a, b]$, on a $\gamma^* = \gamma([a, b])$ est compact.
			\end{proof}

			\begin{prp} Soit $\gamma$, un chemin tracé dans $\Omega \subset \C$, un ouvert. L'application~:
			\begin{equation}
				\int_\gamma \cdot(\omega) \dif \omega : \left(C^0(\gamma^*, \C), \norm \cdot_{\infty, \gamma^*}\right) \to \left(\C, \abs \cdot\right) : f \mapsto \int_\gamma f(\omega)\dif \omega
			\end{equation}
			est une forme linéaire continue.
			\end{prp}

			\begin{proof} Prenons $f, g \in C^0(\gamma^*, \C)$ et $\lambda, \mu \in \C$. On a bien~:
			\begin{equation}
				\int_\gamma \left(\lambda f + \mu g\right)(\omega)\dif \omega = \int_a^b \left(\lambda f(\gamma(s)) + \mu g(\gamma(s))\right)\gamma'(s)\dif s
					= \lambda \int_\gamma f(\omega)\dif \omega + \mu \int_\gamma g(\omega)\dif \omega.
			\end{equation}

			Montrons alors que la forme linéaire est continue. Pour cela montrons qu'elle est Lipschitzienne. Soit $f \in C^0(\gamma^*, \C)$. On a~:
			\begin{align}
				\abs {\int_\gamma f(\omega)\dif \omega} &= \abs {\int_a^b (f \circ \gamma)(s)\gamma'(s)\dif s}
					\leq \int_a^b\abs {(f \circ \gamma)(s)}\abs {\gamma'(s)}\dif s \\
				&\leq \norm {f \circ \gamma}_{\infty, [a, b]} \int_a^b\abs {\gamma'(s)}\dif s = L(\gamma)\norm f_{\infty, \gamma^*}.
			\end{align}
			\end{proof}

			\begin{déf} Soient $[a, b]$ et $[\tilde a, \tilde b]$, deux segments sur lesquels sont définis respectivement $\gamma$, et $\tilde \gamma$, deux
			chemins tracés dans $\C$.

			On appelle changement de paramétrage admissible entre $\gamma$ et $\tilde \gamma$ toute application bijective
			$\varphi \in C^0([a, b], [\tilde a, \tilde b]) \cap C^{1,m}([a, b], [\tilde a, \tilde b])$ strictement croissante sur $[a, b]$ telle que
			$\gamma = \tilde \gamma \circ \varphi$.
			\end{déf}

			\begin{rmq} On impose que $\varphi$ soit strictement croissante car une bijection est obligatoirement strictement monotone, ce qui ne laisse que deux
			\textit{familles} de changement de paramétrages admissibles, à savoir les strictement croissantes et strictement décroissantes. N'accepter qu'une
			seule de ces familles permet de donner un «~sens de parcours~» aux chemins.
			\end{rmq}

			\begin{prp} S'il existe un paramétrage admissible $\varphi$ entre deux chemins $\gamma$ et $\tilde \gamma$ tracés dans $\C$, alors~:
			\begin{enumerate}
				\item $\gamma^* = \tilde\gamma^*$~;
				\item $\forall f \in C^0(\gamma^*, \C) : \int_\gamma f(\omega)\dif \omega = \int_{\tilde \gamma} f(\omega)\dif \omega$.
			\end{enumerate}
			\end{prp}

			\begin{proof}~
			\begin{enumerate}
				\item EXERCICE~;
				\item EXERCICE (avec changement de variable).
			\end{enumerate}
			\end{proof}

			\begin{déf} Soient $\gamma : [a, b] \to \C$ et $\tilde \gamma : [\tilde a, \tilde b] \to \C$, deux chemins tracés dans $\C$ tels que
			$\gamma(b) = \tilde \gamma(\tilde a)$. On appelle \textit{somme de ces chemins} le chemin~:
			\begin{subnumcases}
				{\gamma + \tilde \gamma : [a, b + (\tilde b - \tilde a)] \to \C : t \mapsto}
					\gamma(t) & si $t \leq b$ \\
					\tilde \gamma(\tilde a + t - b) & sinon
			\end{subnumcases}
			\end{déf}

			\begin{ex}~
			\begin{enumerate}
				\item On appelle \textit{cercle orienté positivement de centre $a \in \C$ et de rayon $r > 0$} le chemin fermé~:
				\begin{equation}
					\gamma : [0, 1] \to \C : t \mapsto a + re^{2\pi it}.
				\end{equation}

				On a alors pour longueur~:
				\begin{equation}
					L(\gamma) = \int_0^1 \abs {2\pi i re^{2\pi it}}\dif t = \int_0^1\abs {2\pi r} = 2\pi r.
				\end{equation}

				\item Soient $a, b \in C$. On note $[a, b]$ et on appelle \textit{segment reliant $a$ à $b$} (attention à l'ordre~!) le chemin défini par~:
				\begin{equation}
					\bar \gamma : [0, 1] \to \C : t \mapsto a + t(b-a).
				\end{equation}

				La longueur vaut alors~:
				\begin{equation}
					L(\bar \gamma) = \int_0^1\abs {(b-a)}\dif t = b-a.
				\end{equation}

				\item Soient $a, b, c \in \C$. On appelle \textit{triangle $a \to b \to c$} la somme des chemins $[a, b]$, $[b, c]$, et $[c, a]$.
				À nouveau, l'ordre a de l'importance~!
			\end{enumerate}
			\end{ex}

		\subsubsection{Rappels de connexité dans $\C$}
			\begin{déf} Une partie $E \subset \C$ est dite \textit{non-connexe} s'il existe deux ouverts $U$ et $V$ de $\C$ tels que $E \subset U \cup V$,
			$E \cap U \neq \emptyset \neq E \cap V$ et $E \cap U \cap V = \emptyset$.

			Elle est dite \textit{connexe} dans le cas contraire.
			\end{déf}

			\begin{déf} $E \subset \C$ est convexe lorsque $\forall a, b \in E : [a, b] : [0, 1] \to \C : t \mapsto a + t(b-a)$ est tracé dans $E$.
			\end{déf}

			\begin{prp}\label{prp:convexe => connexe} Soit $E \subset \C$ une partie convexe. Elle est alors connexe.
			\end{prp}

			\begin{proof} Par l'absurde, supposons que $E$ ne soit pas connexe. Alors il existe deux ouverts $U$ et $V$ de $\C$ tel que $E \cup U \cup V$,
			$E \cap U \neq \emptyset \neq E \cap V$, mais $E \cap U \cap V = \emptyset$.

			Soient $u \in E \cap U$, $v \in E \cap V$, $[u, v] : [0, 1] \to \C : t \mapsto u + t(v-u)$. Puisque $E$ est convexe, $\gamma^* \subset E$.

			$\gamma^{-1}(U \cup V) = \gamma^{-1}(E) = [0, 1]$. De plus, $\gamma^{-1}(U)$ est un ouvert de $[0, 1]$ qui contient 0, et $\gamma^{-1}(V)$
			est un ouvert de $[0, 1]$ qui contient 1.

			Si $t \in \gamma^{-1}(U) \cap \gamma^{-1}(V)$, alors $\gamma(t) \in U$, $\gamma(t) \in V$ et $\gamma(t) \in E$. Or $E \cap U \cap V = \emptyset$.

			Donc $\gamma^{-1}(U) \cap \gamma^{-1}(V) = \emptyset$. $\gamma^{-1}(U \cup V) = \gamma^{-1}(U) \cup \gamma^{-1}(V) = [0, 1]$.

			Donc $[0, 1]$ n'est pas connexe. Il y a donc contradiction et $E$ est connexe.
			\end{proof}

			\begin{cor} les disques (ouverts ou fermés) de $\C$ sont donc connexes.
			\end{cor}

			\begin{prp} Lorsque $E = \Omega \subset \C$ est ouvert, $\Omega$ est non-connexe si et seulement si~:
			\begin{itemize}
				\item[$(i)$]   $\Omega = U \cup V$~;
				\item[$(ii)$]  $\Omega \cap U \neq \emptyset$~;
				\item[$(iii)$] $\Omega \cap V \neq \emptyset$~;
				\item[$(iv)$]  $\Omega \cap U \cap V = \emptyset$.
			\end{itemize}
			\end{prp}

			\begin{prp} Soient $E \subset \C$, $x \in E$, $I \neq \emptyset$, $(f_i)_{i \in I}$, une famille de sous-ensembles connexes de $E$ tels que~:
			\begin{equation}
				\forall i \in I : F_i \ni x.
			\end{equation}
			Alors $F \coloneqq \bigcup_{i \in I}F_i$ est un connexe de $E$ contenant $x$.
			\end{prp}

			\begin{proof} Supposons par l'absurde que $F$ n'est pas connexe. Il existe alors deux ouverts $U$ et $V$ de $\C$ tels que~:
			\begin{itemize}
				\item $F \subset U \cup V$~;
				\item $F \cap U \neq \emptyset \neq F \cap V$~;
				\item $F \cap U \cap V = \emptyset$.
			\end{itemize}

			Puisque $\forall i \in I : x \in F_i$, on sait que $x \in F$. De plus, $U$ et $V$ recouvrent $F$. Donc $x \in U$ ou $x \in V$. Par symétrie des hypothèses
			en $U$ et $V$, supposons sans perte de généralité que $x \in U$. Puisque $F \cap V \neq \emptyset$, on sait qu'il existe $x_0 \in F \cap V$. Il existe
			alors un certain $i_0 \in I$ tel que $F_{i_0} \ni x$. On sait donc que $F_{i_0} \cap V \neq \emptyset$ car $x_0 \in F_{i_0} \cap V$ et que
			$F_{i_0} \cap U \neq \emptyset$ car $x \in F_{i_0} \cap U$.

			Or $F_{i_0} \cap U \cap V \subseteq F \cap U \cap V = \emptyset$. On en déduit que $F_{i_0}$ est non-connexe, ce qui est une contradiction.
			\end{proof}

			\begin{prp} Soient $E \subset \C$ et $x \in E$. L'ensemble des connexes de $E$ contenant $x$ est non-vide. De plus, la réunion de ses parties connexes
			est connexe et contient $x$. Cette réunion est le plus grand connexe de $E$ contenant $x$.
			\end{prp}

			\begin{proof} $\{x\}$ est un connexe de $E$ contenant $x$. Donc~:
			\begin{equation}
				X(x) \coloneqq \{C \subset E \tq C \text{ est connexe et } x \in C\} \neq \emptyset.
			\end{equation}

			Notons $C(x) \coloneqq \bigcup_{C \in X(x)}C$. $C(x)$ est un connexe de $E$ contenant $x$ par la proposition précédente. De plus, si $C$ est un connexe de
			$E$ qui contient $x$, alors $C \in X(x)$, et $C \subset C(x)$ par définition. $C(x)$ est donc bien le plus grand connexe de $E$ contenant $x$ au sens de
			l'inclusion.
			\end{proof}

			\begin{déf} Ce plus grand connexe de $E$ contenant $x$ noté $C(x)$ est appelé \textit{composante connexe de $x$ dans $E$}
			\end{déf}

			\begin{ex} L'ensemble $E = \C \setminus \mathbb U$, où $\mathbb U$ est le cercle unitaire $\mathbb U = \{e^{i\theta} \tq \theta \in [0, 2\pi]\}$
			contient deux composantes connexes~:
			\begin{itemize}
				\item $C(0) = \{z \in \C \tq \abs z < 1\} = \mathbb D$, le disque unitaire~;
				\item $C(2i) = \{z \in \C \tq \abs z > 1\} = \C \setminus \overline {\mathbb D}$.
			\end{itemize}
			\end{ex}

			% TODO: ajouter que deux éléments d'une composante connexe ont la même composante connexe.

			\begin{prp} Soit $\Omega \subset \C$ un ouvert. Alors les composantes connexes de $\Omega$ sont des ouverts.
			\end{prp}

			\begin{proof} Soient $x \in E$ et $C(x)$ sa composante connexe. Prenons $y \in C(x)$. Il existe $\delta > 0$ tel que $D(y, \delta[ \subset \Omega$
			par ouverture de $\Omega$. Or $D(y, \delta[$ est convexe, donc connexe par la proposition~\ref{prp:convexe => connexe}.

			Donc $D(y, \delta[ \subset C(y)$. Or $C(x) = C(y)$. Ainsi, $D(y, \delta[ \subset C(x)$ et donc $C(x)$ est ouvert.
			\end{proof}

			\begin{prp}\label{prp:continuité conserve connexité} Soit $\varphi : E \tocont \C$ avec $E \subset \C$ connexe. Alors $\varphi(E) \subset \C$ est connexe.
			\end{prp}

			\grantedproof

			\begin{déf} On appelle \textit{région} (ou \textit{domaine}) \textit{de $\C$} tout ouvert connexe de $\C$.
			\end{déf}

			\begin{déf} Soit $\Omega \subset \C$. On définit l'ensemble $H(\Omega)$ par l'ensemble des fonctions holomorphes de $\Omega$ dans $\C$~:
			\begin{equation}
				H(\Omega) \coloneqq \left\{f : \Omega \to \C \tq f \text{ est holomorphe}\right\}.
			\end{equation}
			\end{déf}

		\subsubsection{Indice d'un point par rapport à un chemin fermé}
			\begin{déf} Soit $\gamma : [a, b] \to \C$, un chemin fermé. Posons $\Omega \setminus \gamma^*$. $\Omega$ est ouvert (car $\gamma^*$ est compact donc fermé).
			Pour $z \in \Omega$, on définit \textit{l'indice de $z$ par rapport à $\gamma$} par~:
			\begin{equation}
				\Ind_\gamma(z) \coloneqq \frac 1{2i\pi}\int_\gamma \frac {\dif \omega}{\omega - z}.
			\end{equation}
			\end{déf}

			\begin{lem}\label{lem:e^z=1} Soit $z \in \C$. $\frac z{2i\pi} \in \Z \iff \exp(z) = 1$.
			\end{lem}

			\begin{proof} Soit $z = a + ib\in \C$. Supposons $e^z = 1$. On a donc $1 = e^ae^{ib}$. On a alors $a=0$ et $b=2k\pi$ pour $k \in \Z$. Dès lors~:
			\begin{equation}
				\frac z{2i\pi} = \frac {ib}{2i\pi} = \frac b{2\pi} = \frac {2k\pi}{2\pi} = k \in \Z.
			\end{equation}

			Supposons alors $\frac z{2i\pi} \eqqcolon k \in \Z$. On a bien $\Re(z) = 0$, et donc~:
			\begin{equation}
				e^z = e^{i\Im(z)} = e^{i \cdot 2ki\pi} = e^{-2k\pi} = 1.
			\end{equation}
			\end{proof}

			\begin{thm} Sous les notations de la définition ci-dessus, on a~:
			\begin{enumerate}
				\item la fonction $\Ind_\gamma$ est définie sur $\Omega$~;  % définie sur $\C$ ?
				\item $\Ind_\gamma \in H(\Omega)$~;
				\item $\Ind_\gamma$ est à valeurs dans $\Z$~;
				\item $\Ind_\gamma$ est constante sur les composantes connexes de $\Omega$~;
				\item $\Ind_\gamma$ vaut $0$ sur la composante connexe non-bornée de $\Omega$
			\end{enumerate}
			\end{thm}

			\begin{proof}~
			\begin{enumerate}
				\item La fonction $\gamma^* \to \C : \omega \mapsto \frac 1{\omega - z}$ est $C^0$ sur $\gamma^*$ car son dénominateur ne s'annule jamais (et est continu
				sur $\gamma^*$) lorsque $z \in \Omega$. Ainsi~:
				\begin{equation}
					\Ind_\gamma(z) = \frac 1{2i\pi}\int_a^b\frac {\gamma'(s)}{\gamma(s)-z}\dif s
				\end{equation}
				est bien défini sur $\Omega$.
				\item $s \mapsto \gamma'(s)$ est de classe $C^{0,m}$ sur $[a, b]$ (quitte à la définir en $0$ en un certain nombre de points) car $\gamma$ est un chemin
				tracé dans $\Omega$. La fonction $s \mapsto \gamma(s) - z$ est de classe $C^{0,m}$ sur $[a, b]$ et ne s'annule pas. Donc par le
				Théorème~\ref{thm:frac C0m DSP}, on a $\Ind_\gamma \in H(\Omega)$.
				\item Soit $\omega \in \C$. On veut montrer que pour tout $z \in \Omega$, on a~:
				\begin{equation}
					\exp\left(\int_\gamma\frac {\dif \omega}{\omega-z}\right) = 1
				\end{equation}
				pour appliquer le Lemme~\ref{lem:e^z=1}.

				Posons $\varphi : [a, b] \to \C^* : t \mapsto \int_a^t\frac {\gamma'(s)}{\gamma(s) - z}$ à $z \in \Omega$ fixé.

				Observons que $\varphi \in C^0([a, b], \C) \cap C^{1,m}([a, b], \C)$,et sauf sur un nombre fini de points, on a~:
				\begin{equation}\label{eq:phi'/phi = gamma'/gamma-z}
					\frac {\varphi'(t)}{\varphi(t)} = \frac {\gamma'(t)}{\gamma(t) - z}.
				\end{equation}

				Observons que~:
				\begin{equation}\label{eq:d/dt(phi/gamma-z)}
					\od {}t\left(\frac {\varphi(t)}{\gamma(t) - z}\right) = \frac {\varphi'(t)(\gamma(t)-z) - \varphi(t)\gamma'(t)}{\left(\gamma(t) - z\right)^2},
				\end{equation}
				sauf en un nombre fini de points. Par~\eqref{eq:phi'/phi = gamma'/gamma-z}~et~\eqref{eq:d/dt(phi/gamma-z)}, on trouve~:
				\begin{equation}
					\od{}t\left(\frac {\varphi(t)}{\gamma(t)-z}\right) = 0,
				\end{equation}
				sauf en un nombre fini de points. Dès lors $t \mapsto \frac {\varphi(t)}{\gamma(t) - z}$ est constante sur $[a, b]$.

				On sait $\varphi(a) = e^0 = 1$, et donc pour $t \in [a, b]$~:
				\begin{equation}
					\frac {\varphi(t)}{\gamma(t) - z} = \frac {\varphi(a)}{\gamma(a) - z} = \frac 1{\gamma(a) - z},
				\end{equation}
				d'où~:
				\begin{equation}
					\forall t \in [a, b] : \varphi(t) = \frac {\gamma(t) - z}{\gamma(a) - z},
				\end{equation}
				ainsi~:
				\begin{equation}
					\varphi(b) = \frac {\gamma(b) - z}{\gamma(a) - z} = \frac {\gamma(a) - z}{\gamma(a) - z} = 1,
				\end{equation}
				d'où $\Ind_\gamma(z) \in \Z$.
				\item Soit $C(x) \subset \Omega$ une composante connexe de $x \in \Omega$. $\Ind_\gamma : \Omega \to \Z$ est continue. Donc $\Ind_\gamma(C(x))$ est un
				connexe de $\Z$ par la Proposition~\eqref{prp:continuité conserve connexité}, et donc un singleton. $\Ind_\gamma$ est constante sur $C(x)$.
				\item $\gamma^*$ est compact. En particulier, $\gamma^*$ est borné dans $\C$. Donc il existe $R > 0$ tel que $\gamma^* \subset D(0, R[$, d'où~:
				\begin{equation}
					\C \setminus D(0, R[ \subset \Omega.
				\end{equation}

				Donc $\Omega$ a exactement une composante connexe non-bornée. Pour $z$ dans cette composante connexe, on a~:
				\begin{equation}
					\abs {\Ind_\gamma(z)} = \abs {\frac 1{2\pi}}\abs {\int_\gamma \frac {\dif \omega}{\omega - z}}
						\leq \frac 1{2\pi}\frac {\abs {\int_a^b\abs {\gamma'(s)\dif s}}}{d(z, \gamma^*)} = \frac {L(\gamma)}{2\pi d(z, \gamma^*)} \xrightarrow[\abs z \to \pinfty]{} 0.
				\end{equation}

				On en déduit $\Ind_\gamma(z) = 0$ pour $z$ assez grand, et donc $\Ind_\gamma(z) = 0$ sur la composante connexe.
			\end{enumerate}
			\end{proof}

			\begin{ex} $\gamma : [0, 2\pi] \to \C : \theta \mapsto a + re^{i\theta}$, pour $a \in \C$ et $r > 0$. Donc~:
			\begin{equation}
				\Ind_\gamma(a) = \frac 1{2i\pi}\int_0^{2\pi}\frac {ire^{i\theta}}{(a + re^{i\theta}) - a}\dif \theta = \frac {2i\pi}{2i\pi} = 1.
			\end{equation}

			De plus, $\Ind_\gamma(a + i\frac 32r) = 0$ par le théorème.
			\end{ex}

		\subsubsection{Le théorème de Cauchy local}
			\begin{lem} Soit $\Omega \subset \C$, un ouvert. Soient $f : \Omega \tocont \C$ admettant une primitive holomorphe dans $\Omega$, $\gamma : [a, b] \to \C$,
			un chemin fermé tracé dans $\Omega$. Alors~:
			\begin{equation}
				\int_\gamma f(\omega)\dif \omega = 0.
			\end{equation}
			\end{lem}

			\begin{proof} Soit $F \in H(\Omega)$ tel que $F' = f$ sur $\Omega$. Alors~:
			\begin{equation}
				\int_\gamma f(\omega)\dif \omega = \int_a^b F'(\gamma(s))\gamma'(s)\dif s = \int_a^b \od {}s\left(F \circ \gamma\right)(s)\dif s = \left[F \circ \gamma\right]_a^b = 0
			\end{equation}
			par fermeture de $\gamma$.
			\end{proof}

			\begin{cor} Pour $n \in \Z$~:
			\begin{itemize}
				\item si $n \leq -2$, alors $z \mapsto \frac {z^{n+1}}{n+1}$ est une primitive holomorphe de $z \mapsto z^n$ dans $\C^*$. Donc pour out chemin tracé dans
				$\C^*$, on a $\int_\gamma \omega^n\dif \omega = 0$~;
				\item si $n \geq 0$, alors $z \mapsto \frac {z^{n+1}}{n+1}$ est une primitive holomorphe de $z \mapsto z^n$ dans $\C$. Donc pour tout chemin $\gamma$
				fermé tracé dans $\C$, on a $\int_\gamma \omega^n\dif \omega = 0$.
			\end{itemize}
			\end{cor}

			\begin{rmq} Que dire pour $n=-1$ de $\int_\gamma \frac {\dif \omega}{\omega}$ pour $\gamma$, un chemin fermé tracé dans $\C^*$ ? On voit que~:
			\begin{equation}
				\int_\gamma\frac {\dif \omega}\omega = 2i\pi \Ind_\gamma(0).
			\end{equation}

			Donc la valeur dépend du chemin $\gamma$ choisi. On en déduit que $z \mapsto z^{-1}$ n'a pas de primitive holomorphe dans $\C^*$.
			\end{rmq}

			\begin{thm}[Cauchy dans un triangle] Soit $\Omega \subset \C$, un ouvert. Soient $p \in \Omega$ et $f : \Omega \tocont \C$ tel que
			$f \in H(\Omega \setminus \{p\}, \C)$.

			Soit $(a, b, c) \in \Omega^3$ tel que $\Delta \coloneqq \Conv(a, b, c)\subset \Omega$. On note $\gamma_\Delta \coloneqq a \to b \to c$. Alors~:
			\begin{equation}
				\int_{\gamma_\Delta} f(\omega)\dif \omega = 0.
			\end{equation}
			\end{thm}

			\begin{proof} Considérons les trois cas séparément~:
			\begin{itemize}
				\item[{[Cas 1]}] $p \not \in \Conv(a, b, c) = \Delta$~;
				\item[{[Cas 2]}] $p \in \{a, b, c\}$~;
				\item[{[Cas 3]}] $p \in \Conv(a, b) \setminus \{a, b, c\}$.
			\end{itemize}

			Pour le cas 1, notons $a'$, le milieu de $[b, c]$, $b'$, le milieu de $[a, c]$, et $c'$, le milieu de $[a, b]$.
			On pose~:
			\begin{equation}
				J \coloneqq \int_{[a \to b \to c]}.
			\end{equation}

			Remarquons que~:
			\begin{equation}
				J = \int_{[a \to c' \to b']}f(\omega)\dif \omega) + \int_{[c' \to b \to a']} f(\omega)\dif \omega + \int_{[a' \to c \to b']} f(\omega)\dif \omega
					+ \int_{[a' \to b' \to c']}f(\omega)\dif \omega.
			\end{equation}

			Dans la somme ci-dessus, un des termes est de module $\geq \frac {\abs J}4$. Pour ce triangle-là, on reproduit le découpage. On produit ainsi
			une suite $(\gamma_{\Delta_n})_{n \in \N}$ de triangles tels que~:
			\begin{equation}
				\forall n \in \N : \abs {\int_{\gamma_{\Delta_n}} f(\omega)\dif \omega} \geq \frac {\abs J}4.
			\end{equation}

			De plus, $L(\gamma_{\Delta_n}) \leq 2^{-n}L(\gamma_\Delta)$.

			Posons $z_0 \in \bigcap_{n \in \N} \Delta_n$. Puisque $z_0 \in \Omega \setminus \{p\}$, $f$ est holomorphe en $z_0$. Fixons $\varepsilon > 0$.
			Il existe $r > 0$ tel que~:
			\begin{equation}
				\forall z \in \Omega : \abs {z-z_0} < r \Rightarrow \abs {f(z) - f(z_0) - (z-z_0)f'(z_0)} < \varepsilon\abs {z - z_0}.
			\end{equation}

			Choisissons $N \in \N$ assez grand pour que pour $z \in \Delta_n$~:
			\begin{equation}
				\forall n \geq N : \Delta_n \subset B(z_0, r[ \text{ et } \abs {z-z_0} \leq 2^{-n}L(\gamma_{\Delta_n}).
			\end{equation}

			On a alors pour $n \geq N$~:
			\begin{equation}
				\int_{\gamma_{\Delta_n}}f(\omega)\dif \omega = \int_{\gamma_{\Delta_n}}\left[f(\omega) - f(z) - (\omega-z_0)f'(z_0)\right]\dif \omega
			\end{equation}
			car~:
			\begin{equation}
				\int_{\gamma_{\Delta_n}}\left(f(z_0) - (\omega-z_0)f'(z_0)\right)\dif \omega = 0
			\end{equation}
			avec ce qui précède (existence d'une primitive holomorphe).

			Ainsi, $\abs {\int_{\gamma_{\Delta_n}}f(\omega)\dif \omega} \leq L(\gamma_{\Delta_n})\cdot \norm {f-f(z_0) - (\cdot-z_0)f'(z_0)}_\infty
			\leq 2^{-n}L(\gamma_\Delta)\varepsilon2^{-n}L(\gamma_\Delta)$.

			Finalement~:
			\begin{equation}
				\abs J \leq 4^n\abs {\int_{\gamma_{\Delta_n}}f(\omega)\dif \omega} \leq L(\gamma_\Delta)^2\varepsilon.
			\end{equation}

			Ainsi, pour tout $\varepsilon > 0$, on a $\abs J \leq \varepsilon$. Donc $\abs J = 0$.

			Pour le second cas, posons $p = c$ dans perte de généralité. On a alors~:
			\begin{equation}
				\int_{[a \to b \to c]}f(\omega)\dif \omega = \underbrace {\int_{[a \to y \to b]}f(\omega)\dif \omega}_{= 0}
					+ \underbrace {\int_{[a \to y \to b]} f(\omega)\dif \omega}_{= 0}
					+ \underbrace {\int_{[x \to c \to y]}f(\omega)\dif \omega}_{\xrightarrow[x, y \to p=c]{} 0} = 0,
			\end{equation}
			par continuité de $f$ en $c=p$.

			Pour le cas 3, on divise le triangle $[a \to b \to c]$ en trois triangles $[a \to p \to b]$, $[b \to p \to c]$, et $[c \to p \to a]$.
			Par le cas 2, on trouve $\int_{\gamma_\Delta} f(\omega)\dif \omega = 0$.
			\end{proof}

			\paragraph{Rappel~: théorème de Green-Riemann}
				\begin{thm} Soient $f, g \in C^1\left(\Omega \subset \R^2, \K \in \{\R, \C\}\right)$. Soit $U$ un ouvert \textit{simple} et \textit{régulier} et son bord
				$\partial U$ orienté positivement. Alors~:
				\begin{equation}
					\int_{\partial U}\left(f(x, y)\dif x + g(x, y)\dif y\right) = \iint_U\left(\pd gx(x, y) + \pd fy(x, y)\right)\dif x\dif y.
				\end{equation}
				\end{thm}

				\begin{proof}[Preuve alternative de Cauchy dans un triangle pour $f \in H(\Omega)$] Pour $\Delta$ le triangle, on a, formellement~:
				\begin{align}
					\int_{\partial \Delta}f(z)\dif z &= \int_{\partial \Delta}\left(u(x, y) + iv(x, y)\right)\left(\dif x + i \dif y\right) \\
						&= \int_{\partial \Delta}\left(u(x, y)\dif x - v(x, y)\dif y\right) + i\int_{\partial\Delta}\left(u(x, y)\dif y + v(x, y)\dif x\right) \\
						&= \iint_\Delta \left[-\pd vx(x, y) - \pd ux(x, y)\right]\dif x\dif y + i\iint_\Delta\left[\pd ux - \pd vy\right]\dif x\dif y = 0,
				\end{align}
				par Cauchy-Riemann.
				\end{proof}

			\begin{thm}[Théorème de Cauchy dans un convexe] Soient $\Omega \subset \C$ un connexe ouvert et $p \in \Omega$.
			Soit $f \in C^0(\Omega, \C) \cap H(\Omega \setminus \{p\})$. Alors il existe $F \in H(\Omega)$ telle que $F' = f$ dans $\Omega$.
			\end{thm}

			\begin{proof} Soient $a, z_0, z \in \Omega$. Par convexité de $\Omega$, le triangle (plein) $a \to z_0 \to z$ est dans $\Omega$. Posons~:
			\begin{equation}
				F(z) \coloneqq \int_{[a, z]}f(\omega)\dif \omega.
			\end{equation}

			On trouve donc~:
			\begin{equation}
				F(z) - F(z_0) = \int_{[a, z]}f(\omega)\dif \omega - \int_{[a, z_0]}f(\omega)\dif \omega = \int_{[z_0, z]}f(\omega)\dif \omega.
			\end{equation}

			En effet, on sait, par Cauchy dans un triangle~:
			\begin{equation}
				\int_{[a, z_0]} f(\omega)\dif \omega + \int_{[z_0, z]} f(\omega)\dif \omega + \int_{[z, a]} f(\omega)\dif \omega = 0.
			\end{equation}

			On en déduit~:
			\begin{equation}
				\int_{[a, z]} f(\omega)\dif \omega = - \int_{[z, a]} f(\omega)\dif \omega = \int_{[a, z_0]} f(\omega)\dif \omega + \int_{[z_0, z]} f(\omega)\dif \omega.
			\end{equation}

			Ainsi, lorsque $z \neq z_0$, on a~:
			\begin{equation}
				\frac {F(z)-F(z_0)}{z-z_0} = \frac 1{z-z_0}\int_{[z_0, z]}f(\omega)\dif \omega,
			\end{equation}
			et~:
			\begin{equation}
				\frac {F(z)-F(z_0)}{z-z_0} - f(z_0) = \frac 1{z-z_0}\int_{[z_0, z]}\left(f(\omega) - f(z_0)\right)\dif \omega.
			\end{equation}

			Or $f$ est continue en $z_0$. Donc soit $\varepsilon > 0$. Il existe $\delta > 0$ tel que $D(z_0, \delta[ \subset \Omega$, et~:
			\begin{equation}
				\forall \omega \in D(z_0, \delta[ : \abs {f(\omega) - f(z)} < \varepsilon.
			\end{equation}

			Ainsi, pour $z \neq z_0 \tq \abs {z_0-z} < \delta$, on a~:
			\begin{equation}
				\abs {\frac {F(z)-F(z_0)}{z-z_0} - f(z_0)} \leq \frac 1{\abs {z-z_0}}\abs {\int_{[z_0, z]}\left(f(\omega) - f(z_0)\right)\dif \omega}
					\leq \frac 1{\abs {z-z_0}}\varepsilon\abs {z-z_0} = \varepsilon.
			\end{equation}

			On a donc $F$ $\C$-dérivable en $z_0$ et $F'(z_0) = f(z_0)$, et ce, pour tout $z_0 \in \Omega$. Par continuité de $f$, on a alors $F \in H(\Omega)$
			avec $F' = f$ sur $\Omega$.
			\end{proof}

			\begin{cor} Pour tout $\gamma$ fermé tracé dans $\Omega$, on a~:
			\begin{equation}
				\int_\gamma f(\omega)\dif \omega = 0.
			\end{equation}
			\end{cor}

			\begin{proof} Pour $\gamma : [a, b] \to \C$, un chemin fermé tracé dans $\Omega$, on trouve~:
			\begin{equation}
				\int_\gamma f(\omega)\dif \omega = \int_a^bF'(\gamma(s))\gamma'(s)\dif s = \left[F \circ \gamma\right]_a^b = 0.
			\end{equation}
			\end{proof}

		\subsubsection{Formule de Cauchy dans un ouvert convexe}
			\begin{thm} Soient $\Omega \subset \C$ un ouvert convexe, $f \in H(\Omega)$, $\gamma$ fermé tracé dans $\Omega$. Alors~:
			\begin{equation}
				\forall z \in \Omega \setminus \gamma^* : f(z)\Ind_\gamma(z) = \frac 1{2i\pi}\int_\gamma\frac {f(\omega)}{\omega - z}\dif \omega.
			\end{equation}
			\end{thm}

			\begin{rmq} Ce théorème affirme donc que connaitre une fonction holomorphe sur un chemin fermé dans son domaine permet de la connaître partout à
			l'intérieur de ce chemin.

			Il est donc très restrictif d'être holomorphe~: on ne peut pas changer une fonction en certains points, ou sur un sous-ensemble de l'intérieur d'un
			chemin fermé sans devoir la changer sur les bords de ce chemin.
			\end{rmq}

			\begin{proof} Soit $z \in \Omega \setminus \gamma^*$. Considérons la fonction~:
			\begin{subnumcases}
				{g_z : \Omega \to \C : \omega \mapsto}
					\frac {f(\omega) - f(z)}{\omega-z} & si $\omega \neq z$ \\
					f'(z) & sinon
			\end{subnumcases}

			$g_z$ est holomorphe sur $\Omega \setminus \{p\}$ car $f \in H(\Omega)$, et donc $g_z$ est continue sur $\Omega \setminus \{z\}$. Or $f$ est $\C$-dérivable
			(car holomorphe), et donc $g-z$ est continue en $z$. $g_z$ est donc continue sur $\Omega$.

			Par Cauchy dans un ouvert connexe, on a~:
			\begin{equation}
				\int_\gamma g_z(\omega)\dif \omega = 0.
			\end{equation}

			Ainsi, pour $z \not \in \gamma^*$, on a~:
			\begin{align}
				\frac 1{2i\pi}\int_\gamma\frac {f(\omega)-f(z)}{\omega - z}\dif \omega &= 0 \\
				\frac 1{2i\pi}\int_\gamma\frac {f(\omega)}{\omega-z}\dif \omega &= \frac 1{2i\pi}\int_\gamma\frac {f(z)}{\omega - z}\dif \omega \\
				\frac 1{2i\pi}\int_\gamma\frac {f(\omega)}{\omega-z}\dif \omega &= f(z)\Ind_\gamma(z).
			\end{align}
			\end{proof}

			\begin{déf} Pour $z \in \C$ et $R > 0$, on définit $\mathcal C^+(z_0, R]$, le cercle centré en $z_0$ et de rayon $R$ orienté positivement.

			Symétriquement, $\mathcal C^-(z_0, R]$ est le cercle centré en $z_0$ de rayon $R$ orienté négativement.
			\end{déf}

			\begin{thm}[Théorème de représentabilité en série de puissances] Soit $\Omega \subset \C$ un ouvert. Soit $f \in H(\Omega)$. Alors \textit{$f$ est
			développable en série de puissances}, i.e.~:
			\begin{equation}
				\forall z_0 \in \Omega : \forall R > 0 \tq D(z_0, R[ \subset \Omega : \exists (c_n)_{n \in \N} \in \C^\N \tq R(\sum c_nz^n) \geq R,
			\end{equation}
			et~:
			\begin{equation}
				\forall z \in D(z_0, R[ : f(z) = \sum_{n \in \N}c_n(z-z_0)^n.
			\end{equation}
			\end{thm}

			\begin{proof} Soient $z_0 \in \Omega$ et $R > 0$ tels que $D(z_0, R[ \subset \Omega$. Soit $r \in (0, R)$. Posons $\gamma = \mathcal C^+(z_0, r]$.

			Appliquons la formule de Cauchy à $f$ sur $D(z_0, R[$ (qui est un convexe ouvert) par le chemin fermé $\gamma$. Pour $\abs {z-z_0} < r$, on a~:
			\begin{equation}
				f(z)\Ind_\gamma(z) = \frac 1{2i\pi}\int_\gamma \frac {f(\omega)}{\omega - z}\dif \omega.
			\end{equation}

			Or $\Ind_\gamma(z) = 1$. Donc $f(z) = \frac 1{2i\pi}\int_\gamma \frac {f(\omega)}{\omega - z}\dif \omega$. Or la fonction qui envoie $z$ sur
			$\int_\gamma\frac {f(\omega)}{\omega-z}\dif \omega$ est développable en série de puissances par le théorème~\ref{thm:frac C0m DSP}.

			Ainsi, il existe $(c_n)_{n \in \N} \in \C^\N \tq R(\sum c_nz^n) \geq r$, et~:
			\begin{equation}
				\forall z \in D(z_0, r[ : f(z) = \sum_{n \geq 0}c_n(z-z_0)^n.
			\end{equation}

			On en déduit que $f \in C^\infty(D(z_0, r[)$ et~:
			\begin{equation}
				\forall n \geq 0 : c_n = \frac {f^{(n)}(z_0)}{n!}.
			\end{equation}

			On a alors $\forall r \in (0, R) : R(\sum c_nz^n) \geq r$, et donc $R(\sum c_nz^n) \geq R$. Et on en conclut finalement~:
			\begin{equation}
				\forall z \in D(z_0, R[ : f(z) = \sum_{n \geq 0}\frac {f^{(n)}(z_0)}{n!}(z-z_0)^n.
			\end{equation}
			\end{proof}

			\begin{cor} Si $f \in H(\Omega)$, alors $f' \in H(\Omega)$.
			\end{cor}

			\begin{proof} On sait que $f$ est développable en série de puissances sur $\Omega$ par le théorème de représentabilité. Donc $f$ est dérivable,
			et de dérivée continue (par les théorèmes de séries de puissances). Dès lors, $f' \in H(\Omega)$.
			\end{proof}

			\begin{cor} $H(\Omega) = C^\infty(\Omega)$.
			\end{cor}

			\begin{thm}[Théorème de Morera] Soient $\Omega \subset \C$ ouvert et $f \in C^0(\Omega, \C)$. On suppose que pour tout triangle $\Delta \subset \Omega$,
			on a~:
			\begin{equation}
				\int_{\partial \Delta} f(\omega)\dif \omega = 0.
			\end{equation}

			Alors $f \in H(\Omega)$.
			\end{thm}

			\begin{proof} Soient $z_0 \in \Omega$, $U \subset \Omega$, un ouvert convexe tel que $z_0 \in U$. On pose pour $z \in U$~:
			\begin{equation}
				F(z) \coloneqq \int_{[z_0, z]}f(\omega) \dif \omega.
			\end{equation}

			Pour $z \in U \setminus \{z_0\}$, on a~:
			\begin{equation}
				\frac {F(z) - F(z_0)}{z-z_0} - f(z_0) = \frac 1{z-z_0}\int_{[z_0, z]}\frac {f(\omega)}{\omega-z}\dif \omega.
			\end{equation}

			Comme précédemment, la continuité en $z_0$ de $f$ assure que $F$ est $\C$-dérivable en $z_0$, avec $f(z_0) = F'(z_0)$.

			Par continuité de $f$, on a $F \in H(\Omega)$. Ainsi, par développement en série de puissances, on a $f = F' \in H(\Omega)$ par le corollaire précédent.
			\end{proof}

	\subsection{Conséquences du théorème de représentabilité en série de puissances}
		\subsubsection{Ordre d'un zéro --- Principe des zéros isolés}
			\begin{déf} Soit $A \subset \C$. On appelle \textit{point limite de $A$} tout $z \in \C$ tel que $\exists a : \N \to A : n \mapsto a_n$ injective
			telle que $a_n \xrightarrow[n \to \pinfty]{} z$.
			\end{déf}

			\begin{prp} $z \in \C$ est un point limite de $A \subset \C$ lorsque~:
			\begin{equation}
				\forall \varepsilon > 0 : D(z, \varepsilon[ \cap \left(A \setminus \{z\}\right) \neq \emptyset,
			\end{equation}
			ou encore~:
			\begin{equation}
				\forall \varepsilon > 0 : \abs {D(z, \varepsilon[ \cap \left(A \setminus \{z\}\right)} \not \in \N.
			\end{equation}
			\end{prp}

			\begin{proof} EXERCICE.
			\end{proof}

			\begin{ex}~:
			\begin{itemize}
				\item Les points limites de $[0, 1] \cup \{i\}$ sont $[0, 1]$~;
				\item les points limites de $(-i, 0)$ sont $[-i, 0]$.
			\end{itemize}
			\end{ex}

			\begin{thm}[Principe des zéros isolés] Soient $\Omega \subset \C$, une région, et $f \in H(\Omega)$. On pose~:
			\begin{equation}
				Z(f) \coloneqq \left\{z \in \Omega \tq f(z) = 0\right\} = f^{-1}(\{0\}).
			\end{equation}

			On a deux possibilités~:
			\begin{itemize}
				\item soit $Z(f) = \Omega$ (i.e. $f \equiv 0$)~;
				\item soit $Z(f)$ n'a pas de point limite dans $\Omega$.

				Dans le second cas, on a~:
				\begin{equation}
					\forall z_0 \in Z(f) : \exists! (m, g) \in \N^* \times H(\Omega) \tq g(z_0) \neq 0 \text{ et } \forall z \in \Omega : f(z) = (z-z_0)^mg(z).
				\end{equation}

				$m$ est appelé \textit{l'ordre de $z_0$ en tant que zéro de la fonction $f$}.
			\end{itemize}
			\end{thm}

			\begin{proof} Soit $A$ l'ensemble des points limites de $Z(f)$ dans $\Omega$. Par continuité de $f$, $Z(f) = f^{-1}(\{0\})$ est fermé. En particulier,
			on a $A \subset Z(f)$.
			Montrons que $A$ est fermé dans $\Omega$ (admis pour l'instant, en attente de la preuve).

			Soit $a \in Z(f)$. Soit $r > 0 \tq D(a, r[ \subset \Omega$. Par le théorème de représentabilité~:
			\begin{equation}
				\exists (c_n)_{n \in \N} \in \C^\N \tq R(\sum c_nz^n) \geq r \text{ et } \forall z \in D(a, r[ : f(z) = \sum_{n \in \N}c_n(z-a)^n.
			\end{equation}

			On a alors deux possibilités~:
			\begin{enumerate}
				\item $\forall n \in \N : c_n = 0 \Rightarrow f \equiv 0$~;
				\item $\exists n \in \N \tq c_n \neq 0$. De plus, $n \geq 1$ car $f(a) = 0 \Rightarrow c_0 = 0$.
			\end{enumerate}

			Posons $m \coloneqq \min\{n \geq 1 \tq c_n \neq 0\}$. Pour $z \in \Omega$, on pose~:
			\begin{subnumcases}
				{g(z) =}
					(z-a)^{-m}f(z) & si $z \neq a$ \\
					c_m & sinon.
			\end{subnumcases}

			On sait $g \in H(\Omega \setminus \{a\})$ et $f(z) = (z-a)^mg(z)$ pour $z \neq a$. Pour de tels $z$, on a~:
			\begin{equation}
				g(z) = (z-a)^{-m}f(z) = \sum_{n \in \N}c_{n+m}(z-a)^n.
			\end{equation}

			Cette formule est encore vraie en $z-a$ car~:
			\begin{equation}
				g(a) = c_m = \sum_{n \in \N}c_{n+m}\delta_{0,n} = \sum_{n \in \N}c_{n+m}(z-a)^n.
			\end{equation}

			Dès lors, on en déduit $g \in H(\Omega)$, et $g(a) \neq 0$. Donc un tel couple $(m, g)$ existe.

			\textbf{L'unicité est laissée en exercice}.

			Par continuité de $g$ sur $\Omega$, et puisque $g(a) \neq 0$, il existe $\delta > 0$ tel que~:
			\begin{equation}
				D(a, \delta[ \subset \Omega \text{ et } g(D(a, \delta[) \subset \C^*.
			\end{equation}

			Ainsi, $g$ ne s'annule pas sur $D(a, \delta[$, donc $f$ ne s'annule pas sur $D(a, \delta[ \setminus \{a\}$, et donc $a$ est un point isolé de $Z(f)$.
			\end{proof}

\end{document}
