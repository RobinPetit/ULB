\documentclass{article}

\usepackage{palatino, eulervm}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fullpage}
\usepackage[bottom]{footmisc}
\usepackage[parfill]{parskip}
\usepackage{amsmath, amsthm, amsfonts, amssymb}
\usepackage{titlesec}  % reduce space between paragraphs
\usepackage{mathtools}
\usepackage{mathdots}
\usepackage{commath}
\usepackage{hyperref}
\usepackage{interval}
\usepackage{stmaryrd}

\makeatletter
\def\thm@space@setup{%
	\thm@preskip=.4cm
	\thm@postskip=\thm@preskip%
}
\makeatother

\title{Calcul différentiel et intégral}
\author{R. Petit}
\date{Année académique 2015 - 2016}

\DeclareMathOperator{\arctanh}{arctanh}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\Imf}{Im}
\DeclareMathOperator{\intr}{int}
\DeclareMathOperator{\C}{\mathbb C}
\DeclareMathOperator{\K}{\mathbb K}
\DeclareMathOperator{\N}{\mathbb N}
\DeclareMathOperator{\Q}{\mathbb Q}
\DeclareMathOperator{\R}{\mathbb R}
\DeclareMathOperator{\Z}{\mathbb Z}
\DeclareMathOperator{\adh}{adh}
\DeclareMathOperator{\tq}{ t.q. }
\DeclareMathOperator{\Larea}{\mathcal L}
\DeclareMathOperator{\Uarea}{\mathcal U}
\DeclareMathOperator{\vol}{vol}
\DeclareMathOperator{\aire}{aire}
\DeclareMathOperator{\divop}{div}
\DeclareMathOperator{\rotop}{rot}
\DeclareMathOperator{\curl}{curl}

\newcommand{\scpr}[2]{{\left\langle#1, #2\right\rangle}}
\newcommand{\frrn}[2]{#1 : #2 \subseteq \R \to \R^n}
\newcommand{\frr}[2]{#1 : #2 \subseteq \R \to \R}
\newcommand{\frmr}[2]{#1 : #2 \subseteq \R^m \to \R}
\newcommand{\frmrn}[2]{#1 : #2 \subseteq \R^m \to \R^n}
\newcommand{\ab}{\interval ab}
\newcommand{\fabr}[1]{#1 : \ab \to \R}
\newcommand{\evf}[4]{\left[#1\left(#2\right)\right]_{#2=#3}^{#2=#4}}
\newcommand{\normfty}[1]{\norm {#1}_\infty}
\newcommand{\divg}[1]{\scpr \nabla {#1}}
\newcommand{\rot}[1]{\nabla \times #1}
\newcommand{\diffvec}[3]{\left(\dif #1, \dif #2, \dif #3\right)}
\newcommand{\difxyz}{\diffvec xyz}
\newcommand{\geomsum}[2]{\frac {1 - #1^{#2+1}}{1 - #1}}
\newcommand{\intint}[2]{\left\llbracket#1, #2\right\rrbracket}  % integer interval
\newcommand{\aireint}{\aire_{\text{int}}}
\newcommand{\aireext}{\aire_{\text{ext}}}

\titlespacing\paragraph{0pt}{1pt plus 1pt minus 1pt}{4pt plus 1pt minus 1pt}
\titlespacing\subparagraph{0pt}{1pt plus 0pt minus 0pt}{1pt plus 1pt minus 1pt}

% amsthm
\newtheorem{thm}{Théorème}[section]
\newtheorem{prp}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollaire}
\newtheorem{lem}[thm]{Lemme}
\renewcommand{\proofname}{\it{Démonstration}}
\theoremstyle{definition}
\newtheorem{déf}[thm]{Définition}
\theoremstyle{remark}
\newtheorem*{rmq}{Remarque}

\begin{document}

\pagenumbering{Roman}
\maketitle
\tableofcontents
\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

\newpage

\section{Intuitions}
	\subsection{Les dérivées}
		\subsubsection{Définition}
			Alors que l'analyse nait au XVII$^e$ siècle, ce n'est que pendant le XIX$^e$ que les outils nécessaires à une
			expression rigoureuse ont été à disposition des mathématiciens. L'analyse a pour notion centrale celle de \textit{variation}.
			Intuitivement, la notion de variation instantanée d'une quantité $f(t)$ peut être décrite de la sorte~:
			\[V(t, \Delta t) = \frac {f(t + \Delta t) - f(t)}{\Delta t}\]

			pour une valeur $\Delta t$ étant \textit{de plus en petite}. On dit donc que $\Delta t$ \textit{tend vers 0}. Attention
			cependant car le manque de rigueur et le manque d'outils adaptés à la manipulation de données infinitésimales amènent à des
			paradoxes et des résultats illogiques voire inexplicables.

			Selon la définition de variation vue ci-dessus, nous pouvons exprimer la dérivée comme étant la variation instantanée d'une
			fonction $f : \R \to \R$ que nous notons $f'(t)$. Plus précisément, la variation \textit{instantanée} implique
			que $\Delta t$ soit \textit{infiniment petit}, ce qui s'écrit ainsi~:
			\[f'(x) = \lim_{\Delta t \to 0}\frac {f(t + \Delta t) - f(t)}{\Delta t}\]

			\begin{rmq} Pour des \textit{petites valeurs} de $\Delta t$, l'approximation suivante est admissible~:
			\[f(t + \Delta t) \simeq f(t) + f'(t)\Delta t\]
			\end{rmq}

		\subsubsection{Exemples}
			\paragraph{Fonction affine}
				Soit une fonction $f : \R \to \R : x \mapsto ax + b$. La dérivée de $f$ est l'expression suivante~:
				\[f'(x) = \lim_{h \to 0}\frac {f(x + h) - f(x)}{h} = \lim_{h \to 0}\frac {a(x + h) + b - (ax + b)}{h} =
				  \lim_{h \to 0}\frac {ax + ah + b - ax - b}{h} = \lim_{h \to 0}\frac {ah}{h} = a\]

				Il \textit{doit} sembler intuitif qu'étant donné que la fonction $f$ est une fonction dont le graphe est une droite, sa
				dérivée (donc sa variation instantanée) est constante sur tout son domaine du fait que la variation d'une fonction uniformément
				(dé)croissante est constante.

				Plus spécifiquement, si $a = 0$, la fonction est une fonction \textit{constante} : $f : \R \to \R : x \mapsto b$.
				De ce fait, sa dérivée doit être nulle car une fonction croissante n'a pas de variation (par définition). La quantification de
				cette variation doit donc être représentée par 0. Cela peut se montrer également aisément depuis la définition de la dérivée~:
				\[f'(x) = \lim_{h \to 0}\frac {f(x+h) - f(x)}{h} = \lim_{h \to 0}\frac {b - b}{h} = 0\]

			\paragraph{Fonction du second degré}
				Soit une fonction $f : \R \to \R : x \mapsto ax^2 + bx + c$. Sa dérivée peut être calculée ainsi~:

				\[
					\begin{aligned}
						  f'(x) &= \lim_{h \to 0}\frac {f(x+h) - f(x)}{h} = \lim_{h \to 0}\frac {a(x+h)^2 + b(x+h) + c - (ax^2 + bx + c)}{h} \\ &=
						  \lim_{h \to 0}\frac {ax^2 + 2axh + ah^2 + bx + bh + c- ax^2 - bx - c}{h} = \lim_{h \to 0}\frac {2axh + ah^2 + bh}{h} =
						  \lim_{h \to 0} 2ax + ah + b = 2ax + b
					\end{aligned}
				\]

				Ici, la dérivée est une fonction de $x$, ce qui indique que selon la valeur de $x$ à laquelle nous voulons évaluer la
				dérivée, la variation représentée est susceptible de différer.

			\paragraph{Fonction valeur absolue}
				Soit une fonction $f : \R \to \R : x \mapsto \left\{\begin{aligned}&\text{$+x$ si $x \ge 0$} \\&\text{$-x$ si $x < 0$}\end{aligned}\right.$.
				Afin de déterminer l'accroissement instantané de $f$ au point d'abscisse $x=0$, il faut repasser par la définition~:
				\[f'(a) = \lim_{h \to 0}V_f(a, h)\]

				Donc $f'(0)$ peut être déterminé de la manière suivante~:
				\[f'(0) = \lim_{h \to 0}V_f(0, h) = \lim_{h \to 0}\frac {f(0+h) - f(0)}{h} = \lim_{h \to 0}\frac {f(h)}{h}\]

				Les cas $h < 0$ et $h \ge 0$ doivent être traités séparément de par la définition de la fonction. Pour le premier cas,
				la dérivée est~:
				\[f'(0) = \lim_{h \to 0} \frac hh = 1\]

				Et pour le second cas,
				\[f'(0) = \lim_{h \to 0} \frac {-h}{h} = -1\]

				Le résultat est tout à fait cohérent par rapport au graphique de la fonction $x \mapsto |x|$ car cette fonction a deux
				accroissements différents~: l'un à gauche de 0, l'autre à droite de 0. Cependant, il est impossible d'exprimer la valeur
				de la dérivée de $f$ au point 0. On dit de $f$ qu'elle n'est pas dérivable en 0.

			\paragraph{Fonction exponentielle}
				Soit une fonction $f : \R \to \R : x \mapsto a^x$ avec $a \in \R_0^+$. À nouveau, pour définir sa
				dérivée, il nous faut repasser par la définition~:
				\[f'(x) = \lim_{h \to 0}\frac {a^{x+h} - a^x}{h} = \lim_{h \to 0}\frac {a^x(a^h - 1)}{h} = a^x\lim_{h \to 0}V(0, h) = a^xf'(0)\]

				Ce développement nous indique que pour connaitre la dérivée de $f$ au point $x$, il nous faut connaitre la dérivée de $f$
				au point $0$. Il est possible d'observer sur des esquisses de graphique que $f'(0)$ dépend de $a$ de manière croissante.
				Faire croître $a$ impliquera une croissance de $f'(0)$. Il existe cependant un nombre $\in \R$ tel que $f'(0) = 1$.
				Ce nombre est $e \simeq 2.718$, ce qui implique $f'(x) = e^xf'(0) = e^x = f(x)$. La fonction $x \mapsto e^x$ (et ses
				multiples) sont leur propre dérivée.

		\subsubsection{Dérivée d'ordre supérieur}
			Étant donné que la dérivée d'une fonction quelconque $f$ (en supposant qu'elle est dérivable) est également une fonction \\
			($f' : \R \to \R : x \mapsto f'(x)$), nous pouvons à nouveau dériver cette fonction (en supposant que la dérivée
			soit toujours dérivable). Il est alors question de \textit{dérivée seconde}, telle que $f''(x) = (f')'(x) = ((f)')'(x)$.
			Pour noter la $k^e$ dérivée, il existe la notation suivante~: $f^{(k)}$ (car répéter $k$ fois le symbole \textit{prime} (')
			est contre-productif en temps de lecture \textbf{et} d'écriture).

			Ces mêmes dérivées d'ordre supérieur à 1 ont leur importance et leur cohérence~: tant $f'(x)$ est la variation de la quantité
			$f(x)$, tant $f''(x)$ est la variation de $f'(x)$. La dérivée seconde représente donc l'accroissement de l'accroissement
			(ou la variation de la variation). Elle nous donne donc une information sur comment la variation évolue (en dynamique,
			si on représente la position d'un mobile par la fonction $x(t)$, $x'(t)$ représente la variation de la position, à savoir la
			vitesse (donc $x'(t) = v(t)$). Cependant, $x''(t)$ représente la variation de la vitesse, à savoir l'accélération, d'où
			$x''(t) = v'(t) = a(t)$).

			De plus, la dérivée seconde a une autre interprétation graphique~: si $f''(x) > 0$, nous savons que $f'(x)$ a une pente positive
			donc $f'(x)$ est croissante. Cela implique que $f(x)$ est croissante aussi mais de plus en plus croissante. Autrement dit,
			le graphe de $f(x)$ est \textit{concave} aux alentours de $(x, f(x))$. De manière similaire, lorsque $f''(x) < 0$, $f(x)$ est
			de moins en moins décroissante et donc le graphe de $f(x)$ est \textit{convexe} aux alentours de $(x, f(x))$.

		\subsubsection{Notation de Leibniz}
			En reprenant la définition de la variation donnée plus haut, nous pouvons définir $\delta f$ et $\delta x$ comme étant
			respectivement la variation de la quantité $f(x)$ et la variation de la quantité $x$. Nous avons donc~:
			\[V(x, h) = \frac {f(x+h) - f(x)}{h} = \frac {\delta f}{\delta x}\]

			Cela implique que nous ayons $\delta f = f(x + h) - f(x)$, ce qui est bien la variation de la quantité $f(x)$, et que nous
			ayons $\delta x = h = (x + h) - (x)$, ce qui est bien la variation de la quantité $x$.

			Or, nous avions défini la dérivée comme étant la variation instantanée, à savoir $f'(x) = \lim_{h \to 0}V(x, h)$, qui selon
			la notation de Leibniz correspond à $f'(x) = \lim_{h \to 0}\frac {\delta f}{\delta x}$. Leibniz a cependant instauré une
			seconde notation correspondant non plus à la variation comme la notation $\delta$ mais bien à la dérivée. Cette notation est~:
			\[\od{f}{x} = \lim_{h \to 0}\frac {\delta f}{\delta x}\]

			Attention cependant à ne pas utiliser cette valeur comme étant un quotient de nombres réels~: la notation $\od fx$ n'a de sens
			que lorsque la limite a été faite. Et comme $\lim_0 \frac {F(h)}{G(h)} \neq \frac {\lim_0 F(h)}{\lim_0 G(h)}$, nous ne pouvons
			pas séparer $\dif f$ et $\dif x$\footnote{Du moins pas dans le cas d'une dérivée.}.

			Leibniz a également eu besoin d'une notation pour la $k^e$ dérivée. Ayant considéré $\frac {\dif{}}{\dif x}$ comme étant
			une opération à réaliser $k$ fois, il a noté la $k^e$ dérivée de la sorte~:

			\[\od[k] fx\]

	\subsection{Règles de dérivation}
		\paragraph{Somme}
			Soient $f, g : \R \to \R$, deux fonction dérivables en $a$. De manière intuitive, nous pouvons dire que
			$(f+g)'(a) = f'(a) + g'(a)$ car l'accroissement de la fonction de somme est la somme des accroissements~: au point $a$,
			la fonction $(f+g)$ \textit{subit} un accroissement égal à l'accroissement de $f$ plus l'accroissement de $g$.

		\paragraph{Produit}
			Soient $f, g : \R \to \R$, deux fonction dérivables en $a$. Contrairement à ce que nous serions tentés de dire
			naïvement, nous ne pouvons pas définir $(fg)'(a) = f'(a)g'(a)$. C'est à Leibniz que l'on doit cette démonstration.

			Considérons un rectangle de dimensions $f(a)$ et $g(a)$. La quantité $(fg)(a)$ correspond à l'aire de ce rectangle.
			Si on passe de $a$ à $a+h$, nous obtenons un nouveau rectangle de dimensions $f(a+h)$ et $g(a+h)$. Selon l'approximation
			vue au point 1.1.1., nous savons que $f(a+h) \simeq f(a) + f'(a)h$. L'aire du nouveau rectangle est donc $A = f(a+h)g(a+h)
			\simeq (f(a) + f'(a)h)(g(a) + g'(a)h) = f(a)g(a) + f'(a)g(a)h + f(a)g'(a)h + f'(a)g'(a)h^2$.
			Rappelons tout de même que la dérivée ici est égale à~:
			\[\begin{aligned}(fg)'(a) &= \lim_{h \to 0}\frac{(fg)(a+h) - (fg)(a)}{h} = \lim_{h \to 0}\frac {f'(a)g(a)h + f(a)g'(a)h + f'(a)g'(a)h^2}{h} \\
									  &= \lim_{h \to 0}f'(a)g(a) + f(a)g'(a) + f'(a)g'(a)h = f'(a)g(a) + f(a)g'(a)\end{aligned}\]

			Nous avons donc la variation instantanée de la fonction $(fg)$ au point $a$~: $(fg)'(a) = f'(a)g(a) + f(a)g'(a)$.

		\paragraph{Composition}
			Soient $f, g : \R \to \R$, deux fonctions telles que $g$ est dérivable en $a$, et $f$ est dérivable en $g(a)$.
			Il est toujours possible de trouver la dérivée de la fonction composée $(f \circ g)(x) = f(g(x))$ à l'aide de l'approximation
			$f(a+h) \simeq f(a) + f'(a)h$ tant que $h$ est \textit{petit}. Notre dérivation devient donc~:
			\[
				\begin{aligned}
					(f \circ g)'(a) &= \lim_{h \to 0}\frac {(f \circ g)(a+h) - (f \circ g)(a)}{h} = \lim_{h \to 0}\frac {f(g(a+h)) - f(g(a))}{h}
					= \lim_{h \to 0}\frac {f(g(a) + g'(a)h) - f(g(a))}{h} \\
									&= \lim_{h \to 0}\frac {f(g(a)) + f'(g(a))g'(a)h - f(g(a))}{h} = f'(g(a))g'(a)
				\end{aligned}
			\]

		\paragraph{Réciproque}
			Soit $f : \R \to \R$. Si $f$ est bijective, nous pouvons définir sa fonction réciproque $f^{-1} : \R \to \R$.
			Si en plus, nous supposons que $f'(x) \neq 0 \forall x \in \R$, nous pouvons exprimer la dérivée de la fonction
			réciproque comme suit~:
			\[(f^{-1})'(f(a)) = \frac {1}{f'(a)}\]

			selon le développement suivant (nécessitant la dérivée d'une composée)~: en sachant que la dérivée de la fonction identité
			est 1 ($(x \mapsto x)'(x) = 1$) et que la composée d'une fonction $f$ avec sa réciproque ($f^{-1}$) correspond à la
			fonction identité, nous pouvons trouver la dérivée de la réciproque.
			\[
				\begin{aligned}
					id'(x) &= 1 \\
					(f^{-1} \circ f)'(x) &= 1 \\
					(f^{-1})'(f(x))f'(x) &= 1 \\
					(f^{-1})'(f(x)) &= \frac {1}{f'(x)}
				\end{aligned}
			\]

			Ou encore~:

			\[(f^{-1})'(a) = \frac {1}{f'((f^{-1})(a))}\]

		\begin{rmq} De manière graphique, ce résultat peut s'interpréter de la manière suivante~: la réciproque d'une fonction bijective est son image par
		symétrie orthogonale d'axe $y = x$. En traçant cet axe de symétrie, on peut observer qu'aux points $(a, f(a)) \in \Gamma_f$ et
		$(b, (f^{-1})(b)) \in \Gamma_{f^{-1}}$ tel que $b = f(a)$, nous avons un rapport entre les pentes des tangentes. Plus précisément, nous pouvons
		observer que la pente de la tangente au second point ($(f^{-1})'(b) = (f^{-1})'(f(a))$) correspond à l'inverse de la pente de la tangente au second
		point. Donc nous avons bien la même égalité~: $(f^{-1})'(f(a)) = \frac {1}{f'(a)}$. \end{rmq}

		\paragraph{Exemple des fonctions exponentielles et logarithmiques} Il a été vu plus haut que la fonction $x \mapsto \exp(x)$
		admettait pour dérivée elle-même. Nous avons également vu ci-dessus que $(f^{-1})'(a) = \frac {1}{f'((f^{-1})(a))}$.
		En étant tenté de trouver $f(x)$ telle que $f'(x) = f(x) \, \forall x \in \dom f$, nous aurions donc $(f^{-1})'(a) = \frac {1}{f((f^{-1})(a))} = \frac 1a$.
		Cependant, nous connaissons une fonction de ce genre~: la fonction exponentielle de base $e$. Sa fonction inverse, la fonction
		logarithmique de base $e$ a sa dérivée calculée de la sorte~:
		\[\log'(x) = \frac {1}{\exp(\log(x))} = \frac 1x\]

	\subsection{Les intégrales}
		\subsubsection{Définition}
			En analyse, il existe un outil permettant de calculer l'aire en dessous d'une courbe. Cet outil s'appelle \textit{intégrale}.
			Soit une fonction $f : \R \to \R$, nous définissons l'aire entre la courbe de $f$ et l'axe horizontal $y = 0$
			définie entre les droites $x = a$ et $x = b$ de la manière suivante~:
			\[A = \int_a^bf(x)\dif x\]

			\begin{rmq} Cette aire est dite \textit{signée}. C'est à dire que les parties \textit{en dessous} de l'axe $y = 0$ sont représentés par une valeur
			réelle négative alors que les parties \textit{au dessus} de l'axe $y = 0$ sont représentés par une valeur réelle positive. \end{rmq}

			Afin d'approximer l'aire que nous tentons de déterminer, nous découpons l'intervalle $[a, b]$ en $n$ \textit{morceaux} $[x_{i-1}, x_i] \, 1 \leq i \leq n$
			avec $x_i = a + (b-a)\frac in$. Ces \textit{arrêtes} nous permettent d'obtenir des rectangles de hauteur respective $f(x_{i-1})$.
			L'aire de chacun de ces rectangles est donc $b \times h = (x_i - x_{i-1})f(x_{i-1})$. L'approximation de l'aire recherchée (donc de
			l'intégrale) est la somme des aires de ces rectangles.
			\[\int_a^bf(x) \dif x \simeq \sum_{i=1}^n(x_i - x_{i-1})f(x_{i-1})\]

			Pour obtenir l'aire exacte, il faut faire tendre $n$ vers $+\infty$.

			\paragraph{Exemple de la fonction carrée}
				Si nous désirons trouver l'aire sous la courbe de $x \mapsto x^2$ sur $[0, t]$, nous la déterminons ainsi~:

				\[\begin{aligned}
					\int_0^tx^2\dif x &= \lim_{n \to \infty}\sum_{i=1}^n(x_i-x_{i-1})(x_{i-1})^2 = \lim_{n \to \infty}\sum_{i=1}^n\frac tn\left(t\frac {i-1}{n}\right)^2 =
					\lim_{n \to \infty}\frac {t^3}{n^3}\sum_{i=1}^n(i-1)^2 \\
									  &= \lim_{n \to \infty}\frac {t^3}{n^3}\frac {n(2n-1)(n-1)}{6} = \lim_{n \to \infty}\frac {t^3(2n-1)(n-1)}{6n^2} =
					t^3\lim_{n \to \infty}\frac {2n^2 + 1 - 3n}{n^2} \\
									  &= t^3\lim_{n \to \infty}\left(2 + \frac {1}{n^2} - \frac 3n\right) = t^3\frac 26 = \frac {t^3}{3}
				\end{aligned}\]

	\subsection{Théorème fondamental du calcul différentiel et intégral}
		\subsubsection{1ère version}
				En analyse, il existe un théorème (parfois appelé \textit{théorème fondamental de l'analyse}) qui définit la dérivation et l'intégration
				comme deux opérations inverses l'une de l'autre. Avant d'exprimer le théorème, tentons de donner un peu d'intuition au rapport
				entre dérivation et intégration.

				Soit $F(x)$, une fonction réelle définie par $F(x) = \int_a^xf(t)\dif t$ telle que $f$ est une fonction réelle continue. Admettons que
				$F(x)$ est dérivable sur l'intégralité de son domaine. Nous pouvons donc déterminer sa dérivée comme étant l'accroissement instantané~:
				\[F'(x) = \lim_{h \to 0}V_F(x, h) = \lim_{h \to 0}\frac {F(x+h) - F(x)}{h} = \lim_{h \to 0} \frac 1h\int_x^{x+h}f(t)\dif t\]

				Ce que représente $\int_x^{x+h}f(t)\dif t$ est l'aire en dessous de la courbe de $f$ entre les points d'abscisse $x$ et $x+h$.
				$h$ étant tout petit (tendant vers l'infini), nous pouvons approximer $f(t)$ constant entre $x$ et $x+h$. L'aire représentée par
				l'intégrale est donc $\simeq hf(x)$. Nous pouvons donc poursuivre le calcul de la dérivée de $F$~:
				\[F'(x) = \lim_{h \to 0} \frac 1h\int_x^{x+h}f(t)\dif t = \lim_{h \to 0} \frac 1hhf(x) = f(x)\]

				Nous avons donc $F(x) = \int_a^xf(t)\dif t$ et $f = F'(x)$. Nous pouvons dès à présent énoncer une première version du théorème~:

				\textit{Soit $f : \R \to \R$, une fonction continue sur $\R$, alors la fonction $F(x)$ définie par}
				\[F(x) = \int_a^xf(t)\dif t\]

				\textit{est dérivable sur l'intégralité de son domaine tel que $F'(x) = f(x)$.}

		\subsubsection{2nde version}
				Il existe une deuxième manière d'énoncer ce théorème. À nouveau, tentons de le trouver intuitivement. En ayant une fonction
				$f : \R \to \R$ dérivable en tout point de son domaine, nous pouvons considérer l'intégrale de $a$ en $b$ de $f'(x)$
				comme étant la somme des variations instantanées de $f$ sur l'intervalle $[a, b]$. Tentons maintenant de définir plus précisément
				ce que vaut cette intégrale.
				\[\int_a^bf'(t)\dif t = \lim_{n \to \infty}\sum_{i=1}^n\frac {b-a}{n}f'(x_{i-1})\]

				\paragraph{Rappel} Au point 1.1.1., nous avons vu l'approximation suivante~: $f(a+h) \simeq f(a) + f'(a)h$. En réorganisant cette
				approximation, nous avons $f'(a)h \simeq f(a+h) - f(a)$. Dans notre cas, si nous posons $h = \frac {b-a}{n}$, nous pouvons poursuivre
				notre intégrale.
				\[
					\begin{aligned}
						\int_a^bf'(t)\dif t &= \lim_{n \to \infty}\sum_{i=1}^nhf'(x_{i-1}) =  \lim_{n \to \infty}\sum_{i=1}^nf(x_{i-1} + h) - f(x_i) \\
											&=  \lim_{n \to \infty}f(x_n) - f(x_0) = f(b) - f(a)
					\end{aligned}
				\]

				Nous avons considéré cette intégrale comme étant la somme des variations instantanées de $f$ entre $a$ et $b$, et le résultat de
				ce développement est que la somme des variations instantanées est égale à la variation totale pour aller de $a$ à $b$.

				Le théorème peut donc être exprimé d'une deuxième manière~:

				\textit{Soit $f : \R \to \R$, une fonction réelle dérivable, alors}
				\[\int_a^bf'(t)\dif t = f(b) - f(a).\]

	\subsection{Primitives}
			En analyse, il est fréquent de devoir trouver $F$ tel que $F'(x) = f(x)$. C'est donc une recherche de fonction. Cependant, comme
			nous venons de le voir, trouver une fonction en connaissant sa dérivée revient à réaliser une intégrale (version 1 du théorème fondamental
			de l'analyse). Une solution $F$ satisfaisant $F'(x) = f(x)$ est appelée une \textit{primitive} de $f$. $\int f$ ou $\int f(t)\dif t$ sont
			les manières les plus courantes d'écrire \textit{primitive de $f$}\footnote{On parle parfois également d'intégrale indéfinie.}.
			Cependant, comme le laisse comprendre la seconde version du théorème fondamental de l'analyse, il existe une infinité de primitives,
			toutes définies à \textit{une constante près}. Nous généralisons donc «  la » primitive de $f$ en $\int f + C$, $C \in \R$.

	\subsection{Règles d'intégration}
			Tout comme il y a des règles de dérivation (point 1.1.5.), il existe des règles d'intégration.

			\paragraph{Produit}
				Tout comme $(fg)'(x) \neq f'(x)g'(x)$, $\int (fg)(x)\dif x \neq \int f(x)\dif x \int g(x) \dif x$. Pour réussir à intégrer un
				produit, il faut partir de la règle de Leibniz pour la dérivation~:
				\[
					\begin{aligned}
						(fg)'(x) &= f'(x)g(x) + f(x)g'(x) \\
						\int_a^b (fg)'(x) \dif x &= \int_a^b (f'(x)g(x) + f(x)g'(x)) \dif x \\
						[(fg)(x)]_a^b &= \int_a^b (f'(x)g(x) + f(x)g'(x)) \dif x \\
						[(fg)(x)]_a^b &= \int_a^b f'(x)g(x) \dif x + \int_a^b f(x)g'(x) \dif x \\
						\int_a^b f'(x)g(x) \dif x &= [(fg)(x)]_a^b - \int_a^b f(x)g'(x) \dif x
					\end{aligned}
				\]

				Il faut donc, pour pouvoir intégrer un produit, considérer un des deux facteurs comme étant une dérivée (qu'il faudra donc
				intégrer pour avancer).

			\paragraph{Composition}
				La règle \textit{d'intégration en chaine} est également appelée \textit{changement de variable}. Elle peut être exprimée comme
				suit. Soient $f$ et $g$, deux fonctions continûment dérivables, $g(\alpha) = a$ et $g(\beta) = b$, alors~:
				\[\int_a^bf(x)\dif x = \int_\alpha^\beta f(g(t))g'(t)\dif t.\]

				Cela veut dire qu'une intégration peut être résolue en \textit{transformant} l'intégrale de manière à faire apparaitre une
				composition. La justification de cette formule peut être donnée comme suit~:
				\[(F(g(t)))'(x) = F'(g(t))g'(t) = f(g(t))g'(t)\]

				Donc~:
				\[\int_\alpha^\beta f(g(t))g'(t) \dif t = F(g(\beta)) - F(g(\alpha)) = F(b) - F(a) = \int_a^bf(t)\dif t.\]

	\subsection{Les équation différentielles}
		Une équation différentielle est une équation dont l'inconnue est une fonction $f$ et dans laquelle les dérivées de $f$ apparaissent.

		\subsubsection{équation différentielle linéaire d'ordre 1}
			Comme le dit la pseudo-définition ci-dessus, une équadiff est une équation où l'inconnue est une fonction. Le cas de $F = \int_a^xf(t)\dif t$
			est un cas particulier d'une grande famille d'équadiffs que l'on appelle \textit{équation différentielle linéaire d'ordre 1}.
			Cette famille est caractérisée par la forme suivante~:
			\[f'(x) + p(x)f(x) = q(x).\]

			Une manière de résoudre une telle équation est de déterminer une fonction auxiliaire $a(x)$ que l'on va multiplier de part et d'autre
			de l'égalité~:
			\[a(x)f'(x) + a(x)p(x)f(x) = q(x)a(x)\]

			Le but de cette manipulation est de pouvoir faire ressortir $a(x)f'(x) + a'(x)f(x)$, ce qui est égal à $(af)'(x)$. Cependant, pour cela,
			il faut choisir $a(x)$ telle que $a'(x) = a(x)p(x)$. Il existe une solution~:
			\[a(x) : x \mapsto e^{\int p(x)\dif x}\]

			Maintenant que nous avons cette fonction, il suffit de résoudre $(af)'(x) = (aq)(x)$. Si $(aq)$ est continue, le théorème fondamental de
			l'analyse assure l'existence d'une primitive $b(x) = \int a(x)q(x)\dif x$. À présent, nous savons que si $(af)'(x) = (aq)(x)$, alors
			$(af)(x) = b(x)$, ou encore $f(x) = \frac {b(x)}{a(x)}$.

			Nous avons donc une solution à l'équation de départ~:
			\[f(x) = \frac {\int \left(e^{\int p(x)\dif x}\right)q(x)\dif x}{e^{\int p(x)\dif x}}.\]

		\subsubsection{Unicité de la solution et problème de Cauchy}
			Lorsque l'on \textit{transforme} le problème initial (équadiff linéaire d'ordre 1) en un problème conditionné (en précisant
			$f(x_0) = y_0$), nous limitons le nombre de solutions à 1. Si nous avons deux fonctions $f_1$ et $f_2$, solutions d'une équadiff linéaire
			d'ordre 1, et que nous définissons une autre fonction $g$ telle que $g(x) = (f_1 - f_2)(x)$, ladite fonction $g$ est une solution de
			l'équation suivante~:
			\[g'(x) + p(x)g'(x) = 0\]

			respectant, de plus $g(x_0) = 0$.

			Dans l'équation ci-dessus, le fait que $q(x) = 0 \forall x$ implique que $(ag)'(x) = 0 \forall x$ également, donc $(ag)(x) = C \forall x$.
			Autrement dit, $(ag)(x)$ est une fonction constante telle que $(ag)(x) = C \forall x$. Comme on sait que $g(x_0) = 0$, on sait que
			$e^{P(x_0)}g(x_0) = 0$ (où $P(x)$ est une primitive de $p(x)$), donc $C = 0$. Cependant, comme $(e^P)(x)$ ne peut s'annuler, il faut $g(x) = 0 \forall x$.

			Ce qui veut donc dire que $f_1$ et $f_2$, les deux fonctions solutions trouvées pour une équadiff linéaire d'ordre 1, sont les mêmes (vu que leur différence
			est la fonction nulle). Il existe donc \textbf{une et une seule} solution à l'équadiff linéaire d'ordre 1 conditionnée (problème de Cauchy).

		\subsubsection{équadiffs linéaire d'ordre 2 à coefficients constants}
			Une équadiff d'ordre 2 est sous la forme suivante~:
			\[\od[2]{}{x}f(x) + a\od{}{x}f(x) + bf(x) = 0\]

			avec deux paramètres réels $a, b \in \R$. Le terme \textit{linéaire} vient du fait que si $f_1, f_2 \in \R^{\R}$ sont deux solutions de
			cette équation, alors $c_1f_1 + c_2f_2$ en est également une (avec $C_1, C_2 \in \R$). Le fait que cette équation soit d'ordre 2 veut également
			dire que l'ensemble des solutions est un espace vectoriel de dimension 2.

			Pour résoudre une telle équation, il faut d'abord passer par ce que l'on appelle l'\textit{équation caractéristique}. Cette équation est la suivante~:
			\[\lambda^2 + a\lambda + c = 0.\]

			Cette équation étant du second degré, il faut séparer trois cas possibles~:
			\begin{itemize}
				\item l'équation admet deux solutions réelles distinctes $\lambda_1$ et $\lambda_2$~;
				\item l'équation admet une seule solution réelle $\lambda$~;
				\item l'équation n'admet aucune solution réelle.
			\end{itemize}

			Dans le premier cas (deux solutions distinctes), l'équation différentielle peut être réécrite sous la forme factorisée suivante~:
			\[\left(\od{}{x} - \lambda_1\right)\left(\od{}{x} - \lambda_2\right)f = 0.\]

			Les solutions $f_1(x) = e^{\lambda_1 x}$ et $f_2(x) = e^{\lambda_2 x}$ sont possible à \textit{deviner}. Le principe de linéarité exprimé juste au-dessus
			permet d'affirmer donc que $f(x) = C_1e^{\lambda_1 x} + C_2e^{\lambda_2 x}$ représente la famille des solutions paramétrées par $C_1$ et $C_2$.

			Dans le second cas (solution unique), l'équadiff peut être réécrite sous la forme factorisée suivante~:
			\[\left(\od{}{x} - \lambda\right)^2f = \od[2]{}{x}f + \lambda^2f - 2\lambda\od{}{x}f = 0.\]

			En posant $g(x) = \od{}{x}f(x) - \lambda f(x)$, nous avons $\od{}{x}g(x) = \od[2]{}{x}f(x) - \lambda\od{}{x}f(x)$. Cela nous permet de réécrire (encore une
			fois) l'équadiff sous la forme suivante~:
			\[\od[2]{}{x}f(x) - \lambda\od{}{x}f(x) - \lambda\left(\lambda\od{}{x}f(x) - \lambda f(x)\right) = \od{}{x}g(x) - \lambda(g(x)) = 0.\]

			La solution est $g(x) = Ke^{\lambda x}$. Or $g(x) = \od{}{x}f(x) - \lambda f(x)$. Donc il faut encore résoudre l'équation à l'aide de la méthode vue ci-dessus
			(ordre 1) afin de trouver la solution suivante. La solution finale est~:
			\[f(x) = \frac {\int \left(e^{\int p(x) \dif x}\right) q(x) \dif x}{e^{\int p(x) \dif x}}\]

			avec $p(x) = -\lambda$ et $q(x) = g(x) = K_1e^{\lambda x}$. D'où~:
			\[
				f(x) = \frac {\int \left(e^{\int -\lambda \dif x}\right) K_1e^{\lambda x} \dif x}{e^{\int -\lambda x \dif x}}
				     = \frac {\int K_2e^{-\lambda x}K_1e^{\lambda x} \dif x}{K_3e^{-\lambda x}}
				     = \left(K\int\dif x\right)e^{\lambda x}K_3^{-1} = K_3^{-1}K(x + C)e^{\lambda x} = (C_1x + C_2)e^{\lambda x}.
			\]

			Dans le dernier cas, l'équation caractéristique n'a pas de solution réelle. Il faut donc aller chercher du côté des nombres complexes. Les coefficients
			étant réels, les deux solutions complexes doivent être conjuguées l'une de l'autre (si $z = \alpha + \beta i$ et $\overline z = \alpha - \beta i$, alors
			$z\overline z = \alpha^2 + \beta^2 \in \R$ et $z + \overline z = 2\beta \in \R$). L'équadiff devient donc~:
			\[\left(\od{}{x} - z\right)\left(\od{}{x} - \overline z\right)f = 0.\]

			L'équation ressemble fortement à celle du premier cas, donc la solution générale est $f(x) = b_1e^{zx} + b_2e^{\overline z x}$ avec $b_1, b_2 \in \mathbb C$.
			Cependant, comme $\exp(zx) = \exp(\alpha x + i\beta x) = e^{\alpha x} \exp(i\beta x) = e^{\alpha x}(\cos(\beta x) + i\sin(\beta x))$ (et donc
			$\exp(\overline z) = e^{\alpha x}(\cos(\beta x) - i\sin(\beta x))$), en choisissant respectivement $b_1 = b_2 = \frac 12$ et $b_1 = -b_2 = -\frac i2$,
			on obtient respectivement $f(x) = e^{\alpha x}\cos(\beta x)$ et $f(x) = e^{\alpha x}\sin(\beta x)$. Nous avons donc deux solutions, et à nouveau, par
			linéarité, nous pouvons exprimer la famille des solutions paramétrée par $C_1, C_2 \in \R$~:
			\[f(x) = C_1e^{\alpha x}\cos(\beta x) + C_2e^{\alpha x}\sin(\beta x).\]

			Ici, notre solution ne fait plus intervenir quoi que ce soit de complexe ($\in \mathbb C$), tous les coefficients sont réels ($\alpha$ et $\beta$ sont des
			\textit{constantes} dépendantes de l'équation caractéristique).

		\subsubsection{équations de Newton}
			Après avoir étudié des équations linéaires, regardons une autre famille d'équations~: les \textbf{équations de Newton}. Cette famille est représentée
			par la forme suivante~:
			\[\od[2]{}{t}x(t) = f(x(t))\]

			avec $x : \R \to \R : t \mapsto x(t)$, l'inconnue et $f$, la fonction \textbf{continue} de \textit{force}. Une telle équation représente
			la position $x(t)$ en fonction du temps d'un mobile (de masse unitaire) soumis à une force $f$ dépendant de la position. Si on multiplie l'équation
			de part et d'autre par la quantité $x'(t)$, on obtient
			\[x^{(2)}(x)x'(t) - f(x(t))x'(t) = 0\]

			Ce que l'on peut intégrer afin d'avoir
			\[\frac 12(x'(t))^2 - F(x(t)) - K = 0,\]

			avec $F$, une primitive de $f$, ou encore
			\[(x'(t))^2 - 2F(x(t)) = E\]

			avec $E$, la constante d'intégration. Si l'équation différentielle est conditionnée (problème de Cauchy) telle que $x(0) = x_0$ et $x'(0) = v_0$, alors
			une solution pour cette équation est $x'(t) = \sqrt{E_0 + 2F(x(t))}$ avec $E_0 = v_0^2 - 2F(x_0)$.

				\paragraph{Exemple~: les équations de Fisher} Les équations de Fisher sont sous la forme suivante~: $u''(t) = (u - u^3)(t)$. La fonction $f$ de force
				est ici $f : \R \to \R : u(t) \mapsto u(t) - u^3(t)$, et a pour primitive $F : \R \to \R : \frac 12u^2(t) - \frac 14u^4(t) + C$.
				Pour des raisons de simplicité, ici, $C$ se verra attribuer la valeur $-\frac 14$. Donc $F(u(t)) = -\frac 14(u^2(t) - 1)^2$.

				Étant donné les solutions constantes à l'équation $u : t \mapsto K_u$ avec $K_u \in \{-1, 0, 1\}$, nous cherchons $u$ telle que~:
				\[\lim_{t \to \pm \infty}u(t) = \pm 1,\]
				et~:
				\[\lim_{t \to \pm\infty}u'(t) = 0.\]
				
				L'intégration première nous donne $u'(t)^2 = \frac 12(u^2(t) - 1)^2$, ou encore~:
				\[u'(t) = \frac {\sqrt{2}}{2}(u^2(t) - 1).\]

				Dans les intervalles de temps tels que $u'(t) > 0$ et $-1 \neq u(t) \neq 1$, nous avons~:

				\[\begin{aligned}
					\int_{t_0}^t \frac {1}{u^2 - 1}\dif u &= \int_{t_0}^t \frac {\sqrt{2}}{2} = \frac{\sqrt{2}}{2} (t - t_0) \\
					\arctanh(u(t)) - \arctanh(u(t_0)) &= \frac{\sqrt{2}}{2} (t - t_0)
				\end{aligned}\]

				Cependant, nous savons qu'$\exists t_0$ tel que $u(t_0) = 0$ vu que nous cherchons $-1 < u(t) < 1$. Supposons $t_0 = 0$, de manière à ce que
				l'équation devienne $\arctanh(u(t)) = \frac {\sqrt{2}}{2}t$ d'où $u(t) = \tanh(\frac t{\sqrt 2})$.

	\subsection{Problèmes et paradoxes}
		La majeure partie de l'analyse mathématique est basée sur la notion de limite, et surtout de limite infinie. Si cette notion est utilisée sans
		suffisamment de rigueur, un certain nombre de paradoxes peuvent apparaitre. Par exemple, soit la série $x, x^2, x^3, ...$ telle que
		$x_i = x^i$. Pour en connaitre la limite infinie, nous pouvons faire $L = \lim_{n\to\infty}x^n = \lim_{m\to\infty}x^{m+1} = x\lim_{m\to\infty}x^m = xL$.
		Nous avons donc $L = xL$, ou encore, $\forall x\neq1, L = 0$. Ce qui peut sembler contre-intuitif pour $x = 2$ ou $x = 3$ par exemple.
		Pareillement, si l'on désire mesurer l'hypoténuse d'un triangle $ABC$ avec $A = (0, 0), B = (1, 0), C = (0, 1)$, on peut approximer la
		longueur de l'hypoténuse comme la longueur d'un « escalier » de $N$ marches. Chaque marche est composée de deux segments de longueur $\frac 1N$.
		La longueur de l'escalier est donc $\frac {2N}{N}$, ou encore 2. Alors que le théorème de Pythagore nous dit que l'hypoténuse est de longueur
		$\sqrt 2$.

\newpage
\section{Les nombres réels}
	Le traitement de l'analyse peut être très puissant, cependant comme vu ci-dessus, un manque de rigueur peut amener à des contradictions voire à
	des résultats insensés. C'est pour cette raison qu'il faut instaurer cette rigueur dès les notions élémentaires telles que les nombres.

	\subsection{Axiomatique des nombres}
		\paragraph{Rappel} Les nombres sont organisés de la sorte~: $\N \subset \mathbb Z\subset \mathbb Q \subset \R$.

		Il faut savoir que chaque ensemble est défini selon des axiomes, et que c'est à partir de ces axiomes et de déductions logiques que sont
		construits les ensembles \textit{plus gros}. Ici, les axiomes correspondant aux ensembles $\N, \mathbb Z$ et $\mathbb Q$ sont considérés
		comme \textit{évidents} et seuls ceux de $\R$ sont explicités.

		\subsubsection{Axiomes de $\R$}
			Avant de citer les axiomes de $\R$, il faut introduire la notion de majorant et de minorant (pour l'axiome de complétude).

			Soit $A \subset \R$. On dit que $A$ est majoré $\iff \exists M \in \R | \forall a \in A, a \leq M$.

			De manière similaire, on dit que $A$ est minoré $\iff \exists m \in \R | \forall a \in A, a \geq m$.

			\begin{rmq} Le minorant/majorant ne doit pas nécessairement appartenir à $A$. \end{rmq}

			\begin{enumerate}
				\item $\R$ est un \textit{corps}
					\begin{itemize}
						\item $(\R, +)$ est un \textit{groupe commutatif}, donc la loi d'addition satisfait les conditions
							  suivantes~: associativité, commutativité, existence d'un élément neutre et existence d'un inverse~;
						\item $(\R \backslash \{0\}, .)$ est un \textit{groupe commutatif}~;
						\item La multiplication est distributive sur l'addition.
					\end{itemize}
				\item $(\R, \leq)$ est un corps entièrement ordonné
					\begin{itemize}
						\item la relation d'ordre $\leq$ satisfait les propriétés suivantes~: réflexivité, transitivité, antisymétrie
							  et ordre total~;
						\item $a \leq b \Rightarrow a + z \leq b + z \, \forall z \in \R$~;
						\item $a, b \leq 0 \Rightarrow ab \geq 0$.
					\end{itemize}
				\item $\R$ satisfait l'axiome de complétude. C'est cet axiome qui différencie grandement $\R$ de
					  $\mathbb Q$. Cet axiome de complétude dit ceci~:

					  \begin{itemize}
						\item $\forall A \subset \R$, si $A$ est non-vide et majoré, alors $A$ possède un majorant minimum appelé
							  \textit{supremum} de $A$ et noté $\sup A$.
						\item $\forall A \subset \R$, si $A$ est non-vide et minoré, alors $A$ possède un minorant maximum appelé
							  \textit{infimum} de $A$ et noté $\inf A$.
					  \end{itemize}

					  Et l'ensemble des rationnels $\mathbb Q$ ne respecte pas cet axiome.
			\end{enumerate}

		\subsubsection{Résultat de l'axiome~: les racines}
			Commençons par énoncer la propriété d'Archimède qui dit que $\forall y\in \R, \exists n \in \N \, | \, n > y$.
			La preuve de ce principe fait intervenir la notion de majorant vue ci-dessus.

			Soit $S = \{n \in \N \, | \, n \leq y\}$ avec $y \in \R$. Par définition, $y$ majore $S$. Il faut différencier les cas où
			$\#S = 0$ et $\#S > 0$. Dans le premier cas, $0 > y$, donc $n = 0$ est le nombre naturel recherché. Dans le second, posons $s = \sup S$.
			Comme $s \in S$, $s - 1$ n'est pas un majorant de $S$, $\Rightarrow \exists m \in S \, | \, s - 1 < m$, ou encore $m + 1 > s$. Or $s$
			majore $S$ donc $m + 1 \not \in S$. Si $m + 1 \not \in S$, alors $m+1 > y$. Donc $n = m + 1$ est le nombre naturel recherché.

			S'en suit le corollaire suivant~: $\forall y \in \R^+, \exists n \in \N \, | \, y > \frac 1n$. La preuve repose sur le lemme
			précédent~: soient $y \in \R^+, x = \frac 1y$. Le lemme d'Archimède dit qu'$\exists n \in \N \, | \, x < n$. Donc
			$\frac 1y < n$, ou encore $\frac 1n < y$. Il faut effectivement $y > 0$ car sinon lors de la dernière étape, nous avons $\frac 1n > y$,
			ce qui n'est pas possible pour $n \in \N$.

			Maintenant, intéressons-nous aux racines carrées et à l'affirmation de leur existence. Tentons de démontrer qu'
			$\exists x \in \R \, | \, x^2 = N \forall N \in \R$. Afin de démontrer ceci, procédons par l'absurde. Soient
			$S = \{y \in \R \, | \, y^2 \leq N\}$ et $x = \sup S$. Prouvons maintenant que $x^2 = N$.

			Si $x^2 < N$, posons $x_n = x + \frac 1n$ avec $n \in \N_0$. Donc
			$x_n^2 = x^2 + \frac {2x}{n} + \frac {1}{n^2} \leq x^2 + \frac {2x+1}{n} \, \forall n \in \mathbb N_0$. Comme $N - x^2 > 0$ et $2x + 1 > 0$ (vu
			que $x \geq 0$ du fait que $0 \in S$), la quantité $\frac {N - x^2}{2x + 1}$ est strictement positive également, donc
			$\exists n \in \N \, | \, \frac 1n \leq \frac {N - x^2}{2x + 1}$. Pour ce même $n$, nous avons $x_n^2 < N$ car
			$x_n^2 < x^2 + (2x+1)\frac 1n < x^2 + (2x+1)\frac {N - x^2}{2x + 1} = x^2 + N - x^2 = N$. Donc $x_n^2 \in S$, cependant $x_n = x + \frac 1n > x$,
			ce qui n'est pas possible.

			Inversement, si $x^2 > N$, on définit $x_n = x - \frac 1n$. D'où $x_n^2 >x^2 - \frac {2x+1}{x^2 - N}$. De plus, les quantités $x^2 - N$
			et $2x + 1$ sont toutes deux strictement positives, donc $\exists n \in \N \, | \, \frac 1n < \frac {x^2 - N}{2x + 1}$. Ce qui
			mène à $x_n^2 > N$. Or $x_n = x - \frac 1n < x$. Donc $x_n$ ne peut être un majorant de $S$. Cela implique qu'$\exists y \in S \, | \, y \geq x_n$,
			ou encore $y^2 \geq x_n^2 > N$, ce qui n'est pas possible car $y \in S \Rightarrow y^2 \leq N$.

			Donc si $x^2 \not < N$ et $x^2 \not > N$, alors $x^2 = N$. De plus, nous avons une définition d'une racine carrée (que l'on peut étendre à
			la racine $n^e$)~:
			\[x^\frac 1n = \sqrt[n] x = \sup \{y \in \R^+ \, | \, y^n \leq x\}.\]

			\begin{lem} Il s'en déduit que $\forall q = \frac mn \in \mathbb Q, x^q = (x^{\frac 1n})^m$. \end{lem}

			\begin{rmq} Ici, les puissances irrationnelles ne sont pas définies. Une telle définition viendra par la suite. \end{rmq}

	\subsection{Densité des rationnels}
		Nous avons vu l'ensemble $\R$ et sa partie rationnelle ($\mathbb Q$). Cependant, quelle est la proportion de ces nombres rationnels
		et donc quelle est la proportion de nombres irrationnels étant donné le résultat suivant~: $\forall x < y \in \R, \exists q \in \mathbb Q \, | \, x < q < y$ ?

		Commençons par prouver ce résultat. Séparons le problème en deux: le cas où $y - x > 1$ et le cas où $x < y$ quelconque. Dans le premier cas,
		$\exists n \in \N \, | \, n > x$ où $n$ est le plus petit entier plus grand que $x$. On en déduit que $n-1 \leq x$, ou encore
		$n \leq x + 1 < y$, ou encore $x < n < y$, prenons $q = n$. Dans le second cas, Archimède dit qu'$\exists m \in \N \, | \, m > \frac {1}{y - x}$.
		Cette inégalité peut se réécrire $my - mx > 1$, ce qui correspond au cas précédent. Prenons $q = \frac nm$, et nous avons $x < q < y$.

		Cela permet de se dire qu'il n'y a pas trop de points qui ont été ajoutés pour passer de $\mathbb Q$ à $\R$. Cependant, il faut savoir
		que $\#\R > \#\mathbb Q$, bien que ces deux ensembles soient infinis.

	\subsection{Inégalité triangulaire}
		L'inégalité triangulaire dit que $\forall x, y \in \R, |x + y| \leq |x| + |y|$. Pour le prouver, il faut savoir que $ab \geq 0 \Rightarrow |a + b| = |a| + |b|$.
		Cependant, quand $a \leq 0 \leq b$, alors $a - |b| = - |a| - |b| \leq a + b \leq |a| + b = |a| + |b|$. Or, pour avoir $x \leq -y$ et $x \geq y$,
		il faut $|x| \leq |y|$. Donc $|a + b| \leq ||a| + |b|| = |a| + |b|$.

		De manière similaire, on peut dire que $\forall x, y \in \R, |x - y| \geq |x| - |y|$. Il suffit pour le prouver de poser $x = a - b$ et $y = b$.
		De là, le lemme précédent s'applique de manière à ce que $|a| \leq |a - b| + |b|$, ce qui peut se réécrire comme suit~: $|a - b| \geq |a| - |b|$.

	\subsection{Autres corps}
		Avant tout, il faut définir ce qu'est un corps.

		L'ensemble $\mathbb K$ est un corps si, muni des opérations d'addition et de produit, il respecte les trois propriétés suivantes~:
		\begin{itemize}
			\item $(\mathbb K, +)$ est un groupe commutatif~;
			\item $(\mathbb K \backslash \{0\}, .)$ est un groupe commutatif~;
			\item le produit est distributif sur l'addition.
		\end{itemize}

		C'est principalement le corps des nombres réels qui sera étudié dans ce cours, cependant il en existe plein d'autres.

\newpage
\section{Les suites}
	Après avoir utilisé la notion de « limite » pour \textit{définir} les notions de dérivée et d'intégrale, il est nécessaire de définir précisément
	cette notion de limite (ou de \textit{convergence}).

	Une suite réelle (dans $\R$) est une liste infinie de $x_i \in \R$ indexés par $i \in \N$. Cette suite se note $(x_n)$, $(x_n)_n$, ou
	encore $(x_n)_{n \in \N}$.

	\subsection{Convergence et divergence}
		On dit d'une suite $(x_n)$ qu'elle converge en $a \in \R \iff \forall \epsilon > 0, \exists N \in \N \, | \, n \geq N \Rightarrow |x_n - a| < \epsilon$.
		Cela se note~:
		\[\lim_{n \to \infty}x_n = a.\]

		Il reste cependant à prouver qu'une suite convergente n'a qu'une seule limite. Pour ce faire, procédons par l'absurde. Supposons que
		$x_n \to a$ et $x_n \to b$ quand $n \to \infty$ et supposons que $a \neq b$. Selon la définition ci-dessus,
		$\exists N_1 \in \N \, | \, n \geq N_1 \Rightarrow |x_n - a| < \epsilon$ et
		$\exists N_2 \in \N \, | \, n \geq N_2 \Rightarrow |x_n - b| < \epsilon$. En prenant $\epsilon = \frac 12|a - b| > 0$ car $a \neq b$
		et $N = \max\{N_1, N_2\}$, l'inégalité triangulaire dit que $|a - x_N + x_N - b| \leq |a - x_N| + |x_N - b| < 2\epsilon$. Donc $|a - b| < |a - b|$.
		L'hypothèse disant $\epsilon > 0$ est fausse, ce qui implique $\epsilon = 0$, ou encore $a = b$.

		Pour définir une convergence en l'infini positif, on procède de la sorte~: $\lim_{n\to\infty} x_n = \infty \iff \forall K \in \R^+, \exists N \in \N \, | \, n \geq N \Rightarrow x_n > K$.
		De manière similaire, pour l'infini négatif~: $\lim_{n\to\infty}x_n = -\infty \iff \forall K \in \R^+, \exists N \in \N \, | \, n \geq N \Rightarrow x_n < K$.

		On dit d'une suite qui ne converge en aucun réel qu'elle est divergente ($\infty \not \in \R$ !).

		\subsubsection{Techniques de démonstration de divergence ou de convergence}
			Soit $(x_n)$, une suite convergente. Alors $(x_n)$ est bornée. Autrement dit, $\exists K \in \R^+ \, | \, \forall n \in \N, |x_n| \leq K$.

			Pour prouver ceci, il faut d'abord montrer que pour $a = \lim x_n$, $\forall \epsilon > 0, \exists N \in \N \, | \, n \geq N \Rightarrow |x_n - a| < \epsilon$.
			Donc $|x_n| = |x_n - a + a| \leq |x_n - a| + |a| < \epsilon + |a|$. Autrement dit, à partir de $n = N$, $(x_n)$ est borné par $\epsilon + |a|$.
			Il reste donc un nombre fini d'éléments à traiter. Donc $K = \max \{|x_0|, ..., |x_{N-1}|, \epsilon + |a|\}$ borne l'entièreté de $(x_n)$.

			De plus, les opérations sur suites sont définies telles que, pour $(x_n)$ et $(y_n)$ convergentes respectivement en $a$ et $b$~:

			\begin{itemize}
				\item $(x_n + y_n)$ est convergente en $a + b$~;
				\item $(x_ny_n)$ est convergente en $ab$~;
				\item $b \neq 0 \Rightarrow (\exists M \, | \, n \geq M \Rightarrow y_n \neq 0) \land (x_n/y_n)_{n \geq M}$ est convergente en $\frac ab$.
			\end{itemize}

			Ces assertions se démontrent comme suit.

			\paragraph{Somme de suites} Soit $\epsilon > 0$, $\exists N_1 \, | \, \forall n \geq N_1, |x_n - a| < \frac \epsilon2$. Pareillement pour
			$N_2 \ | \, \forall n \geq N_2, |y_n - b| < \frac \epsilon2$. En prenant $N = \max \{N_1, N_2\}$, on a $|(x_n + y_n) - (a + b)| \leq |x_n - a| + |y_n - b| < \epsilon$.

			\paragraph{Produit de suites} La suite $(x_ny_n)$ est bornée, donc $\exists K \, | \, |x_n| \leq K$. Donc $|x_ny_n - ab| = |x_n(y_n - b) - b(x_n - a)|$.
			Autrement dit, $|x_ny_n - ab| \leq |x_n||y_n - b| + |b||x_n - a| = K|y_n - b| + |b||y_n - b|$.
			Avec $\epsilon > 0$, $\exists N_1 \, | \, \forall n \geq N_1, |y_n - b| < \frac {\epsilon}{2K}$ et $\exists N_2 \, | \, \forall n \geq N_2, |x_n - a| l \frac {\epsilon}{|b| + 1}$
			(il faut mettre $|b| + 1$ dans le cas où $b = 0$. Avec $N = \max \{N_1, N_2\}$, $\forall n \geq N$, on a~:
			\[|x_ny_n - ab| \leq K|y_n - b| + |b||x_n - a| < \frac \epsilon2 + \frac \epsilon2 = \epsilon.\]

			\paragraph{Quotient de suites} En prouvant que $(\frac {1}{y_n}) \to \frac 1b$, le quotient découle du produit.
			Soit $\epsilon = \frac {|b|}{2} > 0$, $\exists M \, | \, n \geq M \Rightarrow |y_n - b| < \epsilon$. Donc, par l'inégalité triangulaire,
			$|y_n| \geq |b| - |y_n - b| > |b| - \epsilon = \frac {|b|}{2}$. La suite $(y_n)_{n \geq M}$ est bien définie.Maintenant prouvons sa
			convergence en $\frac 1b$~:
			\[\left|\frac {1}{y_n} - \frac 1b\right| = \left|\frac {b - y_n}{by_n}\right| \leq \frac {2}{|b|^2}|b - y_n|.\]

			Pour $\epsilon > 0$ fixé, $\exists N \geq M \, | \, \forall n \geq N, |y_n - b| < \frac {|b|^2}{2}\epsilon$.
			Donc $|\frac {1}{y_n} - \frac 1b| < \frac {2}{|b|^2}\frac {|b|^2}{2}\epsilon = \epsilon$.

			\paragraph{Exemple du quotient de polynômes de degré $k$} Soient $a_i, b_i \in \R$ avec $0 \leq i \leq k$. La suite $(x_n)$ définie par
			$x_n = \frac {\sum_{i = 0}^ka_in^i}{\sum_{i = 0}^kb_in^i}$ converge en $\frac {a_k}{b_k}$. Pour le prouver, il faut diviser le numérateur
			et le dénominateur par $n^k$. La suite devient $x_n = \frac {\sum_{i = 0}^ka_in^{i-k}}{\sum_{i = 0}^kb_in^{i-k}}$. Or $\frac {1}{n^K}$
			converge en 0 $\forall K > 0$. Donc $\lim_{n\to\infty}x_n = \lim_{n\to\infty}\frac {\sum_{i = 0}^ka_in^{i-k}}{\sum_{i = 0}^kb_in^{i-k}} = \frac {a_k}{b_k}$.

			\paragraph{Théorème du sandwich} Soient $(a_n)$ et $(b_n)$, deux suites réelles convergentes vers $l$. Si $(x_n)$ est une suite satisfaisant
			$a_n \leq x_n \leq b_n \forall n \geq N_0$, alors $x_n \to l$.

			Soit $\epsilon > 0$. Par définition, on sait~:
			\[\exists N_1, N_2 \, | \, (n \geq N_1 \Rightarrow |a_n - l| < \epsilon) \land (n \geq N_2 \Rightarrow |b_n - l| < \epsilon).\]
			
			Donc $\forall n \geq N = \max \{N_0, N_1, N_2\}, -\epsilon < a_n - l \leq x_n - l \leq b_n - l < \epsilon$. Autrement dit, $|x_n - l| < \epsilon$.

			Il en découle un corollaire disant que si $(y_n)$ est une suite bornée (pas forcément convergente) et $z_n \to 0$, alors $y_nz_n \to 0$. Pour le prouver,
			on sait qu'$\exists K > 0 \, | \, |y_n| < K$. Autrement dit, $-K < |y_n| < K$, d'où $-K|z_n| \leq |y_n||z_n| \leq K|z_n|$. Puisque $|y_n||z_n| = |y_nz_n|$ et
			$z_n \to 0$, alors $|y_nz_n| \to 0$.

			\paragraph{Règle de l'exponentielle} Cette règle dit que si $(a_n)$ est une suite réelle positive convergent en 0, alors $(a_n^p)$ avec $p \in \mathbb R$
			converge également en 0. Pour le prouver, il faut prendre $\epsilon > 0$ et $\epsilon' = \epsilon^{p^{-1}}$. Par définition,
			$\exists N \, | \, n \geq N \Rightarrow |a_n| < \epsilon'$. Cependant, $a_n \geq 0$, donc $0 \leq a_n < \epsilon' = \epsilon^{p^{-1}}$. D'où
			$0 \leq a_n^p < \epsilon$.

			\paragraph{Suites convergentes en 0} Les suites suivantes convergent en 0, la plupart se démontrent avec le binôme de Newton et/ou le théorème du sandwich.

			\begin{itemize}
				\item $(\frac {1}{n^p})$ avec $p > 0$~;
				\item $(c_n$) avec $|c| < 1$~;
				\item $(n^pc^n)$ avec $p \in \R, |c| < 1$~;
				\item $(\frac {c^n}{n!})$ avec $p \in \R$~;
				\item $\frac {n^p}{n!}$ avec $p \in \R$.
			\end{itemize}

			La première proposition se démontre avec la règle de l'exponentielle vu que $(n^{-1}) \to 0$. La seconde se démontre avec le théorème du sandwich en
			posant $c = \frac {1}{1+a}$ avec $a > 0$. La troisième se démontre de manière similaire. La quatrième se démontre avec le principe d'Archimède et le théorème du
			sandwich. La dernière se réécrit $\frac {n^p}{n!} = \frac {n^p}{2^n}\frac {2^n}{n!}$ et est donc un produit de deux suites convergentes en 0.

			Le procédé pour déterminer la convergence d'une suite est donc de trouver un maximum de suites convergentes en 0 puis d'appliquer les opérations sur les
			limites de suite. Regardons maintenant du côté des suites divergentes.

			Soit $x_n \to \infty$ et $(y_n) \subset \R$. S'il $\exists A \in \R \, | \, \forall n, y_n > A$, alors $x_n + y_n \to \infty$. Également, si $A > 0$,
			alors $x_ny_n \to \infty$. La première affirmation se démontre comme suit~: soit $K > 0$, $\exists N \, | \, \forall n \geq N, x_n > K - A$, donc
			$x_n + y_n > K$. Pour le produit, $\exists N \, | \, \forall n \geq N, x_n > \frac KA$ donc $x_ny_n > K$.

			\paragraph{Règle de la réciproque} Soit $(x_n)$ une suite réelle qui tend vers $\pm \infty$. Alors $\frac1{x_n} \to 0$. De plus, soit $(x_n)$, une
			suite réelle non-nulle. S'$\exists M \, | \, n \geq M \Rightarrow x_n > 0$, alors $x_n \to \infty$. Similairement, s'$\exists M \, | \, n \geq M
			\Rightarrow x_n < 0$, alors $x_n \to -\infty$. Pour démontrer la première affirmation, il faut avoir $\epsilon > 0$, donc $\frac 1\epsilon > 0$.
			On sait qu'$\exists N \, | \, n \geq N \Rightarrow x_n > \frac 1\epsilon$. ou encore $0 < \frac 1{x_n} < \epsilon$ (car $x_n > \frac 1\epsilon > 0$).
			Pour démontrer la seconde, il faut avoir $K > 0$. On sait qu'$\exists N \, | \, n \geq N \Rightarrow x_n < \frac 1K$. Ou encore $\frac 1{x_n} > K$.

			Soient $(x_n)$, une suite réelle et une suite strictement croissante $n_1 < n_2 < \ldots$. La suite $(x_{n_k})$ est une \textit{sous-suite} de $(x_n)$.

			Il en découle le lemme suivant~: Soit $(x_n)$, une suite réelle convergente en $a$. Alors toute sous-suite de $(x_n)$ converge également en $a$.
			Pour le démontrer, il faut repartir de la définition et donc, puisque $x_n \to a$, alors $\exists n \, | \, n \geq N \Rightarrow |x_n - a| < \epsilon$
			avec $\epsilon \in \R^+_0$. Or, comme $(n_k)$ est une série naturelle, $n_k \geq k \; \forall k$ (preuve par récurrence). Si $k \geq N$,
			alors $|x_{n_k} - a| < \epsilon$. Donc $x_{n_k} \to a$.

			On peut en déduire le corollaire suivant~: si $(x_n)$ a deux sous-suites convergentes mais ayant des limites différentes, alors $(x_n)$ ne converge pas.

		\subsubsection{Les suites monotones}
			Une suite est dite \textit{croissante} si $x_n \leq x_{n+1} \; \forall n$ et est dite \textit{décroissante} si $x_n \geq x_{n+1} \; \forall n$.
			Si une suite est soit croissante, soit décroissante, elle est dite \textit{monotone}. Sur base de cette définition, il existe un théorème important,
			celui de la convergence des suites monotones~:

			Soit $(x_n)$, une suite monotone bornée. $(x_n)$ est convergente telle que quand $(x_n)$ est croissante, $\lim x_n = \sup\{x_n \, | \, n \in \N\}$
			et quand $(x_n)$ est décroissante, $\lim x_n = \inf\{x_n \, | \, n \in \N\}$.

			Pour le démontrer, il faut définir $S = \{x_n \, | \, n \in \N\}$, borné par hypothèse. De là, il faut supposer $(x_n)$ soit croissante soit
			décroissante (la démonstration est similaire) et prouver que $a = \sup\{x_n \, | \, x \in \N\}$ si $x_n \to a$. Soit $\epsilon > 0$. Par définition,
			$a$ est le majorant minimal de $S$. Donc $a-\epsilon$ ne majore pas $S$, donc $\exists x_N > a-\epsilon$. Or, comme $(x_n)$ est croissante,
			$n \geq N \Rightarrow x_n \geq x_N > a-\epsilon$. Mais $x_n \leq a$ car $a$ majore $S$. Donc $a-\epsilon < x_n \leq a$. Ou encore $-\epsilon < x_n - a < 0$,
			d'où $|x_n - a| < \epsilon$.

			On sait donc qu'une suite monotone non bornée diverge en $\pm \infty$ et qu'une suite bornée converge en $\sup\{x_n \, | \, n \in \N\}$ ou en
			$\inf\{x_n \, | \, n \in \N\}$.

	\subsection{Théorème de Bolzano-Weierstrass}
		Regardons comment construire deux suites monotones à partir d'une suite quelconque. Soit une suite $(x_n)$, commençons par définir~:
		\[S_m := \{x_n | m \leq n\}.\]

		Si $(x_n)$ est majorée, alors $s_n = \sup S_n$ est fini pour tout $n$ et $(s_n)$ est une suite décroissante car $m \geq n \Rightarrow S_m \subseteq S_n \Rightarrow
		\sup S_m \leq \sup S_n$. De même, si $(x_n)$ est minorée, alors $i_n = \inf S_n$ est fini pour tout $n$ et $(i_n)$ est une suite croissante.

		\begin{déf} Soit $(x_n) \subseteq \R$.

		\begin{itemize}
			\item Si $(x_n)$ est majorée, alors $(s_n)$ est bien définie, et on écrit~:
			\[\limsup_{n\to\infty}x_n := \lim_{n\to\infty}s_n\]

			que l'on appelle la \textit{limite supérieure} de $(x_n)$.

			\item Si $(x_n)$ n'est pas majorée, alors on écrit $\limsup_{n\to\infty}x_n = \infty$.

			\item Si $(x_n)$ est minorée, alors $(i_n)$ est bien définie, et on écrit~:
			\[\liminf_{n\to\infty}x_n := \lim_{n\to\infty}i_n\]

			que l'on appelle la \textit{limite inférieure} de $(x_n)$.

			\item Si $(x_n)$ n'est pas minorée, alors on écrit $\liminf_{n\to\infty}x_n = -\infty$.

			\item Constatons donc que pour une suite $(x_n)$ bornée, $\limsup_{n\to\infty}x_n$ et $\liminf_{n\to\infty}x_n$ sont tous les deux finis.
		\end{itemize}
		\end{déf}

		\begin{thm}[Bolzano-Weierstrass]\label{thm:BolzWeieR} Soit $(x_n)$ une suite bornée. Alors il existe une sous-suite de $(x_n)$ qui converge en
		$\limsup_{n\to\infty}x_n$ et une autre qui converge en $\liminf_{n\to\infty}x_n$.

		De plus, pour une sous-suite convergente $(x_{n_k})$ quelconque, $\liminf_{n \to \infty}x_n \leq \lim_{k\to\infty}x_{n_k} \leq \limsup_{n\to\infty}x_n$.

		\[\liminf_{n\to\infty}x_n \leq \lim_{k\to\infty}x_{n_k} \leq \limsup_{n\to\infty}x_n.\]
		\end{thm}

		\begin{proof} Soit $(x_n)$ une suite bornée. Montrons qu'il existe un sous-suite convergent en $\limsup_{n\to\infty}x_n$. La démonstration
		pour la sous-suite convergent en $\liminf_{n\to\infty}x_n$ étant similaire. Définissons cette suite $(x_{n_k})$ terme à terme.

		Prenons $n_0 = 0$. C'est à dire que la sous-suite $(x_{n_k})$ débute en $x_0$. Regardons maintenant l'ensemble $S_{0+1} = \{x_1, x_2, \ldots\}$.
		Comme $s_1$ est le supremum de $S_1$, il doit exister $x_m \in S_1$ tel que $s_1-1 < x_m \leq s_1$. Prenons alors pour $n_1$ le plus petit $m$
		satisfaisant cette condition. Comme $\forall \alpha \in S_1, \alpha > x_0 = n_0$, il est évident que $n_1 > n_0$.

		Regardons maintenant l'ensemble $S_{1+n_1} = \{x_{1+n_1}, x_{2+n_1}, \ldots\}$. De manière similaire au point précédent, il est possible de trouver pour
		$n_2$ un $m$ tel que $x_m \in S_{1+n_1}$ et $s_{1+n_1}-\frac 12 < x_m \leq s_{1+n_1}$. Il est tout aussi évident que $n_2 > n_1$.

		En réitérant ce procédé $k$ fois, on obtient $n_0 < n_1 < n_2 <\ldots < n_k$ satisfaisant~: $s_{1+n_i} - \frac 1{i+1} < x_{n_{i+1}} \leq s_{1+n_i}$.

		On obtient donc au final une sous-suite $(x_{n_k})$ de $(x_n)$ telle que $s_{1+n_k} - \frac 1{k+1} < x_{n_{k+1}} \leq s_{1+n_k}$. De plus, la suite
		$(s_{1+k_k})$ est une sous-suite de $(s_n)$. On a donc $s_{1+n_k} \to \limsup_{n\to\infty}x_n$ et $s_{1+n_k} - \frac 1{k+1} \to \limsup_{n\to\infty}x_n$.
		Par le théorème du sandwich, on peut déterminer que $x_{n_k} \to \limsup_{n\to\infty}x_n$ également.

		De plus (en admettant l'existence de $(i_{n_k})$ car la preuve est similaire à celle de $(s_{n_k})$), on a~:
		\[\forall k \in \N, i_{n_k} \leq x_{n_k} \leq s_{n_k}.\]

		Et comme $(i_{n_k})$ et $(s_{n_k})$ sont des sous-suites d'une suite convergente, elles sont elles-mêmes convergentes. Donc par un résultat précédent,
		$\lim_{k\to\infty}i_{n_k} \leq \lim_{k\to\infty}x_{n_k} \leq \lim_{k\to\infty}s_{n_k}$, ou encore
		$\liminf_{n\to\infty}x_n \leq \lim_{k\to\infty}x_{n_k} \leq \limsup_{n\to\infty}x_n$. Ce qui prouve la deuxième partie du théorème. \end{proof}

		\begin{cor} On peut établir un corollaire de ce théorème~: une suite $(x_n) \subseteq \R$ converge si et seulement si~:
		\[\liminf_{n\to\infty}x_n = \limsup_{n\to\infty}x_n = L \in \R,\]
		en quel cas, $x_n \to L$. \end{cor}

		\begin{proof} La démonstration se fait par le théorème du sandwich dans un sens et par le théorème de Bolzano-Weierstrass dans l'autre sens. \end{proof}

		\begin{rmq} Nous avons donc maintenant trois résultats primordiaux sur la convergence des suites~:

		\begin{enumerate}
			\item Toute suite convergente est bornée~;
			\item toute suite monotone et bornée est convergente~;
			\item Toute suite bornée possède une sous-suite convergente.
		\end{enumerate}
		\end{rmq}

	\subsection{Le critère de Cauchy}

		Jusqu'ici, la notion de convergence fait intervenir \textit{directement} la notion de limite, ce qui sous-entend qu'il faut connaitre la limite avent de
		commencer la preuve de la convergence. Il existe donc une notion permettant de prouver la convergence sans expliciter la limite.

		\begin{déf} Une suite $(x_n)$ est dite \textit{de Cauchy} si $\forall \epsilon > 0, \exists N > 0 \ | \, m, n \geq N \Rightarrow |x_n - x_m| < \epsilon$.
		\end{déf}

		Cette définition n'implique pas uniquement qu'il faut que $|x_n - x_{n+1}|$ tende vers zéro mais bien qu'il faut que \textbf{pour tout} $m, n \geq N$,
		ces deux éléments tendent vers zéro.

		\begin{lem} Soit $(x_n) \subseteq \R$ une suite convergente. $(x_n)$ est de Cauchy. \end{lem}

		\begin{proof} Soit une suite $(x_n)$ convergente en $a \in \R$. Par définition,
		$\forall \epsilon > 0, \exists N > 0 | n \geq N \Rightarrow | x_n - a| < \epsilon$. Soit $\epsilon' = \frac \epsilon2$. Soit ce $N$ découlant de la
		définition de convergence. Prenons $m, n \geq N$. On a donc $|x_m - x_n| = |x_m - a + a - x_n| < |x_m - a | + |x_n - a| = \epsilon'$. \end{proof}

		\begin{thm}[Critère de Cauchy] Une suite $(x_n)$ converge si et seulement si elle est de Cauchy. \end{thm}

		\begin{proof} L'implication $\Rightarrow$ est donnée par le lemme précédent. Il faut encore prouver l'implication $\Leftarrow$.

		Commençons par montrer que $(x_n)$ est bornée et puis appliquons Bolzano-Weierstrass. Par hypothèse, $(x_n)$ est de Cauchy. Donc $\forall \epsilon > 0,
		\exists N > 0 \, | \, m, n \geq N \Rightarrow |x_n - x_m| < \epsilon$. Soit $\epsilon$, prenons ce $N$ qui découle de la définition de suite de Cauchy.
		On a donc $\forall n \geq N, |x_n| = |x_n + x_N - x_N| \leq |x_n - x_N| + |x_N| < \epsilon + |x_N|$. Construisons $K = \max(\{|x_0|, |x_1|, |x_2|, \ldots,
		\epsilon + |x_N|\})$. On a donc $|x_n| < K \forall n \geq N$, ce qui implique que $(x_n)$ est bornée.

		Appliquons maintenant le théorème de Bolzano-Weierstrass qui dit qu'il existe une sous-suite $(x_{n_k})_k$ qui converge en une valeur $a \in \R$ quand
		$k \to \infty$. Par la convergence, il existe $N_1$ tel que $\forall k \geq N_1, |x_{n_k} - a| < \frac \epsilon2$. Et par la définition de suite de Cauchy,
		il existe $N_2$ tel que $m, n \geq N_2 \Rightarrow |x_n - x_m| < \frac \epsilon2$. Soit $\delta$ tel que $\delta \geq N_1$ et $n_\delta \geq N_2$.
		On a alors $\forall n \geq N_2, |x_n - a| = |x_n - x_\delta + x_\delta - a| \leq |x_n - x_\delta| + |x_\delta - a| < \epsilon$.

		Il y a donc également convergence de la suite $(x_n)$ en $a \in \R$ quand $n \to \infty$. \end{proof}

\newpage
\section{Fonctions continues}
	\subsection{Limite d'une fonction en un point}
			\begin{déf} Soient $a, b \in \R$ tels que $a < b$. On note~:
				\[\begin{aligned}
					\interval {a}{b} &:= \{x \in \R \, | \, a \leq x \leq b\} \\
					]a, b] &:= \{y \in \R \, | \, a < x \leq b\} \\
					[a, b[ &:= \{x \in \R \, | \, a \leq x < b\} \\
					]a, b[ &:= \{x \in \R \, | \, a < x < b\} \\
					[a, \infty[ &:= \{x \in \R \, | \, a \leq x\} \\
					]a, \infty[ &:= \{x \in \R \, | \, a < x\} \\
					]-\infty, b] &:= \{x \in \R \, | \, x \leq b\} \\
					]-\infty, b[ &:= \{x \in \R \, | \, x < b\} \\
					]-\infty, \infty[ &:= \R.
				\end{aligned}\]

				Ces ensembles sont appelés \textit{intervalles}. \end{déf}

			\begin{déf} Soit $U \subseteq \R$. $U$ est dit ouvert $\iff \forall x \in U, \exists \epsilon > 0 \, | \, ]x-\epsilon, x+\epsilon[ \subseteq U$.
			$U$ est dit fermé $\iff \R \setminus U$ est ouvert. \end{déf}

			\begin{déf} Soient $A \subseteq \R$ et $a \in A$. $a$ est dit intérieur à $A$ s'$\exists \delta > 0 \, | \, ]a-\delta, a+\delta[ \subseteq A$.
			L'ensemble des points $a$ tels que $a$ est intérieur à $A$ est noté $\intr A$. \end{déf}

			\begin{rmq} $\forall A \subseteq \R$, $\intr A \subseteq A$. De plus, un ensemble peut être simultanément ouvert et fermé~: $]-\infty, \infty[$
			est ouvert car $\forall x \in R, \exists \epsilon > 0 \, | \, ]x-\epsilon, x+\epsilon[ \in \R$ et est fermé car
			$\R \setminus ]-\infty, \infty[ = \interval [open] 00$ est ouvert. \end{rmq}

			\begin{déf} Soit $a \in \R$. Un voisinage de $a$ est un ensemble contenant un intervalle de la forme $]c, d[$ avec $c < a < d$. \end{déf}

			\begin{déf} Soient $A \subseteq \R$ et $a \in \R$. $a$ est adhérent à $A$ si $\forall \delta > 0, ]a-\delta, a+\delta[ \cap A \neq \emptyset$.
			On note $\adh A$ l'ensemble des points adhérents à $A$. \end{déf}

			\begin{déf} Soient $f : U \subseteq \to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$. La limite de $f$ dans $B$ lorsque
			$x$ tend vers $a$ existe dans $\R$ et vaut $L \in \R$ si~:
			\[\forall \epsilon > 0, \exists \delta > 0 \, | \, \forall x \in (U \cap B), |x-a| < \delta \Rightarrow |f(x)-L| < \epsilon.\]

			Cela se note $f(x) \to L$ lorsque $x \to a$ dans $B$ ou~:
			\[\lim_{\underset{x \in B}{x \to a}} f(x) = L.\]
			\end{déf}

			\begin{rmq} Ici, $\epsilon$ permet de déterminer un voisinage autour de $L$, la limite, alors que $\delta$ permet de déterminer un voisinage autour
			de $a$. \end{rmq}

			\begin{thm}[Unicité de la limite] Soient $f : U \subseteq \R \to \R$, $B \subseteq \R$, $a \in \adh(B \cap U)$. Soient $L_1, L_2 \in \R$ tels que~:
			\[\begin{aligned}
				\lim_{\underset{x \in B}{x \to a}} f(x) &= L_1 \\
				\lim_{\underset{x \in B}{x \to a}} f(x) &= L_2.
			\end{aligned}\]

			Alors $L_1 = L_2$. \end{thm}

			\begin{proof} Soient $f : U \subseteq \R \to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$. Prenons $\epsilon = \frac {|L_1 - L_2|}3$.
			Par hypothèse, on sait qu'il existe $\delta_1 \, | \, \forall x \in U \cap B, |x-a| < \delta_1 \Rightarrow |f(x) - L_1| < \epsilon$ et
			$\delta_2 \, | \, \forall x \in U \cap B, |x-a| < \delta_2 \Rightarrow |f(x) - L_2| < \epsilon$.

			Soit $x_0 \in U \cap B$ tel que $|x_0 - a| < \min(\{L_1, L_2\})$. On sait alors que
			$f(x_0) \in ]L_1-\epsilon, L_1+\epsilon[$ et $f(x_0) \in ]L_2-\epsilon, L_2+\epsilon[$.
			Or, par choix de $\epsilon$, ces deux ensembles sont d'intersection vide. Il y a donc contradiction et $L_1 = L_2$. \end{proof}

			\begin{déf} La limite de $f : U \subseteq \R \to \R$ en $x \to a$ existe et vaut $L \in \R$ si la limite de $f$ en $x \to a$ dans $B = \R$
			existe et vaut $L$. On note cela~:
			\[\lim_{x \to a} f(x) = L \in \R.\]
			\end{déf}

		\subsubsection{Limites pointées, gauches et droites}

			Voyons dans quels contextes il est intéressant de manipuler le $B \subseteq \R$.

			\begin{déf} Soit $a \in \R$. Un voisinage de $a$ est pointé s'il contient un intervalle $]c, d[ \setminus \{a\}$. Un voisinage de $a$ est
			\textit{de droite} s'il contient un intervalle $[a, d[$ et peut être pointé si l'intervalle est sous la forme $]a, d[$. De manière similaire,
			un voisinage est \textit{de gauche} si l'intervalle est sous la forme $]c, a]$ et peut également être pointé. \end{déf}

			\begin{déf}[Définition des limites à gauche, à droite et pointées] Soit $f : U \to \R$ où $U$ est un intervalle contenant un voisinage pointé de $a$.
			La limite à gauche, respectivement à droite, respectivement pointée de $f$ en $x \to a$ existe et vaut $L \in \R$ si la limite de $f$ dans
			$B = ]-\infty, a[$, respectivement $]a, \infty[$, respectivement $\R \setminus \{a\}$ en $x \to a$ existe et vaut $L$.

			Cela se note respectivement~:
			\[\lim_{\underset{<}{x \to a}}f(x) = L, \;\;\lim_{\underset{>}{x \to a}} f(x) = L, \;\; \lim_{\underset{\neq}{x \to a}} f(x) = L.\]
			\end{déf}

			\begin{rmq} Pour pouvoir parler de limites de $f$ soit à gauche, soit à droite, soit pointée, il faut impérativement que $f$ soit définie dans les
			alentours de $a$. Plus précisément, il faut $a \in \adh(A \cap B)$ avec $B$ défini selon le cas. \end{rmq}

			\begin{lem} Soient $a \in \R$ et $f : U \to \R$ tels que $U$ définit un voisinage pointé de $a$. Alors $f$ possède une limite pointée en $a$ si et
			seulement si $f$ possède une limite à gauche en $a$ et une limite à droite en $a$ telles que $\lim_{x\to a^+}f(x) = \lim_{x \to a^-}f(x) = L \in \R$.
			Dans ce cas, on a $\lim_{\underset{\neq}{x \to a}} f(x) = L$. \end{lem}

			\begin{prp} Soient $f : U\to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$. La limite de $f$ dans $B$ en $x \to a$ existe et vaut
			$L \in \R$ si et seulement si $\forall A \subseteq B$, la limite de $f$ dans $A$ en $x \to a$ existe et vaut $L$. \end{prp}

			\begin{proof} Pour la condition suffisante ($\Leftarrow$), on sait que pour \textbf{tout} $A \subseteq B$, la limite existe. En prenant $A = B$,
			on sait que la limite existe également dans $B$ et vaut $L \in \R$.

			Pour la condition nécessaire ($\Rightarrow$), observons que
			$\forall \epsilon > 0, \exists \delta > 0 \, | \, x \in (B \cap U) \land (|x-a| < \delta) \Rightarrow (|f(x)-L| < \epsilon)$.
			On sait alors que pour $\epsilon \in \R_0^+$ fixé et pour $A \subseteq B$, il existe $\delta \in \R_0^+$ tel que $x \in A \Rightarrow |f(x) - L| < \epsilon$.
			\end{proof}

			\begin{prp} Soient $f : U \to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$. La limite de $f$ dans $B$ en $x \to a$ existe et vaut
			$L \in \R$ si et seulement si pour toute suite $(x_n)_{n \in \N} \subseteq B$ telle que $x_n \to a$, la suite
			$(f(x_n))_{n \in \mathbb N} \subseteq \R$ converge en $L$. \end{prp}

			\begin{proof} Soit $\epsilon > 0$. On sait par hypothèse qu'$\exists \delta > 0$ tel que
			$x \in (B\cap U) \land |x-a| < \delta \Rightarrow |f(x)-L| < \epsilon$. Soit $(x_n) \subseteq B$ telle que $x_n \to a$. On sait donc
			qu'$\exists N > 0$ tel que $n > N \Rightarrow |x_n - a| < \epsilon$. Si ceci est vrai pour tout $\epsilon$, ça l'est plus précisément pour $\delta$
			déterminé par l'hypothèse. On a donc $\forall \epsilon > 0, \exists N > 0 \, | \, n > N \Rightarrow |f(x_n) - L| < \epsilon$ ou encore
			$f(x_n) \to L$ pour $n \to \infty$.
			
			Montrons maintenant que si toutes les suites de $B$ convergentes en $a$ implique $f(x_n) \to L$, alors la limite de $f$ en $a$ existe et vaut $L$.
			Fonctionnons par l'absurde~: soit $(x_n) \subseteq B$ une suite dans $B$ telle que $x_n \to a$ quand $n \to \infty$ et $(f(x_n)) \to L \in \R$ mais
			alors que $f(x) \not \to L$ pour $x \to a$. On sait alors qu'$\exists \epsilon_0 > 0 \, | \, \forall \delta > 0$ il n'y a pas de convergence de
			$f(x)$ en $L$. Soit $\epsilon_0$. Prenons $\delta = \frac 1n$. On en déduit que $\forall n \geq 1$, il existe $x_n \in U$ tel que $|x_n-a| < \frac 1n$ et
			$|f(x_n) - L| \geq \epsilon_0$ donc tel que $x_n \to a$ mais $f(x_n) \not \to L$ lorsque $n \to \infty$. Or par hypothèse, on sait que
			$(f(x_n)) \to L$. Ce qui est une contradiction avec $|f(x_n) - L| \geq \epsilon_0$. Donc $f(x) \to L$ pour $x \to a$. \end{proof}

		\subsubsection{Règles de calcul de limite de fonctions}

			\begin{thm} Soient $f, g : U \subseteq \R \to \R$ et $a \in \R$ tels que $f$ et $g$ sont définies dans le voisinage de $a$. Soient $L_f$ et $L_g$ tels
			que $\lim_{x \to a}f(x) = L_f$ et $\lim_{x \to a}g(x) = L_g$. alors~:
			\[\begin{aligned}
				\lim_{x \to a}(f+g)(x) &= L_f + L_g \\
				\lim_{x \to a}(fg)(x) &= L_fL_g.
			\end{aligned}\]

			Si $L_g \neq 0$, la fonction $\frac fg(x)$ existe dans le voisinage de $a$ telle que~:
			\[\begin{aligned}\lim_{x \to a}\frac fg(x) = \frac {L_f}{L_g}.\end{aligned}\]
			\end{thm}

			\begin{proof} Soient $f, g : U \to \R$ convergents respectivement en $L_f$ et $L_g$ quand $x \to a$.

			\subparagraph{Addition} Montrons que la limite de la somme vaut la somme des limites.
			Soit $\epsilon > 0$. On sait par la définition des limites de $f$ et $g$ qu'il existe $\delta_1$ tel que $\forall x \in U, |x-a| < \delta_1
			\Rightarrow |f(x) - L_f| < \frac \epsilon2$
			et $\delta_2$ tel que $\forall x \in U, |x-a| < \delta_2 \Rightarrow |g(x)-L_g| < \frac \epsilon2$. Prenons alors $\delta = \min(\{\delta_1, \delta_2\})$.
			On sait dès lors que $\forall x \in U, |f(x) + g(x) - L_f - L_g| = |f(x)-L_f + g(x)-L_g| \leq |f(x)-L_f| + |g(x)-L_g| < \frac \epsilon2 + \frac \epsilon2 =
			\epsilon$.

			\subparagraph{Multiplication} Montrons que le produit des limite vaut le produit des limites. Montrons tout d'abord qu'il existe un voisinage autour de $a$
			tel que $f(x)$ est bornée.

			Soit $\epsilon = 1$. On sait alors qu'il existe $\delta_1$ tel que $\forall x \in U, |x-a| < \delta_1 \Rightarrow |f(x) - L_f| < 1$. Ce $\delta_1$
			définit un voisinage de $a$ où $|f(x)| < |L_f| + 1 \forall x$.

			Montrons ensuite que $\lim_{x \to a}(fg)(x) = L_fL_g$.

			Soit $\epsilon \in \R_0^+$. On sait qu'il existe $\delta_2$ tel que $\forall x \in U, |x-a| < \delta_2 \Rightarrow |f(x)-L_f| < \frac {\epsilon}{2(|L_g|+1)}$
			et $\delta_3$ tel que $\forall x \in U, |x-a| < \delta_3 \Rightarrow |g(x)-L_g| < \frac {\epsilon}{2|L_f|}$. Prenons alors
			$\delta = \min(\{\delta_i | i \in [3]\})$. On a alors $\forall x \in U$ tel que $|x-a| < \delta$~:
			\[\begin{aligned}
				|f(x)g(x) - L_fL_g| &= |f(x)(g(x) - L_g) + L_g(f(x) - L_f)| \leq |f(x)||g(x) - L_g| + |L_g||f(x) - L_f| \\
									&< (|L_f| + 1)\frac {\epsilon}{2(|L_f|+1)} + |L_g|\frac {\epsilon}{2|L_g|} = 2\frac {\epsilon}{2} = \epsilon.
			\end{aligned}\]

			\subparagraph{Quotient} Montrons que la limite du quotient vaut le quotient des limites. Commençons par montrer qu'il existe un voisinage de $a$ où
			$g(x) \neq 0$.

			Soit $\epsilon = \frac {|L_g|}2$. On sait alors qu'il existe $\delta_V$ tel que $\forall x \in U, |x-a| < \delta_V \Rightarrow |g(x)-L_g| < \frac {|L_g|}2$.
			Ou encore pour ce même $\delta_V$, $|g(x)| > \frac {|L_g|}2$. Donc $|g(x)|$ est strictement positif. Définissons alors $V := U \cap ]a-\delta_V, a+\delta_V[$,
			un voisinage de $a$ sur lequel la fonction $\frac fg(x)$ est bien définie.
			
			Prouvons maintenant que $\frac 1g(x) \to \frac 1{L_g}$ quand $x \to a$, et le résultat découlera de la proposition précédente.
			
			Soit $\epsilon > 0$. On sait qu'il existe $\delta > 0$ tel que $\forall x \in U, |x-a| < \delta \Rightarrow |g(x) - L_g| < \frac {|L_g|^2\epsilon}2$.
			On sait dès lors~:
			\[\left|\frac 1{g(x)} - \frac 1{L_g}\right| = \frac {|g(x)-L_g|}{|g(x)||L_g|} < \frac {|g(x)-L_g|}{\frac {|L_g|}{2}|L_g|}
			  = \frac {2|g(x)-L_g|}{|L_g|^2} < \frac {2(\frac {|L_g|^2\epsilon}{2})}{|L_g|^2} = \epsilon.\]
			
			\end{proof}
			  
			\begin{thm} le théorème du sandwich des suites a un homologue pour les fonctions~: soient $f, g, h : \R \to \R$ trois fonctions telles que
			$\forall x \in \R, f(x) \leq g(x) \leq h(x)$ avec $\lim_{x \to a}f(x) = \lim_{x \to a}h(x) = L \in \R$. Alors $\lim_{x \to a}g(x) = L$.
			\end{thm}
			
			\begin{proof} Définissons les suites $(f(x_n))_n, (g(x_n))_n$ et $(h(x_n))_n$. On sait que $\forall k \in {\R}^{\R}, \lim_{x \to a}k(x)$ existe
			et vaut $L_k$ si et seulement si pour toute suite $(x_n) \subseteq \R$ convergente en $a \in \R$, on a $k(x_n) \to L_k$ quand $n \to \infty$.
			
			On sait dès lors que $f(x_n) \to L$ et $h(x_n) \to L$. Par le théorème du sandwich, on sait que $g(x_n) \to L$ également. \end{proof}
			
			\begin{thm}[Conservation des inégalités] Soient $f, g : U \subseteq \R \to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$.
			Si $\exists \delta > 0$ tel que $\forall x \in ]a-\delta, a+\delta[ \cap U \cap B, f(x) \leq g(x)$ et $\lim_{x \to a}f(x)$ et $\lim_{x \to a}g(x)$
			existent dans $\R$, alors $\lim_{x \to a}f(x) \leq \lim_{x \to a}g(x)$. \end{thm}
			
		\subsubsection{Limites infinies et limites à l'infini}
			
			\begin{déf} Soient $f, g : U \subseteq \R \to \R$, $B \subseteq \R$, $a \in \adh(U \cap B)$. La limite de $f$ dans $B$ en $a$ existe et vaut
			$+\infty$ si $\forall K > 0, \exists \delta > 0 \, | \, \forall x \in (U \cap B), |x-a| < \delta \Rightarrow f(x) > K$. Cela se note~:
			\[\lim_{x \to a}f(x) = \infty.\]
			
			De manière similaire, la limite de $f$ dans $B$ en $a$ vaut $-\infty$ si $\forall K > 0, \exists \delta > 0 \, | \, \forall x \in (U \cap B), |x-a| <
			\delta \Rightarrow f(x) < -K$.
			Ce la se note~:
			\[\lim_{x \to a}f(x) = -\infty.\]
			\end{déf}

			\begin{rmq} les cas particuliers où $B = \R, B = ]-\infty, a], B = [a, +\infty[, B = \R \setminus \{a\}$ donnent les définitions de limite
			standard infinie, limite à gauche, limite à droite et limite pointée infinies. \end{rmq}

			\begin{déf} Soit $f : U \subseteq \R \to \R$ où $U$ n'est pas majoré. La limite de $f$ en « l'infini » vaut $L \in \R$ si
			$\forall \epsilon > 0, \exists M > 0 \, | \, \forall x \in U, x > M \Rightarrow |f(x) - L| < \epsilon$. Cela se note~:
			\[\lim_{x \to \infty}f(x) = L.\]

			De manière similaire, si $U$ n'est pas minoré, la limite de $f$ en « moins l'infini » vaut $L \in \R$ si
			$\forall \epsilon > 0, \exists M > 0 \, | \, \forall x \in U, x < -M \Rightarrow |f(x) - L| < \epsilon$. Cela se note~:
			\[\lim_{x \to -\infty}f(x) = L.\]
			\end{déf}

	\subsection{Définition de la continuité}
		\begin{déf} Soient $a \in \R$, $f : I \subseteq \R \to \R$ une fonction définie sur un voisinage de $a$. La fonction $f$ est continue en $a$ si 

		\[\lim_{x \to a}f(x) = f(a).\]

		Ou encore si

		\[\forall \epsilon > 0, \exists \delta > 0 \tq \abs{x-a} < \delta \Rightarrow \abs{f(x) - f(a)} < \epsilon.\]

		Cette définition comporte trois points importants~: il faut que la limite de $f$ en $a$ existe, que $f$ soit définie en $a$ et que la fonction prenne la valeur
		de sa limite en $a$.
		\end{déf}

		\begin{prp} Venant de la définition de limite de fonction sur base des suites, il est possible d'exprimer la continuité d'une fonction en terme
		de suites. Soient $a \in \R$, $f : I \subseteq \R \to \R$ une fonction définie sur un voisinage de $a$. Alors la fonction $f$ est continue en
		$a$ si et seulement si pour toute suite $(x_n)$ convergente en $a$, la suite $(f(x_n))$ converge en $f(a)$. \end{prp}

		\begin{prp} Les règles de calcul concernant la continuité sont assez évidentes. Soient $a \ni \R$, $f, g : I \subseteq \R \to \R$ deux fonctions
		définies sur un voisinage de$a$ et continues en $a$. Alors $f+g$ est également continue en $a$, $fg$ est également continue en $a$ et si
		$g(a) \neq 0$, alors $\frac fg$ est continue en $a$. De plus soit $h : V \subseteq \R \to \R$ une fonction définie sur un voisinage de $f(a)$
		et contenant $f(I)$, l'image de $f$. Si $h$ est continue en $f(a)$, alors $h \circ f : I \to \R$ est continue en $a$. \end{prp}

		\begin{proof} Les premières propositions viennent directement des règles sur les limites. Pour la composée, prenons $(x_n)$ une suite convergent en $a$.
		Par continuité de $f$, on sait que $f(x_n)$ converge en $f(a)$ et par continuité de $h$, on sait que $h(f(x_n))$ converge en $h(f(a))$. Dès lors, on sait que
		$h \circ f$ est continue en $a$. \end{proof}

		\begin{déf} Tout comme pour les limites, on parle de continuité à gauche (respectivement à droite) si la limite de $x$ tendant vers $a$ à gauche
		(respectivement à droite) vaut $f(a)$. \end{déf}

		\begin{déf} Une fonction $\fabr f$ est \textit{continue} si elle est continue en tout point $c \in ]a, b[$, continue à droite en
		$a$ et à gauche en $b$. \end{déf}
	
	\subsection{Théorème des bornes atteintes}

		\begin{thm} Soit $\fabr f$ une fonction continue. Alors $f$ est bornée \textbf{et} $f$ atteint ses bornes. Dès lors~:
		\[\exists m, M \in \ab \tq \forall x \in \ab : f(m) \leq f(x) \leq f(M).\]
		\end{thm}

		\begin{proof} Montrons d'abord que $f$ est bornée et puis montrons qu'elle atteint ses bornes.

		Supposons par l'absurde que $f$ n'est pas majorée. Dès lors, $\forall n \in \N : \exists x_n \in \ab \tq f(x_n) > n$. La suite $(x_n)$ est bornée car
		$\forall n : x_n \in \ab$. Par le théorème de Bolzano-Weierstrass, on sait qu'il existe $(x_{n_k})_k$ une sous-suite de $(x_n)$ convergeant en
		$x \in \R$. Par la continuité de $f$ en $x$ (hypothèse), la suite $(f(x_{n_k}))_k$ converge en $f(x)$. Or par construction de $(x_n)$, la suite $(f(x_{n_k}))_k$
		diverge vers $+\infty$. Il y a donc contradiction. Idem pour le minorant.

		Montrons maintenant que $f$ atteint ses bornes. Par l'absurde, supposons que $f$ n'atteint pas ses bornes. Prenons
		$M \coloneqq \sup\{f(x) \tq x \in \ab\}$ qui existe et est fini. Supposons que $\nexists c \in \ab \tq f(c) = M$. Posons alors~:
		\[g(x) \coloneqq \frac 1{M - f(x)}.\]

		$g$ est continue et donc bornée par $K \in \R$. Dès lors,
		\[\forall x \in \ab : \frac 1{M-f(x)} \leq K.\]

		Ce qui implique (du fait que $f(x) < A$ et $K > 0$) que $M-f(x) \geq \frac 1K$ ou encore $f(x) \leq M - \frac 1K$. Ce qui contredit le fait que $M$ est le
		majorant de $\{f(x) \tq x \in \ab\}$. Idem pour la borne inférieure. \end{proof}
	
	\subsection{Théorème de la valeur intermédiaire}
		
		\begin{thm} Soit $\fabr f$ une fonction continue. Soit $\gamma$ strictement entre $f(a)$ et $f(b)$.
		Alors $\exists c \in ]a, b[ \tq f(c) = \gamma$. \end{thm}

		\begin{proof} Supposons $f(a) < \gamma < f(b)$, le cas $f(b) < \gamma < f(a)$ se montre de la même manière.

		Soit $S \coloneqq \{x \in \ab \tq f(x) < \gamma\} \subseteq \dom f$. Par définition de $S$, on sait que $a \in S$ et que $S$ est majoré par $b$.
		Il existe une suite $(x_n)$ dans $S$ telle que $x_n \to \sup S$. Donc $\forall n \in \N : x_n \in S$ donc $f(x_n) < \gamma$. De plus,
		$\lim_{n\to+\infty}f(x_n) = f(\sup S) \leq \gamma$. Notons $c \coloneqq \sup S$. On sait donc que $f(c) \leq \gamma$.

		Supposons par l'absurde $f(c) < \gamma$. Soit $\epsilon \coloneqq \gamma - f(x) > 0$. Par la continuité de $f$ en $c$, on sait
		qu'$\exists \delta > 0 \tq \abs{x-c} < \delta \Rightarrow \abs{f(x)-f(c)} < \epsilon = \gamma-f(c)$. On sait que $a < c < b$.

		Prenons $x \coloneqq c + \frac {\min\{\delta, b-c\}}2$. Alors $x \in \ab$, $\abs{x -c} < \delta$ et $x > c$. Donc $\abs{f(x) - f(c)} < \epsilon$,
		ou encore~:
		\[f(c) - \gamma < f(x) - f(c) < \gamma - f(c).\]

		On sait donc $f(x)-f(c) < \gamma-f(c)$ ce qui implique $f(x) < \gamma$. Ce qui veut dire que $x \in S$. Or, par construction de $x$, $x > c = \sup S$.
		Il y a une contradiction. Donc l'hypothèse $f(c) < \gamma$ est fausse. Dès lors, $f(c) = \gamma$. \end{proof}

		\paragraph{Exemple} On peut, par ce théorème, montrer que tout polynôme de degré $n$ impair admet au moins une racine réelle~: soit
		$P(x) \coloneqq \sum_{i=1}^{n}a_ix^i$. Prenons $Q(x) \coloneqq \frac {P(x)}{a_n}$. Dès lors, les limites en les infinis de $Q$ sont ces mêmes infinis~:
		$\lim_{x\to\pm\infty}Q(x) = \pm\infty$. On sait dès lors qu'il existe deux réels $a$ et $b$ tels que $Q(a) < 0$ et $Q(b) > 0$. Par le théorème, on sait qu'il
		existe un $c \in ]a, b[$ tel que $Q(c) = 0$. Si $Q(c) = 0$, alors $P(c) = 0$ par définition de $Q$.
	
	\subsection{Théorème de l'intervalle et de la réciproque}
		
		\begin{prp} Soit $I \subseteq \R$. Alors $I$ est un intervalle si et seulement si $\forall \alpha, \beta \in I : \interval \alpha\beta \subset I$.
		\end{prp}

		\begin{thm}[de l'intervalle] Si $f : I \to \R$ est continue et $I$ est un intervalle, alors $f(I)$ est un intervalle. De plus, si $I$ est fermé borné,
		$f(I)$ l'est aussi. \end{thm}

		\begin{déf} Soit $f : U \subseteq \R \to \R$ une fonction définie sur un ensemble $U$ $f$ est strictement croissante (respectivement strictement
		décroissante) si pour tout $x, y \in U : x < y \Rightarrow f(x) < f(y)$ (respectivement $x < y \Rightarrow f(x) > f(y)$). $f$ est simplement croissante
		(respectivement simplement décroissante) si pour tout $x, y \in U : x < y \Rightarrow f(x) \leq f(y)$ (respectivement $x < y \Rightarrow f(x) \geq f(y)$).
		De plus, $f$ est strictement monotone si elle est strictement croissante ou strictement décroissante et est simplement monotone si elle est simplement
		croissante ou simplement décroissante. \end{déf}

		\begin{thm}[de la réciproque] Soit $f : I \subseteq \R \to \R$ une fonction continue et strictement monotone définie sur un intervalle $I$. Alors
		$f$ est injective et sa réciproque $f^{-1} : f(I) \to I$ est aussi continue. \end{thm}

		\begin{proof} Soient $x, y \in I$ tels que $x \neq y$. Soit $f^{-1} : f(I) \to \R$ la réciproque de $f$. Supposons $f$ strictement croissante
		(le cas strictement décroissant est similaire). Soit $p \in \intr f(I)$ (on sait que $f(I)$ est un intervalle). Montrons que $f^{-1}$ est continue en
		$p$. Prenons $q \in I$ tel que $f(q) = p$. On sait que $q$ n'est pas une extrémité de $I$ car les extrémités de $I$ correspondent aux extrémités de
		$f(I)$ (par la monotonie stricte de $f$) et que $p$ est dans l'intérieur de $f(I)$. Prenons $\epsilon > 0$ tel que $]q-\epsilon, q+\epsilon[ \subset I$.
		Posons $c \coloneqq f(q-\epsilon)$ et $d \coloneqq f(q+\epsilon)$. On sait $c < p < d$ par la monotonie stricte de $f$. Soit
		$\delta \coloneqq \min(pc, d-p)$. Dès lors, $\delta > 0$ et $\interval {p-\delta}{p+\delta} \subset f(\interval {q-\epsilon}{q+\epsilon})$, ou encore
		$f^{-1}(\interval {p-\delta}{p+\delta}) \subset \interval {q-\epsilon}{q+\epsilon}$. Nous avons donc $\delta$ tel que
		$\abs{x-p} < \delta \Rightarrow \abs{f^{-1}(x)-f^{-1}(p)} < \epsilon$. Ce qui montre que $f^{-1}$, la réciproque de $f$ est continue en $p$. La continuité
		à droite et à gauche de $f^{-1}$ aux extrémités de $f(I)$ se montrent de la même manière. \end{proof}
	
	\subsection{Continuité uniforme}
		La notion de continuité de $f$ en $a$ est une notion locale. Il en existe une version globale~: la continuité uniforme.

		\begin{déf} Soit $f : U \subseteq \R \to \R$ une fonction définie sur un ensemble réel $U$. $f$ est uniformément continue sur $U$ si~:
		\[\forall \epsilon > 0 : \exists \delta > 0 \tq \forall x, y \in U : \abs{x-y} < \delta \Rightarrow \abs{f(x)-f(y)} < \epsilon.\]

		Cette définition englobe la continuité simple mais requiert en plus que la valeur de $\delta$ trouvée ne dépende pas du point $a$ choisi. Comme
		cette définition englobe l'autre, une fonction uniformément continue est également continue. \end{déf}

		\begin{déf} Deux suites $(x_n), (y_n) \subset \R$ sont équivalentes si $\abs{x_n-y_n} \to 0$ pour $n \to +\infty$. Donc si l'une de des suites
		converge en $L$, l'autre converge également en $L$. \end{déf}

		\begin{prp} Soit $f : U \subseteq \R \to \R$. La fonction $f$ est uniformément continue si et seulement si $\forall (x_n), (y_n) \subset U$
		équivalentes, les suites $(f(x_n)), (f(y_n))$ sont équivalentes. \end{prp}

		\begin{proof} Soit $f : U \to \R$ uniformément continue. Soient $(x_n), (y_n) \subset \R$ deux suites équivalentes. Soit $\epsilon > 0$. On sait
		qu'il existe $\delta$ tel que $\forall x, y \in U : \abs{x-y} < \delta \Rightarrow \abs{f(x)-f(y)} < \epsilon$ par l'uniforme continuité de $f$. De
		plus, par l'équivalence des suites, on sait qu'il existe $N \in \N$ tel que $\abs{x_n-y_n} < \delta$. On a donc
		$\abs{x_n-y_n} < \delta \Rightarrow \abs{f(x_n)-f(y_n)} < \epsilon$. Les deux suites sont bien équivalentes. Montrons maintenant que l'équivalence de
		$(f(x_n))$ et $(f(y_n))$ implique l'uniforme continuité de $f$.
		
		Supposons par l'absurde qu'il existe $\epsilon_0 > 0$ tel qu'il n'existe pas de $\delta_0 > 0$. En particulier, $\delta_0 = \frac 1n$ ne convient pas,
		quel que soit $n > 0$. Prenons donc $n > 0$. Alors il existe $x_n, y_n \in U$ tels que $\abs{x_n-y_n} < \frac 1n$ et $\abs{f(x_n)-f(y_n)} \geq \epsilon_0$.
		On a donc les suites $(x_n), (y_n)$ équivalentes mais pas les suites $(f(x_n))$ et $(f(y_n))$ ce qui est une contradiction avec l'hypothèse. \end{proof}

		\begin{thm} Si $\fabr f$ est une fonction continue, alors $f$ est uniformément continue. \end{thm}

		\begin{proof}[version 1] Reprenons le format de démonstration précédente~: supposons par l'absurde que $f$ n'est pas continue. Dès lors, on sait qu'il
		existe $\epsilon_0 > 0$ tel qu'il n'existe pas de $\delta$ satisfaisant la définition. Plus précisément, $\forall n > 0 : \exists x_n, y_n \in \ab$
		tels que $\abs{x_n-y_n} < \frac 1n$ et $\abs{f(x_n) - f(y_n)} \geq \epsilon_0$. Maintenant, prenons deux sous-suites $(x_{n_k})$ et $(y_{n_k})$. Puisque
		$(x_n)$ et $(y_n)$ sont des suites bornées, les sous-suites $(x_{n_k})$ et $(y_{n_k})$ sont convergentes par le théorème de Bolzano-Weierstrass. Comme
		$\abs{x_n-y_n} \to 0$, on sait que les sous-suites convergentes convergent vers la même valeur~: $\lim_{k\to+\infty}x_{n_k} = \lim_{k\to+\infty}y_{n_k}$.
		Posons $p \coloneqq \lim_{k\to+\infty}x_{n_k}$. Comme les suites considérées sont dans $\ab$, on sait que $p \in \ab$
		également. Or par la continuité de $f$, on sait que $f(x_{n_k}) \to f(p)$ et $f(y_{n_k}) \to f(p)$. Dès lors, $\abs{f(x_{n_k})-f(y_{n_k})} \to 0$.
		Or $\abs{f(x_n) - f(y_n)} \geq \epsilon_0 \forall n$. Il y a donc contradiction. \end{proof}

		\begin{lem} Soit $(x_n)$ une suite bornée. Si $\forall (x_{n_k})$ sous-suite de $(x_n) : x_{n_k} \to L$, alors $x_n \to L$. \end{lem}

		\begin{proof} Par Bolzano-Weierstrass, il existe une sous-suite $(x_{n_k})$ telle que $x_{n_k} \to \liminf (x_n) = L$ et une
		sous-suite $(x_{m_k})$ telle que $x_{m_k} \to \limsup (x_n) = L$. Si $\liminf(x_n) = \limsup (x_n) = L$, alors $x_n \to L$. \end{proof}

		\begin{proof}[version 2] Montrons que si $x_n - y_n \to 0$, alors $f(x_n) - f(y_n) \to 0$. Comme $f(x_n)-f(y_n)$ est bornée, prenons
		$f(x_{n_k})-f(y_{n_k})$, une sous-suite convergente et nommons la limite $L$. Par Bolzano-Weierstrass, on sait qu'il existe deux sous-suites
		$(x_{n_{k_l}})$ et $(y_{n_{k_l}})$ qui convergent (respectivement en $u$ et $v$). Alors on peut dire que $f(x_{n_{k_l}}) - f(x_{n_{k_l}}) \to f(u) - f(v) = L$.
		Or, comme $x_{n_{k_l}} - y_{n_{k_l}} \to 0$, on sait que $u = v$. Dès lors, on peut dire que $L = 0$. \end{proof}

		\begin{thm} Soit $f : ]a, b[ \to \R$ une fonction uniformément continue. Alors il existe un \textit{prolongement continu} $\fabr {\bar f}$ de $f$.
		\end{thm}

		\begin{proof} Prenons la fonction 
		\[\fabr {\bar f} : x \mapsto \left\{\begin{aligned}&\lim_{x \to a^+}f(x) &\text{ si } x = a\\&f(x) &\text{ si } x \in ]a, b[\\&\lim_{x \to b^-}f(x) &\text{ si } x = b\end{aligned}\right.\]

		Dès lors, la fonction $\bar f$ est uniformément continue. \end{proof}

		\begin{rmq} Soit $f : A \to \R$ une fonction uniformément continue. $\forall E \subset A$, $f_{|_E}$ est également uniformément continue. \end{rmq}
	
	\subsection{Fonctions à valeur vectorielle}

		\begin{déf} Soient $x = (x_1, \ldots, x_n), y = (y_1, \ldots, y_n) \in \R^n$. On définit la distance entre $x$ et $y$ par~:
		\[\norm{x-y} = \sum_{i=1}^n(x_i-y_i)^2.\]

		De plus, la \textit{norme} d'un vecteur $x$ est la distance entre lui-même et l'origine. Donc $\norm x = \sum_{i=1}^nx_i^2$. \end{déf}

		\begin{déf} Soient $x, y \in \R^n$. On définit le produit scalaire de $x$ et $y$ par~:
		\[\scpr xy = \sum_{i=1}^nx_iy_i.\]
		\end{déf}

		\begin{rmq} $\sqrt {\norm x} = \scpr xx$. \end{rmq}

		\begin{prp} Soient $x, y, z \in \R^n, \lambda, \mu \in \R$. Alors~:

		\begin{enumerate}
			\item $\scpr {\lambda x + \mu y}z = \lambda \scpr xz + \mu \scpr yz$~;
			\item $\scpr xy = \scpr yx$~;
			\item $\scpr xx \geq 0$ et $\scpr xx = 0 \Rightarrow x = 0$.
		\end{enumerate}
		\end{prp}

		\begin{thm}[Inégalité de Cauchy-Schwartz] Soient $x, y \in \R^n$. Alors $\abs {\scpr xy} \leq \norm x\norm y$ avec égalité si et seulement si $x$ et
		$y$ sont colinéaires. \end{thm}

		\begin{thm}[Inégalité triangulaire] Soient $x, y \in \R^n$. Alors~:
		\[\norm {x+y} \leq \norm x + \norm y.\]
		\end{thm}

		\begin{proof}
		\[\norm {x+y}^2 = \scpr {x+y}{x+y} = \scpr xx + 2\scpr xy + \scpr yy = \norm x^2 + 2\scpr xy + \norm y^2 \leq \norm x^2 + 2\norm x\norm y + \norm y^2 =
		(\norm x + \norm y)^2.\]
		\end{proof}

		\begin{déf}\label{bouleOuverte} Soient $r > 0$ et $a \in \R^n$. On définit la boule ouverte en $a$ de rayon $r$ par~:
		\[B(a, r) \coloneqq \{x \in \R^n : \norm {x-a} < r\}.\]
		\end{déf}

		\begin{déf} Soient $A \subset \R^n$ et $a \in \R^n$. $a \in \adh A$ si et seulement si $\forall \epsilon > 0 : B(a, \epsilon) \cap A \neq \emptyset$.
		\end{déf}

		\begin{déf} Soient $f : A \subset \R^m \to \R^n$, $a \in \adh A$ et $L \in \R^n$. On définit la limite de $f$ pour $x \to a$ par~:
		\[\forall \epsilon > 0 : \exists \delta > 0 \tq \forall x \in A : \norm {x-a} < \delta \Rightarrow \norm {f(x)-L} < \epsilon.\]
		\end{déf}

		\begin{rmq} \[\lim_{x \to a}f(x) = l \in \R^n \iff \forall \epsilon > 0 : \exists \delta > 0 \tq f(A \cap B(a, \delta)) \subseteq B(L, \epsilon).\]
		\end{rmq}

		\begin{déf} Soient $f : A \subset \R^m \to \R^n$, $a \in A$. $f$ est continue en $a$ si $\lim_{x \to a}f(x) = f(a)$. \end{déf}

		\begin{déf} Soit $f : A \subset \R^m \to \R^n$. Les composantes de $f$ sont les fonctions $f_i : A \subset \R^m \to \R^n$ telles que
		$f(x) = (f_1(x), f_2(x), \ldots, f_n(x))$. \end{déf}

		\begin{lem} Soit $f : A \subset \R \to \R^n$ une fonction définie sur un voisinage de $a \in \R$. La fonction $f$ possède une limite en $x \to a$ si
		et seulement si toutes les composantes $f_i : A \to \R^n$ possèdent une limite en $x \to a$. Alors~:
		\[\lim_{x \to a}f(x) = \left(\lim_{x \to a}f_1(x), \lim_{x \to a}f_2(x), \ldots, \lim_{x \to a}f_n(x)\right).\]
		\end{lem}

		\begin{proof} Montrons d'abord que $f(x) \to L \Rightarrow \forall i : f_i(x) \to L_i$. Montrons ensuite l'autre sens de l'implication.

		Soit $f : A \subset \R \to \R^n$ une fonction convergent en $L = (L_1, \ldots, L_n) \in \R^n$ en $x \to a$. Soit $\epsilon > 0$. On sait qu'il existe
		$\delta > 0$ tel que pour tout $x \in A$, $\abs{x-a} < \delta \Rightarrow \norm{f(x)-L} < \epsilon$. Dès lors~:
		\[\abs{f_i(x) - L_i} = \sqrt {(f_i(x) - L_i)^2} \leq \norm {f(x)-L} < \epsilon.\]

		Pour montrer l'implication dans l'autre sens, prenons $\epsilon > 0$. $\forall i \in \{1, \ldots, n\}~:
		\exists \delta_i \tq \abs{x-a} < \delta_i \Rightarrow \abs{f_i(x)-L_i} < \frac \epsilon{\sqrt n}$. On définit $\delta \coloneqq \min\{\{\delta_i\}_i\}$.
		Dès lors, si $\abs{x-a} < \delta$, on a~:
		\[\norm {f(x)-L} = \sqrt {\sum_{i=1}^n(f_i(x)-L_i)^2} < \sqrt {\sum_{i=1}^n\left(\frac \epsilon{\sqrt n}\right)^2} = \sqrt {n\frac {\epsilon^2}{n}} = \epsilon.\]
		\end{proof}

		\begin{lem} La fonction $f : A \subseteq \R \to \R^n$ est continue en $a$ si et seulement si toutes ses composantes sont continues en $a$. \end{lem}

		\begin{proof} Par le lemme précédent, la démonstration est triviale. \end{proof}

		\begin{lem} Soient $f, g : A \subseteq \R \to \R^n$ deux fonctions continues en $a \in A$. Alors les fonctions $\norm f, f+g$ et $fg$ sont également
		continues en $a$. \end{lem}

		\begin{rmq} Étant donné que la continuité d'une fonction à valeur vectorielle est équivalente à la continuité de ses composantes, les théorèmes sur
		les fonctions les fonctions réelles s'appliquent facilement aux fonctions à valeur vectorielle. \end{rmq}

		\begin{prp} Soit $\fabr f^n$ continue. Alors il existe $p, q \in \ab$ tels que $\forall x \in \ab~:
		\norm {f(q)} \leq \norm {f(x)} \leq \norm {f(p)}$. \end{prp}

		\begin{proof} Par le le lemme précédent, on sait que $\norm f$ est continue. En appliquant le théorème des bornes atteintes sur chaque composante,
		la proposition est démontrée. \end{proof}

\newpage
\section{Fonctions dérivables}\label{sec:foncdérivables}
	\subsection{Définitions}
	
		\begin{déf} Soit $f : I \to \R$ une fonction définie sur un intervalle ouvert $I$ contenant $a$. La fonction $f$ est dérivable en $a$ si la limite
		suivante existe~:
		\[\lim_{h \to 0}\frac {f(a+h)-f(a)}h.\]

		Si la limite existe, on la note $f'(a)$. Si la fonction $f$ est dérivable sur tout point $a$ de son domaine, $f$ est dérivable. On définit la fonction dérivée de
		$f$ par $f' : I \to \R : x \mapsto f'(x)$. \end{déf}

		\begin{déf} Il existe des classes de dérivabilité notées $C^k$ pour $k \in \N$. Si $f$ est continue, alors $f \in C^0$. De plus, si $f$ est dérivable
		et $f'$ est $C^k$, alors $f \in C^{k+1}$.  Et si $\forall k \in \N : f \in C^k$, alors on note $f \in C^\infty$. \end{déf}

		\begin{prp} Soit $f : A \subseteq \R \to \R$ dérivable en $a$. Alors $f$ est continue en $a$. \end{prp}

		\begin{proof}
		\[\lim_{x \to a}f(x) = \lim_{x \to a}f(a) + \lim_{x \to a}\frac {f(x)-f(a)}{x-a}(x-a) = f(a) + f'(a) \cdot 0 = f(a).\]
		\end{proof}

		\begin{thm}[Règles de calcul] Soient $f, g : I \subseteq \R \to \R$ deux fonctions définies sur un intervalle ouvert $I$ et dérivables en $a \in I$. Alors~:

		\begin{enumerate}
			\item $(f+g)'(a) = f'(a) + g'(a)$~;
			\item $(fg)'(a) = f'(a)g(a) + f(a)g'(a)$~;
			\item si $g(a) \neq 0$, alors $\left(\frac fg\right)'(a) = \frac {f'(a)g(a)-f(a)g'(a)}{g(a)^2}$.
		\end{enumerate}
		\end{thm}

		\begin{proof} Le premier point découle directement des règles de calcul sur la somme de limites. Le second point se montre en réécrivant la limite~:
		\[\lim_{x\to a}\frac {(fg)(x) - (fg)(a)}{x-a} = \lim_{x \to a}f(x)\frac {g(x)-g(a)}{x-a} + \lim_{x \to a}g(x)\frac {f(x)-f(a)}{x-a} = f'(a)g(a) + f(a)g'(a).\]

		Le dernier se montre d'abord par $\left(\frac 1g\right)'(a) = -\frac {g'(a)}{g(a)^2}$~:
		\[\lim_{x \to a}\frac {\frac 1g(x) - \frac 1g(a)}{x-a} = \lim_{x \to a}\frac {g(a)-g(x)}{(x-a)g(a)g(x)} = -\frac {g'(a)}{g(a)^2}.\]

		En utilisant le point 2 et cette propriété, le quotient est démontré. \end{proof}

		\begin{thm} Soient $f, g$ deux fonctions telles que $f$ est dérivable en $a$ et $g$ est dérivable en $f(a)$. Alors la composée $g \circ f$ est dérivable
		en $a$ et vaut~: \[g'(f(a))f'(a).\] \end{thm}

		\begin{proof} Soient les fonctions suivantes~:
		\[F(x) \coloneqq \left\{\begin{aligned}&\frac {f(x)-f(a)}{x-a} &\text{ si } x \neq 0 \\ &f\prime(a) &\text{ si } x = a\end{aligned}\right.,\]
		et~:
		\[G(x) \coloneqq \left\{\begin{aligned}&\frac {g(x)-g(f(a))}{x-f(a)} &\text{ si } x \neq f(a)\\&g\prime(f(a)) &\text{ si } x = f(a)\end{aligned}\right.\]

		Les fonctions $F$ et $G$ sont respectivement continues en $a$ et $f(a)$. De plus, $\forall x \in \dom f : f(x) = f(a) + (x-a)F(x)$ et
		$\forall x \in \dom g : g(x) = g(f(a)) + (x-f(a))G(x)$. Dès lors, on peut calculer $g \circ f$~:

		\begin{align*}
			(g \circ f)(x) &= g(f(x)) = g(f(a) + (x-a)F(x)) = g(f(a)) + (f(a) + (x-a)F(x) - f(a))G(f(a) + (x-a)F(x)) \\
		                   &= (g \circ f)(a) + (x-a)F(x)G(f(x)).
		\end{align*}

		On peut dès lors calculer la dérivée en faisant~:
		\[(g \circ f)'(a) = \lim_{x \to a}\frac {(g \circ f)(x) - (g \circ f)(a)}{x-a} = \lim_{x \to a}\frac {(x-a)F(x)G(f(x))}{x-a} = \lim_{x \to a}F(x)G(f(x)).\]

		Et puisque le produit de fonctions continues est toujours une fonction continue, par la continuité, cette valeur vaut $F(a)G(f(a)) = f'(a)g'(f(a))$.
		\end{proof}

		\begin{thm}[de la réciproque] Soit $f : I \to J$ une bijection continue réelle entre deux intervalles ouverts. Si $f$ est dérivable en $a \in I$ telle
		que $f'(a) \neq 0$, alors la réciproque $f^{-1}$ est dérivable en $f(a)$ et vaut~:
		\[\left(f^{-1}\right)'(f(a)) = \frac 1{f'(a)}.\]
		\end{thm}

		\begin{proof} Puisque $f$ est une bijection continue, elle est strictement monotone. Donc par un théorème précédent, on sait que $f^{-1} : J \to I$
		est également continue. Posons~:
		\[G(y) \coloneqq \frac {f^{-1}(y) - a}{y - f(a)}.\]

		Dès lors~:
		\[\lim_{y \to f(a)} G(y) = \lim_{y \to f(a))}G(f(f^{-1}(y))) = \lim_{x \to \lim_{y \to f(a)}f^{-1}(y)}\frac {x-a}{f(x)-f(a)} = \frac 1{f'(a)}.\]
		\end{proof}
	
	\subsection{Extrema}
		
		\begin{déf} Soient $f : U \subseteq \R \to \R$ et $a \in U$. Le point $a$ est un minimum local de $f$ si
		$\exists \epsilon > 0 \tq \forall x \in U : \abs{x-a} < \epsilon \Rightarrow f(a) \leq f(x)$. De même, $a$ et un maximum local si
		$\exists \epsilon > 0 \tq \forall x \in U : \abs{x-a} < \epsilon \Rightarrow f(a) \geq f(x)$. Si $a$ est un minimum local ou un maximum local, alors $a$ est
		un extremum local. \end{déf}

		\begin{prp} Soit $f : U \subseteq \R \to \R$ une fonction dérivable. Soit $a \in \intr U$ un extremum de $f$. Alors $f'(a) = 0$. \end{prp}

		\begin{proof} Montrons le cas où $a$ est un minimum local (le cas du maximum est identique). Par la définition du minimum, on sait qu'il existe
		$\epsilon$ tel que $\forall x \in U : \abs{x-a} < \epsilon \Rightarrow f(a) \leq f(x)$. Dès lors~:

		\begin{align*}
			&\forall x \in U \tq x < a : \frac {f(x)-f(a)}{x-a} \leq 0, \\
			&\forall x \in U \tq x > a : \frac {f(x)-f(a)}{x-a} \geq 0.
		\end{align*}

		Or, comme par hypothèse $f$ est dérivable en $a$, la limite pour $x \to a$ existe. Il faut donc $f'(a) = 0$. \end{proof}

		\begin{déf} Soit $f$ une fonction dérivable. Un point $a \in \dom f$ tel que $f'(a) = 0$ est appelé point critique. \end{déf}

	\subsection{Théorème de la moyenne}

		\begin{lem}[Théorème de Rolle] Soit $f : \ab \subseteq \R \to \R$ continue et dérivable sur $]a, b[$. Si $f(a) = f(b)$, alors
		$\exists c \in ]a, b[ \tq f'(c) = 0$. \end{lem}

		\begin{proof} La fonction $f$ est définie sur un intervalle fermé borné. Donc par le théorème des bornes atteintes, on sait qu'il existe $m, M \in \R$
		tels que $f(m)$ est le minimum de $f$ et $f(M)$ est le maximum de $f$. Si $f(m) = f(M)$, alors la fonction est constante. Alors prenons $c = \frac {a+b}2$.
		Sinon, si $m \neq a$ et $m \neq b$, prenons $c = m$ car $m$ est un extremum. Par la proposition précédente, $f'(c) = 0$. \end{proof}

		\begin{thm}[de la moyenne/des accroissements finis] Soit $f : \ab \subseteq \R \to \R$ continue sur $\ab$ et dérivable sur $]a, b[$.
		Alors il existe $c \in ]a, b[$ tel que~:
		\[f'(c) = \frac {f(b)-f(a)}{b-a}.\]
		\end{thm}

		\begin{proof} Soit $G(x)$ une fonction continue sur $\ab$ et dérivable sur $]a, b[$ définie par~:
		\[G(x) \coloneqq f(x) - \frac {f(b)-f(a)}{b-a}x.\]

		Dès lors, $G(b)-G(a)=0$. Donc par le théorème de Rolle, on sait qu'il existe $c$ tel que $G'(c) = 0$. Or
		\[G'(x) = f'(x) - \frac {f(b)-f(a)}{b-a} = 0.\]

		On a donc bien $f'(c) = \frac {f(b)-f(a)}{b-a}$. \end{proof}

		\begin{prp} Soit $\fabr f$ continue sur $\ab$ et dérivable sur $]a, b[$.

		\begin{itemize}
			\item[$(i)$]   Si $\forall x \in ]a, b[ : f'(x) > 0$, alors $f$ est strictement croissante  sur $]a, b[$~;
			\item[$(ii)$]  Si $\forall x \in ]a, b[ : f'(x) < 0$, alors $f$ est strictement décroissante sur $]a, b[$~;
			\item[$(iii)$] Si $\forall x \in ]a, b[ : f'(x) = 0$, alors $f$ est constante sur $]a, b[$.
		\end{itemize}
		\end{prp}

		\begin{proof} Puisque $f$ est définie et continue sur un intervalle borné fermé, pour tout $x_1 < x_2 \in ]a, b[$, on sait que~:
		\[\exists c \in ]a, b[ \tq f'(c) = \frac {f(x_1)-f(x_2)}{x_1-x_2}.\]

		Donc si $f'(c) > 0$ (premier cas), il faut $f(x_2) > f(x_1)$, si $f'(c) < 0$ (second cas), il faut $f(x_2) < f(x_1)$ et si $f'(c) = 0$ (dernier cas), il faut
		$f(x_1) = f(x_2)$. On a donc $f$ soit strictement croissante, soit strictement décroissante soit constante. \end{proof}

		\begin{thm}[Comparatif de la moyenne] Soient $\fabr {f, g}$ continues sur $\ab$ et dérivables sur $]a, b[$. Si pour tout $x \in ]a, b[$, on a $g(x) \neq 0$,
		alors il existe $c \in ]a, b[$ tel que~:
		\[\frac {f'(c)}{g'(c)} = \frac {f(b)-f(a)}{g(b)-g(a)}.\]
		\end{thm}
	
	\subsection{Règle de l'Hospital}
		
		\begin{thm} Soient $f, g : \interval cd \to \R$ deux fonctions continues sur $\interval cd$ et dérivables sur $]c, d[$. Soit $a \in ]c, d[$.
		Si $f(a) = g(a) = 0$, alors~:
		\[\lim_{x \to a}\frac {f(x)}{g(x)} = \lim_{x \to a}\frac {f'(x)}{g'(x)}\]

		si cette limite existe. \end{thm}

		\begin{proof} Calculons d'abord la limite à droite, puis la limite à gauche. Soit $x \in ]a, d]$. Dès lors, par le théorème comparatif de la moyenne
		sur des fonctions $f$ et $g$ réduites au domaine $[a, x]$, on sait qu'il existe $\xi(x) \in ]a, x[$ tel que~:
		\[\frac {f'(\xi(x))}{g'(\xi(x))} = \frac {f(x)-f(a)}{g(x)-g(a)} = \frac {f(x)-0}{g(x)-0} = \frac {f(x)}{g(x)}.\]

		De plus, quand $x \to a^+$, il faut $\xi(x) \to a^+$ car $a < \xi(x) < x$. On sait donc que~:
		\[\lim_{x \to a^+}\frac {f(x)}{g(x)} = \lim_{\xi(x) \to a^+}\frac {f'(\xi(x))}{g'(\xi(x))}.\]

		De manière similaire, en prenant $x \in [c, a[$, on trouve $\xi(x) \in ]x, a[$ tel que~:
		\[\frac {f'(\xi(x))}{g'(\xi(x))} = \frac {f(x)-f(a)}{g(x)-g(a)} = \frac {f(x)}{g(x)}.\]

		À nouveau, quand $x \to a^-$, il faut $\xi(x) \to a^-$. Dès lors~:
		\[\lim_{x \to a^-}\frac {f(x)}{g(x)} = \lim_{\xi(x) \to a^-}\frac {f'(\xi(x))}{g'(\xi(x))}.\]

		Donc si les deux limites existent et sont égales, on a~:
		\[\lim_{x \to a}\frac {f(x)}{g(x)} = \lim_{x \to a}\frac {f'(x)}{g'(x)}.\]
		\end{proof}
		
		\begin{prp} Soient $f, g : \interval cd \to \R$ continues sur $\interval cd$ et dérivables sur $]c, d[$. Si $\lim_{x \to a^+}g(x) = \pm\infty$,
		$\forall x : x \in ]a, d[ \Rightarrow g'(x) \neq 0$, et si $\lim_{x \to a^+}\frac {f'(x)}{g'(x)} = L \in \overline \R$ alors~:

		\[\lim_{x \to a^+}\frac {f(x)}{g(x)} = L.\]
		\end{prp}

		\begin{proof} Soient $x < y \in ]a, d[$. Il existe $\xi \in ]x, y[$ tel que~:
		\[\frac {f(x)-f(y)}{g(x)-g(y)} = \frac {f'(\xi)}{g'(\xi)}.\]

		Comme $g(x) \to \pm \infty$ quand $x \to a$, on sait qu'il existe un voisinage de $a$ où $g(x) \neq 0$. De plus, en réécrivant~:
		\[f(x) = f(y) + (f(x)-f(y)) = f(y) + (g(x)-g(y))\frac {f(x)-f(y)}{g(x)-g(y)} = f(y) + (g(x)-g(y))\frac {f'(\xi)}{g'(\xi)},\]

		on peut trouver~:
		\[\frac {f(x)}{g(x)} = \frac 1{g(x)}\left[f(y) + (g(x)-g(y))\frac {f'(\xi)}{g'(\xi)}\right] =
		\frac {f(y)}{g(x)} + \left(1 - \frac {g(y)}{g(x)}\right)\frac {f'(\xi)}{g'(\xi)}.\]

		Soit $\epsilon > 0$. Prenons $\delta_1 > 0$ tel que $\forall \chi \in ]a, a+\delta_1[ : \abs{\frac {f'(\chi)}{g'(\chi)} - L} < \epsilon_1$.
		Fixons $y = a+\delta_1$. On a $\frac {g(y)}{g(x)} \to 0$ quand $x \to a$ car $g(y)$ est fixé et $g(x) \to +\infty$ (pareil pour $\frac {f(y)}{g(x)} \to 0$).
		Dès lors, il existe $\delta_2$ tel que $\forall x : x \in ]a, a+\delta_2[ \Rightarrow \abs{\frac {g(y)}{g(x)}} < \epsilon_2$ et
		$\abs{\frac {f(y)}{g(x)}} < \epsilon_3$.
		Prenons $(\epsilon_1, \epsilon_2, \epsilon_3) = \left(\frac \epsilon6, \min\left\{\frac \epsilon{3|L|+1}, \frac 12\right\}, \frac \epsilon3\right)$.
		Prenons $\delta \coloneqq \min \{\delta_1, \delta_2\}$. On a donc~:
		
		\begin{align*}
			\forall x : x \in ]a, a+\delta[ : \abs{\frac {f(x)}{g(x)} - L}
			&= \abs{\frac {f(y)}{g(x)} + \left(1 - \frac {g(y)}{g(x)}\right)\left(\frac {f'(\chi)}{g'(\chi)}-L+L\right) - L} \\
			&= \abs{\frac {f(y)}{g(x)} + \left(\frac {f'(\chi)}{g'(\chi)}-L\right)-\frac {g(y)}{g(x)}\left(\frac {f'(\chi)}{g'(\chi)}-L\right)+L-\frac {g(y)}{g(x)}L-L} \\
			&< \epsilon_3 + \epsilon_1 + \epsilon_2\epsilon_1 + \abs L\epsilon_2 \\
			&< \frac \epsilon3 + \frac \epsilon6 + \frac \epsilon6 + \frac \epsilon3 = \epsilon.
		\end{align*}
		\end{proof}
	
	\subsection{Dérivées de fonctions à valeur dans $\R^n$}
		
		\begin{déf} Soit $\frrn fA$. Soit $a \in \intr A$. $f$ est dérivable en $a$ si~:
		\[\lim_{h \to 0}\frac {f(a+h)-f(a)}h\]

		existe dans $\R^n$ cet élément. \end{déf}

		\begin{lem} La fonction $\frrn fA$ est dérivable en $a \in \intr A$ si et seulement si toutes ses composantes sont dérivables. En ce cas,
		$f'(a) = (f_1'(a), \ldots, f_n'(a))$. \end{lem}

		\begin{thm} Soient $f : \R \to \R$ dérivable en $a \in \R$ et $g : \R \to \R^n$ dérivable en $f(a)$. Alors $(g \circ f)'(a) = f'(a)g'(f(a))$. \end{thm}

		\begin{proof} Par le lemme précédent et la règle de dérivation de composée pour les fonctions réelles. \end{proof}

		\begin{thm} Soient $f : \R \to \R, g : \R^n \to \R, h : \R \to \R^n$ et $a \in \R$ tels que $f, g, h$ sont dérivables en $a$. Alors~:

		\begin{enumerate}
			\item $(fg)'(a) = f'(a)g(a) + f(a)g'(a)$~;
			\item $(\scpr gh)'(a) = \scpr {h'(a)}{g(a)} + \scpr {h(a)}{g'(a)}$~;
			\item $\left(\frac gf\right)'(a) = \frac {g'(a)f(a) - g(a)f'(a)}{f(a)^2}$~;
			\item $(g+h)'(a) = g'(a) + h'(a)$.
		\end{enumerate}
		\end{thm}
	
	\subsection{Dérivées de fonctions vectorielles}
		
		\begin{déf} Le graphe d'une fonction $\frmr fA$ est l'ensemble des couples~:
		\[\Gamma_f \coloneqq \{((x, f(x)) \in \R^n \times \R \cong \R^{n+1} \tq x \in A\}.\]
		\end{déf}

		\begin{déf} Soit $\frmr fA$. Soit $a \in \intr A$. La $j$ème dérivée partielle de $f$ (pour $1 \leq j < m$) est  donnée par~:
		\[\lim_{h \to 0}\frac {f(a_1, \ldots, a_j+h, \ldots, a_n) - f(a)}h = \lim_{h \to 0}\frac {f(a+he_j)-f(a)}h.\]

		si cette limite existe et se note $\pd f{x_j}(a)$ ou $(\partial_jf)(a)$. Si la limite n'existe pas, alors $f$ n'est pas dérivable en $a$.
		\end{déf}

		\begin{déf} On définit le gradient de $f : \R^m \to \R$ en $a$ par le vecteur des dérivées partielles~:
		\[(\nabla f)(a) = ((\partial_1 f)(a), \ldots, (\partial_m f)(a)) = ((\partial_i f)(a))_i \in \R^m.\]
		\end{déf}

		\begin{déf} $e_j$ est le $j$ème vecteur de la base canonique de $\R^n$. \end{déf}

		\begin{déf} Soit $f : \R^m \to \R$, $a \in \R^m$ et $v \in \R^m$. $f$ est dérivable en $a$ dans la direction $v$ si la limite suivante
		existe dans $\R$~:

		\[\lim_{h \to 0}\frac {f(a+hv)-f(a)}h.\]

		On note $(\partial_v f)(a)$ ce réel et on l'appelle dérivée directionnelle de la fonction $f$ dans la direction $v$ au point $a$. \end{déf}

		\begin{rmq} Comme $e_j$ est un vecteur de la base de $\R^m$, $(\partial_j f)(a) = (\partial_{e_j} f)(a)$ est une dérivée directionnelle. \end{rmq}

		\begin{déf} Soient $f : \R^m \to \R$, $a \in \R^m$. La fonction $f$ est différentiable s'il existe $u \in \R^m$ tel que~:

		\[\lim_{x \to a}\frac {f(x) - f(a) - \scpr u{x-a}}{\norm {x-a}} = 0 \in \R.\]
		\end{déf}

		\begin{prp} Soient $f : \R^m \to \R$, $a \in \R^m$. Si $f$ est différentiable en $a$, alors la fonction
		$\partial_\cdot f : \R^m \to \R : v \mapsto (\partial_v f)(a)$ est définie pour tout vecteur $v$ et est linéaire. De plus,
		$\forall v \in \R^m : (\partial_v f)(a) = \scpr {(\nabla f)(a)}v$. \end{prp}

		\begin{proof} Soit $x(t) \coloneqq a + tv$. Donc, $x(t) \to a$ si $t \to 0$. Dès lors~:

		\begin{align*}
			\lim_{t \to 0} \frac {f(a+tv)-(f(a) + \scpr u{tv})}{\abs t} &= 0 \\
			\lim_{t \to 0} \frac {f(a+tv)-f(a)}{\abs t} - \lim_{t \to 0}\frac {\scpr u{tv}}{\abs t} &= 0 \\
			(\partial_v f)(a) - \scpr uv \lim_{t \to 0}\frac tt &= 0 \\
			(\partial_v f)(a) &= \scpr uv.
		\end{align*}

		Dès lors, pour $v = e_j$, on a $(\partial_{e_j} f)(a) = (\partial_j f)(a) = \scpr uv = \scpr u{e_j} = u_j$. Donc
		$u = ((\partial_1 f)(a), \ldots, (\partial_m f)(a)) = (\nabla f)(a)$. \end{proof}

\newpage
\section{Intégrales de Riemann}
		\subsection{Définitions}
	
		\begin{déf} une partition de $\ab$ est la donnée $\{x_i \tq 0 \leq i \leq n\} \subset \ab$ telle que $a = x_0 < \ldots < x_n = b$. \end{déf}

		\begin{déf} Soit $\fabr f$ bornée. Soit $P = \{x_i\}_{i \in [n]}$  une partition de $\ab$. Pour tout $1 \leq i \leq n$, on définit~:

		\begin{align*}
			m_i &\coloneqq \inf \{f(x) \tq x \in \interval {x_{i-1}}{x_i}\}, \\
			M_i &\coloneqq \sup \{f(x) \tq x \in \interval {x_{i-1}}{x_i}\}.
		\end{align*}

		On définit ensuite~:
	
		\begin{align*}
			\mathcal L(f, P) &\coloneqq \sum_{i=1}^n(x_i-x_{i-1})m_i, \\
			\mathcal U(f, P) &\coloneqq \sum_{i=1}^n(x_i-x_{i-1})M_i.
		\end{align*}

		Qui sont respectivement l'aire signée de la somme des rectangles inférieurs et supérieurs. À partir de cela, on définit~:
	
		\begin{align*}
			\mathcal L(f) &\coloneqq \sup \{\mathcal L(f, P) \tq P \text{ est une partition de } \ab\}, \\
			\mathcal U(f) &\coloneqq \inf \{\mathcal U(f, P) \tq P \text{ est une partition de } \ab\}.
		\end{align*}
		\end{déf}

		\begin{déf} une fonction $\fabr f$ bornée est intégrable si $\mathcal U(f) = \mathcal L(f)$. On note cette valeur $\int_a^bf(x)\dif x$ ou encore
		$\int_a^b f$. \end{déf}

		\begin{lem} Si $\fabr f$ est bornée et $P$ est une partition de $\ab$, $y \in \ab$. On définit $P' \coloneqq P \cup \{y\}$. Alors~:
		\[\Larea(f, P) \leq \Larea(f, P') \leq \Uarea(f, P') \leq \Uarea(f, P).\]
		\end{lem}

		\begin{proof} Soit $r \in \{1, \ldots, n\}$ tel que $x_{r-1} < y < x_r$. On sait que $\Larea(f, P) = m_1(x_1-x_0) + \ldots + m_n(x_n-x_{n-1})$ et on
		sait que $\Larea(f, P') = m_1(x_1-x_0) + \ldots + \alpha(y-x_{r-1}) + \beta(x_r-y) + \ldots + m_n(x_n-x_{n-1})$ où
		$\alpha = \inf \{f(x) \tq x \in \interval {x_{r-1}}y\}$ et $\beta = \inf \{f(x) \tq x \in \interval y{x_r}\}$. Dès lors~:

		\begin{align*}
			\Larea(f, P')-\Larea(f, P) &= \alpha(y-x_{r-1}) + \beta(x_r-y) - m_r(x_r-x_{r-1}) = x_{r-1}(-\alpha+m_r)+ x_r(\beta-m_r) + \alpha y - \beta y + m_ry - m_ry \\
			                           &= (\alpha-m_r)(y-x_{r-1}) + (\beta-m_r)(x_r-y).
		\end{align*}

		Étant donné que $x_{r-1} < y < x_r$, on sait que les secondes parenthèses sont positives. De plus, comme les intervalles dont $\alpha$ et $\beta$ sont
		les minima sont inclus dans l'intervalle dont $m_r$ est le minimum, il est nécessaire que $\alpha \geq m_r$ et $\beta \geq m_r$. Les premières
		parenthèses sont dès lors également positives. Si $\Larea(f, P')-\Larea(f, P) \geq 0$, alors $\Larea(f, P') \geq \Larea(f, P)$. La partie pour les aires
		supérieures est identique. \end{proof}

		\begin{cor} Soient $P, P'$ deux partitions de $\ab$ telles que $P \subset P'$, alors $\Larea(f, P') \leq \Larea(f, P) \leq \Uarea(f, P) \leq \Uarea(f, P')$.
		\end{cor}

		\begin{proof} En écrivant $P' = P \cup \{y_1, \ldots, y_k\}$ et en appliquant $k$ fois le lemme précédent. \end{proof}

		\begin{lem} Soient $P$ et $P'$ deux partitions quelconques du même intervalle $\ab$. Alors~:
		\[\Larea(f, P) \leq \Uarea(f, P').\]
		\end{lem}

		\begin{proof} Soit $P'' = P \cup P'$. Dès lors, on sait $P'' \subset P'$ et $P'' \subset P$. Donc~:
		\[\Larea(f, P) \leq \Larea(f, P'') \leq \Uarea(f, P'') \leq \Uarea(f, P').\]
		\end{proof}

		\begin{prp} Soit $\fabr f$ bornée. Alors $\Larea(f) \leq \Uarea(f)$. \end{prp}

		\begin{proof} On sait que pour tout $P, P'$ partitions de $\ab$, $\Larea(f, P) \leq \Uarea(f, P')$. En particulier, en prenant le $\sup$
		à gauche et l'$\inf$ à droite, l'inégalité reste vraie. Donc~:
	
		\begin{align*}
			\sup \{\Larea(f, P) \tq P \text{ est une partition de } \ab\} &\leq \inf \{\Uarea(f, P) \tq P \text{ est une partition de } \ab\} \\
			\Larea(f) &\leq \Uarea(f)
		\end{align*}
		\end{proof}
	
	\subsection{Fonctions intégrables}
		\begin{prp}[Critère de Riemann] Soit $\fabr f$ bornée. Alors $f$ est intégrable si et seulement $\forall \epsilon > 0 : \exists P$ une partition
		de $\ab$ telle que $\Uarea(f, P) - \Larea(f, P) < \epsilon$. \end{prp}

		\begin{proof} Montrons d'abord l'implication $\Rightarrow$. On sait par hypothèse que $\Larea(f) = \Uarea(f)$. Soit $\epsilon > 0$. On sait
		qu'il existe $P_1, P_2$ partitions de $\ab$ tels que $\Uarea(f, P_1) < \Uarea(f)+\frac \epsilon2$ et $\Larea(f, P_2) > \Larea(f) + \frac \epsilon2$.
		Posons $P \coloneqq P_1 \cup P_2$. Dès lors~:
		\[
			\Larea(f) - \frac \epsilon2 \leq \Larea(f, P_2) \leq \Larea(f, P) \leq \Larea(f)
			  \stackrel{\text{par hypothèse}}=
			\Uarea(f) \leq \Uarea(f, P) \leq \Uarea(f, P_1) \leq \Uarea(f) + \frac \epsilon2.
		\]

		On sait donc $\Uarea(f, P) < \Uarea(f, P_1) < \Uarea(f)+\frac \epsilon2$ et $-\Larea(f, P) < -\Larea(f, P_2) < -\Larea(f)+\frac \epsilon2$. Donc~:
		\[\Uarea(f, P) - \Larea(f, P) < \Uarea(f)+\frac \epsilon2-\Larea(f)+\frac \epsilon2 = \epsilon.\]

		Montrons maintenant l'autre sens de l'implication $\Leftarrow$. Soit $\epsilon > 0$. On sait qu'il existe $P$ une partition de $\ab$ telle que
		$\Uarea(f, P) - \Larea(f, P) < \epsilon$. De plus, on sait $0 \leq \Uarea(f)-\Larea(f) \leq \Uarea(f, P)-\Larea(f, P) < \epsilon$. On a donc une
		quantité $\Uarea(f)-\Larea(f)$ ne dépendant pas de $\epsilon$ mais étant plus petite qu'$\epsilon$ pour tout $\epsilon > 0$. Il faut dès lors
		$\Uarea(f)-\Larea(f) = 0$. \end{proof}

		\begin{thm} Si $\fabr f$ est continue, alors $f$ est intégrable. \end{thm}

		\begin{proof} Soit $\epsilon > 0$. Si $f$ est continue, alors elle est uniformément continue. Donc il existe $\delta$ tel que~:
		\[\forall x, y : \abs{x-y} < \delta \Rightarrow \abs{f(x)-f(y)} < \frac \epsilon{b-a}.\]

		On construit $P$ telle que $\forall 1 \leq 1 \leq n : x_i-x_{i-1} < \delta$. Pour tout $i$, on sait qu'il existe $x_*$ et $x^*$ tels que
		$M_i = f(x^*)$ et $m_i = f(x_*)$ par le théorème des bornes atteintes. Donc~:
		\[\Uarea(f, P)-\Larea(f, P) = \sum_{i=1}^n(M_i-m_i)(x_i-x_{i-1}) = \sum_{i=1}^n(f(x^*)-f(x_*))(x_i-x_{i-1}) \leq \frac \epsilon{b-a}\sum_{i=1}^n(x_i-x_{i-1})
		= \frac \epsilon{b-a}(b-a) = \epsilon.\]
		\end{proof}

	\subsection{Propriétés des intégrales}
		
		\begin{déf} Soit $\fabr f$ intégrable. On définit~:
		\[\int_a^bf(x)\dif x = -\int_b^af(x)\dif x.\]
		\end{déf}

		\begin{prp} Soient $\fabr {f, g}$ intégrables. Soient $\alpha, \beta \in \R$ et $c \in \ab$. Alors~:

		\begin{enumerate}
			\item $\int_a^b\left(\alpha f(x) + \beta g(x)\right)\dif x = \alpha \int_a^bf(x)\dif x + \beta\int_a^bg(x)\dif x$~;
			\item $\int_a^bf(x)\dif x = \int_a^cf(x)\dif x + \int_c^bf(x)\dif x$~;
			\item Si $\forall x \in \ab : f(x) \leq g(x)$, alors $\int_a^bf(x)\dif x \leq \int_a^bg(x)\dif x$~;
			\item $\abs f$ est intégrable et $\abs{\int_a^bf(x)\dif x} \leq \abs{\int_a^b\abs{f(x)}\dif x}$.
		\end{enumerate}
		\end{prp}
	
	\subsection{Théorème fondamental de l'analyse}

		\begin{thm}[Théorème fondamental du calcul et intégral] Soit $\fabr f$ continue (donc intégrable). Alors la fonction $\fabr F : x \mapsto \int_a^xf$
		est l'unique primitive de $f$ sur $\ab$ qui s'annule en $a$. \end{thm}

		\begin{proof} Soit $c \in \ab$. Soit $\epsilon > 0$. Par la continuité (uniforme) de $f$ en $c$, il existe $\delta$ tel que
		$\abs{x-c}<\delta \Rightarrow \abs{f(x)-f(c)} < \epsilon$. De plus, notons que~:

		\begin{align*}
			\abs{\frac {F(x)-F(c)}{x-c}-f(c)} = \abs{\frac {\int_a^xf - \int_a^cf}{x-c}-f(x)} = \abs{\frac {\int_c^xf - f(c)(x-c)}{x-c}}
			= \abs{\frac {\abs{\int_c^x(f(t)-f(c))\dif t}}{x-c}} \leq \frac 1{\abs{x-c}}\abs{\int_c^x\abs{f(t)-f(c)}\dif t}
		\end{align*}

		Et si $\abs{x-c} < \delta$, alors $\forall t \in \interval cx : \abs{t-c} < \delta $. Et donc $\abs{f(t)-f(c)} < \epsilon$. Donc~:
		\[\frac 1{\abs{x-c}}\abs{\int_c^x\abs{f(t)-f(c)}\dif t} < \frac 1{\abs{x-c}}\abs{\int_c^x\epsilon\dif t} = \epsilon.\]

		On a donc montré que pour $x \to c$, on a $\frac {F(x)-F(c)}{x-c} = F'(c) \to f(c)$.

		Montrons maintenant que $F$ est l'\textbf{unique} primitive s'annulant en $a$. Soit $\fabr G$ telle que $G' = f$ et $G(a) = 0$. Montrons que $G = F$~:
		\[(G-F)'=(f-f) = 0.\]

		On sait donc que $G-F$ est une fonction constante. Et comme $G(a) = F(a) = 0$, on sait que $\forall x \in \ab : (G-F)(x) = 0$. \end{proof}

		\begin{cor} Soit $\fabr f$ intégrable. Soit $F \coloneqq \int f$. On a~:
		\[\int_a^b f(x)\dif x = F(b) - F(a) = \evf Fxab.\]
		\end{cor}

		\begin{proof} Les fonctions suivantes~:

		\begin{align*}
			x \mapsto \int_a^xf(t)\dif t, \\
			x \mapsto F(x) - F(a),
		\end{align*}

		sont deux primitives de $f$ s'annulant en $a$ et donc sont égales. \end{proof}

		\begin{prp} Soient $\fabr {f, g} \in C^1$. Alors~:

		\[\int_a^bf(x)g'(x)\dif x = \evf {(fg)}xab - \int_a^bf'(x)g(x)\dif x.\]
		\end{prp}

		\begin{proof} Soit $h(x) \coloneqq (fg)'(x) = f'(x)g(x) + f(x)g'(x)$. Par le théorème fondamental, on sait~:
		\[\int_a^bh'(x)\dif x = \int_a^b(f'(x)g(x) + f(x)g'(x))\dif x = \evf hxab.\]

		Ou encore~:
		\[\int_a^bf'(x)g(x)\dif x = \evf hxab - \int_a^bf(x)g'(x)\dif x.\]
		\end{proof}

		\begin{prp} Soient $\fabr f$ continue et $\fabr g \in C^1$. Posons $\alpha \coloneqq g(a), \beta \coloneqq g(b)$. Alors~:
		\[\int_a^bf(x)\dif x = \int_\alpha^\beta(f \circ g)(t)g'(t)\dif t.\]
		\end{prp}

		\begin{proof} Soient $F(x) \coloneqq \int_a^xf(t)\dif t$ et $G(t) \coloneqq (F \circ g)(t)$. Dès lors, on sait que $G'(t) = (f \circ g)(t)g'(t)$.
		De plus, l'intégration bornée donne~:
		\[\int_\alpha^\beta (f \circ g)(t)g'(t)\dif t = \int_\alpha^\beta G'(t)\dif t = \evf Gt\alpha\beta
		= (F \circ g)(\beta)-(F \circ g)(\alpha) = \int_a^b f(x)\dif x.\]
		\end{proof}

	\subsection{Les intégrales impropres}

		\begin{déf} Soit $f : [a, +\infty[ \to \R$ une fonction bornée et intégrable sur tout $\interval ab$ pour $b > a$. Alors si la limite suivante existe~:
		\[\lim_{b \to +\infty}\int_a^bf(x)\dif x,\]

		on dit que $\int_a^{+\infty} f(x)\dif x$ converge. On appelle une telle limite une intégrale impropre. Si la limite n'existe pas, on dit que
		$\int_a^\infty f(x)\dif x$ diverge. De manière similaire, soit $f : ]-\infty, b] \to \R$ une fonction bornée et intégrable sur tout $\interval ab$ pour
		$a < b$. Alors si la limite suivante existe~:
		\[\lim_{a \to -\infty}\int_a^bf(x)\dif x,\]

		on dit que $\int_{-\infty}^bf(x)\dif x$ converge. Pour une fonction $f : \R \to \R$, bornée, si les deux intégrales impropres suivantes existent~:

		\[\int_{-\infty}^0f(x)\dif x, \\
		\int_0^{+\infty}f(x)\dif x,\]

		alors on définit l'intégrale suivante~:

		\[\int_{-\infty}^{+\infty}f(x)\dif x = \int_{-\infty}^0f(x)\dif x + \int_0^{+\infty}f(x)\dif x.\]
		\end{déf}

		\begin{déf} De manière similaire, pour des fonctions non bornées, on a les définitions suivantes. Soit $f : ]a, b] \to \R$. Si $f$ est intégrable
		sur tout $\interval cb$ avec $c \in ]a, b]$, alors on définit~:
		\[\int_a^bf(x)\dif x = \lim_{c \to a}\int_c^bf(x)\dif x.\]

		De même, soit $f : [a, b[ \to \R$ intégrable sur tout $\interval ac$ pour $c \in [a, b[$. On définit alors~:
		\[\int_a^bf(x)\dif x = \lim_{c \to b}\int_a^cf(x)\dif x.\]

		Et finalement soit $f : ]a, b[ \to \R$. Si les deux intégrales impropres existent, on définit~:

		\[\int_a^bf(x)\dif x = \int_a^{\frac {a+b}2}f(x)\dif x + \int_{\frac {a+b}2}^bf(x)\dif x.\]
		\end{déf}

		\begin{prp}[Critère de comparaison] Soient $f, g : [a, +\infty[ \to \R$ intégrables sur $\interval ab$ pour tout $b > a$. Si
		$\forall x > a : 0 \leq f(x) \leq g(x)$ et $\int_a^{+\infty}g(x)\dif x$ converge, alors $\int_a^{+\infty}f(x)\dif x$ converge également telle que~:
		\[\int_a^{+\infty}f(x)\dif x \leq \int_a^{+\infty}g(x)\dif x.\]
		\end{prp}

		\begin{proof} Comme $f(x) \leq g(x)$ pour tout $x > a$, on sait que pour tout $b > a$, on a~:
		\[\int_a^bf(x)\dif x \leq \int_a^bg(x)\dif x.\]

		De plus, les fonctions suivantes sont croissantes car $f$ et $g$ sont toujours positives~:

		\begin{align*}
			&F : b \mapsto \int_a^bf(x)\dif x, \\
			&G : b \mapsto \int_a^bg(x)\dif x.
		\end{align*}

		Dès lors, $F$ est majorée et bornée par $\int_a^{+\infty}g(x)\dif x$. La limite de $F(b)$ pour $b \to +\infty$ existe. \end{proof}

	\subsection{Longueur de courbes}
		
		\begin{déf} Une courbe différentiable est une application $\gamma : \ab \subseteq \R \to \R^n$ de classe $C^1$ telle que
		$\forall t : t \in \ab \Rightarrow \gamma'(t) \neq 0 \in \R^n$. \end{déf}

		\begin{déf} L'ensemble $C \coloneqq \Imf \gamma$ est la courbe différentiable associée à $\gamma$. \end{déf}

		\begin{déf} Soit $\gamma : \ab \to \R^n$ une courbe différentiable. Si $\gamma(a) = \gamma(b)$, alors $\gamma$ est un lacet. Et si $\gamma$
		est un lacet tel que $\gamma'(a) = \gamma'(b)$ (dérivées respectivement « à droite » et « à gauche »), alors $\gamma$ est un lacet différentiable.
		\end{déf}

		\begin{déf} Si $\gamma$ est une courbe injective sur $]a, b[$, alors $\gamma$ est dite simple. \end{déf}

		\begin{déf} Soit $\gamma$ une courbe paramétrée simple et différentiable. On définit sa longueur par~:
		\[L(\gamma) \coloneqq \int_a^b\norm{\gamma'(t)}\dif t.\]
		\end{déf}

		\begin{prp} Soient $f : \ab \to \interval cd \in C^1$ bijective, $\gamma : \ab \to \R^n$ et $\eta : \interval cd \to \R^n$ deux courbes
		paramétrées différentiables. Supposons que $\gamma = (\eta \circ f)$ (que $\gamma$ est une reparamétrisation de $\eta$). Alors
		$L(\gamma) = L(\eta)$. \end{prp}

		\begin{proof} Supposons $f$ croissante. Dès lors (en posant $u \coloneqq f(t)$)~:

		\begin{align*}
			L(\gamma) &= L(\eta \circ f) = \int_a^b\norm{(\eta \circ f)'(t)}\dif t = \int_a^b\norm{(\eta' \circ f)(t)f'(t)}\dif t =
			\int_a^b\norm{(\eta' \circ f)(t)}\abs{f'(t)}\dif t \\
			          &= \int_a^b\norm{(\eta' \circ f)(t)}f'(t)\dif t = \int_{f(a)}^{f(b)}\norm{\eta'(u)}\dif u = \int_c^d\norm{\eta'(u)}\dif u = L(\eta).
		\end{align*}
		
		Si $f$ était décroissant, il aurait fallu mettre un $-$ en sortant $f'(t)$ de la valeur absolue mais les bornes d'intégration $f(a)$ et $f(b)$ auraient
		respectivement donné $d$ et $c$. Donc en rentrant le moins dans les bornes d'intégration, on obtient le même résultat. \end{proof}

		\begin{déf} Soit $\gamma : \ab \to \R^n$ une courbe paramétrée différentiable telle que $\forall t : \norm{\gamma'(t)} = 1$. Alors $\gamma$
		est une paramétrisation par longueur. \end{déf}

		\begin{rmq} La longueur d'une telle courbe est $b-a$ et la distance entre deux points $p, q$ quelconques est $\abs{p-q}$. \end{rmq}

		\begin{lem} Toute courbe paramétrée différentiable possède une paramétrisation par longueur. \end{lem}

		\begin{proof} Soit $\gamma : \ab \to \R^n$ une courbe paramétrée différentiable. Définissons $\ell : \ab \to \R^+ : t \mapsto \int_a^t\gamma'(u)\dif u$.
		$\ell$ est une fonction strictement croissante et une bijection $\in C^1$. Soit $\eta = (\gamma \circ \ell^{-1})$. On a~:
		\[\norm{\eta'(t)} = \norm{\gamma((\ell^{-1})(t))(\ell^{-1})'(t)} = \norm {\gamma'((\ell^{-1})(t))} \frac 1{\abs{\ell'((\ell^{-1})(t))}}.\]

		Et comme, par définition, $\ell'(t) = \norm{\gamma'(t)}$, on a $\norm{\eta'(t)} = 1$. \end{proof}

	\subsection{Intégrales curvilignes}
		
		\begin{déf} Soient $f : E \subseteq \R^n \to \R$ et $\gamma : \ab \to \R^n$ avec $\Imf \gamma \subseteq E$. On définit l'intégrale de $f$ le
		long de la courbe $\gamma$ par~:
		\[\int_\gamma f\dif s \coloneqq \int_a^bf(\gamma(t))\norm{f'(t)}\dif t.\]
		\end{déf}

		\begin{déf} Un champ de vecteurs est une application $f : E \subseteq \R^n \to \R^n$. \end{déf}

		\begin{déf} Soient $f : E \subseteq \R^n \to \R^n$ un champ de vecteurs continu et $\gamma : \ab \to \R^n$ une courbe paramétrée différentiable.
		On définit le travail de $f$ le long de $\gamma$ par~:
		\[\int_\gamma\scpr f{\dif s} \coloneqq \int_a^b\scpr {f(\gamma(t))}{\gamma'(t)}\dif t.\]
		\end{déf}

		\begin{prp} Soient $\gamma$ et $\eta$ deux paramétrisations d'une même courbe. Alors $\int_\gamma\scpr f{\dif s} = \pm\int_\eta\scpr f{\dif s}$.
		Le signe moins peut apparaitre si $\gamma$ et $\eta$ dont d'orientation opposée. \end{prp}

	\subsection{Champs conservatifs}
		
		\begin{déf} Soit $f : E \subseteq \R^n \to \R^n$ un champ de vecteurs. $f$ est conservatif si il existe $F : E \to \R$ tel que $f = \nabla F$.
		\end{déf}

		\begin{prp} Soit $f : E \subseteq \R^n \to \R^n$ un champ conservatif. Alors $\int_\gamma\scpr f{\dif s}$ ne dépend que des extrémités de
		$\gamma$. \end{prp}

		\begin{proof} Soit $F$ tel que $f = \nabla F$.
		\[\int_\gamma\scpr f{\dif s} = \int_a^b\scpr {\nabla F(\gamma(t))}{\gamma'(t)}\dif t = \int_a^b(F(\gamma(t)))'(t)\dif t = F(\gamma(b))-F(\gamma(a)).\]
		\end{proof}

\newpage
\section{Fonctions continues à plusieurs variables}
	\subsection{Introduction}
		\begin{rmq} C'est d'abord l'espace euclidien $\R^n$ qui sera étudié ici. \end{rmq}

		\begin{déf} Soit $f : \R^m \to \R^n$ une fonction. On dit que $f$ est une fonction à $m$ variables et à valeur vectorielle. \end{déf}

		\paragraph{Rappel} Soit $U \subseteq \R^m$. On dit que $U$ est un voisinage de $a \in \R^m$ s'il existe $\delta > 0$ tel que pour tout $x \in \R^m$,
		si $\norm{x-a} < \delta$, alors $x \in U$.

	\subsection{Norme et distance dans $\R^n$}
		Le choix de la norme euclidienne pour exprimer la distance dans $\R^n$ est arbitraire. Mais une fonction de distance $d : \R^n \times \R^n \to \R$ doit
		respecter certaines propriétés.

		\begin{déf} Soit $X$ un ensemble. Une \textbf{distance} sur $X$ est une application $d : X \times X \to \R$ qui respecte les propriétés suivantes
		$\forall x, y \in \R^n$~:

		\begin{enumerate}
			\item $d(x, y) \geq 0$~;
			\item $d(x, y) = 0 \iff x = y$~;
			\item $d(x, y) = d(y, x)$~;
			\item $\forall z \in \R^n : d(x, z) \leq d(x, y) + d(y, z)$.
		\end{enumerate}
		\end{déf}

		\begin{déf} Une \textbf{norme} sur $\R^n$ est une application $\norm \cdot : \R^n \to \R$ respectant les propriétés suivantes
		$\forall x, y \in \R^n, \lambda \in \R$~:

		\begin{enumerate}
			\item $\norm x \geq 0$~;
			\item $\norm {\lambda x} = \abs \lambda \norm x$~;
			\item $\norm x = 0 \iff x = 0$~;
			\item $\norm {x+y} \leq \norm x + \norm y$.
		\end{enumerate}
		\end{déf}

		\begin{prp} La définition d'une norme sur $\R^n$ implique l'existence d'une distance $d : \R^n \times \R^n \to \R : (x, y) \mapsto \norm {x-y}$.
		\end{prp}

		\begin{proof} Soient $x, y, z \in \R^n$. Montrons les propriétés nécessaires pour une distance~:
		
		\begin{enumerate}
			\item $d(x, y) = \norm {x-y} \geq 0$~;
			\item $0 = d(x, y) = \norm {x-y} \iff x-y = 0 \iff x = y$~;
			\item $d(x, y) = \norm {x-y} = \norm{(-1)(y-x)} = \abs {-1}\norm {y-x} = \norm {y-x} = d(y, x)$~;
			\item $d(x, z) = \norm {x-z} = \norm {(x-y)+(y-z)} \leq \norm {x-y} + \norm {y-z} = d(x, y) + d(y, z)$.
		\end{enumerate}
		\end{proof}

		\begin{lem} Soient $x, y \in \R^n$. Alors $\abs {\scpr xy} \leq \sqrt {\scpr xx}\sqrt {\scpr yy}$. \end{lem}

		\begin{proof} Tout d'abord, supposons $y = 0$. On a alors~:
		\[\abs {\scpr x0} = 0 = \sqrt{\scpr xx}\sqrt {\scpr 00}.\]
		L'inégalité large est donc vérifiée puisque les deux membres sont égaux.

		On sait que $\forall \lambda \in \R : 0 \leq \scpr {x - \lambda y}{x - \lambda y} = \scpr xx - \lambda \scpr yx - \lambda(\scpr xy - \lambda \scpr yy)$.
		Supposons maintenant $y \neq 0$. Prenons $\lambda$ tel que $\scpr xy - \lambda \scpr yy = 0$ (c.-à-d. $\lambda = \frac {\scpr xy}{\scpr yy}$).
		On a donc~:
		\[0 \leq \scpr xx - \lambda yx = \scpr xx - \frac {\scpr xy}{\scpr yy}\scpr yx = \scpr xx - \frac {\scpr xy^2}{\scpr yy}.\]
		\end{proof}

	\subsection{Convergence des suites dans $\R^n$}
		\begin{déf}[Convergence de suite dans $\R^n$] Soit $(x_n)_n \subset \R^n$. $(x_n)_n$ converge en $x \in \R^n$ si~:
		\[\forall \epsilon > 0 : \exists K(\epsilon) \in \N \tq \forall k > K(\epsilon) : \norm {x_k - x} < \epsilon,\]
		ce que l'on écrit~:
		\[\lim_{k \to +\infty}\norm{x_k - x} = 0.\]
		\end{déf}

		\begin{déf} Soit $(x_k) \subset \R^n$ une suite. $(x_{k\,i})_k$ désigne la suite de la $i$ème composante de $(x_k)$. \end{déf}

		\begin{prp}\label{convergenceRnComposantes} La suite $(x_n)_n \subset \R^n$ converge en $x \in \R^n$ si et seulement si chaque suite de composante
		$(x_{k\,i})_k \subset \R$ converge en $x_i$. \end{prp}

		\begin{proof} Dans le sens direct, on suppose $x_k \to x \in \R^n$. Prenons $1 \leq i \leq n$, alors~:
		\[\abs {x_{k\,i} - x_i} \leq \norm {x_k - x} \to 0.\]
		Dans le sens indirect, on suppose que $\forall i \in \{1, \dotsc, n\} : x_{k\,i}$. Alors~:
		\[\norm {x_k - x}^2 \coloneqq \sum_{i=1}^n(x_{k\,i} - x_i)^2 \to 0.\]
		\end{proof}

		\begin{cor} Soit $(x_k)_k \subset \R^n$. Pour tout $1 \leq p, q \leq n-1$, la suite $(x_k)_k \coloneqq (y_k, z_k)_k \subset \R^p \times \R^q$
		converge si et seulement si $(y_k)$ converge dans $\R^p$ et $qz_k)$ converge dans $\R^q$. \end{cor}

		\begin{proof} Trivial par la proposition~\ref{convergenceRnComposantes}. \end{proof}

		\begin{déf} Une suite $(x_k)_k \subset \R^n$ est dite de Cauchy si $\forall 1 \leq i \leq n$, la suite $(x_{k\,i})_k \subset \R$ est de Cauchy. \end{déf}

		\begin{thm} La suite $(x_k)_k \subset \R^n$ converge si et seulement si elle est de Cauchy. \end{thm}

		\begin{lem}[Inégalité de Young] \[\forall a, b \in \R^+, p, q \in (0, +\infty) \tq p+q=pq : \frac {a^p}p + \frac {b^q}q \geq ab.\] \end{lem}

		\begin{proof} Par les propriétés de la fonction $\log$ (concavité), on a~:
		\[\log(ab) = \log\left(a^{\frac pp}\right) + \log\left(b^{\frac qq}\right) = \frac 1p\log\left(a^p\right) + \frac 1q\log\left(b^q\right) \leq \log\left(\frac {a^p}p + \frac {b^q}q\right).\]
		La fonction $\exp$ est croissante, donc on peut composer les deux membres de l'inégalité avec $\exp$, et on obtient bien l'inégalité de Young.
		\end{proof}

		\begin{déf} Soit $x \in \R^n$ un vecteur. On définit la norme $\norm \cdot_p : \R^n \to \R$ telle que~:
		\[\norm x_p = \left(\sum_{k=1}^n\abs{x_k}^p\right)^{\frac 1p}.\]
		\end{déf}

		\begin{thm}[Inégalité de Minkowski\footnote{Généralisation de l'inégalité de Cauchy-Schwartz en dimension $n \geq 1$}]
		Soient $p, q \in [1, +\infty)$ tels que $p+q = pq$. Alors $\forall x, y \in \R^n :$
		\[\abs {\sum_{i=1}^nx_iy_i} \leq \sum_{i=1}^n\abs{x_iy_i} \leq \left(\sum_{i=1}^n\abs {x_i}^p\right)^{\frac 1p} \left(\sum_{i=1}^n\abs {y_i}^q\right)^{\frac 1q}.\]
		\end{thm}

		\begin{proof} Par Young, on a le cas où $\sum_{k = 1}^n\abs {x_k}^p = \sum_{k=1}^n\abs{y_k}^q = 1$~:
		\[\abs{x_k}\abs{y_k} \leq \frac {\abs{x_k}^p}p + \frac {\abs {y_k}^q}q.\]
		En sommant sur $k$ de $1$ à $n$, on obtient~:
		\[\sum_{k = 1}^n\abs{x_k}\abs{y_k} \leq \frac 1p\sum_{k=1}^n\abs{x_k}^p + \frac 1q\sum_{k=1}^n\abs{y_k}^q = \frac 1p + \frac 1q = 1
		= \left(\sum_{k=1}^n\abs {x_k}^p\right) \left(\sum_{k=1}^n\abs{y_k}^q\right).\]

		Si $\norm x_p \neq 1$ ou $\norm y_q \neq 1$, alors on renormalise les vecteurs avec $x' \coloneqq \frac x{\norm x_p}$, $y' \coloneqq \frac y{\norm y_q}$.
		On a alors~:
		\[\frac 1{\norm x_p\norm y_q}\sum_{k=1}^n\abs{x_ky_k} = \sum_{k=1}^n\abs{x'_ky'_k} \leq 1,\]
		ou encore~:
		\[\sum_{k=1}^n\abs{x_ky_k} \leq \norm x_p\norm y_q.\]
		\end{proof}

		\begin{déf} On définit $\normfty \cdot \coloneqq \lim_{p \to +\infty} \norm \cdot_p$. Ce qui donne, pour $x \in \R^n$~:
		\[\normfty x = \max_{k=1, \dotsc, n}\abs{x_k}.\]
		\end{déf}

		\begin{déf}\label{equivNormes} Les normes $\norm \cdot_a$ et $\norm \cdot_b$ sont dites équivalentes s'il existe deux constantes $C_1, C_2 \in \R_0^+$
		telles que~:
		\[\forall x \in \R^n : C_1\norm x_b \leq \norm x_a \leq C_2\norm x_b.\]
		\end{déf}

		\begin{rmq} On observe alors que~:
		\[(\norm x_p)^p = \sum_{k=1}^n\abs{x_k}^p \leq n(\max_k\abs{x_k})^p \leq n\sum_{k=1}^n\abs{x_k}^p.\]
		En prenant la racine $p$ème des inégalités, on obtient~:
		\[\norm x_p \leq \sqrt[p]n \normfty x \leq \sqrt[p]n \norm x_p.\]

		Selon la définition~\ref{equivNormes}, on remarque que pour tout $p$, les normes $\norm \cdot_p$ et $\normfty \cdot$ sont équivalentes. On remarque
		donc que la notion de convergence (définie par $\norm \cdot_2$, la norme euclidienne) aurait pu être définie selon une norme $\norm \cdot_p$
		quelconque.\footnote{Pour être complet, toutes les normes sur $\R^n$ sont équivalentes, mais ce résultat dépasse le contenu de ce cours.}
		\end{rmq}

	\subsection{Limite d'une fonction en un point}
		\begin{déf} Soit $f : \R^m \to \R^n$. Si $m > 1$, on dit que $f$ est une fonction à plusieurs variables réelles, et si $n > 1$, on dit que $f$ est à
		valeurs vectorielles. \end{déf}

		\begin{déf}[Adéhrence en dimension $n > 1$] Soit $A \subseteq \R^n$. Un point $a \in \R^n$ est dit \textbf{adhérent} à $A$ si
		$\forall \delta > 0 : \exists x \in A \tq \norm {x-a} < \delta$.
		
		L'ensemble des points adhérents à $A$ s'appelle l'\emph{adhérence} de $A$ et se note $\adh A$. \end{déf}

		\begin{lem} Soient $A \subseteq \R^n$ et $a \in \R^n$. $a \in \adh A$ si et seulement si $\exists (x_k) \subset A \tq x_k \to a$. \end{lem}

		\begin{proof} Pour le sens indirect, soit $(x_k) \subset A$ une suite convergente en $a \in \R^n$.
		Soit $\delta > 0$. On sait qu'il existe $N(\delta) \in \N^*$ tel que $\forall n \geq N(\delta) : \norm {x_n - a} < \delta$. Donc $a \in \adh A$.

		Pour le sens direct, on prend $a \in \adh A$. On sait donc que pour tout $\delta > 0$, il existe $x(\delta) \in A \tq \norm {x(\delta) - a} < \delta$.
		On construit alors une suite $(x_k) \subset A$ telle que $x_k \coloneqq x(1/k)$ pour tout $k \geq 1$. Dès lors, soit $\delta > 0$. On sait qu'il existe
		$N = \left\lceil\frac 1\delta\right\rceil$ tel que $\forall n > N : \norm{x_n - a} < \delta$. Donc $(x_k) \subset A$ par définition, et on vient de
		montrer que $x_k \to a$. \end{proof}

		\begin{déf} La boule ouverte a été définie au~\ref{bouleOuverte}~; on définit également la boule fermée de centre $a$ et de rayon $r$ par~:
		\[\bar B(a, r) \coloneqq \{x \in \R^n \tq \norm{x-a} \leq r\} = B(a, r) \cup \{x \in \R^n \tq \norm{x-a} = r\}.\]
		\end{déf}

		\begin{rmq} L'adhérence d'un ensemble $A$ peut également se définir à l'aide de la boule ouverte~:
		\[\adh A = \{x \in \R^n \tq \forall r > 0 : B(x, r) \cup A \neq \emptyset\}.\]
		\end{rmq}

		\begin{rmq} La boule fermée $\bar B(a, r)$ est l'adhérence de la boule ouverte $B(a, r)$. \end{rmq}

		\begin{déf}[Intérieur en dimension $n > 1$] Soit $A \subset \R^n$. Un élément $a \in \R^n$ est dit \emph{intérieur} à $A$ si
		$\exists \delta > 0 \tq \forall x \in \R^n : \norm {x-a} < \delta \Rightarrow x \in A$. \end{déf}

		\begin{rmq} Géométriquement, on dit que l'intérieur $\intr A$ est l'ensemble~:
		\[\intr A \coloneqq \{x \in \R^n \tq \exists r > 0 \tq B(x, r) \subseteq A\}.\]
		\end{rmq}

		\begin{déf} La frontière de $A$ est l'ensemble $\adh A \setminus \intr A$. On le note $\partial A$. \end{déf}

		\begin{déf} Un ensemble $A \subseteq \R^n$ est~:
		\begin{itemize}
			\item \emph{ouvert} si et seulement si $A = \intr A$~;
			\item \emph{fermé} si et seulement si $A = \adh A$.
		\end{itemize}
		\end{déf}

		\begin{déf}[Limite d'une fonction multivariée à valeur vectorielle]\label{def:limfrmrn} Soient $f : U \subseteq \R^m \to \R^n$, $B \subset \R^m$, et
		$a \in \adh(U \cap B)$. La limite de $f$ dans $B$ pour $x$ tendant vers $a$ existe et vaut $L \in \R^n$ si~:
		\[\forall \epsilon > 0 : \exists \delta > 0 \tq \forall x \in \adh(U \cap B) : \norm{x-a} < \delta \Rightarrow \norm{f(x)-L} < \epsilon.\]
		On écrit cela~:
		\[\lim_{\stackrel B{x \to a}}f(x) = L\qquad\qquad\text{ ou }\qquad\qquad\lim_{\stackrel {x \to a}{x \in B}}f(x) = L.\]
		\end{déf}

		\begin{rmq} Dans la définition~\ref{def:limfrmrn}, $\epsilon$ sert à déterminer une boule ouverte $B(L, \epsilon)$ et $\delta$ sert à déterminer une
		boule ouverte $B(a, \delta)$ tels que~:
		\[f\left(B(a, \delta) \cap (B \cap U)\right) \subset B(L, \epsilon).\]
		\end{rmq}

		\begin{déf} La limite de $f : \R^m \to \R^n$ en $x$ tendant vers $a \in \R^m$ existe et vaut $L \in \R^n$ si et seulement si la limite de $f$ dans
		$B = \R^m$ existe et vaut $L$ pour $x$ tendant vers $a$. On note cela~:
		\[\lim_{x \to a}f(x) = L.\]
		\end{déf}

		\begin{déf} Soient $f : \R^m \to \R^n$, et $a \in \adh\left(\dom f \setminus \{a\}\right)$. Alors la limite pointée~:
		\[\lim_{\stackrel {x \to a}{x \neq a}}f(x) = L\]
		est vérifiée si la limite de $f$ dans $B = \R^m \setminus \{a\}$ existe et vaut $L$.
		\end{déf}

		\begin{prp} Soient $f : U \subset \R^m \to \R^n$, $B \subset \R^m$, et $a \in \adh (B \cap U)$. La limite de $f$ dans $B$ pour $x \to a$ existe et vaut
		$L$ si et seulement si $\forall A \subseteq B : $ la limite de $f$ dans $A$ de $x \to a$ existe et vaut $L$.
		\end{prp}

		\begin{prp}\label{prp:limssisuitesconvergent} Soient $f : \R^m \to \R^n$, et $a \in \adh(\dom f)$. Alors~:
		\[\lim_{x \to a}f(x) = L \iff \forall (x_n)_n \subset \dom f : x_n \to a \Rightarrow (f(x_n))_n \to L.\]
		\end{prp}

		\begin{proof} Supposons d'abord que $f(x) \to L$ quand $x \to a$.

		Soit $\epsilon > 0$. On sait qu'il existe $\delta > 0$ tel que~:
		\[\forall x \in \R^m : \norm{x-a} < \delta \Rightarrow \norm{f(x)-L} < \epsilon.\]
		Soit $(x_k) \subset \R^m$ une suite convergente en $a$. On sait qu'il existe $N > 0$ tel que~:
		\[\forall n > N : \norm{x_n-a} < \delta.\]
		Dès lors~:
		\[\norm{x_n-a} < \delta \Rightarrow \norm{f(x_n)-L} < \epsilon.\]

		Supposons ensuite que pour toute suite $(x_k) \subset \R^m$ convergente en $a$, on ait $(f(x_k))_k \subseteq \R^n$ convergente en $L$.
		Supposons par l'absurde que $f(x) \not \to L$ pour $x \to a$. On sait alors qu'il existe $\epsilon_0$ tel que~:
		\[\forall \delta_0 > 0 : \exists x \in \R^m \tq \norm{x_n-a} < \delta \land \norm{f(x_n)-L} \geq \epsilon_0.\]
		En particulier, pour $\delta_0 = \frac 1n$, on trouve~:
		\[\forall n \geq 1 : \exists x_n \in \R^m \tq \norm{x_n-a} < \delta_0 = \frac 1n \land \norm{f(x_n)-L} \geq \epsilon_0.\]
		Or, par hypothèse, o na supposé que pour toute suite $(x_k) \subset \R^m \tq x_k \to a$, la suite $(f(x_k))_k$ convergeait en $L$.
		Ce qui est une contradiction, et donc $f(x) \to L$.
		\end{proof}

	\subsection{Continuité d'une fonction}
		\begin{déf} Soient $f : \R^m \to \R^n$, et $a \in \dom f$. On dit que $f$ est continue en $a$ si~:
		\[\lim_{x \to a}f(x) = f(a),\]
		c'est-à-dire~:
		\[\forall \epsilon > 0 : \exists \delta >0 \tq \forall x \in \dom f : \norm {x-a} < \delta \Rightarrow \norm {f(x)-f(a)} < \epsilon.\]
		\end{déf}

		\begin{déf} Une fonction $f : \R^m \to \R^n$ est dite continue si elle est continue en tout point de son domaine. \end{déf}

		\begin{prp} Soient $f : \R^m \to \R^p$ et $g : \R^p \to \R^n$ deux fonctions continues. Alors $(g \circ f)$ est également continue. \end{prp}

		\begin{proof} Soit $\epsilon > 0$. On sait (par la continuité de $g$) qu'il existe $\delta_g > 0$ tel que~:
		\[\forall y \in \R^p : \norm{y - f(a)} < \delta_g \Rightarrow \norm{g(y) - g(f(a))} < \epsilon.\]
		On sait également, par continuité de $f$ qu'il existe $\delta_f > 0$ tel que~:
		\[\forall x \in \R^m : \norm{x - a} < \delta_f \Rightarrow \norm{f(x)-f(a)} < \delta_g.\]
		Dès lors, on trouve~:
		\[\forall x \in \R^m : \norm{x-a} < \delta_f \Rightarrow \norm{f(x)-f(a)} < \delta_g \Rightarrow \norm{g(f(x)) - g(f(a))} < \epsilon.\]
		\end{proof}

		\subsubsection{Fonctions à valeur réelle}
		\begin{déf} On appelle la fonction $\pi_j : \R^m \to \R$ définie par~:
		\[\pi_j((x_1, \dotsc, x_m)) = x_j\]
		la $j$ème fonction de projection.
		\end{déf}

		\begin{lem} La fonction de projection $\pi_j$ est une fonction continue. \end{lem}

		\begin{proof} Soient $\epsilon > 0$, et $a \in \R^m$. Prenons $\delta = \epsilon$. En supposant $\norm{x-a} < \delta$, on trouve~:
		\[\epsilon = \delta > \norm{x-a} = \sqrt {(x_j-a_j)^2 + \sum_{j \neq i}(x_i-a_i)^2} \geq \sqrt {(x_j-a_j)^2} = \abs {\pi_j(x)-\pi_j(a)}.\]
		On a bien $\abs {\pi_j(x)-\pi_j(a)} < \epsilon$.
		\end{proof}

		\begin{prp}\label{prp:continuitérmparsuites} Soient $f : \R^m \to \R$, et $a \in \R^m$. Alors $f$ est continue en $a$ si et seulement si~:
		\[\forall (x_k)_k \subset \R : x_k \to a \Rightarrow (f(x_k))_k \to f(a).\]
		\end{prp}

		\begin{proof} Corollaire direct de la proposition~\ref{prp:limssisuitesconvergent}. \end{proof}

		\begin{rmq} Ce résultat nous permet d'écrire~:
		\[\lim_{k \to +\infty}f(x_k) = f\left(\lim_{k \to +\infty}x_k\right)\]
		pour $f$ continue, ce qui est assez pratique pour les règles de calcul.
		\end{rmq}

		\begin{cor} Soit $f : \R^m \to \R$. S'il existe deux suites $(x_k), (y_k) \subset \R^m$ convergentes en $a$ telles que~:
		\[\lim_{k \to +\infty}f(x_k) \neq \lim_{k \to +\infty}f(y_k),\]
		alors $f$ n'est pas continue. \end{cor}

		\begin{rmq} Ce corollaire est en réalité la contraposée de la proposition~\ref{prp:continuitérmparsuites}. \end{rmq}

		\subsubsection{Fonctions à valeur vectorielle}
		\begin{thm} Une fonction $f = (f_1, \dotsc, f_n) : \R^m \to \R^n$ est continue en $a \in \R^m$ si et seulement si les $n$ fonctions $f_i$ sont
		continues en $a$.
		\end{thm}

		\begin{proof} Si $f$ est continue, alors $\forall i \in \{1, \dotsc, n\} : f_i \coloneqq (\pi_i \circ f)$ est continue par composée de fonctions
		continues.

		Si $f_i$ est continue pour tout $i$, on calcule~:
		\[\lim_{x \to a}\norm{f(x)-f(a)} \coloneqq \lim_{x \to a}\sqrt{\sum_{i=1}^n\abs{f_i(x)-f_i(a)}^2}
		= \sqrt {\sum_{i=1}^n\lim_{x \to a}\abs{f_i(x)-f_i(a)}^2} = 0,\]
		ce qui est équivalent à $\lim_{x \to a}f(x) = f(a)$.
		\end{proof}

		\begin{rmq} Pour déterminer la continuité de $f$, on peut déterminer la continuité de ses composantes. On ne peut cependant pas déterminer la continuité
		des restrictions de la fonction selon les différents arguments !
		\end{rmq}
	
	\subsection{Valeur intermédiaire et ensembles connexes par arcs}
		\begin{déf} Soit $E \subset \R^n$. On dit que l'ensemble $E$ est \emph{connexe par arcs} s'il existe $\gamma : [0, 1] \to E$ une application continue
		telle que $\gamma(0) = p$ et $\gamma(1) = q$ pour tout $p, q \in E$.

		$\gamma$ est appelé un \emph{chemine de $p$ à $q$}.
		\end{déf}

		\begin{prp} L'ensemble des ensembles connexes par arcs de $\R$ sont les intervalles. \end{prp}

		\begin{proof} Montrons que les intervalles sont des ensembles connexes. Soit $I = [p, q]$ un intervalle. Alors $\gamma : t \mapsto tq + (1-t)p$ est
		un chemin de $p$ à $q$.

		Soit $I$ un ensemble connexe par arcs. S'il y a un chemin de $p$ à $q$ dans $I$, alors $\forall r \in \R \tq p < r < q$, il faut $r \ni I$.
		\end{proof}

		\begin{thm}[Image continue d'un ensemble connexe par arcs]\label{thm:imageconnexepararcs} Soit $f : E \subset \R^m \to \R^n$ où $E$ est un ensemble
		connexe par arcs. Si $f$ est continue sur $E$, alors $f(E)$ est connexe par arcs.
		\end{thm}

		\begin{proof} Prenons $p, q \in f(E)$. Par définition, on sait qu'il existe $a, b \in E$ tels que $f(a) = p$ et $f(b) = q$. Soit $\gamma$ un chemin
		de $a$ à $b$ dans $E$. Par continuité de $\gamma$ on sait que $(f \circ \gamma)$ est continue et représente un chemin de $p$ à $q$.
		\end{proof}

		\begin{cor} Soit $f : E \subset \R^m \to \R$ une fonction continue où $E$ est un ensemble connexe par arcs. Si $\alpha < \beta$ sont deux valeurs
		prises par $f$, alors $\forall c \in (\alpha, \beta) : \exists m \in E \tq f(m) = c$.
		\end{cor}

		\begin{proof} Corollaire direct du théorème~\ref{thm:imageconnexepararcs} : $f(E)$ est un intervalle. \end{proof}

		\begin{prp} Il n'existe pas de bijection continue $f : \R^m \to \R$ pour $m > 1$. \end{prp}

		\begin{proof} Supposons par l'absurde qu'il existe $f : \R^n \to \R$ continue et bijective. Prenons $g : \R^m \setminus \{0\} \to \R \setminus \{x\}$,
		la restriction de $f$ à $\R \setminus \{x\}$ où $x = f(0) \in \R^m$. On observe que $\R^m \setminus \{0\}$ est un ensemble connexe par arcs alors
		que $\R \setminus \{x\}$ n'est pas un intervalle, ce qui contredit le théorème~\ref{thm:imageconnexepararcs}.
		\end{proof}

		\begin{rmq} Pour être complet, il est possible de déterminer une bijection de $\R^n$ dans $\R$ mais elle ne peut être continue. Ces ensembles ont donc
		la même cardinalité. Il existe un résultat similaire disant qu'il n'existe pas de bijection continue $\R^m \to \R^n$ pour $n \neq m$, mais qui dépasse
		le contenu de ce cours.
		\end{rmq}

	\subsection{Bornes atteintes et ensembles compacts}
		\begin{thm}[Théorème de Bolzano-Weierstrass dans $\R^n$]\label{thm:BolzWeierRn} Dans $\R^m$, toute suite bornée $(x_k)_k$ contient une sous-suite
		convergente.
		\end{thm}

		\begin{proof} On note $x_{k\,i}$ la $i$ème composante du vecteur $x_k$. Comme $x_k$ est bornée par hypothèse, on sait que pour tout $i$, la suite
		$(x_{k_i})_k$ est également bornée. Par le théorème~\ref{thm:BolzWeieR} de Bolzano-Weierstrass, on sait que chaque composante de la suite contient
		une sous-suite convergente.

		On nomme $(x_{k_{l^{(1)}}\,1})_k$ une sous-suite convergente de $(x_{k\,1})_k$. On s'intéresse à la suite $(x_{k_{l^{(1)}}})_k$. On prend également une
		sous-suite convergente de $(x_{k\,2})_k$ que l'on appelle $(x_{k_{m^{(2)}}\,2})_k$. On prend alors les éléments communs entre $(x_{k_{l^{(1)}}})_k$
		et $(x_{m^{(2)}})_k$ et on appelle cette suite $(x_{k_{l^{(2)}}})$. On répète cet argument au total $m$ fois afin d'obtenir une suite
		$(x_{k_{l^{(m)}}})$ qui est l'intersection entre tous les $(x_{k_{m^{(i)}}})$. Cette suite est une sous-suite de $(x_k)_k$ est est convergente en
		toutes ses composantes par construction.
		\end{proof}

		\begin{prp} Un ensemble $E \subset \R^m$ est fermé et borné si et seulement si~:
		\[\forall (x_k)_k \subset E : \exists (x_{k_l})_l \text{ convergente en }e \in E.\]
		\end{prp}

		\begin{proof} Supposons d'abord que $E$ est fermé et borné. Par Bolzano-Weierstrass, on sait que toute suite admet une sous-suite convergente en une
		valeur $e$ adhérente à $E$. Or puisque $E$ est fermé, on sait que $e \in E$.

		Supposons maintenant que toute suite dans $E$ admet une sous-suite convergente. Prenons $x \in \adh E$. Soit $(x_k)_k \subset E$, une suite convergente
		en $x$. On sait donc qu'il existe une sous-suite de $(x_k)$ qui converge dans $E$ par hypothèse, et par unicité de la limite, on sait que la sous-suite
		converge en $x$. Donc $x \in E$, et donc $\adh E \subset E$, ou encore, $E$ est fermé.

		On montre que $E$ est borné par l'absurde en trouvant $y_i \in E \setminus B(y_{i-1}, R_i)$, qui montre que la suite $(y_i)_i$ n'admet pas de sous-suite
		convergente, ce qui est une contradiction.
		\end{proof}

		\begin{déf} Un ensemble $E \subset \R^m$ est dit \emph{compact} si toute suite $(x_k) \subset E$ admet une sous-suite convergente dans $E$. \end{déf}

		\begin{thm}[Théorème des bornes atteintes]\label{thm:bornesatteintesrn} Soit $\frmr fE$ une fonction continue. Si $E$ est un ensemble fermé borné,
		alors $f(E) \subseteq \R$ est un ensemble fermé borné. En particulier, $f$ est bornée et atteint ses bornes.
		\end{thm}

		\begin{proof} Notons $m, M$ respectivement les quantités $\inf_{x \in E}f(x)$ et $\sup_{x \in E}f(x)$. Montrons que $f$ est bornée, et donc supposons par
		l'absurde que $m = -\infty$ ou $M = +\infty$.  On trouve donc une suite $(x_k) \subset E$ telle que $\abs {f(x_k)} \to +\infty$ (ou encore
		$\forall k \in \N : \abs{f(x_k)} \geq k$). Puisque $E$ est un ensemble fermé borné, on peut extraire $(x_{k_n})$ une sous-suite de $(x_k)$ convergente
		en $x \in E$. Par continuité de $f$, on sait que $f(x_{k_n})$ est également une suite convergente en $f(x)$. La fonction $f$ est donc bornée, ce qui
		contredit l'hypothèse.

		Montrons ensuite que $m \in f(E)$. (L'argument pour montrer $M \in f(E)$ est identique.) Par définition de l'infimum, il existe $(x_k) \subset E$ une
		suite telle que $f(x_k)$ est convergente en $m$. Puisque $E$ est fermé borné, on peut extraire une sous-suite $(x_{k_n})$ telle que $x_{k_n} \to x \in E$.
		On sait alors $f(x_{k_n}) \to f(x)$ par continuité de $f$, et donc $m \in f(E)$.

		Montrons maintenant que $f(E)$ est fermé. Prenons $c \in \adh f(E)$. On sait qu'il existe une suite $(x_k) \subset f(E)$ convergente en $c$. Si $x_k$
		est dans $E$, alors il existe $z_k \in E$ tel que $x_k = f(z_k)$. La suite $(z_k) \subset E$ possède une sous-suite $(z_{k_n})$ convergente en $x \in E$.
		Par continuité de $f$, on sait que $f(z_k) \to f(x) \in f(E)$. Or, par unicité de la limite, on a $c = f(x) \in f(E)$.
		\end{proof}

	\subsection{continuité uniforme}
		\begin{déf} Soit $\frmr fE$ une fonction. Si~:
		\[\forall \epsilon > 0 : \exists \delta > 0 \tq \forall x, y \in E : \norm{x-y} < \delta \Rightarrow \abs{f(x)-f(y)} < \epsilon,\]
		alors on dit que que $f$ est \emph{uniformément continue}.
		\end{déf}

		\begin{déf} Une fonction $\frmr fE$ est dite \emph{Lipschitzienne} si~:
		\[\exists \alpha > 0 \tq \forall x, y \in E : \norm{f(x)-f(y)} \leq \alpha\norm{x-y}.\]
		\end{déf}

		\begin{lem} Une fonction Lipschitzienne est uniformément continue. \end{lem}

		\begin{proof} Soit $\frmr fE$ une fonction Lipschitzienne. Soit $\epsilon > 0$. Prenons $\delta = \frac \epsilon\alpha$ où $\alpha$ est le paramètre
		lipschitzien de $f$. Supposons que $\norm{x-y} < \delta$, on trouve alors~:
		\[\abs {f(x)-f(y)} \leq \alpha\norm{x-y} < \alpha\delta = \epsilon.\]
		\end{proof}

		\begin{rmq} On remarque également que l'uniforme continuité implique la continuité. \end{rmq}

		\begin{prp} Soit $\frmr fE$ une fonction continue définie sur $E$ fermé borné. Alors $f$ est uniformément continue. \end{prp}

		\begin{proof} Par Bolzano-Weierstrass. \end{proof}

\newpage
\section{Fonctions différentiables à plusieurs variables}
	\subsection{Rappels}
		On appelle la \emph{différentielle} en $a$ l'application $(\partial_vf)(a)$ qui envoie $v$ sur $\scpr {(\nabla f)(a)}v$, où
		$(\nabla f)(a)$ est le gradient de $f$ au point $a$, donné par le vecteur $\left((\partial_if)(a)\right)_i$.

		\begin{déf} Soit $\frmrn fE$ et soient $a \in \intr E, v \in \R^m$. On dit que \emph{$f$ est dérivable au point $a$ dans la direction $v$} si~:
		\[\forall i \in \{1, \dotsb, n\} : (\partial_if)(a) \coloneqq \lim_{h \to 0}\frac {f_i(a+hv)-f_i(a)}h\qquad\text{existe}.\]
		Dans ce cas, on note~:
		\[(\partial_vf)(a) \coloneqq \lim_{h \to 0}\frac {f(a+hv)-f(a)}h = \left((\partial_vf_i)(a)\right)_i.\]
		\end{déf}

		La définition de différentiabilité des fonctions réelles a été vue à la section~\ref{sec:foncdérivables}. $\frmr fE$ est différentiable en $x \in \R^m$
		s'il existe $u \in \R^m$ tel que~:
		\[\lim_{x \to a}\frac {f(x)-f(a)-\scpr u{x-a}}{\abs {x-a}} = 0.\]

		Si un tel vecteur $u$ existe, il faut que que ce vecteur $u$ soit le gradient $(\nabla f)(a)$ de $f$. On peut dès lors déterminer les différentes
		dérivées directionnelles d'une fonction $\frmr fE$ en connaissant son gradient~:
		\[(\partial_vf)(a) = \scpr {(\nabla f)(a)}v.\]

		On extrapole cette formule dans le cas de fonctions à valeur vectorielle~:
		\[(\partial_vf)(a) = \left((\partial_vf_i)(a)\right)_i = \left(\scpr{(\nabla f_i)(a)}v\right)_i.\]

		Cette formule peut s'écrire de manière matricielle~:
		\[(\partial_vf_i)(a) =
		\begin{pmatrix}
			\pd {f_1}{x_1} & \pd {f_1}{x_2} & \ldots & \pd {f_1}{x_m} \\
			\pd {f_2}{x_1} & \pd {f_2}{x_2} & \ldots & \pd {f_2}{x_m} \\
			     \vdots    &      \vdots    & \ddots &     \vdots     \\
			\pd {f_n}{x_1} & \pd {f_n}{x_m} & \ldots & \pd {f_n}{x_m}
		\end{pmatrix}
		\begin{pmatrix}
			v_1 \\ v_2 \\ \vdots \\ v_m
		\end{pmatrix}.\]

		\begin{déf} La matrice $J_f(a) \coloneqq \left[\pd {f_i}{x_j}\right]$ est appelée la \emph{matrice Jacobienne} de $\frmrn fE$ au point $a$. \end{déf}

		\begin{rmq} La matrice jacobienne (ou le jacobien) est une généralisation du gradient : $J_f(a)$ est une matrice à $n$ lignes si $f$ est à valeurs dans
		$\R^n$. Donc si $n = 1$ (cas de fonction à valeur réelle), alors la matrice ne contient qu'une seule ligne et est donc le vecteur gradient.
		\end{rmq}

	\subsection{Différentiabilité de fonctions à valeur vectorielle}
		\begin{déf} Soient $\frmrn fE$ et $a \in \intr E$. Ont dit que $L : \R^m \to \R^n$ est une \emph{approximation linéaire de $f$ en $a$} si~:
		\[\lim_{x \to a}\frac {\norm {f(x)-f(a)-L(x-a)}}{\norm {x-a}} = 0.\]
		\end{déf}

		\begin{déf} Soient $\frmrn fE$ et $a \in \intr E$. On dit que $f$ est \emph{différentiable au point $a$} s'il existe une approximation linéaire de $f$
		en $a$. Cette application linéaire est appelé la \emph{différentielle} de $f$ en $a$ et se note $(Df)(a)$ ou $(\dif f)(a)$.
		\end{déf}

		\begin{lem} Soient $\frmrn fE$ et $a \in \intr E$. Soient $L_1, L_2$ deux approximations linéaires de $f$ autour de $a$. Alors $L_1 = L_2$. \end{lem}

		\begin{proof} On observe que la somme de deux applications linéaires reste une application linéaire. De plus, on trouve~:
		\begin{align*}
			\frac {\norm {(L_1-L_2)(x-a)}}{\norm{x-a}} &= \frac {\norm {(L_1-L_2)(x-a) + f(a) - f(x) - f(a) + f(x)}}{\norm {x-a}} \\
			&\leq \frac {\norm{-L_1(x-a)-f(a)+f(x)}}{\norm {x-a}} + \frac {\norm {f(x)-f(a)-L_2(x-a)}}{\norm {x-a}}.
		\end{align*}

		Par définition, en faisant tendre $x$ vers $a$ dans ces deux termes, on obtient 0. On a~:
		\[0 \leq \frac {\norm{(L_1-L_2)(x-a)}}{\norm{x-a}} \leq \frac {\norm{-L_1(x-a)-f(a)+f(x)}}{\norm {x-a}} + \frac {\norm {f(x)-f(a)-L_2(x-a)}}{\norm {x-a}}.\]
		Par le théorème du sandwich, on trouve que~:
		\[\lim_{x \to a}\frac {\norm{(L_1-L_2)(x-a)}}{\norm{x-a}} = 0.\]

		Soit $v \in \R^m$ tel que $v \neq 0$. On considère la suite $(x_k) \subset \R^m$ telle que $x_k = a + \frac vk$. Par hypothèse, $a \in \intr E$. Dès
		lors, il existe un voisinage de $a$ entièrement inclus dans $E$. On peut donc trouver $K$ tel que $\forall k > K : x_k \in E$. En faisant tendre $k$
		vers $+\infty$, on retrouve la même limite que juste au-dessus~:
		\[\lim_{k \to +\infty}\frac {\norm{(L_1-L_2)(x_k-a)}}{\norm{x_k-a}} = 0.\]

		Or, par linéarité, on trouve~:
		\[0 = \lim_{k \to +\infty}\frac {\norm{(L_1-L_2)(a+\frac vk-a)}}{\norm{a+\frac vk-a}}
		= \lim_{k \to +\infty}\frac {\frac 1k}{\frac 1k}\frac {\norm{(L_1-L_2)(v)}}{\norm v}.\]

		On trouve effectivement $0 = (L_1-L_2)(v)$ pour tout $v$.
		\end{proof}

		\begin{déf} Soient $f : \R^m \to \R^n$. On dit que $f$ est un petit $o$ de $\norm h$ si~:
		\[\lim_{h \to 0}\frac {\norm {f(h)}}{\norm h} = 0.\]
		Ce la se note $f(h) = o(\norm h)$.
		\end{déf}

		\begin{thm} Soient $\frmrn fE$ et $a \in \intr E$. La fonction $f$ est différentiable en $a$ si et seulement si toutes les composantes $f_i$ pour
		$1 \leq i \leq n$ sont différentiables en $a$. De plus, on détermine $(\dif f)(a) = J_f(a)$ où $J_f(a)$ est la matrice jacobienne.\footnote{
		Pour être entièrement rigoureux, on dit que $(\dif f)(a)$ est \emph{déterminée} par $J_f(a)$ car $(\dif f)(a)$ est une application linéaire et toute
		application linéaire est sous la forme $v \mapsto av$ où $a$ est une matrice. Dans un tel cas, on dit que la matrice $a$ détermine (ou définit)
		l'\emph{application linéaire associée}.}
		\end{thm}

		\begin{proof} Supposons d'abord que $f$ est différentiable en $a$. On sait alors qu'il existe $L$, une application linéaire telle que~:
		\[f(x) = f(a) + L(x-a) + o(\norm{x-a}).\]
		On note $U = [U_{i\,j}]$ la matrice associée à l'application linéaire $L$. Soit $1 \leq k \leq n$. Alors~:
		\begin{align*}
			\lim_{x \to a}\frac {\abs {f_k(x)-f_k(a) - \scpr {U_{k\,\cdot}}{x-a}}}{\norm{x-a}}
			&\leq \lim_{x \to a} \frac 1{\norm{x-a}}\sqrt{\sum_{i=1}^n\abs {f_i(x)-f_i(a)-\scpr{U_{i\,\cdot}}{x-a}}^2} \\
			&= \lim_{x \to a}\frac 1{\norm{x-a}}\norm{\left[f_i(x)-f_i(a)-\sum_{j=1}^m(U_{i\,j}(x_j-a_j))\right]_i} \\
			&= \lim_{x \to a}\frac 1{\norm{x-a}}\norm{[f_i(x)]_i - [f_i(a)]_i - \left[\sum_{j=1}^mU_{i\,j}(x_j-a_j)\right]_i} \\
			&= \lim_{x \to a}\frac 1{\norm{x-a}}\norm{f(x)-f(a)-L(x-a)}.
		\end{align*}

		Or ce dernier membre tend vers 0 par définition de la différentiabilité. Par le théorème du sandwich, en sachant que~:
		\[0 \leq \frac {\abs {f_k(x)-f_k(a) - \scpr {U_{k\,\cdot}}{x-a}}}{\norm{x-a}},\]
		on trouve que~:
		\[\lim_{x \to a}\frac {\abs{f_k(x)-f_k(a)-\scpr{U_{k\,\cdot}}{x-a}}}{\norm{x-a}} = 0,\]
		et donc la composante $f_k$ est différentiable en $a$.

		Supposons maintenant que $f_k$ est différentiable pour tout $1 \leq k \leq n$ et montrons que $f$ est différentiable. On sait que~:
		\[\forall 1 \leq i \leq n : \lim_{x \to a}\frac {f(x)-f(a)-\scpr{U_i}{x-a}}{\norm{x-a}} = 0.\]
		On définit ensuite $U = [U_{i\,j}]$ où $U_{i\,j} \coloneqq (U_i)_j$. Si $L$ est l'application linéaire telle que $L(v) = Uv$, on trouve~:
		\begin{align*}
			\lim_{x \to a}\frac {\norm{f(x)-f(a)-L(x-a)}^2}{\norm{x-a}^2}
			&= \lim_{x \to a}\frac 1{\norm{x-a}^2}\sum_{i=1}^n\abs {f_i(x)-f_i(a)-\scpr{U_{i\,\cdot}}{x-a}}^2 \\
			&= \sum_{i=1}^n\lim_{x \to a}\norm{\frac {\abs{f_i(x)-f_i(a) - \scpr{U_{i\,\cdot}}{x-a}}}{\norm{x-a}}}^2 \\
			&= \sum_{i=1}^n0^2 = 0.
		\end{align*}
		Ce qui veut dire que $f$ est différentiable.

		La seconde partie du théorème est de dire que la différentielle de $f$ est l'application associée à la matrice jacobienne $J_f(a)$.
		Afin de montrer cela, on a vu dans le cas $n=1$ qu'il était obligatoire d'avoir $U_i = (\nabla f_i)(a)$. Par définition de $U$ (mettre les vecteurs
		$U_i$ en lignes pour en faire une matrice), on a bien~:
		\[U_{i\,j} = (U_i)_j = \left((\nabla f_i)(a)\right)_j = \pd {f_i}{x_j}(a).\]
		\end{proof}

		\begin{prp}\label{prp:diffexistanceC} Soient $\frmrn fE$ et $a \in \intr E$. Si $f$ est différentiable en $a$, alors il existe $C, R \in  \R_0^+$ tels
		que~:
		\[\forall x \in B(a, r) : \norm{f(x)-f(a)} \leq \norm{x-a}.\]
		En particulier, $f$ est continue en $a$.
		\end{prp}

	\subsection{Condition suffisante de différentiabilité}
		\begin{déf} Soit $\frmrn fE$ où $E$ est ouvert. Si les dérivées partielles de $f$ existent en tout point $a \in E$ et qu'elles définissent des fonctions
		continues, on dit que $f$ est de classe $C^1$.
		\end{déf}

		\begin{thm}\label{thm:diffssipdcont} Soient $\frmrn fE$ et $a \in \intr E$. S'il existe $\delta > 0$ tel que~:
		\[\forall x \in B(a, \delta), j \in \{1, \dotsc, m\} : \pd f{x_j} : \intr E \to \R\qquad\text{ existe et est continue en $a$},\]
		alors $f$ est différentiable en $a$.
		\end{thm}

		\begin{proof} Considérons le cas où $n=1$. On sait alors revenir au cas $n \gneqq 1$ par le théorème qui dit qu'une fonction $f$ est différentiable si
		et seulement si ses composantes le sont.

		Soient $a \in \intr E, \epsilon > 0$. Par hypothèse, toutes les dérivées partielles sont continues en $a$. Donc, pour tout $1 \leq j \leq m$, il existe
		$\delta_j > 0$ (et $\delta_j < r$\footnote{Cette hypothèse nous permet que les dérivées partielles soient définies sur $B(a, \delta_j) \subset B(a, r)$
		sans souci.}) tel que pour tout $x \in B(a, \delta_j)$, on ait~:
		\[\abs{\pd f{x_j}(x) - \pd f{x_j}(a)} \leq \frac \epsilon m.\]
		On pose $\delta \coloneqq \min_{j = 1, 2, \dotsc, m}\delta_j$. Ainsi, $\forall x \in B(a, \delta)$, l'inéquation précédente est vérifiée.

		Soit $\beta \coloneqq \{e_1, \dotsb, e_m\}$ la base canonique de $\R^m$. On pose la fonction~:
		\[h_1 : [0, 1] \to \R : t \mapsto f(a + t(x-a)_1e_1).\]

		L'évaluation $h_1'(s)$ au point $s \in [0, 1]$ représente la dérivée partielle de $f$ par rapport à $x_1$ au point $a + s(x-a)_1e_1$. Par le théorème de
		la moyenne, on sait qu'il existe $t_1 \in (0, 1)$ tel que~:
		\[f(a + (x-a)_1e_1) - f(a) = h_1(1) - h_1(0) = h_1'(t_1)(x-a)_1 = \pd {}{x_1}f(a + t(x-a)_1e_1)(x-a)_1.\]

		Par choix de $\delta$, on sait que~:
		\[\abs{\pd {}{x_1}f(a + t_1(x-a)_1e_1) - \pd {}{x_1}f(a)} \leq \frac \epsilon m.\]

		En multipliant de part et d'autre par $\abs {(x-a)_1}$, on trouve~:
		\[\abs {f(a + t(x-a)e_1) - f(a) - \pd {}{x_j}f(a)(x-a)_1} \leq \frac \epsilon m\abs {(x-a)_1} \leq \frac \epsilon m \abs {x-a}.\]

		En reproduisant l'argument pour $h_2, \dotsc, h_m$ et en appliquant à nouveau le théorème de la moyenne, on trouve les inégalités suivantes~:
		\[\forall i \in \{1, \dotsc, m\} : \abs{f\left(a+ \sum_{j=1}^i(x-a)_je_j\right) - f\left(a + \sum_{j=1}^{i-1}(x-a)_je_j\right) - \pd f{x_i}(a)}
		\leq \frac \epsilon m \norm{x-a}\]

		En les sommant, on obtient~:
		\[\abs{f(x)-f(a) - \sum_{j=1}^m\pd f{x_j}(a)(x-a)_j} = \abs {f\left(a + \sum_{k=1}^m(x-a)_ke_k\right)-f(a) - \sum_{j=1}^m\pd f{x_k}(a)(x-a)_j}
		\leq \epsilon\norm{x-a},\]
		ce qui prouve que la limite tend vers 0. La fonction $f$ est donc différentiable en $a$.
		\end{proof}

		\begin{rmq} Il est habituellement plus simple d'établir les dérivées partielles et leur continuité que de définir la différentiabilité sur base de la
		définition \emph{brute}. C'est pour cela que le théorème~\ref{thm:diffssipdcont} s'avère très utile dans beaucoup de situations.
		\end{rmq}

	\subsection{Différentiation de fonction composées}
		On a vu que pour $f : \R \to \R^n$ et $g : \R^n \to \R$, la dérivée de la composée était donnée par~:
		\[(g \circ f)(a) = \scpr {(\nabla g)(f(a))}{f'(a)}.\]

		Soient maintenant deux fonctions $f : \R^m \to \R^n$ et $g : \R^m \to \R^p$. Leur composée $(g \circ f) : \R^m \to \R^p$ est bien définie. Supposons
		que $f$ est différentiable en $a$ et a $(\dif f)(a)$ pour différentielle et que $g$ est différentiable en $f(a)$ et a $(\dif g)(f(a))$ pour
		différentielle.

		\begin{thm} Soient $f : A \subset \R^m \to \R^n$ et $g : B \subset \R^n \to \R^p$.Si $f(A) \subseteq B$, $a \in \intr A$, $f(a) \in \intr B$, et si
		$f$ et $g$ sont différentiables respectivement en $a$ et $f(a)$, alors $(g \circ f)$ est différentiable en $a$ et sa différentielle est donnée par~:
		\[(\dif (g \circ f))(a) : \R^m \to \R^p : v \mapsto \left[(\dif g)(f(a)) \circ (\dif f)(a)\right]v = J_g(f(a))J_f(a)v,\]
		où $J_g$ et $J_f$ représentent respectivement la matrice jacobienne de $g$ et de $f$.
		\end{thm}

		\begin{proof} Étant donné que l'on peut réduire la différentiabilité d'une fonction vectorielle à la différentiabilité de ses composantes,
		concentrons-nous sur le cas $p=1$. On pose~:
		\[Q(x) \coloneqq \frac {(\dif g)(f(a))\left(f(x)-f(a)-(\dif f)(a)(x-a)\right)}{\norm {x-a}}
		= \sum_{k=1}^n\pd g{x_k}(f(a))\frac{f_k(x)-f_k(a)-(\dif f_k)(a)(x-a)}{\norm{x-a}}.\]
		Par différentiabilité de $f$, on peut dire que $f_k$ est différentiable en $a$ pour tout $k$. Donc en passant à la limite, on obtient~:
		\[\lim_{x \to a}Q(x) = \sum_{k=1}^n\pd g{x_k}(f(a))\lim_{x \to a}\frac {f_k(x)-f_k(a)-(\dif f_k)(a)(x-a)}{\norm{x-a}} = 0.\]
		Si $f(x) = f(a)$, alors $P(x) = 0$. Sinon, on pose~:
		\begin{align*}
			R(x) &\coloneqq \frac {g(f(x)) - g(f(a)) - (\dif f)(a)(f(x)-f(a))}{\norm{f(x)-f(a)}} \\
			S(x) &\coloneqq \frac {\norm{f(x)-f(a)}}{\norm{x-a}}.
		\end{align*}

		On peut alors réécrire $P(x) = (RS)(x)$. Par la proposition~\ref{prp:diffexistanceC}, on sait que $f(x) \to f(a)$ et par différentiabilité de $g$, on
		sait que $R(x) \to 0$ pour $x \to a$. On sait donc également que $P(x) \to 0$ pour $x \to a$. De là, on trouve~:
		\[0 = \lim_{x \to a}P(x)-Q(x) = \lim_{x \to a}\frac {g(f(x)) - g(f(a)) - (\dif g)(f(a))((\dif f)(a)(x-a)}{\norm {x-a}},\]
		ou encore $(g \circ f)$ est différentiable en $a$ et a $(\dif g)(f(a)) \circ (\dif f)(a)$ pour différentielle.
		\end{proof}

	\subsection{Changement de variables}
		\begin{thm} Soient $U, V \subseteq \R^n$. Soient $f : U \to V$ et $g : V \to \R$ deux fonction admettant des dérivées partielles continues.
		Alors la fonction définie par $h : U \to \R : x \mapsto (g \circ f)(x)$ admet des dérivées partielles définies par~:
		\[\pd h{y_j}(a) = \sum_{i=1}^n\pd g{x_i}(f(a))\pd {f_i}{y_j}(a).\]
		\end{thm}

		\begin{proof} Par le théorème~\ref{thm:diffssipdcont}, on sait que les fonctions $f$ et $g$ sont différentiables. On sait également que sa
		différentielle est donnée par~:
		\[(\dif h)(a) = (\dif g)(f(a)) \circ (\dif f)(a).\]
		On peut exprimer ses dérivées partielles (qui existent puisque $h$ est différentiable) comme~:
		\[\pd h{y_j} = (\dif h)(a)(e_j),\]
		où $e_j$ est un vecteur de la base canonique de $\R^n$.
		On peut alors exprimer la différentielle de $f$ par la différentielle de ses composantes~:
		\[(\dif f)(a)(e_j) = \left[\pd {f_i}{x_j}(a)\right]_i \in \R^n.\]

		Également, on peut écrire pour tout $u, b \in \R^n$~:
		\[(\dif g)(b)(u) = \sum_{i=1}^n\pd g{x_i}(b)u_i.\]
		En posant $u \coloneqq (\dif f)(a)(e_j)$ et $b \coloneqq f(a)$, cela revient à écrire~:
		\[(\dif g)(f(a))((\dif f)(a)(e_j)) = \sum_{i=1}^n\pd g{x_i}(f(a))\pd {f_i}{y_j}(a).\]
		On remarque que~:
		\[(\dif g)(f(a))((\dif f)(a)(e_j)) = \pd h{y_j}.\]
		\end{proof}

		\begin{rmq} En changeant ainsi les variables, on passe de $x_k$ qui sont des variables indépendantes à une collection de fonctions $f_k$ qui sont liées
		aux variables $x_k$.
		\end{rmq}

\newpage
\section{Intégration de fonctions à plusieurs variables}
	\subsection{Introduction}
		\begin{rmq} Il existe deux théories principales de l'intégration : l'intégration de Riemann et l'intégration de Lebesgue. L'intégration de Lebesgue est
		intuitivement \emph{meilleure} que l'intégration de Riemann au sens que si $f$ est une fonction intégrable au sens de Riemann, alors elle l'est également
		au sens de Lebesgue et la valeur de l'intégrale concorde alors qu'il existe des fonctions\footnote{Dont la fonction de Dirichlet définie par
		$f(x) = 1$ si $x \in \Q$ et $f(x) = 0$ si $x \not \in \Q$.} qui sont intégrables au sens de Lebesgue et qui ne le sont pas au sens de Riemann.

		L'intégration de Riemann est cependant plus simple à définir et à comprendre, Lebesgue dépasse donc le cadre de ce cours.
		\end{rmq}

	\subsection{Intégrale de Darboux sur un rectangle}
		\begin{déf} On définit un \emph{rectangle} $R$ dans $\R^n$ par~:
		\[R = \prod_{i=1}^n[a_i, b_i] = \{x \in \R^n \tq \forall 1 \leq i \leq n : a_i \leq x_i \leq b_i\}.\]
		\end{déf}

		\begin{déf} Une partition d'un intervalle $I = [a, b] \subset \R$ est un choix de $k$ nombres $t_i \in I$ tels que~:
		\[a = t_0 < t_1 < \dotsb < t_{k-1} < t_k = b.\]

		Une partition rectangulaire d'un rectangle $R = \prod_k[a_k, b_k] \subset \R^n$ est une partition de chaque intervalle de $R$.
		Cela revient à choisir $t_{i\,j}$ tels que~:
		\begin{align*}
			a_1 &= t_{1\,0} < t_{1\,1} < \dotsb < t_{1\,k_1-1} < t_{1\,k_1} = b_1 \\
			a_2 &= t_{2\,0} < t_{2\,1} < \dotsb < t_{2\,k_2-1} < t_{2\,k_2} = b_2 \\
			\vdots \\
			a_n &= t_{n\,0} < t_{n\,1} < \dotsb < t_{n\,k_n-1} < t_{n\,k_n} = b_n.
		\end{align*}
		\end{déf}

		\begin{déf} On définit $R_j$ pour $1 \leq j \leq p$ (où $p$ est le nombre de \emph{sous-rectangles} de la partition $\mathcal P$) comme étant le $j$ème
		rectangle. On définit ensuite es valeurs maximales et minimales atteintes par rectangle par~:
		\begin{align*}
			m_j &\coloneqq \inf\{f(x) \tq x \in R_j\} \\
			M_j &\coloneqq \sup\{f(x) \tq x \in R_j\}.
		\end{align*}
		\end{déf}

		\begin{déf} Soit $f : R \subset \R^n \to \R$ une fonction définie sur un rectangle de $\R^n$, et $\mathcal P$ une partition rectangulaire de $R$. Les
		sommes~:
		\begin{align*}
			\underline S(f, \mathcal P) &\coloneqq \sum_jm_j\vol(R_j), \\
			\overline  S(f, \mathcal P) &\coloneqq \sum_jM_j\vol(R_j)
		\end{align*}
		s'appellent respectivement la \emph{somme inférieure} et la \emph{somme supérieure} de Darboux de $f$ par rapport à $\mathcal P$.
		\end{déf}

		\begin{déf} On définit les intégrales respectivement \emph{inférieure} et \emph{supérieure} de Darboux de $f$ par~:
		\begin{align*}
			\underline \int_R f(x)\dif x &\coloneqq \sup_{\mathcal P} \underline S(f, \mathcal P), \\
			\overline  \int_R f(x)\dif x &\coloneqq \inf_{\mathcal P} \overline  S(f, \mathcal P).
		\end{align*}
		\end{déf}

		\begin{rmq} Il suit directement de la définition que pour toute partition $\mathcal P$, on a~:
		\[\underline S(f, \mathcal P) \leq \overline S(f, \mathcal P),\]
		et donc, en particulier~:
		\[\underline \int_R f(x)\dif x \leq \overline \int_R f(x)\dif x.\]
		\end{rmq}

		\begin{déf} On dit que la fonction $f : R \subset \R^m \to \R$ est intégrable au sens de Riemann si les intégrales inférieure et supérieure de
		Darboux sont égales~:
		\[\underline \int_R f(x)\dif x = \overline \int_R f(x)\dif x.\]
		Dans ce cas, on définit l'\emph{intégrale de Riemann} de la fonction $f$ par~:
		\[\int_R f(x)\dif x,\]
		qui vaut cette valeur commune.
		\end{déf}

		\begin{déf} Soit $f : R \subset \R^m \to \R^n$. $f$ est dite intégrable sur $R$ si toutes ses composantes sont intégrables, et on définit son intégrale
		par le vecteur correspondant aux intégrales des composantes~:
		\[\int_R f(x)\dif x = \left[\int_R f_i(x)\dif x\right]_i \in \R^n.\]
		\end{déf}

		\begin{lem}\label{lem:intégrablesiepsilonP(epsilon)} Soit $f : R \to \R$ bornée. Si pour tout $\epsilon > 0$, il existe $\mathcal P(\epsilon)$ une
		partition rectangulaire de $R$ telle que~:
		\[\overline S(f, \mathcal P(\epsilon)) - \underline S(f, \mathcal P(\epsilon)) \leq \epsilon,\]
		alors $f$ est intégrable au sens de Riemann.
		\end{lem}

		\begin{rmq} À nouveau, et pour la plupart des résultats qui suivront, le fait que la définition d'une intégrale d'une fonction à valeur vectorielle est
		définie par le vecteur dont les composantes sont les intégrales des composantes implique que les résultats basés sur une fonction
		$f : R \subset \R^m \to \R$ peuvent être étendus aux fonctions $f : R \subset \R^m \to \R^n$.
		\end{rmq}

		\begin{proof} Supposons par l'absurde que le lemme soit faux et donc que sous ces hypothèses, il existe $f$ non intégrable au sens de Riemann\footnote{
		À compter d'ici, le terme \emph{intégrable} fera toujours référence à la notion d'\emph{intégrable au sens de Riemann}.}. On peut alors prendre~:
		\[\epsilon = \inf_{\mathcal P}\overline S(f, \mathcal P) - \sup_{\mathcal P}S(f, \mathcal P) \gneqq 0.\]
		On sait par les hypothèses qu'il existe $\mathcal P(\epsilon)$ une partition de $R$ telle que~:
		\[\overline S(f, \mathcal P(\epsilon)) \leq \underline S(f, \mathcal P(\epsilon)) + \epsilon
		= \underline S(f, \mathcal P(\epsilon)) + \inf_{\mathcal P}\overline S(f, \mathcal P) - \sup_{\mathcal P}\underline S(f, \mathcal P).\]
		De plus, par définition, on sait que~:
		\[\inf_{\mathcal P}\overline S(f, \mathcal P) \leq \overline S(f, \mathcal P(\epsilon))
		\lneqq \underline S(f, \mathcal P(\epsilon)) + \inf_{\mathcal P}\overline S(f, \mathcal P) - \sup_{\mathcal P}\underline S(f, \mathcal P).\]
		En soustrayant $\inf_{\mathcal P}$ de part et d'autre, on obtient~:
		\[\sup_{\mathcal P}\underline S(f, \mathcal P) < \underline S(f, \mathcal P(\epsilon)),\]
		ce qui est une contradiction, par définition du supremum.
		\end{proof}

		\begin{prp} Si $R \subset \R^m$ est un rectangle et $f : E \to \R$ est continue, alors $f$ est intégrable. \end{prp}
		
		\begin{proof} Puisque $E$ est un rectangle, il est fermé borné, et donc $f$ est bornée également (par sa continuité). $f$ est également uniformément
		continue (puisque continue sur un ensemble fermé borné). Dès lors, pour tout $\epsilon > 0$, il existe $\delta > 0$ tel que~:
		\[\forall x, x' \in R : \norm{x-x'} < \delta \Rightarrow \abs{f(x)-f(x')} < \frac \epsilon{\vol(R)}.\]
		En choisissant $\mathcal P$ de manière à ce que chaque $R_j$ soit contenu dans une boule ouverte de diamètre $\delta$. On trouve donc, pour tout
		$R_j$ dans $\mathcal P$~:
		\[M_j(f) - m_j(f) = \sup_{R_j}f(x) - \inf_{R_j}f(x) < \frac \epsilon{\vol(R)}.\]

		On a donc construit une partition $\mathcal P$ telle que~:
		\[\overline S(f, \mathcal P) - \underline S(f, \mathcal P) = \sum_j\left(M_j(f)-m_j(f)\right)\vol(R) < \frac \epsilon{\vol(R)}\vol(R) = \epsilon.\]
		Dès lors, par le lemme~\ref{lem:intégrablesiepsilonP(epsilon)}, sait que $f$ est intégrable.
		\end{proof}

		\begin{prp}[Propriétés des intégrales]
		\begin{enumerate}
			\item Soient $f, g : R \subset \R^m \to \R$ intégrables et $\alpha, \eta \in \R$. Alors $(\alpha f + \beta g)$ est intégrable telle que~:
				\[\int_R (\alpha f + \beta g)(x)\dif x = \alpha \int_R f(x)\dif x + \beta \int_R g(x)\dif x.\]
			\item Soient $f, g : R \subset \R^m \to \R$ intégrables telles que pour tout $x \in R$, on a $f(x) \geq g(x)$, alors~:
				\[\int_R f(x)\dif x \geq \int_R g(x)\dif x.\]
			\item Soit $f : R \subset \R^m \to \R$ intégrable. Alors $\abs{f(x)}$ est également intégrable telle que~:
				\[\int_R \abs{f(x)}\dif x \geq \abs{\int_R f(x)\dif x}.\]
			\item Soit $R \subset \R^m$ un rectangle et soient $P, S$ tels que $R = P \cup S$ où $P$ et $S$ sont également des rectangles n'ayant pour seule
				intersection éventuelle leurs bords. Soit $f : R \to \R$. Alors $f$ est intégrable, si et seulement si $f$  réduit à $P$ et $f$ réduit à $S$
				sont intégrables. De plus~:
				\[\int_R f(x)\dif x = \int_P f(x)\dif x + \int_S f(x)\dif x.\]
		\end{enumerate}
		\end{prp}

		\begin{proof} Les démonstrations sont similaires à celles concernant les fonctions univariées. \end{proof}

		\begin{déf} Soit $E$ un ensemble quelconque (discret, infini dénombrable, infini indénombrable, etc.) Alors on définit la \emph{fonction caractéristique}
		de $E$\footnote{Également appelée \emph{fonction} indicatrice de l'ensemble $E$.} par~:
		\[1_E : x \mapsto \begin{cases}1 &\text{ si }x \in E\\0 &\text{ sinon}\end{cases}.\]
		\end{déf}

		\begin{rmq} Cette fonction indicatrice permet, entre autres, de définir une intégrale sur un ensemble qui n'est pas un rectangle. En effet, soit $S$
		un sous-ensemble de $\R^m$. Si $S$ n'est pas un rectangle, o peut déterminer un rectangle $R$ tel que $S \subset R$. Dans ce cas, on intègre la fonction
		$(1_Sf)(x)$ sur le rectangle $S$. La fonction s'annulant sur tous les points de $R \setminus S$ c'est bien l'aire de $f$ sur $S$ qui est obtenue.

		Attention tout de même~: la fonction $(1_Sf)(x)$ n'est (à priori) pas continue.
		\end{rmq}

	\subsection{Critère de Lebesgue}
		\begin{déf} Soit $N \subset \R^m$. On dit que $N$ est de \emph{mesure nulle} si pour tout $\epsilon > 0$, il existe une collection dénombrable de
		rectangles $R_j$ dans $\R^m$ telle que~:
		\begin{itemize}
			\item[$(i)$]  $\displaystyle N \subset \bigcup_{j=1}^{+\infty}R_j$,
			\item[$(ii)$] $\displaystyle \lim_{k \to +\infty}\sum_{j=0}^k\vol(R_j) \leq \epsilon$.
		\end{itemize}
		\end{déf}

		\begin{rmq} Pour le point $(ii)$, c'est une série qui doit être inférieure à $\epsilon$.  Les séries seront détaillées dans la section~\ref{sec:séries}.
		L'idée derrière est le fait qu'une somme d'un nombre fini de termes est bien définie, mais une somme infinie ne l'est pas directement. Dès lors, on
		passe à la limite sur $k \to +\infty$.
		\end{rmq}

		\begin{rmq} Un ensemble fini ou dénombrable est de mesure nulle. \end{rmq}

		\begin{lem} Soit $f : E \subset \R^{m-1} \to \R$ une fonction continue où $E$ est un ensemble compact. Alors le graphe de $f$ est de mesure nulle.
		\end{lem}

		\begin{proof} Soit $R$ un rectangle tel que $\dom f \subseteq R$. Soit $\epsilon > 0$. Par continuité uniforme (continuité d'une fonction bornée sur un
		ensemble compact), on sait qu'il existe $\delta > 0$ tel que~:
		\[\forall x, x' \in \dom f : \norm{x-x'} < \delta \Rightarrow \abs{f(x)-f(x')} < \frac \epsilon{\vol(R)}.\]
		Soit ensuite $\mathcal P$ une partition rectangulaire de $R$ telle que chaque rectangle $R_j$ a un diamètre\footnote{Le diamètre d'un rectangle
		correspond à la longueur de ses diagonales.} $\lneqq \delta$. Pour chaque rectangle $R_j$, on construit $R_j' \coloneqq R_j \times [m_j, M_j]$.
		Chacun de ces sous-rectangles a un volume donné par~:
		\[\vol(R_j') = \vol(R_j)(M_j-m_j) \lneqq \vol(R_j)\frac \epsilon{\vol(R)}.\]
		En sommant le volume de tous ces rectangles, on trouve un volume inférieur à $\epsilon$.
		\end{proof}

		\begin{thm}[Critère de Lebesgue] Soit $f : R \subset \R^m \to \R$ une fonction définie sur un rectangle. Soit~:
		\[D \coloneqq \{p \in R \tq f \text{ n'est pas continue en } p\}.\]
		Alors $f$ est intégrable si et seulement si $f$ est bornée et $D$ est de mesure nulle.
		\end{thm}

		\begin{rmq} Ce théorème n'est pas démontré ici. \end{rmq}

		\begin{cor}\label{cor:intpositive=0ssiNmesurenulle} Soit $f : R \to \R_0^+$ intégrable et positive définie sur un rectangle .
		Si $\int_R f(x)\dif x = 0$, alors l'ensemble~:
		\[N \coloneqq \{x \in R \tq f(x) \gneqq 0\}\]
		est de mesure nulle. De plus, si $f$ est continue, alors $N = \emptyset$ et $f$ est la fonctions constante nulle.
		\end{cor}

		\begin{proof} Soit $D$ l'ensemble des points de $R$ où $f$ n'est pas continue. Par Lebesgue, on sait que $D$ est de mesure nulle.
		Montrons que $N \subseteq D$. Supposons par l'absurde qu'il existe $p \in N$ tel que $f$ est continue en $p$. Par définition de $N$, on sait que $f(p) > 0$.
		Par la continuité de $f$ en $p$, on sait qu'il existe un rectangle $R' \subset R$ tel que $p \in R'$ et pour tout $x \in R'$, on a~:
		\[f(x) > \frac {f(p)}2.\]
		Puisque $f(x)$ est définie positive, on sait que pour tout $x$ : $(f)(x) \geq (1_{R'}f)(x)$. Dès lors~:
		\[\int_R f(x)\dif x \geq \int_R (1_{R'}f)(x)\dif x = \int_{R'} f(x)\dif x \geq \vol(R')\frac {f(p)}2 \gneqq 0.\]
		Ce qui est une contradiction. Dès lors, on sait $N \subseteq D$. Or un sous-ensemble d'un ensemble dénombrable est dénombrable et donc $N$ est de
		mesure nulle. Également, si $f$ est continue, il est évident que $D = \emptyset$, et donc forcément $N = D$.
		\end{proof}

	\subsection{Théorème de Fubini}
		\begin{thm}[Théorème de Fubini simplifié dans $\R^2$] Soit $f : [a, b] \times [c, d] \to \R$ une fonction continue définie sur un rectangle de $\R^2$.
		Les fonctions $F : [a, b] \to \R$ et $G : [c, d] \to \R$ définies par~:
		\[\begin{cases}F(s) &= \int_c^df(s, t)\dif t \\ G(t) &= \int_a^bf(s, t)\dif s\end{cases}\]
		sont continues. De plus, on a~:
		\[\int_c^dF(s)\dif s = \int_a^bG(t)\dif t = \int_R f(x)\dif x.\]
		\end{thm}

		\begin{proof} On remarque que les fonctions $F$ et $G$ sont bien définies de par la continuité de $f$. De plus, comme $R$ est fermé borné, la fonction
		$f$ est uniformément continue sur $R$. Dès lors, en posant $\epsilon > 0$, on sait qu'il existe $\delta > 0$ tel que pour tout $(s_1, t_1), s_2, t_2$,
		si $\abs {(s_1, t_1)-(s_2, t_2)} < \delta$, alors $\abs{f(s_1, t_1) - f(s_2, t_2)} < \frac \epsilon{d-c}$. Dès lors, soient $s_1, s_2$, en supposant
		$\abs{s_1-s_2} < \delta$, alors on trouve~:
		\[\abs{F(s_1) - F(s_2)} = \abs{\int_c^d\left(f(s_1, t) - f(s_2, t)\right)\dif t} \leq \int_c^d\abs{f(s_1, t)-f(s_2, t)}\dif t
		= \int_c^d\frac \epsilon{d-c} = \epsilon.\]
		On observe donc que $F$ est également uniformément continue. On détermine similairement que $G$ est uniformément continue.

		Soit $\mathcal P$ une partition rectangulaire de $R$. Par définition, on peut écrire~:
		\[\begin{cases}
			\mathcal P_1 &: a = s_0 < s_1 < \dotsb < s_{p-1} < s_p = b \\
			\mathcal P_2 &: c = t_0 < t_1 < \dotsb < t_{q-1} < t_q = d.
		\end{cases}\]

		On note alors $\mathcal P_{ij}$ le rectangle $[s_{i-1}, s_i] \times [t_{j-1}, t_j]$. Les sommes de Darboux peuvent s'exprimer ainsi~:
		\begin{align*}
			\underline S(f, \mathcal P) &= \sum_{i=1}^p\sum_{j=1}^q\abs{[s_{i-1}, s_i]}\abs{[t_{j-1}, t_j]}\inf_{(s, t)\in \mathcal P_{ij}}f(s, t)
			= \sum_{i=1}^p\sum_{j=1}^q(s_i - s_{i-1})(t_j-t_{j-1})\inf_{s \in [s_{i-1}, s_i]}\left(\inf_{t \in [t_{j-1}, t_j]}f(s, t)\right) \\
			&= \sum_{i=1}^p\left((s_i-s_{i-1})\inf_{s \in [s_{i-1}, s_i]}\left[\sum_{j=1}^q(t_j-t_{j-1})\inf_{t \in [t_{j-1}, t_j]}f(s, t)\right]\right).
		\end{align*}

		On remarque cependant que~:
		\[\sum_{j=1}^q(t_j-t_{j-1})\inf_{t \in [t_{j-1}, t_j]}f(s, t) = \underline S(t \mapsto f(s, t), \mathcal P_2) \leq \int_c^df(s, t)\dif t = F(s).\]

		On peut alors réécrire~:
		\[\underline S(f, \mathcal P) \leq \sum_{i=1}^p(s_i-s_{i-1})\inf_{s \in [s_{i-1}, s_i]}F(s) = \underline S(F, \mathcal P_1) \leq \int_a^bF(s)\dif s.\]

		De manière sensiblement équivalente, on montre que~:
		\[\overline S(f, \mathcal P) \geq \int_a^bF(s)\dif s.\]

		On sait cependant que $f$ est intégrable et donc que~:
		\[\int_R f(x)\dif x = \int_a^b F(s)\dif s.\]
		On prouve de la même manière que $\int_R f(x)\dif x = \int_c^dG(t)\dif t$.Ce qui conclut la preuve du théorème.
		\end{proof}

		\begin{thm}[Théorème de Fubini] Soient $R_1 \subset \R^{m_1}, R_2 \subset \R^{m_2}$ et $f : R_1 \times R_2 \to \R$ une fonction intégrable. Alors les
		fonctions suivantes sont intégrables également~:
		\begin{align*}
			R_1 \ni x &\mapsto \underline \int_{R_2} f(x, y)\dif y, \\
			R_1 \ni x &\mapsto \overline  \int_{R_2} f(x, y)\dif y, \\
			R_2 \ni y &\mapsto \underline \int_{R_1} f(x, y)\dif x, \\
			R_2 \ni y &\mapsto \overline  \int_{R_1} f(x, y)\dif x.
		\end{align*}

		De plus~:
		\begin{align*}
			\int_{R_1 \times R_2} f(x)\dif x
				&= \int_{R_1}\left(\underline \int_{R_2} f(x, y)\dif y\right)\dif x \\
				&= \int_{R_1}\left(\overline  \int_{R_2} f(x, y)\dif y\right)\dif x \\
				&= \int_{R_2}\left(\underline \int_{R_1} f(x, y)\dif x\right)\dif y \\
				&= \int_{R_2}\left(\overline  \int_{R_1} f(x, y)\dif x\right)\dif y.
		\end{align*}
		\end{thm}

		\begin{rmq} Lorsque la fonction $x \mapsto f(x, y)$ est intégrable pour tout $y \in R_2$, alors~:
		\[\int_{R_1} f(x, y)\dif x = \underline \int_{R_1} f(x, y)\dif x = \overline \int_{R_1} f(x, y)\dif x.\]

		Dans ce cas, le théorème de  Fubini dit que~:
		\[\int_{R_1 \times R_2} f(x, y)\dif (x, y) = \int_{R_1}\left(\int_{R_2} f(x, y)\dif y\right)\dif x.\]
		De même, si la fonction $y \mapsto f(x, y)$ est intégrable pour tout $x \in R_1$, alors Fubini dit que~:
		\[\int_{R_1 \times R_2} f(x, y)\dif (x, y) = \int_{R_2}\left(\int_{R_1} f(x, y)\dif x\right)\dif y.\]
		\end{rmq}

		\begin{cor} Soit $f : R_1 \times R_2 \to \R$ intégrable. Alors l'ensemble~:
		\[N \coloneqq \{y \in R_2 \tq x \mapsto f(x, y) \text{ n'est pas intégrable }\}\]
		est de mesure nulle.
		\end{cor}

		\begin{proof} Par définition, pour tout $y \in R_2$, n peut définir~:
		\[\Delta(y) \coloneqq \overline \int_{R_1} f(x, y)\dif x - \underline \int_{R_1} f(x, y)\dif x \geq 0.\]

		De plus, $y \in N$ si et seulement si $\Delta(y) \gneqq 0$. Or Fubini dit que $\Delta(y)$ est intégrable et que $\int_{R_2}\Delta(y)\dif y = 0$.
		On peut donc dire que $\Delta(y) = 0$ par le corollaire~\ref{cor:intpositive=0ssiNmesurenulle} sauf quand $y \in E$ tel que $E$ est de mesure nulle.
		\end{proof}

	\subsection{Changements de variables}
		\begin{déf} Soit $M$ une matrice de dimension $n \times m$. On note, pour tout $(i, j) \in \{1, \dotsc, n\} \times \{1, \dotsc, m\}$, $M(i, j)$ pour
		faire référence à la matrice $M$ à laquelle la $i$ème ligne et la $j$ème colonne ont été retirées.
		\end{déf}

		\begin{rmq} Soit $f : \R^n \to \R^n$ une fonction de classe $C^1$. On prend $a, \epsilon \in \R^n$, et on définit $R_\epsilon$ le rectangle~:
		\[R_\epsilon \coloneqq \prod_{k=1}^n[a_k, a_k+\epsilon_k].\]
		Rien ne garantit que $f(R_\epsilon)$ soit toujours un rectangle, cependant, pour $\epsilon$ \emph{tout petit}, on peut utiliser $(\dif f)(a)$, la
		différentielle de $f$ en $a$ pour approximer (linéairement) $f(R_\epsilon)$. La différentielle envoie $R_\epsilon$, un rectangle sur $f(R_\epsilon)$,
		un parallélogramme. On peut définir ce parallélogramme par $P$ engendré par les vecteurs~:
		\[f(a) + \pd f{x_1}(a)\epsilon_1, \;f(a) + \pd f{x_2}(a)\epsilon_2, \; \dotsc, \;f(a) + \pd f{x_n}(a)\epsilon_n.\]

		Il s'ensuit que le volume de ce parallélogramme est donné par le déterminant de la matrice dont les vecteurs de $P$ sont les vecteurs de $P$~:
		\[\vol(f(R_\epsilon)) = \vol(P) = \abs{\det J_f(a)}\prod_{k=1}^n\epsilon_k = \abs{\det J_f(a)}\vol(R_\epsilon).\]

		On en déduit que $\abs{\det J_f(a)}$ définit la variation infinitésimale de volume en $a$ quand les variables sont changées par la fonction $f$.
		\end{rmq}

		\begin{déf} Soit $f : U \subset \R^n \to \R^n$ de classe $C^1$ définie sur $U$ un ouvert. On dit que $f$ est un changement de variables si
		$\det J_f(a) \neq 0$ pour tout $a \in U$.
		\end{déf}

		\begin{thm}[Formule du changement de variables] Soit $f : U \subseteq \R^n \to \R^n$ définie sur $U$ un ouvert. Si $g : f(U) \to \R^n$ est une fonction
		intégrable, alors la fonction $(g \circ f)\abs{\det J_f(a)} : U \to \R^n$ est intégrable et~:
		\[\int_V g(x)\dif x = \int_U g(f(x))\abs{\det J_f(x)}\dif x.\]
		\end{thm}

		\begin{rmq} Ce théorème n'est pas démontré. \end{rmq}

	\subsection{Surfaces dans $\R^3$}
		\begin{déf} Soit $f : U \subset \R^2 \to \R^3$ injective de classe $C^1$ où $U$ est un ouvert. On dit que $f(U)$ est une \emph{surface paramétrisable} si
		pour tout $p \in U : \pd fx$ et $\pd fy$ sont linéairement indépendants. On dit que l'application $f$ est une \emph{paramétrisation} de $S$.
		\end{déf}

		\begin{rmq} Si $V \subset \R^2$ est un ouvert et si $\phi : V \to U$ est une bijection de classe $C^1$ telle que pour tout $q \in V$~:
		$(\dif \phi)(q) : U \to V$ est un isomorphisme, alors $\hat f : V \to \R^3 : x \mapsto (f \circ \phi)(x)$ a la même image que $f$ et de plus, on sait~:
		\[(\dif \hat f)(q) = (\dif f)(\phi(q))(\dif \phi)(q).\]
		De plus, puisque $(\dif \phi)(q)$ est un isomorphisme et $(\dif f)(\phi(q))$ est injective, $(\dif \hat f)(q)$ est également injective. Dès lors, $\hat f$
		est également une paramétrisation de $S$.
		\end{rmq}

		\begin{déf} Une \emph{surface paramétrisable} est un sous-ensemble $S \subset \R^3$ qui est localement paramétrisable. C'est-à-dire que pour tout
		$p \in S$, il existe $V \subset \R^3$ un ouvert contenant $p$ tel que $V \cap S$ est paramétrisable.

		S'il existe un ensemble de paramétrisations telles que toutes les paramétrisations sont localement injectives, alors la surface $S$ est dite
		\emph{plongée}.
		\end{déf}

		\subsubsection{Surfaces orientables}
		\begin{déf} Soient $u, v \in \R^3$. On définit le produit vectoriel~:
		\[u \times v \coloneqq \left(u_2v_3 - u_3v_2, -(u_1v_3 - u_3v_1), u_1v_2 - u_2v_1\right).\]
		\end{déf}

		\begin{rmq} On peut définir le produit vectoriel par le déterminant de la matrice~:
		\[\begin{pmatrix}
			e_x & e_y & e_z \\
			u_1 & u_2 & u_3 \\
			v_1 & v_2 & v_3
		\end{pmatrix}.\]
		\end{rmq}

		\begin{lem} La norme du produit vectoriel de $u, v \in \R^3$ respecte l'égalité suivante~:
		\[\norm{u \times v} = \norm u\norm v\sin\theta,\]
		où $\theta$ est l'angle formé par les deux vecteurs.
		\end{lem}

		\begin{rmq} En supposant $u \neq 0 \neq v$, on trouve que $u \times v = 0$ si et seulement si $u$ et $v$ sont colinéaires. Le produit vectoriel donne
		donc un moyen de tester l'indépendance linéaire entre deux vecteurs.
		\end{rmq}

		\begin{déf} Soit $S$ une surface paramétrée par $f : U \to \R^3$. On définit le \emph{vecteur normal unitaire positif à $S$} par~:
		\[n : U \to \R^3 : p \mapsto \frac {\pd fx(p) \times \pd fy(p)}{\norm{\pd fx(p) \times \pd fy(p)}}.\]
		\end{déf}

		\begin{rmq} Par définition d'une surface paramétrée, les vecteurs $\pd fx(p)$ et $\pd fy(p)$ sont linéairement indépendants. Dès lors diviser la norme
		du produit scalaire a du sens car cette norme est strictement positive.
		\end{rmq}

		Le produit scalaire donne un vecteur orthogonal aux vecteurs $u$ et $v$. Donc par définition, $n(p)$ donne un vecteur (unitaire) orthogonal aux
		dérivées partielles $\pd fx(p)$ et $\pd fy(p)$, et donc par extension, $n(p)$ est également orthogonal à l'image de $(\dif f)(p) : \R^2 \to \R^3$.

		De plus, il existe deux vecteurs unitaires orthogonaux au plan tangent à $f(p)$, à savoir $n(p)$ et $-n(p)$. En choisissant $\hat f$, une autre
		paramétrisation de $S$, on aura toujours un de ces deux vecteurs.

		\begin{déf} Soient $f, \hat f$ deux paramétrisations d'une même surface $S$ paramétrisable. On note respectivement $n$ et $\hat n$ les vecteurs unitaires
		normaux. On dit que $f$ et $\hat f$ sont de même orientation si $n = \widehat n$, et on dit que $f$ et $\hat f$ sont d'orientation opposée si
		$n = \hat n$.
		\end{déf}

		\begin{rmq} Le nombre d'orientations possibles étant réduit à 2, on peut subdiviser toutes le paramétrisations $f$ d'une même surface paramétrisable $S$
		en 2 classes (selon le vecteur normal unitaire).
		\end{rmq}

		\begin{déf} Une surface $S$ est dite orientable si on peut choisir toutes les paramétrisations locales telles qu'elles induisent toutes le même vecteur
		normal (donc de même orientation) en tout point où leur image se rencontre.
		\end{déf}

		\begin{rmq} Sur une surface orientable, on peut dès lors définir le champ normal $S \to \R^3$ de manière globale sur la surface. Une \emph{orientation}
		est alors un choix explicite d'une des deux possibilités du champ.

		Il existe des surfaces non-orientables (c.f. le ruban de Möbius ou la bouteille de Klein).
		\end{rmq}

	\subsection{Aire d'une surface et intégrale sur une surface}
		\begin{déf} Soit $S$ une surface paramétrée par $f : U \subseteq \R^2 \to \R^3$. L'aire de $S$ est définie par~:
		\[A(S) \coloneqq \int_S \norm{\pd fx \times \pd fy}\dif (x, y).\]
		\end{déf}

		\begin{rmq} L'idée derrière cette définition est de définir l'aire d'une surface par la valeur numérique d'un volume de hauteur 1 et de base $S$.
		\end{rmq}

		S'il faut plus d'une paramétrisation locale de la courbe pour définir l'aire, on prend une paramétrisation qui couvre la surface $S$ en y enlevant
		un nombre fini de courbes $C_1, \dotsc, C_n$. On appelle cette surface $\hat S$. L'aire d'une courbe étant nulle il est intuitivement vrai que $S$ et
		$\hat S$ ont la même aire.

		\begin{déf} Soit $g : \R^3 \to \R$ une fonction continue et soit $f : U \subset \R^2 \to \R^3$ une paramétrisation de la surface $S$. L'intégrale
		de $g$ sur $S$ est définie par~:
		\[\int_Sg\dif A \coloneqq \int_U g(f(x, y))\norm{\pd fx \pd fy}\dif (x, y).\]
		\end{déf}

		\begin{rmq} À nouveau, s'il faut plus d'une paramétrisation locale, on définit $\widehat S = S \setminus (\bigcup_{i-1}^nC_i)$ qui est la surface
		$S$ à laquelle un nombre fini de courbes a été retiré. On définit alors~:
		\[\int_S g\dif A = \int_{\hat S}g \dif A.\]
		\end{rmq}

		\subsubsection{Flux d'un champ de vecteurs au travers d'une surface}
		Si $F$ est un champ de vecteur (à savoir si $F : \R^3 \to \R^3$), alors on peut définir la notion de \emph{flux} de vecteur à travers une surface $S$.

		\begin{déf} Soient $f : U \subset \R^2$ une paramétrisation de la surface $S$ définie sur un ouvert $U$ et $F : \R^3 \to \R^3$ un champ de vecteur
		continu. Le \emph{flux de $F$ au travers de $S$} est défini par~:
		\[\int_S\scpr F{\dif n} \coloneqq \scpr F{\pd fx \times \pd fy}\dif (x, y).\]
		\end{déf}

		Dans ce contexte, $\dif n$ représente l'élément normal à la surface $S$ est \emph{vaut}\footnote{Gardons en tête que cette notation n'a un sens \--
		jusqu'à présent et dans ce cours \-- que dans une intégrale} $(\pd fx \times \pd fy)\dif x\dif y$.

		L'orientation de la paramétrisation a beaucoup d'importance : si $\hat f$ est d'orientation opposée à $f$, alors l'intégrale revient à
		\emph{parcourir la surface dans l'autre sens}. Dès lors, la notion de flux n'a de sens que pour les surfaces orientées.

	\subsection{Théorèmes fondamentaux}
		\subsubsection{Divergence et rotationnel d'un champ}
		\begin{déf} Soit $F : U \subset \R^3 \to \R^3 \in C^1$ où $U$ est un ouvert.

		On définit la \emph{divergence de $F$} parla fonction~:
		\[\divg F : U \to \R : (x, y, z) \mapsto \left(\pd {F_1}x + \pd {F_2}y + \pd {F_3}z\right)(x, y, z).\]
		La divergence se note également parfois $\divop F$.

		On définit également le \emph{rotationnel} de $F$ par~:
		\[\rot F : U \to \R^3 : (x, y, z) \mapsto \left(\pd {F_3}y - \pd {F_2}z, \pd {F_1}z - \pd {F_3}x, \pd {F_2}x - \pd {F_1}y\right)(x, y, z).\]
		Le rotationnel se note également parfois $\curl F$ ou $\rotop F$.
		\end{déf}

		On pose les notations suivantes pour $F, G$ deux champs de vecteurs et $f$ une fonction quelconques (tous les trois de classe $C^1$)~:
		\begin{align*}
			\scpr F\nabla(f) &\coloneqq F_1\pd fx + F_2\pd fy + F_3\pd fz, \\
			\scpr F\nabla(G) &\coloneqq \left(\scpr F\nabla(G_i)\right)_i.
		\end{align*}

		\begin{lem} Soient $\phi, \psi$, deux fonctions et $F, G$ deux champs de vecteurs, tous de classe $C^1$. Alors~:
		\begin{enumerate}
			\item $\nabla(\phi\psi) = \phi\nabla\psi + \psi\nabla\phi$~;
			\item $\divg {\phi F} = \phi\divg F + \scpr {\nabla\phi}F$~;
			\item $\rot {\phi F} = \phi(\rot F) - F \times (\nabla \phi)$~;
			\item $\nabla \scpr FG = F \times (\rot G) + G \times (\rot F) + \scpr F\nabla(G) + \scpr G\nabla(F)$~;
			\item $\divg {F \times G} = \scpr G{\rot F} - \scpr F{\rot G}$~;
			\item $\rot (F \times G) = F \divg G - G \divg F + \scpr G\nabla(F) - \scpr F\nabla(G)$.
		\end{enumerate}
		\end{lem}

		\begin{proof} Seuls les points 1, 2 , 3, et 5 sont démontrés ici.

		Point 1~:
		\begin{align*}
			\nabla(\phi\psi) &= \left(\pd {(\phi\psi)}x, \pd {(\phi\psi)}y, \pd {(\phi\psi)}z\right)
				= \left(\pd \phi x\psi + \phi \pd \psi x, \pd \phi y\psi + \phi\pd \psi y, \pd \phi z\psi + \phi\pd \psi z\right) \\
			&= \psi\left(\pd \phi x, \pd \phi y, \pd \phi z\right) + \phi\left(\pd \psi x, \pd \psi y, \pd \psi z\right)
				= \psi\nabla\phi + \phi\nabla\psi.
		\end{align*}

		Point 2~:
		\begin{align*}
			\divg {\phi F} &= \pd {(\phi F)_1}x + \pd {(\phi F)_2}y + \pd {(\phi F)_3}z
				= \pd \phi xF_1 + \phi\pd {F_1}x + \pd \phi yF_2 + \phi\pd {F_2}y + \pd \phi z + \phi\pd {F_3}z \\
			&= \scpr F{\nabla}(\phi) + \scpr {\nabla \phi}F.
		\end{align*}

		Point 3~:
		\begin{align*}
			\rot {\phi F} &= \left(\pd {(\phi F)_3}y - \pd {(\phi F)_2}y, \pd {(\phi F)_1}z - \pd {(\phi F)_3}x, \pd {(\phi F)_2}x - \pd {(\phi F)_1}y\right) \\
			&= \left(
					\pd \phi yF_3 + \phi \pd {F_3}y - \pd \phi zF_2 - \phi \pd {F_2}z,
					\pd \phi zF_1 + \phi \pd {F_1}z - \pd \phi xF_3 - \phi \pd {F_3}x,
					\pd \phi xF_2 + \phi \pd {F_2}x - \pd \phi yF_1 - \phi \pd {F_1}y
				\right) \\
			&= (\nabla \phi \times F) + \phi(\rot F).
		\end{align*}

		Point 5~:
		\begin{align*}
			\divg {F \times G} &= \pd {(F \times G)_1}x + \pd {(f \times G)_2}y + \pd {(F \times G)_3}z
				= \pd {(F_2G_3 - F_3G_2)}x + \pd {(F_3G_1 - F_1G_3)}y + \pd {(F_1G_2 - F_2G_1)}z \\
			&= G_1(\pd {F_3}y - \pd {F_2}z) + G_2(\pd {F_1}z - \pd {F_3}x) + G_3(\pd {F_2}x - \pd {F_1}y)
			 + F_1(\pd {G_2}z - \pd {G_3}y) + F_2(\pd {G_3}x - \pd {G_1}z) + F_3(\pd {G_1}y - \pd {G_2}x) \\
			&= G_1(\rot F)_1 + G_2(\rot F)_2 + G_3(\rot F)_3 - F_1(\rot G)_1 - F_2(\rot G)_2 - F_3(\rot G)_3 \\
			&= \scpr G{\rot F} - \scpr F{\rot G}.
		\end{align*}
		\end{proof}

		\subsubsection{Questions d'orientation}
		\begin{déf} Soit $S \subset \R^3$ une surface et soit $C$ un lacet simple de $S$. Supposons qu'il existe $S_i, S_e \subset S$ tels que
		$S_i \cap S_e = \emptyset$ et $S \setminus C = S_i \cup S_e$. Dans ce cas, on dit que $C$ est le \emph{bord} de $S_i$ (ou de $S_e$), ce qui se note~:
		\[C = \partial S_e = \partial S_e.\]
		\end{déf}

		\begin{rmq} L'orientation de $S$ et le choix de $S_i$ confèrent une orientation à $C$. \end{rmq}

		\begin{déf} Soit $\gamma : [a, b]  \to S$ une paramétrisation de $C$.On dit que $\gamma$ parcourt $C$ dans le sens \emph{positif} si pour tout
		$t \in [a, b]$, le vecteur  donné par $n(\gamma(t)) \times \gamma'(t)$ se trouve sur le côté intérieur de $S$.
		\end{déf}

		\begin{rmq} Par continuité, il suffit de vérifier cette propriété sur un point de $\gamma$. \end{rmq}

		\begin{rmq} On remarque également que si $\gamma : [a, b] \to S$ est une paramétrisation du lacet simple $C$, alors l'image de $\gamma$ est le bord de
		de $S$. Plus formellement~:
		\[\Imf \gamma = \partial S.\]
		\end{rmq}

		\subsubsection{Théorème de Cauchy-Green-Riemann}
		Le théorème CGR (également appelé théorème de Green) est une généralisation du théorème fondamental du calcul différentiel et intégral.

		\begin{déf} Soit $F : U \subset \R^3 \to \R^3$ un champ de vecteur et soit $\gamma : [a, b] \to \R^3$. On introduit la notation suivante~:
		\[F_k\dif x_k \coloneqq (F_k \circ \gamma)(t)\gamma_k'(t)\dif t.\]
		\end{déf}

		\begin{lem}\label{lem:décompositionsurfaces} Toute surface $D \subset \R^2$ peut se \emph{découper} en un ensemble de sous-surfaces de la forme~:
		\[[x_i, x_{i+1}] \times [y_j, f(x)].\]
		\end{lem}

		\begin{rmq} La preuve de ce lemme est technique, on peut donc le supposer comme vrai dans ce contexte. \end{rmq}

		\begin{lem}\label{lem:opdérivationintégration} Soient $F, G : \R^2 \to \R^2$ tels que~:
		\[F(x, y) = \int_{y_0}^yG(x, \gamma)\dif \gamma.\]
		Alors~:
		\[\pd Fx(x, y) = \int_{y_0}^y\pd Gx(x, \gamma)\dif \gamma.\]
		\end{lem}

		\begin{rmq} L'idée derrière ce lemme est de montrer que lorsque la variable d'intégration (ici $x_2$) n'est pas la variable de dérivation (ici $x_1$),
		alors les opérateurs de dérivation et d'intégration peuvent être intervertis.
		\end{rmq}

		\begin{thm}\label{thm:CGR} Soit $D \subset \R^2$ une surface dont le bord est une courbe simple de classe $C^1$ par morceaux. Si $P$ et $Q$ sont de
		classe $C^1$ sur un ouvert contenant $D$, alors~:
		\[\iint_D\left(\pd Qx - \pd Py\right)\dif (x, y) = \oint_{\partial D}(P\dif x + Q\dif y),\]
		où $\partial D$ est orienté dans le sens anti-horlogique.
		\end{thm}

		\begin{proof}\footnote{Ceci est une idée de preuve, la preuve donnée n'est pas rigoureuse.} On commence par décomposer la surface $D$ de telle façon que
		toutes les sous-surfaces soient sous la forme $[x_0, x_1] \times [y, f(x)]$, ce qui est possible grâce au lemme~\ref{lem:décompositionsurfaces}.

		De là, on intègre sur toutes les sous-surfaces et on somme toutes ces intégrales. On obtient alors l'intégrale sur le \emph{vrai bord} du fait que les
		autres côtés (que le bord $\partial D$) se compensent dans la somme (en étant parcourus dans des sens opposés). Dès lors, il suffit d'étudier le
		comportement sur une telle sous-surface pour extrapoler (par la somme).

		Supposons donc que $D$ soit sous la forme $[x_0, x_1] \times [y_0, f(x)]$. On peut donc dire que $\partial D$ est composé de quatre courbes $C^1$,
		respectivement $C_1, C_2, C_3, C_4$ où on peut \emph{visualiser}~:
		\begin{align*}
			C_1 &= \{(x, y_0) \tq x_0 \leq x \leq x_1\} \\
			C_2 &= \{(x_1, y) \tq y_0 \leq y \leq f(x_1)\} \\
			C_3 &= \{(x, f(x)) \tq x_0 \leq x \leq x_1\} \\
			C_4 &= \{(x_0, y) \tq y_0 \leq y \leq f(x_0)\}.
		\end{align*}
		Comme on a supposé $\partial D$ parcouru dans le sens anti-horlogique, on peut supposer que $\partial D$ correspond au parcours des $C_i$ dans l'ordre
		$C_1$ à $C_4$.

		À l'aide du théorème de Fubini et du théorème fondamental, calculons~:
		\[\iint_D\pd Py\dif (x, y) = \int_{x_0}^{x_1}\left(\int_{y_0}^{f(x)}\pd Py\dif y\right)\dif x
		= \int_{x_0}^{x_1}\left(P(x, f(x)) - P(x, y_0)\right)\dif x.\]

		De plus, par définition, on peut exprimer~:
		\[\int_{\partial D}P\dif x = \sum_{i=1}^4\int_{a_i}^{b_i}P\left(\gamma_{i\,1}(t), \gamma_{i\,2}(t) \cdot \gamma_{i\,1}'(t)\right)\dif t.\]
		On observe que les côtés $C_2$ et $C_4$ correspondent aux « \emph{barres verticales} » de $D$, et donc $\gamma_{i\,1}'(t)$ vaut 0 pour tout $t$ dans
		$[a, b]$. Il reste alors à sommer les intégrales pour $i \in \{1, 3\}$. On trouve alors~:
		\[\int_{\partial D}P\dif x = \int_{x_0}^{x_1}P(x, y_0)\dif x + \int_{x_1}^{x_0}P\left(x, f(x)\right)\dif x
		= \int_{x_0}^{x_1}P(x, y_0) - P\left(x, f(x)\right)\dif x = -\iint_D\pd Py\dif (x, y).\]

		Ensuite, on calcule~:
		\[\iint_D\pd Qx\dif (x, y) = \int_{x_0}^{x_1}\left(\int_{y_0}^{f(x)}\pd Qx(x, y)\dif y\right)\dif x.\]
		On note alors~:
		\[U(x, y) \coloneqq \int_{y_0}^yQ(x, s)\dif s.\]

		Par le théorème fondamental, on sait~:
		\[\pd Uy(x, y) = Q(x, y).\]
		Le lemme~\ref{lem:opdérivationintégration} dit que l'on peut écrire l'égalité suivante~:
		\[\pd Ux(x, y) = \int_{y_0}^y\pd Qx(x, s)\dif s.\]

		On sait que $\partial D$ est une courbe fermée, par définition. On sait également que l'intégrale du travail d'un champ de vecteur sur une telle courbe
		est nulle. Dès lors~:
		\[0 = \oint_{\partial D}\scpr {\nabla U}\dif\eta = \oint_{\partial D}\pd Ux\dif x + \oint_{\partial D}\pd Uy\dif y
		= \oint_{\partial D}\pd Ux\dif x + \oint_{\partial D}Q\dif y.\]
		On trouve donc~:
		\[\int_{\partial D}Q\dif y = -\int_{\partial D}\pd Ux\dif x = +\iint_D\pd {}x\left(\pd Uy(x, y)\right)\dif (x, y)
		= \iint_D\pd Qx\dif (x, y).\]

		En sommant $\int_{\partial D}P\dif x$ et $\int_{\partial D}Q\dif y$, on obtient~:
		\[\int_{\partial D}\left(P\dif x + Q\dif y\right) = \iint_D\left(\pd Qx - \pd Py\right)\dif (x, y).\]
		\end{proof}

		\subsubsection{Théorème de Stokes}
		\begin{déf} Une surface $S$ différentiable par morceaux est l'union d'un nombre fini de surfaces paramétrisables qui satisfont les propriétés suivantes~:
		\begin{enumerate}
			\item $\forall i \neq j : \int S_i \cap \int S_j = \emptyset$~;
			\item $\forall i \neq j : \partial S_i \cap \partial S_j$ est~:
			\begin{itemize}
				\item un point~;
				\item l'ensemble vide~;
				\item une courbe $C^1$ par morceaux~;
			\end{itemize}
			\item $\forall i \neq j, k \neq j, k \neq i : \partial S_i \cap \partial S_j \cap \partial S_k$ est~:
			\begin{itemize}
				\item l'ensemble vide~;
				\item un point~;
			\end{itemize}
			\item $\forall p, q \in S : \exists \gamma$, un chemin dans $S$ de $p$ à $q$~;
			\item l'ensemble de toutes les courbes ne se trouvant que sur le bord d'un unique $S_i$ dorme un nombre fini de courbes simples, fermées, disjointes
			et $C^1$ par morceaux.
		\end{enumerate}
		\end{déf}

		\begin{thm}[Théorème de Stokes] Soient $S \subset \R^3$ une surface orientée différentiable par morceaux et $\{C_i\}_{1 \leq i \leq n}$ une collection
		de lacets simples sur $S$ tels que $S \setminus \left(\bigcup_{i=1}^nC_i\right) = S_i \cup S_e$ où $S_i \cap S_e = \emptyset$ et
		$S_i^\complement \cap S_e^\complement = \bigcup_{i=1}^nC_i$. On suppose $S_i$ compact.
		Si $F$ est une champ de vecteurs $C^1$ sur un ouvert contenant $\partial S \cup S$, alors~:
		\[\iint_{S_i}\scpr {\rot F}{\dif n} = \int_{\partial S_i} \scpr F{\dif l} = \sum_{k=1}^n\int_{C_k}\scpr F{\difxyz}.\]
		\end{thm}

		\begin{proof} La preuve donnée ici ne concerne que les surfaces différentiables de classe $C^2$, ce qui est une hypothèse de régularité trop fort.
		Soit $U \subset \R^2$. On prend $f : U \to \R^3$ une paramétrisation de $S_i : f(U)$ telle que $\partial S_i = f(\partial U)$. On suppose
		$f \in C^2$. On prend également $u : \R \to \R^2$ une paramétrisation de $\partial U$. On définit~:
		\[\gamma : [a, b] \subset \R \to \R^3 : t \mapsto (f \circ u)(t),\]
		une paramétrisation de $\partial S_i$. On peut alors écrire~:
		\[\int_{\partial S_i}\scpr F{\difxyz} = \int_a^b\scpr{F(\gamma(t))}{\od \gamma t(t)}\dif t,\]
		où~:
		\[\od \gamma t(t) = \od {(f \circ u)}t(t) = \pd f{u_1}(u(t))\od {u_1}t(t) + \pd f{u_2}(u(t))\od {u_2}t(t).\]
		On peut alors réécrire~:
		\[\int_{\partial S_i}\scpr F{\difxyz}
		= \int_a^b\left(\scpr {F(f(u(t)))}{\pd f{u_1}(u(t))\od {u_1}t(t)} + \scpr{F(f(u(t)))}{\pd f{u_2}(u(t))\od {u_2}t(t)}\right)\dif t.\]

		En posant~:
		\begin{align*}
			P(u) &\coloneqq \scpr {F(f(u))}{\pd f{u_1}(u)}, \\
			Q(u) &\coloneqq \scpr {F(f(u))}{\pd f{u_2}(u)},
		\end{align*}
		on peut réécrire~:
		\begin{align*}
			\int_{\partial S_i}\scpr F{\difxyz}
			&= \int_a^b\left(\scpr {F(f(u(t)))}{\pd f{u_1}(u(t))\od {u_1}t(t)} + \scpr{F(f(u(t)))}{\pd f{u_2}(u(t))\od {u_2}t(t)}\right)\dif t \\
			&= \int_a^b\scpr {F(f(u(t)))}{\pd f{u_1}(u(t))}\od {u_1}t(t)\dif t + \scpr{F(f(u(t)))}{\pd f{u_2}(u(t))}\od {u_2}t(t)\dif t \\
			&= \int_{\partial U}P\dif x + Q\dif y.
		\end{align*}

		Par le théorème CGR (théorème~\ref{thm:CGR}), on peut écrire~:
		\[\int_{\partial U}P\dif x + Q\dif y = \iint_U\left(\pd Q{u_1} - \pd P{u_2}\right)\dif (u_1, u_2).\]

		On peut ensuite chercher à évaluer les dérivées partielles de $P$ et $Q$, ce qui donne~:
		\begin{align*}
			\pd P{u_2} &= \pd {}{u_2}\left(\scpr {F(f(u))}{\pd f{u_1}(u)}\right) = \pd {}{u_2}\sum_{j=1}^3F(f(u))_j\cdot \left(\pd f{u_1}(u)\right)_j
				= \pd {}{u_2}\sum_{j=1}^3F_j(f(u))\pd {f_j}{u_1}(u) \\
			&= \sum_{j=1}^3\pd {}{u_2}\left(F_j(f(u))\pd {f_j}{u_1}(u)\right)
				= \sum_{j=1}^3\left(\pd {}{u_2}(F_j(f(u)))\pd {f_j}{u_2}(u)\pd {f_j}{u_1} + F_j(f(u))\pd {}{u_2}\pd {f_j}{u_1}(u)\right) \\
			&= \sum_{j=1}^3\left[\sum_{k=1}^3\left(\pd {}{x_k}F_j(f(u))\pd {f_k}{u_2}(u)\pd {f_j}{u_1}(u)\right)
					+ F_j(f(u))\textstyle\frac {\partial^2 f}{\partial u_2\partial u_1}(u)\right].
		\end{align*}

		De manière similaire, on trouve~:
		\[\pd Q{u_1} = \sum_{j=1}^3\left[\sum_{k=1}^3\left(\pd {}{x_k}F_j(f(u))\pd {f_k}{u_1}(u)\pd {f_j}{u_2}(u)\right)
					+ F_j(f(u))\textstyle\frac {\partial^2 f}{\partial u_1\partial u_2}(u)\right].\]

		Puisque $f$ est de classe $C^2$, l'ordre de dérivation n'a as d'importance. Dès lors~:
		\[\frac {\partial^2 f}{\partial u_1\partial u_2}(u) = \frac {\partial^2 f}{\partial u_2\partial u_1}(u).\]

		En évaluant $\pd Q{u_2} - \pd P{u_1}$, on peut supprimer les dérivées secondes de $f$ qui se compensent. Il reste donc~:
		\[\pd Q{u_2} - \pd P{u_1} = \sum_{j=1}^3\sum_{k=1}^3\pd {F_j}{x_k}(f(u))\left(\pd {f_k}{u_1}(u)\pd {f_j}{u_2}(u) - \pd{f_k}{u_2}(u)\pd {f_j}{u_1}(u)\right).\]
		On y voit apparaître $\pd f{u_1} \times \pd f{u_2}$. De plus les éléments se somment avec $\rot F$, on retrouve bien~:
		\[\pd Q{u_1} - \pd P{u_1} = \scpr {\rot F}{\pd f{u_1} \times \pd f{u_2}}.\]
		En prenant l'intégrale double sur $U$, on retrouve bien $\iint_U\scpr {\rot F}{\dif n}$ car $\dif n$ est le vecteur normal, or par définition du produit
		vectoriel, le vecteur $\pd f{u_1} \times \pd f{u_2}$ est normal à $\pd f{u_1}$ \textbf{et} $\pd f{u_2}$.
		\end{proof}

		\subsubsection{Théorème de la divergence}
		Le théorème de la divergence est une conséquence directe du théorème de Stokes qui permet de relier l'intégrale de la divergence d'un champ de vecteurs
		défini sur un ensemble borné fermé de $\R^3$ et le flux de ce champ de vecteur à travers le bord de cet ensemble.

		\begin{thm}[Théorème de la divergence]\label{thm:divergence} Soit $\Omega \subset \R^3$ un ensemble borné, fermé et régulier dont le bord est une surface
		différentiable de classe $C^1$ par morceaux. Si $F$ est un champ de vecteur de classe $C^1$ sur un ouvert de $\R^3$ contenant $\Omega \cup \partial \Omega$,
		alors~:
		\[\iiint_\Omega\divg F\dif (x, y, z) = \iint_{\partial \Omega}\scpr F{\dif n}.\]
		\end{thm}

		\begin{rmq} Le théorème n'est pas démontré ici. \end{rmq}

	\subsection{Mesure de Jordan}
		\begin{déf} Soit $D \subset \R^2$ un ensemble borné. On définit une \emph{partition} de $D$ par une collection $D_1, \dotsc, D_m\subset D$ telle que~:
		\begin{align*}
			\bigcup_{i=1}^m D_i &= D, \\
			\forall i \neq j : D_i \cap D_j &= \emptyset.
		\end{align*}
		\end{déf}

		\begin{déf} Soient $f : D \to \R$ et $P$ une partition de $D$. On définit respectivement les \emph{sommes inférieure et supérieure de Darboux} par~:
		\begin{itemize}
			\item $L(f, P) \coloneqq \sum_{i=1}^m m_i(f)\aire(D_i)$~;
			\item $U(f ,P) \coloneqq \sum_{i=1}^m M_i(f)\aire(D_i)$,
		\end{itemize}
		où $m_i$ et $M_i$ sont définis respectivement par~:
		\begin{align*}
			m_i(f) &\coloneqq \inf\{f(x, y) \tq (x, y) \in D_i\}, \\
			M_i(f) &\coloneqq \sup\{(fx, y) \tq (x, y) \in D_i\}.
		\end{align*}
		\end{déf}

		\begin{déf} On définit respectivement les intégrales \emph{inférieure et supérieure de Darboux} par~:
		\begin{align*}
			\Larea(f) &\coloneqq \inf_P L(f, P), \\
			\Uarea(f) &\coloneqq \sup_P U(f, P).
		\end{align*}
		\end{déf}

		\begin{déf} L'ensemble $S \subset \R^2$ est dit \emph{simple} s'il peut s'écrire comme une union de rectangles dont les intérieurs sont disjoints deux
		à deux.
		\end{déf}

		\begin{déf} On définit l'aire d'un espace simple $S = \bigcup_{i=1}^kR_i$ par~:
		\[\aire(S) = \sum_{i=1}^k\aire(R_i).\]
		\end{déf}

		\begin{prp} Soient $S_1, S_2$ deux ensembles simples. Si $S_1 \subseteq S_2$, alors $\aire(S_1) \leq \aire(S_2)$.
		\end{prp}

		\begin{déf} Soit $D \subset \R^2$ un ensemble borné. On définit respectivement ses aires intérieure et extérieure par~:
		\begin{align*}
			\aireint(D) &\coloneqq \sup_{s \subseteq D}\aire(S), \\
			\aireext(D) &\coloneqq \inf_{S \supseteq D}\aire(S).
		\end{align*}
		\end{déf}

		\begin{déf} Un ensemble $D \subset \R^2$ est dit \emph{mesurable au sens de Jordan} si~:
		\[\aireint(D) =\aireext(D).\]

		Si $D$ est mesurable au sens de Jordan, on définit son aire par=
		\[\aire(D) = \aireint(D) = \aireext(D).\]
		\end{déf}

	\subsection{Intégrale de Darboux sur un ensemble Jordan-mesurable}
		\begin{déf} On dit qu'une partition $\mathcal P$ de $D$ est \emph{en sous-ensembles mesurables au sens de Jordan} si
		$\mathcal P = \{D_i\}_{1 \leq i \leq m}$ tel que $D_i$ est mesurable au sens de Jordan pour tout $i$.
		\end{déf}

		\begin{déf} On définit $\mathcal P_D$, l'ensemble des partitions de $D \subset \R^2$ en sous-ensembles mesurables au sens de Jordan.
		\end{déf}

		\begin{déf} Soit $D \subset \R^2$ borné et mesurable au sens de Jordan. La fonction $f : D \to \R$ est dite \emph{intégrable au sens de Riemann} si~:
		\[\Uarea_D(f) = \inf_{P \in \mathcal P_D}U(f, P) = \sup_{P \in \mathcal P_D}L(f, P) = \Larea_D(f).\]
		\end{déf}

		\begin{rmq} Les notations suivantes sont équivalentes~:
		\begin{align*}
			\overline{\iint_D}f(x, y)\dif (x, y) = \Uarea_D(f), \\
			\underline{\iint_D}f(x, y)\dif (x, y) = \Larea_D(f).
		\end{align*}
		\end{rmq}

		\begin{lem} Cette définition est équivalente à la définition d'intégration sur domaine rectangulaire dans le cas où $D$ est un rectangle.
		\end{lem}

		\begin{proof} Supposons que pour tout $\epsilon > $, il existe $P_\epsilon$ une partition rectangulaire telle que~:
		\[U(f, P_\epsilon) - L(f, P_\epsilon) < \epsilon.\]
		On en déduit que $\inf_{P \in \mathcal P_D}U(f, P) = \sup_{P \sin \mathcal P_D}L(f, P)$ car les partitions rectangulaires $\subset \mathcal P_D$.

		Supposons que $\inf_{\mathcal P_D}U(f, P) = \sup_{\mathcal P_D}L(f, P)$ et montrons que pour tout $\epsilon > 0$, il existe une partition $P$ telle que~:
		\[U(f, P) - L(f, P) < \epsilon.\]
		Pour cela, on pose $\epsilon > 0$, et on sait qu'il existe $P_\epsilon \in \mathcal P_D$ telle que~:
		\[U(f, P_\epsilon) - L(f, P_\epsilon) < \frac \epsilon 2.\]
		On note $P_\epsilon = \{D_i\}_{1 \leq i \leq n}$. On définit $\alpha \in \N^*$ et la partition $\mathcal R_\alpha$ composée des sous-rectangles obtenus
		en découpant les côtés de $D$ (qui est rectangulaire) en $2^\alpha$. Chacun de ces sous-rectangles est soit intérieur à un $D_\gamma$ soit contient des
		points d'un ou plusieurs bords des $D_i$.

		On note $\mathcal A_i$ l'ensemble de tous les rectangles intérieurs à $D_i$ et $\mathcal B$ l'ensemble des rectangles intérieurs à aucun $D_i$. On note
		également $\mathcal B_i$ l'ensemble des rectangles de $\mathcal R_\alpha$ qui touchent le bord de $D_i$.

		Les ensembles $S_i$ et $S_i'$ définis comme suit~:
		\begin{align*}
			S_i  &\coloneqq \bigcup_{R \in \mathcal A_i}R, \\
			S_i' &\coloneqq S_i \cup \bigcup_{R \in \mathcal B_i}R
		\end{align*}
		sont respectivement un ensemble simple contenu dans $D_i$ et un ensemble simple contenant $D_i$.
		
		On observe que~:
		\[\lim_{\alpha \to +\infty}\aire(S_i \setminus S_i') = 0.\]

		Dès lors, pour tout $\eta > 0$, il existe $\alpha_0$ tel que si $\alpha \geq \alpha_0$, alors~:
		\[\aire\left(\bigcup_{R \in \mathcal B_i}R\right) < \frac \eta n.\]

		On peut également écrire~:
		\begin{align*}
			L(f, \mathcal R_\alpha) &= \sum_{R \in \mathcal R_\alpha}\aire(R)\inf\{f(x, y) \tq (x, y) \in R\} \\
				&= \sum_{i=1}^n\sum_{R \in \mathcal A_i}\aire(R)\inf\{f(x, y) \tq (x, y) \in R\}
					+ \sum_{R \in \mathcal B}\inf\{f(x, y) \tq (x, y) \in R\}\aire(R).
		\end{align*}

		Et $D_i$ est mesurable au sens de Jordan, donc~:
		\[\sum_{R \in \mathcal A_i}\aire(R) - \aire(D_i) \leq \eta.\]
		Puisque $\mathcal A_i \subset D_i$, on peut écrire~:
		\[\sum_{i=1}^n\sum_{R \in \mathcal A_i}\inf_{(x, y) \in R}f(x, y)\aire(R) \leq \sum_{i=1}^nm_i(f)\sum_{R \in \mathcal A_i}\aire(R),\]
		où $m_i(f)$ est le minimum atteint par $f$ sur $D_i$. On peut donc écrire également~:
		\[\sum_{i=1}^n\sum_{R \in \mathcal A_i}\inf_{(x, y) \in R}f(x, y)\aire(R) \leq \sum_{i=1}^nm_i(f)\aire(D_i) - Mn\eta,\]
		que l'on modifie en~:
		\[L(f, \mathcal R_\alpha) \geq L(f, P_\epsilon) - (Mn + m)\eta.\]
		En prenant $\eta$ suffisamment petit (et donc $\alpha$ suffisamment grand), on peut écrire~:
		\[L(f, \mathcal R_\alpha) \geq L(f, P_\epsilon) - \frac \epsilon 4.\]

		Par un raisonnement similaire, on trouve une partition $\mathcal R_{\alpha'}$ telle que~:
		\[U(f, \mathcal R_{\alpha'} \leq U(f, P_\epsilon) + \frac \epsilon 4.\]

		En prenant $\beta \coloneqq \max(\alpha, \alpha')$, on a $\mathcal R_\beta$, une partition telle que~:
		\[U(f, \mathcal R_\beta) - L(f, \mathcal R_{\beta}) \leq U(f, P_\epsilon) + \frac \epsilon 4 - L(f, P_\epsilon) + \frac \epsilon 4
		< \frac \epsilon 2 + 2 \frac \epsilon 4 = \epsilon.\]
		\end{proof}

		\begin{thm} Soit $f : [a, b] \to \R$ positive et bornée. La région du plan définie par~:
		\[D_f = \{(x, y) \in \R^2 \tq a \leq x \leq b, 0 \leq y \leq f(x)\}\]
		est mesurable au sens de Jordan si et seulement si la fonction $f$ est intégrable au sens de Riemann.
		\end{thm}

		\begin{thm} Soit $f : \R^2 \to \R$ bornée. Si $D \subset \R^2$ est un ensemble ouvert mesurable au sens de Jordan, et si $f$ est intégrable sur $D$,
		alors $(f1_D)$ est intégrable sur tout rectangle ouvert $R$ tel que $D \subseteq R$ et~:
		\[\iint_D f(x, y)\dif (x, y) = \iint_R (f1_D)(x, y)\dif (x, y).\]
		\end{thm}

		\begin{thm} Soit $f : I \times J \to \R$ bornée sur $R = I \times J$. Alors~:
		\begin{align*}
			\overline {\iint_R}f(x, y)\dif (x, y) &\geq \overline {\int_I}\left(\overline {\int_J} f(x, y)\dif y\right)\dif x, \\
			\overline {\iint_R}f(x, y)\dif (x, y) &\geq \overline {\int_J}\left(\overline {\int_I} f(x, y)\dif x\right)\dif y, \\
			\underline {\iint_R}f(x, y)\dif (x, y) &\leq \underline {\int_I}\left(\underline {\int_J} f(x, y)\dif y\right)\dif x, \\
			\underline {\iint_R}f(x, y)\dif (x, y) &\leq \underline {\int_J}\left(\underline {\int_I} f(x, y)\dif x\right)\dif y.
		\end{align*}
		\end{thm}

		\begin{déf} On introduit les fonctions suivantes~:
		\begin{align*}
			\underline {\int_I}f(x, \cdot) : J \to \R : y \mapsto \underline {\int_I} f(x, y)\dif x, \\
			\underline {\int_J}f(\cdot, y) : I \to \R : x \mapsto \underline {\int_J} f(x, y)\dif y, \\
			\overline  {\int_I}f(x, \cdot) : J \to \R : y \mapsto \overline  {\int_I} f(x, y)\dif x, \\
			\overline  {\int_J}f(\cdot, y) : I \to \R : x \mapsto \overline  {\int_J} f(x, y)\dif y.
		\end{align*}
		\end{déf}

		\begin{thm}[Théorème de Fubini] Soit $f : I \times J \to \R$ bornée et intégrable sur $R = I \times J$. Alors les fonctions $\underline {\int_J}f(\cdot, y)$
		et $\overline {\int_J}f(\cdot, y)$ sont intégrables au sens de Riemann sur $I$ et les fonctions $\underline {\int_I}f(x, \cdot)$ et
		$\overline {\int_I}f(x, \cdot)$ sont intégrables au sens de Riemann sur $J$.

		De plus, si la fonction $f(x, \cdot)$ est intégrable sur $J$ pour tout $x \in I$, alors~:
		\[\iint_{I \times J}f(x, y)\dif (x, y) = \int_I\left(\int_J f(x, y)\dif y\right)\dif x.\]
		De même, si la fonction $f(\cdot, y)$ est intégrable sur $I$ pour tout $y \in J$, alors~:
		\[\iint_{I \times J}f(x, y)\dif (x, y) = \int_J\left(\int_I f(x, y)\dif x\right)\dif y.\]
		\end{thm}

\newpage
\section{Les séries}\label{sec:séries}
	\subsection{Suites réelles ou complexes}
		On a une notion de convergence dans $\R^n$, et donc en particulier dans $\R^2$. Cela induit une convergence dans $\C$.

		\begin{rmq} Dans cette section, on note $\K$ un corps désignant tant bien les réels que les complexes. \end{rmq}

		\begin{rmq} Si $z \in \K$, la quantité $\abs z$ représente sa valeur absolue s'il est réel et son module s'il est complexe. \end{rmq}

		\begin{déf} La suite $(z_k) \subset \K$ converge vers $z$ si~:
		\[\forall \epsilon > 0 : \exists N \in \N \tq \forall k \geq N : \abs {z_k-z} < \epsilon.\]
		\end{déf}

		\begin{prp} La suite complexe $(z_k)$ converge si et seulement si les suites réelles définies par $(\Re z_k)$ et $(\Im z_k)$ convergent.
		Dans ces conditions, on a~:
		\[\lim_{k \to +\infty}z_k = \lim_{k \to +\infty}\Re z_k + i\lim_{k \to +\infty}\Im z_k.\]
		\end{prp}

		\begin{déf} Une \emph{série} est une somme infinie des éléments d'une suite. On dit que la suite $(a_k)$ est le \emph{terme général} de la série~:
		\[\sum_{k \geq 0}a_k \equiv \sum_{k=0}^\infty a_k.\]
		\end{déf}

		\begin{déf} On dit que la série $\sum_{k=0}^\infty a_k$ \emph{converge} si la suite des \emph{sommes partielles} dans $\K$ définie par~:
		\[s_n = \sum_{k=0}^n a_k\]
		converge dans $\K$. La limite de cette suite est appelée la \emph{somme} de la série et peut se noter des manières suivantes~:
		\[\sum_{k \geq 0} a_k = \sum_{k=0}^\infty a_k = \sum_k a_k = \sum a_k.\]
		Si cette suite des sommes partielles ne converge pas, on dit que la série diverge.
		\end{déf}

		\begin{prp}
		\begin{itemize}
			\item La série $\sum_k a_k$ converge si et seulement si pour tout $m$, la série $\sum_{k \geq m}a_k$ converge~;\footnote{Mais la somme de ces deux
			séries n'est pas identique.}
			\item soit $(x_k) \subset \R^+$. S'il existe $K \in \N$ et $\epsilon > 0$ tels que pour tout $k \geq K : x_k > \epsilon$, alors la série diverge~;
			\item si la série $\sum_k a_k$ converge, alors le \emph{reste} défini par $\sum_{k=n}^\infty a_k$ tend vers 0 pour $n \to +\infty$.
		\end{itemize}
		\end{prp}

		\begin{prp}\label{prp:siserieconvalorstermevers0} Soient $(a_k) \subset \K$ et $\sum_k a_k$. Si la série $\sum_k a_k$ converge, alors $a_k \to 0$ pour
		$k \to +\infty$.
		\end{prp}

		\begin{proof} Par définition, si la série converge, c'est que la suite des sommes partielles converge. Dès lors, la suite $s_n$ est une suite de Cauchy.
		On peut donc dire que pour $\epsilon > 0$, il existe $N \in \N$ tel que~:
		\[\forall m > n > N : \abs {s_m - s_n} = \abs {\sum_{k \geq n+1}} < \epsilon.\]
		En particulier, on peut exprimer~:
		\[\abs {s_{n+1} - s_n} = \abs {a_{n+1}} < \epsilon.\]
		On y voit donc bien que $a_k \to 0$ pour $k \to +\infty$.
		\end{proof}

		\begin{lem}[Formule de la somme d'une suite géométrique] Soit $(z_k) \subset \K$ une suite géométrique. On peut donc dire que $z_k = z^k$ pour
		$z \in \K$. La $n$ème somme partielle de la série $\sum_k z_k$ est donnée par~:
		\[s_n = \geomsum zn\]
		\end{lem}

		\begin{proof} Par définition de la suite des sommes partielles, on trouve~:
		\[\sum_{k=0}^{n+1}z^k = s_n + z^{n+1} = 1 + \sum_{k=0}^nz^{k+1} = 1 + zs_n.\]
		On a donc~:
		\[s_n + z^{n+1} = 1 + zs_n,\]
		ce qui revient à écrire~:
		\[s_n = \geomsum zn.\]
		\end{proof}

		\begin{cor} Une série de suite géométrique $(z^k) \subset \K$ converge si et seulement si $\abs z \lneqq 1$. \end{cor}

		\begin{proof} Si $\abs z < 1$, alors pour $n \to +\infty$, on a $z^{n+1} \to 0$, et donc $s_n \to \frac 1{1-z}$.

		Si $\abs z \geq 1$, alors le terme général $z_k$ ne tend pas vers 0 pour $k \to +\infty$, et donc par la
		proposition~\ref{prp:siserieconvalorstermevers0}, on sait que la série ne converge pas.
		\end{proof}

	\subsection{Critères de convergence}
		\begin{thm}[Convergence des séries à termes positifs] Soit $(a_k) \subset \R$ une suite positive ($\forall k \in \N : a_k \geq 0$).
		La série $\sum_k a_k$ converge si et seulement si la suite des sommes partielles est bornée.
		\end{thm}

		\begin{proof} La suite des sommes partielles est croissante. Par convergence des suites, on sait qu'elle converge si et seulement si elle est bornée, et
		dans ce cas, $s_n \to \sup s_n$. Sinon, $s_n \to +\infty$.
		\end{proof}

		\begin{thm}[Critère de Leibniz] Soit $(a_k) \subset \R$ une suite définie positive. Si $\lim_{k \to +\infty}a_k = 0$ et si $a_k$ est décroissante, alors
		la série $\sum_k(-1)^ka_k$ converge.
		\end{thm}

		\begin{proof} En \emph{divisant} la suite des sommes partielles en 2 (les indices pairs et impairs), on trouve une suite croissante et une suite
		décroissante~:
		\[s_{2n+2} - s_{2n} = \sum_{k=2n+1}^{2n+2}(-1)^a_k = -a_{2n+1} + a_{2n+2} \leq 0,\]
		et~:
		\[s_{2n+3} - s_{2n+1} = \sum_{k=2n+2}^{2n+3}(-1)^ka_k = a_{2n+2} - a_{2n+3} \geq 0.\]

		Ces deux suites réelles convergent dans $\R$ si et seulement si elles sont respectivement minorée et majorée. On a supposé $a_k$ décroissante.
		Dès lors~:
		\[s_{2n+1} \leq s_{2n} \leq s_0,\]
		et~:
		\[s_{2n} \geq s_{2n+1} \geq s_1.\]

		On note alors $s \coloneqq \inf s_{2n}$ et $t \coloneqq \sup s_{2n+1}$. On observe ensuite~:
		\[t-s = \lim_{n \to \infty}s_{2n+1}-s_{2n} = -\lim_{n \to +\infty}a_{2n+1} = 0.\]
		On a donc $t = s$. Ce la veut dire que pour tout $\epsilon > 0$, il existe $N_1$ et $N_2$ dans $\N$ tels que~:
		\begin{align*}
			\forall n \geq N_1 &: \abs {s_{2n+1}-t} < \epsilon, \\
			\forall n \geq N_2 &: \abs {s_{2n} - s} < \epsilon.
		\end{align*}

		On en déduit que pour $n \geq \max\{N_1, N_2\}$, on trouve $\abs {s_n-t} < \epsilon$.
		\end{proof}

		\begin{thm}[Critère de Cauchy pour les séries] Soit $(a_k) \subset \K$. La série $\sum_k a_k$ converge si et seulement si~:
		\[\forall \epsilon > 0 : \exists N \in \N \tq \forall m > n \geq N : \abs {\sum_{k=n+1}^m a_k} < \epsilon.\]
		\end{thm}

		\begin{proof} En considérant la suite des sommes partielles, on sait qu'elle converge si et seulement si elle est de Cauchy. \end{proof}

	\subsection{Convergence absolue}
		\begin{déf} Soit $(a_k) \subset \K$. On dit que la série $\sum_k a_k$ \emph{converge absolument} si la série $\sum_k \abs {a_k}$ converge.
		\end{déf}

		\begin{rmq} Toute série à termes positifs convergente est absolument convergente.\footnote{Cette remarque peut également se formuler «~Pour une suite
		à termes positifs, la convergence et la convergence absolue sont équivalentes~».}
		\end{rmq}

		\begin{prp} Toute série absolument convergente est convergente. \end{prp}

		\begin{proof} Soit $\sum_k a_k$ une série absolument convergente. On sait que la série $\sum_k \abs {a_k}$ converge, et est donc une suite de Cauchy.
		Dès lors, on sait que pour tout $\epsilon > 0$, il existe $N$ tel que~
		\[\forall m > n \geq N : \abs {\sum_{k=n+1}^m\abs {a_k}} < \epsilon.\]
		Cependant, on peut exprimer~:
		\[\epsilon > \abs {\sum_{k=n+1}\abs {a_k}} = \sum_{k=n+1}^m\abs {a_k} \geq \abs {\sum_{k=n+1}^m a_k}.\]
		On en déduit que la série $\sum_k a_k$ (ou la série des sommes partielles) satisfait également le critère de Cauchy, et est donc convergente.
		\end{proof}

		\begin{thm} Si la série $\sum_k a_k$ possède une série majorant $\sum_k b_k$ telle que $\sum_k b_k$ est convergente, alors $\sum_k a_k$ est absolument
		convergente.
		\end{thm}

		\begin{proof} On prend $\sum_k b_k$ un majorant de $\sum_k a_k$. Par le critère de Cauchy, on sait~:
		\[\forall \epsilon > 0 : \exists N \tq \forall m > n \geq N : \abs {\sum_{k=n+1}^m b_k} < \epsilon.\]

		Par définition du majorant, on sait qu'il existe $K \in \N$ tel que~:
		\[\forall k \geq K : \abs {a_k} \leq b_k.\]
		On en déduit
		\[\forall m > n \geq \max \{N, K\} : \sum_{k=n+1}^m \abs {a_k} \leq \sum_{k=n+1}^m b_k < \epsilon.\]

		Donc $\sum_k \abs {a_k}$ satisfait le critère de Cauchy, elle est donc convergente.
		\end{proof}

		\begin{rmq} La contraposée de ce théorème affirme que si $\sum_k b_k$ est un majorant de $\sum_k a_k$ et que $\sum_k \abs{a_k}$ n'est pas convergente,
		alors $\sum_k b_k$ diverge.
		\end{rmq}

		\begin{cor}[Critère d'équivalence] Soient $\sum_k a_k$ et $\sum_k b_k$ deux séries réelles à termes positifs. Supposons que $\forall k \in \N : b_k \neq 0$.
		On note~:
		\[\alpha \coloneqq \lim_{k \to +\infty}\frac {a_k}{b_k} \in \R^+ \cup \{+\infty\}.\]

		\begin{enumerate}
			\item Si $0 < \alpha < +\infty$, alors $\sum_k a_k$ converge (respectivement diverge) si et seulement si $\sum_k b_k$ converge
				(respectivement diverge)~;
			\item si $\alpha = 0$ et $\sum_k b_k$ converge, alors $\sum_k a_k$ converge~;
			\item si $\alpha = +\infty$ et $\sum_k b_k$ diverge, alors $\sum_k a_k$ diverge.
		\end{enumerate}
		\end{cor}

		\begin{rmq} Dans le cas 1, on remarque que $\alpha$ n'est ni nul ni infini, et donc que $\sum a_k$ et $\sum_k b_k$ tendent vers 0 à la même vitesse.
		\end{rmq}

		\begin{proof} Preuve du point 1. Par hypothèse, il existe $K \in \N$ tel que~:
		\[\forall k \geq K : -\epsilon + \alpha \leq \frac {a_k}{b_k} \leq \alpha + \epsilon.\]
		En prenant $\epsilon$ suffisamment petit, on peut poser~:
		\[\begin{cases}\beta &\coloneqq \alpha - \epsilon > 0, \\\gamma &\coloneqq \alpha + \epsilon.\end{cases}\]

		On en déduit qu'il existe $N$ tel que~:
		\[\forall k \geq N : \beta b_k \leq a_k \leq \gamma b_k.\]

		On a alors $\gamma b_k$, un majorant de $a_k$,  donc par le critère de comparaison, si $\sum_k b_k$ converge, alors $\sum_k a_k$ converge.
		De même, on a $a_k$ un majorant de $\beta b_k$, donc par le critère de comparaison, si $\sum_k a_k$ converge, alors $\sum_k b_k$ converge.

		Les points 2 et 3 se démontrent de manière similaire.
		\end{proof}

		\begin{thm}[Critère de la racine] Soit $(a_k) \subset \K$. On note~:
		\[\alpha \coloneqq \limsup_{k \to +\infty}\sqrt[k]{\abs {a_k}}.\]

		\begin{enumerate}
			\item Si $\alpha < 1$, alors $\sum_k a_k$ converge absolument~;
			\item si $\alpha > 1$, alors $\sum_k a_k$ diverge.
		\end{enumerate}
		\end{thm}

		\begin{rmq} Le critère de la racine ne donne aucune information sur le cas où $\alpha = 1$ car il est possible de trouver des exemples convergents,
		absolument convergents et divergents.
		\end{rmq}

		\begin{proof} Preuve du point 1. Soit $a \in (\alpha, 1)$. Par hypothèse, il existe $K$ tel que~:
		\[\forall k \geq K : \sqrt[k]{\abs {a_k}} < a,\]
		ou encore $\abs {a_k} < a^k$. Or, on sait la série géométrique $\sum_k a^k$ convergente (car $a \in (0, 1)$), et par le critère de comparaison, comme
		$\sum_k a^k$ est un majorant de $\sum_k a_k$, le critère de comparaison dit que $\sum_k a_k$ converge.

		Preuve du point 2. Pour tout $n \in N$, il existe $k \geq n$ tel que $\abs {a_k} \geq 1$. Dès lors, on observe~:
		\[\lim_{k \to +\infty}\abs {a_k} \geq 1.\]

		Le terme général de la série ne tend donc pas vers 0, et par la proposition~\ref{prp:siserieconvalorstermevers0}, on sait que $\sum_k a_k$ diverge.
		\end{proof}

		\begin{thm}[Critère du quotient] Soient $(a_k) \subset \K$ et $K_0 \in \N$ tels que~:
		\[\forall k \geq K_0 : a_k \neq 0.\]

		\begin{enumerate}
			\item S'il existe $K \geq K_0$ et $a < 1$ tels que~:
				\[\forall k \geq K = \frac {\abs {a_{k+1}}}{\abs {a_k}} < a,\]
			alors la série $\sum_k a_k$ est convergente~;
			\item s'il existe $K \geq K_0$ tel que~:
				\[\forall k \geq K : \frac {\abs {a_{k+1}}}{\abs {a_k}} \geq 1,\]
			alors la série $\sum_k a_k$ est divergente.
		\end{enumerate}
		\end{thm}

		\begin{proof} Preuve du point 1. Commençons par montrer que pour tout $k \geq K+1$, on a~:
		\[\abs {a_k} \leq \frac {\abs {a_K}}{a^K}a^k.\]
		Pour $k = K+1$, on a par hypothèse~:
		\[\frac {\abs {a_k}}{\abs {a_K}} \leq a,\]
		ou encore~:
		\[\abs {a_k} \leq \abs {a_K}a = \abs {a_K}a\frac {a^K}{a^K} = \abs{a_K}\frac {a^k}{a^K}.\]
		Ensuite, supposons cette inégalité vraie pour $k > K$, et montrons-la pour $k+1$~:
		\[\abs{a_{k+1}} \leq a\abs{a_k} = a\frac {\abs{a_K}}{a^K}a^k = \frac {\abs {a_K}}{a^K}a^{k+1}.\]
		On peut donc exprimer, pour tout $k \geq K$~:
		\[\abs {a_k} \leq \beta a^k,\]
		où $\beta = \frac {\abs {a_K}}{a^K} \in \R$ est une constante. On peut donc dire que la série $\sum_k a_k$ est majorée par la série $\beta\sum_ka^k$,
		qui converge car $a < 1$, et donc par le critère de comparaison, on peut dire que $\sum_k a_k$ converge.

		Preuve du point 2. Pour tout $m \in \N$, on sait que $\abs {a_{K+m}} \geq \abs {a_K}$. Donc $\abs {a_k}$ ne tend pas vers 0 pour $k \to +\infty$.
		Par la proposition~\ref{prp:siserieconvalorstermevers0}, on sait que $\sum_k a_k$ ne converge pas.
		\end{proof}

	\subsection{Critères de Dirichlet et d'Abel}
		\begin{thm}[Critère de Dirichlet] Soient $(a_k) \subset \R$ et $(b_k) \subset \C$ telles que~:
		\begin{itemize}
			\item[$(i)$]   $\lim_{k \to +\infty} a_k = 0$~;
			\item[$(ii)$]  $\sum_k \abs{a_{k+1}-a_k}$ est convergente~;
			\item[$(iii)$] la suite des sommes partielles de $(b_k)$ est bornée dans $\C$.
		\end{itemize}
		Alors la série $\sum_k (a_kb_k)$ est convergente dans $\C$.
		\end{thm}

		\begin{proof} On note $(B_n)$ la suite des sommes partielles de $\sum_k b_k$ et $(s_n)$ la suite des sommes partielles de $\sum_k a_kb_k$.
		On écrit $s_n$ sous la forme~:
		\[s_n = \sum_{k=0}^na_kb_k = \sum_{k=0}^na_kB_k - \sum_{k=0}^{n-1}a_{k+1}B_k = \sum_{k=0}^{n-1}B_k(a_k-a_{k+1}) - a_nB_n.\]
		On sait par hypothèse que $B_n$ est bornée, et donc il existe $M \lneqq $ tel que~:
		\[\abs {(a_j-a_{j+1})B_j} \leq M\abs{a_j-a_{j+1}}.\]
		Or, par hypothèse également, la série $\sum_j\abs{a_j-a_{j+1}}$ converge. Comme la série convergente $M\sum_k \abs {a_k-a_{k+1}}$ est un majorant de la
		série $\sum_k (a_j-a_{j+1})B_j$. Par le critère de comparaison, on sait que~:
		\[\sigma \coloneqq \lim_{n \to +\infty}\left(\sum_{k=0}^{n-1}(a_k - a_{k+1})B_k\right) \in \R.\]

		On sait également par hypothèse que la suite $(a_k)$ tend vers 0 pour $k \to +\infty$. Par les règles de calcul des suites, on sait que la suite
		$(a_nB_n)$ tend également vers 0 pour $n \to +\infty$. Dès lors, par les règles de calcul sur les suites, on trouve que la suite des sommes partielles de
		$\sum_k a_kb_k$ converge.
		\end{proof}

		\begin{rmq} Lorsque $(a_k)$ est décroissante, le critère de Dirichlet peut s'exprimer comme étant de critère d'Abel. \end{rmq}

		\begin{thm}[Critère d'Abel] Soient $(a_k) \subset \R$ et $(b_k) \subset \C$ telles que~:
		\begin{itemize}
			\item[$(i)$]   $\lim_{k \to +\infty} a_k = 0$~;
			\item[$(ii)$]  $a_{k+1} < a_k$ pour tout $k$~;
			\item[$(iii)$] la suite des sommes partielles de $(b_k)$ est bornée dans $\C$.
		\end{itemize}
		Alors la série $\sum_k (a_kb_k)$ est convergente dans $\C$.
		\end{thm}

		\begin{proof} On observe que~:
		\[\sum_{k=0}^n \abs{a_k-a_{k+1}} = \sum_{k=0}^n (a_k-a_{k+1}) = a_0 + \sum_{k=0}^n a_k - \sum_{k=0}^n a_k - a_{n+1} = a_0 - a_{n+1}.\]
		Dès lors~:
		\[\lim_{n \to +\infty}\sum_{k=0}^n\abs{a_k - a_{k+1}} = a_0 - \lim_{n \to +\infty}a_{n+1} = a_0.\]
		Donc la série $\sum_k\abs{a_k-a_{k+1}}$ converge. En appliquant le critère de Dirichlet, on trouve effectivement que $\sum_k a_kb_k$ converge.
		\end{proof}

	\subsection{Opérations sur les séries}
		\begin{déf} Soient la suite $(a_k) \subset \K$ et $\sigma : \N \to \N$ bijective. On dit que la série $\sum_k a_{\sigma(k)}$ est un \emph{réarrangement}
		de la série $\sum_k a_k$.
		\end{déf}

		\begin{rmq} Si $\sigma$ est une permutation de $\N$ telle que seul un nombre fini de valeurs $k$ ne sont pas envoyées sur elle-même, alors la série
		$\sum_k a_{\sigma(k)}$ converge si et seulement si $\sum_k a_k$ converge et leur somme sera la même.
		\end{rmq}

		\begin{thm}[Théorème de réarrangment de Riemann] Soit $(a_k) \subset \R$ telle que $\sum_k a_k$ converge sans converger absolument. Alors pour tout
		$s \in \R$, il existe $\sigma : \N \to \N$ bijective telle que $\sum_k a_{\sigma(k)} = s$. De plus, il existe $\sigma$ une permutation de $\N$ telle que
		$\sum_k a_{\sigma(k)}$ diverge.
		\end{thm}

		\begin{rmq} Ce théorème n'est pas démontré ici car il dépasse le niveau du cours. \end{rmq}

		\begin{déf} Soit $(a_k) \subset \R$ telle que $\sum_k a_k$ converge sans converger absolument. Alors on dit que la convergence de $\sum_k a_k$ est
		\emph{conditionnelle}. Soit $(b_k) \subset \R$ telle que $\sum_k b_k$ converge absolument. Alors on dit que la convergence de $\sum_k b_k$ est
		\emph{inconditionnelle}
		\end{déf}

		\begin{rmq} Cette dénomination vient du fait qu'une suite qui converge sans converger absolument peut converger vers n'importe quel réel (et peut ne pas
		converger) selon le réarrangement. Une série qui converge absolument ne dépend pas du réarrangement (voir théorème~\ref{thm:convabsolurearrangement}).
		\end{rmq}

		\begin{thm}\label{thm:convabsolurearrangement} Si $\sum_k a_k$ est une série absolument convergente et $\sigma$ est une permutation de $\N$, alors
		$\sum_k a_{\sigma(k)}$ est absolument convergente et la somme ne dépend pas de $\sigma$.
		\end{thm}

		\begin{proof} Commençons par montrer que $\sum_k a_{\sigma(k)}$ converge absolument, et nous montrerons que la somme ne dépend pas de $\sigma$ ensuite.

		Soit $n \in \N$. On pose $\tilde n \coloneqq \max_{0 \leq i \leq n}\sigma(i)$. Cela implique que $\sigma(\intint 0n) \subset \intint0{\tilde n}$.
		On en déduit~:
		\[\sum_{k=0}^n \abs {a_{\sigma(k)}} \leq \sum_{k=0}^{\tilde n} \abs {a_k} \leq \sum_{k \geq 0}\abs {a_k}.\]
		La série est donc bornée et est donc absolument convergente.

		On note respectivement $(\hat s_n)$ et $(s_n)$ les suites des sommes partielles de $\sum_k \abs {a_{\sigma(k)}}$ et $\sigma_k \abs{a_k}$.
		Soit $\epsilon > 0$. Par le critère de Cauchy, on sait qu'il existe $N$ tel que~:
		\[\forall m > n \geq N : \sum_{k=n+1}^m \abs {a_k} < \frac \epsilon 2.\]
		On prend alors $\hat N \geq N$ tel que~:
		\[\intint 0N \subseteq \sigma\left(\intint 0{\hat N}\right).\]
		Soit $n \geq \hat N$. On pose $A_n \coloneqq \sigma\left(\intint 0n\right) \setminus \intint 0N$. On peut exprimer~:
		\[\abs{s_n - \hat s_n} = \abs{\sum_{k=0}^n a_k - \sum_{k=0}^n a_{\sigma(k)}} = \abs {\sum_{k=\hat N+1}^n a_k + \sum_{k \in A_n} a_k}
			\leq \sum_{k=\hat N+1}^n\abs {a_k} + \sum_{k \in A_n}\abs {a_k} \leq \sum_{k=\hat N+1}^n\abs {x_k} + \sum_{k=\hat N+1}^{\tilde n}\abs {a_k}
			< \frac \epsilon 2 + \frac \epsilon 2.\]

		On a donc montré que pour tout $n \geq \hat N$ et $\epsilon > 0$, on a $\abs {s_n - \hat s_n}$, ou encore $s_n = \hat s_n$. En prenant la limite,
		l'égalité reste et on montre l'égalité des sommes des suites.
		\end{proof}

		\begin{déf} Soit $(a_{ij})_{i, j \in \N}$ une \emph{suite double}. La \emph{série double} $\sum_{i,j}a_{ij}$ dite \emph{sommable} si~:
		\[\sup_{n \in \N}\sum_{i,j=0}^n\abs {a_{ij}} < +\infty\]
		\end{déf}

		\begin{prp} Si $\sum_k a_k$ et $\sum_k b_k$ sont deux séries absolument convergentes. Alors la série double $\sum_{i, j}a_{ij}$ est sommable. \end{prp}

		\begin{thm}[Théorème de Fubini pour les séries] Soit $(a_{ij}) \subset \K$. Si la série double $\sum_{i,j}a_{ij}$ est sommable, alors pour tout
		$\sigma : \N \to \N^2$\footnote{$\N$ et $\N^2$ ont bien le même cardinal, pour s'en convaincre, on peut imaginer une bijection qui remplit $\N^2$
		par ses diagonales. Il y a également la fonction de couplage de Cantor bien connue qui le montre.}, la série $\sum_k a_{\sigma(k)}$ est absolument
		convergente et la somme $s$ est indépendante de $\sigma$.

		De plus, pour tout $i, j \in \N$, les séries $\sum_k a_{ik}$ et $\sum_k a_{kj}$ sont absolument convergentes et~:
		\[\sum_i\left(\sum_j a_{ij}\right) = \sum_j\left(\sum_i a_{ij}\right) = s.\]
		\end{thm}

		\begin{rmq} Ce résultat n'est pas démontré. \end{rmq}

		\begin{déf} Le \emph{produit de Cauchy} de deux séries $\sum_k a_k$ et $\sum_k b_k$ est défini par la série~:
		\[\sum_n\sum_{\sum_{k=0}^n}a_kb_{n-k}\]
		\end{déf}

		\begin{rmq} Une conséquence du théorème de Cauchy pour les suites est que si $\sum_k a_k$ et $\sum_k b_k$ sont deux séries absolument convergentes,
		alors leur produit de Cauchy est une série absolument convergente qui a pour somme $\left(\sum_k a_k\right)\left(\sum_k b_k\right)$.

		De plus, le produit de Cauchy de deux suites convergentes de manière conditionnelle peut de pas converger.
		\end{rmq}

	\subsection{Séries de puissances}
		\begin{déf} Soit $(a_k) \subset \K$ une suite. On appelle la série $\sum_k a_k(z-z_0)^k$ une série de puissances où $z_0 \in \K$.
		\end{déf}

		\begin{thm}\label{thm:rayondeconvergence} Soit $(a_k) \subset \K$. Alors il existe $\rho_a \in [0, +\infty]$ , appelé \emph{rayon de convergence} tel
		que pour tout $z$ tel que $\abs {z-z_0} < \rho_a$, la série de puissances $\sum_k a_k(z-z_0)^k$ converge absolument et pour tout $z$ tel que
		$\abs{z-z_0} > \rho_a$, la série de puissances diverge. De plus, la \emph{formule de Hadamard} donne~:
		\[\rho_a = \frac 1{\limsup_{k \to +\infty} \sqrt[k] {\abs{a_k}}}.\]
		\end{thm}

		\begin{proof} On observe~:
		\[\limsup_{k \to +\infty} \sqrt[k] {\abs{a_k(z-z_0)^k}} = \abs{z-z_0}\limsup_{k \to +\infty}\abs {a_k} = \frac {\abs {z-z_0}}{\rho_a}.\]
		Il faut donc, par le critère de la racine, que $\abs{z-z_0} \lneqq \rho_a$ pour une convergence absolue et $\abs{z-z_0} \gneqq \rho_a$ pour une
		divergence.
		\end{proof}

		\begin{déf} Le disque de contre $z_0$ et de rayon $\rho_a$ est appelé \emph{rayon de convergence}. \end{déf}

		\begin{prp} Soit $(a_k) \subset \K$ telle que~:
		\[\lim_{k \to +\infty}\abs {\frac {a_k}{a_{k+1}}}\]
		existe dans $\R \cup \{\pm \infty\}$. Alors le rayon de convergence de la série de puissances $\sum_k a_k(z-z_0)^k$ est donné par~:
		\[\rho_a = \lim_{k \to +\infty}\abs {\frac {a_k}{a_{k+1}}}.\]
		\end{prp}

		\begin{proof} On calcule~:
		\[\lim_{k \to +\infty}\abs {\frac {a_{k+1}(z-z_0)^{k+1}}{a_k(z-z_0)^k}} = \frac {\abs{z-z_0}}{\lim_{k \to +\infty}\abs {\frac {a_k}{a_{k+1}}}}.\]
		On en conclut que la série $\sum_k a-K(z-z_0)^k$ converge si $\abs {z-z_0} < \lim_{k \to +\infty}\abs{\frac {a_k}{a_{k+1}}}$ et diverge si
		$\abs {z-z_0} > \lim_{k \to +\infty}\abs{\frac {a_k}{a_{k+1}}}$. Par le théorème~\ref{thm:rayondeconvergence}, on en déduit que~:
		\[\rho_a = \lim_{k \to +\infty}\abs{\frac {a_k}{a_{k+1}}}.\]
		\end{proof}

	\subsection{L'ensemble des réels n'est pas dénombrable}
		\begin{déf} Un ensemble $X$ est dit~:
		\begin{itemize}
			\item \emph{fini} s'il existe $m \in \N$ et une bijection $f : X \to \intint 1m$~;
			\item \emph{infini} s'il n'est pas fini~;
			\item \emph{dénombrable} s'il existe une bijection $f : X \to \N$~;
			\item \emph{au plus dénombrable} s'il est fini ou dénombrable.
		\end{itemize}
		\end{déf}

		\begin{déf} Deux ensembles $X$ et $Y$ sont dits \emph{équipotents} s'il existe $f : X \to Y$ bijective.
		\end{déf}

		\begin{prp}\label{prp:propriétésensdénombrables} Soient $X$ et $Y$ deux ensembles au plus dénombrable. Alors les ensembles suivantes~:
		\begin{itemize}
			\item $A \subset X$~;
			\item $X \cup Y$~;
			\item $X \times Y$,
		\end{itemize}
		sont au plus dénombrables.
		\end{prp}

		\begin{proof} Si $A$ est fini, il n'y a rien à démontrer. Si $A$ est infini (et donc $X$ est infini), alors on sait qu'il existe $g : X \to \N$ une
		bijection. On peut restreindre cette bijection à $g{\mid_A} : A \to g(A) \subseteq \N$, qui reste une bijection. $A$ est donc dénombrable.

		Si $X$ et $Y$ sont finis, on sait que $X \cup Y$ est fini. Si $X$ est fini et $Y$ infini (le cas symétrique s'en déduit), alors il existe $m \in \N$ et
		$f : X \to \intint 1m$ et il existe également $g : Y \to \N$. On définit alors~:
		\[h : X \cup Y \to \N : x \mapsto f(x-1)1_X(x) + g(x)1_Y(x).\]
		On remarque que $h$ est bijective. De même, si $X$ et $Y$ sont tous deux infinis, il existe $f : X \to \N$ et $g : Y \to \N$ bijectifs. On définit
		à nouveau~:
		\[h : X \cup Y \to \N : x \mapsto 2f(x)1_X(x) + (2g(x)+1)1_Y(x).\]
		On remarque également que $h$ est bijective.

		Si $X$ et $Y$ sont tous deux finis, alors $X \times Y$ est fini également. Si $X$ et $Y$ sont tous deux infinis, alors l'idée de la preuve est de
		construire une fonction $\N \to X \times Y$ qui remplit le \emph{plan discret} $X \times Y$ en diagonale, et qui, par construction est bijective.\footnote{
		À nouveau, la fonction de couplage de Cantor peut être utilisée, et c'est une fonction $X \times Y \to \N$ donc qui prend le problème dans l'autre sens.}
		Si $X$ est fini et $Y$ infini (ou symétriquement), l'argument est le même.
		\end{proof}

		\begin{prp} Une union dénombrable d'ensembles au plus dénombrables est toujours dénombrable. \end{prp}

		\begin{prp} L'ensemble $\Q$ des rationnels est dénombrable. \end{prp}

		\begin{déf} On prend $f : \Q \to \Z \times \Z_0 : \frac pq \mapsto (p, q)$. On voit que $f$ est injective, et donc $f{\mid_{\Q}}$ est une bijection
		avec un sous-ensemble de $\Z \times \Z_0$ qui, lui, est dénombrable. On sait également $\N \subset \Q$ et donc $\Q$ ne peut être fini.
		$\Q$ doit donc forcément être dénombrable.
		\end{déf}

		\begin{rmq} La théorie des séries permet de justifier la notion de développement décimal illimité. En effet~:
		\[1.999\ldots = 1 + \sum_{k \geq 1}\frac 9{10} = 1 + 9\sum_{k \geq 1}10^{-k} = 1 + 9\frac {10^{-1}}{1 - 10^{-1}} = 1 + 1 = 2.\]
		\end{rmq}

		Tout nombre $x \in [0, 1]$ peut se développer dans une base $b \geq 2$ de la manière suivante~:
		\[x = \sum_{k \geq 1}x_kb^{-k}.\]

		\begin{thm}\label{thm:Rindénombrable} L'ensemble $\R$ des nombres réels n'est pas dénombrable. \end{thm}

		\begin{proof} Montrons que $[0, 1)$ est indénombrable. Supposons par l'absurde qu'il existe une bijection $f : \N \to [0, 1)$. Cette fonction
		\emph{ordonne} l'intervalle $[0, 1)$ par un ensemble $f(0), f(1), \ldots$ Chacun de ces nombres a une représentation par développement décimal unique~:
		\[f(n) = \sum_{k \geq 1}(x_n)_k10^{-k}.\]

		On considère la suite $(a_k)$ définie par~:
		\[a_k = \begin{cases}0 &\text{ si } (a_{k-1})_k \neq 0 \\ 1 &\text{ si }(a_{k-1})_k = 0\end{cases} = 1 - \delta_{0 (a_{k-1})_k}.\]
		On prend $a \in [0, 1)$ défini par $\sum_{k \geq 1}a_k10^{-k}$. Par construction, on sait pour tout $k \in \N$ que $a \neq f(k)$. On peut donc
		dire $a \not \in f(\N) = [0, 1)$, or $f$ est supposée injective. C'est une contradiction, et donc $[0, 1)$ n'est pas dénombrable.
		\end{proof}

		\begin{cor} L'ensemble $\R \setminus \Q$ des nombres irrationnels est indénombrable. \end{cor}

		\begin{proof} Supposons par l'absurde que $\R \setminus \Q$ est dénombrable. Par la proposition~\ref{prp:propriétésensdénombrables}, on sait que
		$\R = (\Q \setminus \Q) \cup \Q$ est dénombrable. Or on vient de voir dans le théorème~\ref{thm:Rindénombrable} que $\R$ est indénombrable. Il faut donc
		$\R \setminus \Q$ indénombrable.
		\end{proof}

		\begin{rmq} Il est assez intuitif de se dire qu'en retirant un ensemble \emph{uniquement dénombrable} d'un ensemble indénombrable, la différence
		ensembliste reste indénombrable.
		\end{rmq}

\newpage
\section{Approximation de Taylor}
	\subsection{Approximation polynomiale des fonctions d’une variable}
		\begin{thm}[Théorème de Taylor] Soient $f : I \to \R$, $a \in \intr I$, et $k \geq 1$. Si $f$ est $k$ fois dérivable en $a$, alors il existe une unique
		polynôme~:
		\[P(x) = \sum_{i=0}^kc_i(x-a)^i\]
		tel que~:
		\[\lim_{x \to a}\frac {f(x)-P(x)}{(x-a)^k} = 0.\]
		De plus, $P = T_k(f, a)$ où~:
		\[T_k(f, a)(x) \coloneqq \sum_{n=0}^k\frac {f^{(n)}(a)}{n!}(x-a)^n.\]
		\end{thm}

		\begin{déf} On appelle le polynôme $T_k(f, a)$ le \emph{polynôme de Taylor de $f$ autour de $a$}. \end{déf}

		\begin{déf} Si $f \in C^\infty$, on définit la \emph{série de Taylor de $f$ autour de $a$} par~:
		\[T(f, a)(x) \coloneqq \sum_{n=0}^\infty\frac {f^{(n)}(a)}{n!}(x-a)^n.\]
		\end{déf}

		\begin{rmq} La série de Taylor est une série de puissance et converge donc pour tout $x$ tel que $\abs {x-a} < \rho$, où~:
		\[\rho = \frac 1{\limsup_{n \to +\infty}\frac {f^{(n)}(a)}{n!}}.\]

		Si $\rho \gneqq 0$, la série $T(f, a)$ définit une fonction (car il y a convergence) sur un voisinage de $a$. Il faut alors montrer que $T(f, a) = f$.
		\end{rmq}

		\begin{déf} On dit que la fonction $f : I \to \R$ est \emph{analytique} si~:
		\[\forall a \in I : \exists \delta \tq \forall x \in (a-\delta, a+\delta) : f(x) = T(f, a)(x).\]
		\end{déf}

		\begin{proof}[Preuve du théorème de Taylor] Montrons d'abord l'unicité, et montrons ensuite l'existence sur base de l'unicité.

		Soient $P_1, P_2$ deux polynômes de degré $\leq k$ tels que~:
		\[\lim_{x \to a}\frac {f(x) - P_1(x)}{(x-a)^k} = \lim_{x \to a}\frac {f(x) - P_2(x)}{(x-a)^k} = 0.\]

		Par les règles de calcul sur les limites, on a~:
		\[\lim_{x \to a}\frac {P_1(x)-P_2(x)}{(x-a)^k} = 0.\]
		On note $P(h) \coloneqq P_1(a+h)-P_2(a+h)$. On peut également écrire~:
		\[P(h) = \sum_{i=0}^ka_i^i.\]
		On a donc $P(h) \to 0$ pour $h \to 0$. Il faut donc avoir $a_i = 0$ pour tout $i$. On a donc $P_1(x)-P_2(x) = 0$, ce qui montre que le polynôme est unique.

		Montrons maintenant qu'il existe. Plus précisément, montrons que $T_k(f, a)$ est ce polynôme.

		On pose $R_k(f, a)(x) = f(x) - T_k(f, a)(x)$. Montrons que $\frac {R_k(f, a)(x)}{x-a} \to 0$ pour $x \to a$. Prouvons-cela par induction sur $k$.
		Pour $k=1$, par définition de la dérivée, on trouve~:
		\[\lim_{x \to a}\frac {f(x)-T_1(f, a)}{x-a} = \lim_{x \to a}\frac {f(x) - f(a) - f'(a)(x-a)}{x-a} = \lim_{x \to a}\frac {f(x)-f(a)}{x-a} - f'(a) = 0.\]
		Pour le pas de récurrence, on calcule~:
		\[R_{k+1}(f, a)(x) = R_{k+1}(f, a)(x) - R_{k+1}(f, a)(a).\]
		Par le théorème de la moyenne, on sait qu'il existe $\xi$ compris strictement entre $x$ et $a$ tel que~:
		\[R_{k+1}(f, a)(x) = R_{k+1}(f, a)'(\xi)(x-a).\]
		En calculant le terme de droite, on trouve~:
		\begin{align*}
			R_{k+1}(f, a)'(\xi) &= f'(\xi) - \sum_{n=1}^{k+1}\frac {f^{(n)}(\xi)}{n!}n(x-a)^{n-1} = f'(\xi) - \sum_{n=0}^k\frac {f^{(n+1)}(\xi)}{n!}(x-a)^n \\
			&= f'(\xi) - \sum_{n=0}^k\frac {(f')^{(n)}(\xi)}{n!}(x-a)^n = f'(\xi) - T_k(f', a)(\xi) = R_k(f', a)(\xi).
		\end{align*}
		On procède ensuite à la récurrence~:
		\[\frac {R_{k+1}(f, a)(x)}{(x-a)^{k+1}} = \frac {R_k(f', a)(\xi)(x-a)}{(x-a)^{k+1}} = \frac {R_k(f', a)(\xi)}{(\xi - a)^k}\frac {(\xi-a)^k}{(x-a)^k}.\]
		On en déduit alors~:
		\[\lim_{x \to a}\abs {\frac {R_{k+1}(f, a)(x)}{(x-a)^{k+1}}} = \epsilon(k)\lim_{\xi \to a}\abs {\frac {R_k(f', a)(\xi)}{(\xi-a)^k}},\]
		où~:
		\[\epsilon(k) = \frac {\abs {(\xi-a)^k}}{\abs {(x-a)^k}} \leq 1.\]
		On a donc bien~:
		\[\lim_{x \to a}\abs {\frac {R_{k+1}(f, a)(x)}{(x-a)^{k+1}}} = 0.\]
		\end{proof}

		\begin{thm}[Formule du reste de Lagrange] Sous les hypothèses du théorème de Taylor, si de plus $f^{(k)}$ est continue et $f^{(k+1)}$ existe, alors
		pour tout $x$, il existe $\zeta$ strictement entre $x$ et $a$ tel que~:
		\[R_k(f, a)(x) = \frac {f^{(k+1)}(\zeta)}{(k+1)!}(x-a)^{k+1}.\]
		\end{thm}

		\begin{cor} Soit $f \in C^\infty(I=(x_1, x_2))$. Si~:
		\[\forall x_0 \in I : \exists \delta > 0, M > 0 \tq ([x_0 \pm \delta] \subset I) \land (\forall k \in \N, x \in (x_0 \pm \delta) : \abs {f^{(k)}(x)} \leq k!M^k),\]
		alors $f$ est analytique.
		\end{cor}

		\begin{proof} On remarque que le rayon de convergence est $\geq \frac 1M$ par~:
		\[\frac 1\rho = \sqrt[k] {\frac {f^{(k)}(a)}{k!}} \leq \sqrt[k] {M^k} = M.\]
		Dès lors~:
		\[\sum_{k \geq 0}\frac {f^{(k)}(a)}{k!}(x-a)^k\]
		converge pour tout $x$ tel que $\abs {x-a} < \frac 1M$. On calcule ensuite~:
		\[\abs {R_k(f, a)(x)} = \frac 1{k!}\abs {f^{(k)}(a)(x-a)^k} \leq \frac 1{k!}\abs {k!M^k(x-a)^k} = M^k(x-a)^k.\]
		Or on sait $\abs {x-a} \lneqq \frac 1M$. Dès lors~:
		\[\abs {M(x-a)}^k \to 0.\]
		\end{proof}

	\subsection{Fonctions de plusieurs variables}
		\begin{déf} Soit $f : E \subset \R^m \to \R$. On dit que $f$ est \emph{continûment dérivable} si les dérivées partielles de $f$ sont continues.
		Cela se note $f \in C^1$.
		\end{déf}

		\begin{rmq} Comme pour les fonction $f : \R \to \R$, on définit $f \in C^n$ par récurrence si $f \in C^{n-1}$ est les dérivées partielles de $f^{(n)}$
		existent et sont continues ($f^{(n)}$ est évidemment un vecteur de dérivées partielles).
		\end{rmq}

		\begin{déf} On dit que $f : E \subset \R^m \to \R$ est $k$ fois continûment dérivable (ou $f \in C^k$) si toutes les dérivées partielles~
		\[\frac {\partial^k}{\partial x_{i_k}\partial x_{i_{k-1}}\dotsb\partial x_{i_2}\partial x_{i_1}}\]
		existent et sont continues. On appelle ces dérivées partielles les \emph{dérivées partielles d'ordre $k$ de $f$.}
		\end{déf}

		\begin{déf} La fonction $f : E \subset \R^m \to \R^n$ est dite $k$ fois continûment dérivable si toutes ses composantes $f_i$ avec $1 \leq i \leq n$
		sont de classe $C^k$.
		\end{déf}

		\begin{thm}[Théorème de Clairault] Soit $f : E \subset \R^m \to \R$ une fonction définie sur un ouvert $E$ de classe $C^2$. Alors pour tout $a \in E$
		et $1 \leq i, j \leq n$, on a~:
		\[\pd {}{x_i}\left(\pd f{x_j}\right)(a) = \pd {}{x_j}\left(\pd f{x_i}\right)(a).\]
		\end{thm}

		\begin{rmq} Ce résultat n'est pas démontré. \end{rmq}

		\begin{cor} Soit $f : E \subset \R^m \to \R^n$ une fonction de classe $C^k$ définie sur un ouvert $E$. Pour tout $\gamma \leq k$, les dérivées partielles
		d'ordre $\gamma$ de $f$~:
		\[\frac {\partial^\gamma f}{\partial x_{i_\gamma}\partial x_{i_{\gamma-1}}\dotsb\partial x_{i_2}\partial x_{i_1}}\]
		ne dépendent pas de l'ordre de dérivation (l'ordre des $i_j$).
		\end{cor}

		\begin{déf} Soit $\alpha \in \N^m$. On dit que $\alpha$ est un \emph{multi-indice} et sa longueur est définie par~:
		\[\overline \alpha \coloneqq \sum_{k=1}^m\alpha_m.\]
		\end{déf}

		\begin{déf} Soient $f : E \subset \R^m \to \R$ une fonction de classe $C^k$ définie sur un ouvert $E$ et $\alpha \in \N^m$ un multi-indice tel que
		$\overline \alpha = k$. On introduit la notation suivante~:
		\[\partial^\alpha f \coloneqq \frac {\partial^{\overline \alpha}f}{\partial x_1^{\alpha_1}\dotsb\partial x_m^{\alpha_m}}.\]
		\end{déf}

		\begin{déf} Si $\alpha \in \N^m$ est un multi-indice et $y \in \R^m$ un vecteur, on définit~:
		\begin{enumerate}
			\item $\R \ni \alpha ! \coloneqq \prod_{k=1}^m\alpha_k!$~;
			\item $\R \ni y^\alpha \coloneqq \prod_{k=1}^ny_k^{\alpha_k}$.
		\end{enumerate}
		\end{déf}

		\begin{déf} Soient $f : E \subset \R^m \to \R$ une fonction de classe $C^k$ définie sur un ouvert, $a \in E$, et $1 \leq r \leq k$. On définit le
		polynôme $P_r(f, a)$ par~:
		\[P_r(f, a)(x) = \sum_{\alpha \in \N \tq \overline \alpha = r}\frac 1{\alpha!}(x-a)^\alpha\partial^\alpha f(a).\]
		\end{déf}

		\begin{déf} Soit $f : E \subset \R^m \to \R$ une fonction de classe $C^k$ définie sur un ouvert $E$. On appelle le \emph{polynôme de Taylor de $f$
		autour de $a$ d'ordre $k$} le polynôme $T_k(f, a) : E \to \R$ défini par~:
		\[T_k(f, a)(x) = f(a) + \sum_{i=1}^kP_i(f, a)(x).\]
		\end{déf}

		\begin{lem}\label{lem:dérivéeFTaylor} Soit $f : B(a, R) \subset \R^m \to \R$ de classe $C^{k+1}$. Pour tout $t \in (0, 1), 1 \leq r \leq k+1$, on a~:
		\[\frac 1{r!}\od[r] Ft(t) = \sum_{\alpha \in \N^m \tq \overline \alpha = r}\frac 1{\alpha !}(x-a)^\alpha\partial^\alpha f(a + t(x-a)).\]
		\end{lem}

		\begin{thm}[Théorème de Taylor à $n$ variables] Soit $f : E \subset \R^m \to \R$ de classe $C^{k+1}$. Si pour tout $a \in \intr E, R > 0$, la boule
		$B(a, R) \subset E$, alors il existe $C > 0$ tel que pour tout $x \in \bar B(a, R)$~:
		\[\norm {f(x) - T_k(f, a)(x)} \leq C\norm{x-a}^{k+1}.\]
		\end{thm}

		\begin{proof} On pose~:
		\[F(t) = f(a + t(x-a)).\]
		On observe que~:
		\[f(x) - f(a) = F(1) - F(0) = \sum_{r=1}^k\od[r] Ft(0) + R.\]
		On sait également qu'il existe $\kappa \in (0, 1)$ tel que~:
		\[R = \frac 1{(k+1)!}\od[k+1]Ft(\kappa).\]

		Par le lemme~\ref{lem:dérivéeFTaylor}, on sait que~
		\[\frac 1{r!}\od[r]Ft(0) = P_r(f, a)(x).\]
		On peut donc réécrire le reste comme~:
		\[R(x) = \sum_{\alpha \in \N^m \tq \overline \alpha = k+1}\frac 1{\alpha!}(x-a)\alpha\partial^\alpha f(a + \kappa(x-a)).\]

		Par hypothèse, $f \in C^{k+1}$, et donc les fonctions $\partial^\alpha f$ sont continues sur $E$ pour $\overline \alpha = k+1$. On peut donc dire que
		pour tout $x \in \bar B(a, R)$, ces fonctions sont bornées. Donc il existe $M > 0$ tel que~:
		\[\abs{\partial^\alpha f(a + t(x-a))} < M.\]

		On sait également que $\alpha! < 1$ et donc par l'inégalité triangulaire, on peut exprimer~:
		\[\abs {R(x)} \leq M\sum_{\alpha \in \N^m \tq \overline \alpha=k+1}\abs {(x-a)\alpha}.\]

		En observant~:
		\[\abs {y^\alpha} \leq \norm y^{\overline \alpha},\]
		on peut alors poser $C \coloneqq MN$ où $N$ est le nombre de multi-indices de longueur $k+1$. On a alors~:
		\[\abs {R(x)} \leq C\norm{x-a}^{k+1}.\]
		\end{proof}

		\begin{déf} On définit la matrice Hessienne de la fonction $f : \R^m \to \R$ de classe $C^2$ par~:
		\[H_f(a) = \left[\md f2{x_i}{}{x_j}{}(a)\right]_{1 \leq i, j \leq m}.\]
		\end{déf}

		\begin{rmq} La matrice Hessienne d'une fonction $f$ est symétrique.

		De plus, on observe que $T_2(f, a)(x) = f(a) + \scpr {(\nabla f)(a)}{x-a} + \frac 12(x-a)^TH_f(x-a)$.
		\end{rmq}

	\subsection{Condition suffisante d’extrémalité}
		\begin{déf} Soit $f : E \subset \R^m \to \R$. Un point $a \in E$ est un \emph{maximum local} de $f$ si~:
		\[\exists \delta > 0 \tq \forall x \in B(a, \delta) \cap E : f(x) \leq f(a).\]
		Similairement, $a$ est un \emph{minimum local} si~:
		\[\exists \delta > 0 \tq \forall x \in B(a, \delta) \cap E : f(x) \geq f(a).\]

		Si $a$ est soit un minimum local, soit un maximum local, on dit que $a$ est un \emph{extremum local}.
		\end{déf}

		\begin{lem} Soient $f : E \subset \R^m \to \R$ et $a \in \intr E$. Si $f$ est dérivable en $a$ dans la direction $v \in \R^m$ et si $a$ est un extremum
		local de $f$, alors $\partial_v f(a) = 0$.
		\end{lem}

		\begin{proof} Soit $f_v : \R \to \R : t \mapsto f(a + tv)$. On remarque alors que~:
		\[\od {f_v}t(0) = \partial_v f(a).\]

		De plus, si $a$ est un extremum local de $f$, alors $t$ est un extremum local de $f_v$. Donc $\od {f_v}t(0) = 0$.
		\end{proof}

		\begin{cor} Soit $f : E \subset \R^m \to \R$ différentiable en $a \in E$. Si $a$ est un extremum local de $f$, alors $(\nabla f)(a) = 0$.
		\end{cor}

		\begin{proof} Par le lemme précédent, on sait que pour tout $v \in \R^m$ où $v \neq 0$, $\partial_v f(a) = 0$. Or on sait, puisque $f$ est différentiable
		en $a$, $\pd fv = \scpr {(\nabla f)(a)}v$. Il faut donc $(\nabla f)(a) = 0$.
		\end{proof}

		\begin{déf} On appelle les points annulant le gradient d'une fonction $f$ les \emph{points critiques} de $f$.
		\end{déf}

		\begin{rmq} Les extrema locaux sont donc des points critiques d'une fonction $f$ multivariée. \end{rmq}

		\begin{rmq} L'annulation du gradient est une condition nécessaire mais pas suffisante pour la présence d'un extremum local. Un exemple est la fonction
		$f : \R^2 \to \R : (x, y) \mapsto x^2 - y^2$. Son gradient s'annule en $(0, 0)$, or pour tout $x$, on trouve $f(x, 0) \gneqq f(0, 0) = 0 \gneqq f(0, x)$.
		\end{rmq}

		\begin{prp} Soient $f : I \to \R$ de classe $C^k$ et $a \in I$. Supposons $f^{(n)}(a) = 0$ pour $1 \leq n < k$ et $f^{(k)}(a) \neq 0$. Alors~:
		\begin{enumerate}
			\item si $k$ est pair et $f^{(k)}(a) > 0$, alors $a$ est un minimum local de $f$~;
			\item si $k$ est pair et $f^{(k)}(a) < 0$, alors $a$ est un maximum local de $f$~;
			\item si $k$ est impair, alors $a$ n'est pas un extremum de $f$.
		\end{enumerate}
		\end{prp}

		\begin{déf} Soit $M$ une matrice symétrique carrée $n \times n$. La forme quadratique~:
		\[Q_M : \R^n \to \R : x \mapsto x^TMx\]
		est dite~:
		\begin{itemize}
			\item \emph{définie positive} si $\forall x \neq 0 : Q_M(x) \gneqq 0$~;
			\item \emph{définie négative} si $\forall x \neq 0 : Q_M(x) \lneqq 0$~;
			\item \emph{indéfinie} si elle n'est ni positive ni négative.
		\end{itemize}
		\end{déf}

		\begin{thm} Soit $f : E \subset \R^n \to \R$ une fonction de classe $C^3$ définie sur un ouvert. Soit $a \in E$, un point critique de $f$. Alors
		\begin{enumerate}
			\item Si $H_f(a)$ est définie positive, alors $a$ est un minimum local de $f$~;
			\item si $H_f(a)$ est définie négative, alors $a$ est un maximum local de $f$~;
			\item si $H_f(a)$ est indéfinie, alors $a$ n'est pas un extremum de $f$.
		\end{enumerate}
		\end{thm}

		\begin{proof} Par hypothèse, $(\nabla f)(a) = 0$. Donc on peut exprimer l'approximation de Taylor d'ordre 2 de $f$ autour de $a$ par~:
		\[T_2(f, a)(x) = f(a) + \frac 12(x-a)^TH_f(a)(x-a),\]
		que l'on peut réécrire en~:
		\[T_2(f, a)(x) - f(a) = \frac 12\sum_{i=1}^n\sum_{j=1}^n\md f2{x_i}{}{x_j}{}(a)(x-a)_i(x-a)_j.\]

		Par le théorème de Taylor dans $\R^n$, on sait qu'il existe $C > 0, r > 0$ tels que $\bar B(a, r) \subset E$ et~:
		\[\abs {f(x)-T_2(f, a)(x)} \leq C\norm{x-a}^3.\]

		Quand $\norm {x-a} \leq 1$, on sait $\norm {x-a}^3 \leq \norm {x-a}^2$. Donc pour $x-a$ suffisamment petit, c'est le comportement de $(x-a)^TH_f(a)(x-a)$
		qui domine le comportement de $f(x)-T_2(f, a)(x)$.

		Supposons $H_f(a)$ définie positive (le cas définie négative se démontre de la même manière). On pose~:
		\[S^{n-1} \coloneqq \left\{x \in \R^n \tq \norm x = 1\right\}.\]
		L'ensemble $S^{n-1}$ et fermé borné, et donc par le théorème des bornes atteintes, on peut noter $\alpha$\footnote{Que l'on sait $\gneqq 0$ car $H_f(a)$
		est supposée définie positive.} le minimum de $Q$ restreint à $S^{n-1}$. On trouve alors pour tout $x \neq 0$~:
		\[Q(x) = Q\left(\norm x\frac x{\norm x}\right) = \norm x^2Q\left(\frac x{\norm x}\right) \geq \norm x^2\alpha.\]

		Prenons alors $\epsilon > 0$ tel que $\epsilon < \frac \alpha{4C}$. Si $\norm {x-a} < \epsilon$, on trouve~:
		\[\abs {f(x)-T_2(f, a)(x)} \leq C\norm{x-a}^3 \leq C\norm{x-a}^2\frac \alpha{4C} = \frac \alpha 4\norm{x-a}^2
		\leq \frac \alpha 4Q(x-a) = \frac 14(x-a)^TH_f(a)(x-a).\]

		Dès lors, prenons $x \in B(a, r)$. On observe~:
		\begin{align*}
			f(x)-f(a) &= f(x) - T_2(f, a)(x) + T_2(f, a)(x) - f(a) = f(x) - T_2(f, a)(x) + \frac 12(x-a)^TH_f(a)(x-a) \\
			&\geq -\frac 14(x-a)^TH_f(a)(x-a) + \frac 12(x-a)^TH_f(a)(x-a) = \frac 14(x-a)^TH_f(a)(x-a) \geq 0.
		\end{align*}
		On en déduit donc que $a$ est un minimum local de $f$.

		Si $H_f(a)$ est indéfinie, on sait qu'il existe $u, v \in \R^n$ tels que~:
		\[u^TH_f(a)u \gneqq 0 \gneqq v^TH_f(a)v.\]
		En raisonnant de la même manière, on trouve, avec $\delta \to 0$~:
		\[f(a + \delta u \gneqq f(a) \gneqq f(a + \delta v).\]
		On en déduit que $a$ n'est pas un extremum de $f$.
		\end{proof}

\newpage
\section{Fonctions implicites et réciproques}
	\subsection{Théorème du point fixe de Banach}
		\begin{déf} Soit $X$ un ensemble et soit $f : X \to X$. On dit que $x \in X$ est un \emph{point fixe de $f$} si $f(x) = x$.
		\end{déf}

		\begin{déf} La fonction $f : \R^m \to \R^n$ est dite \emph{lipschitzienne} s'il existe $L > 0$ tel que~:
		\[\forall x, y \in \R^m : \norm{f(x)-f(y)} \leq L\norm{x-y}.\]
		Le plus petit $L > 0$ satisfaisant cette condition est appelé la \emph{constante de Lipschitz} de $f$.
		\end{déf}

		\begin{lem} Toute fonction lipschitzienne est uniformément continue.
		\end{lem}

		\begin{proof} Soit $f : \R^m \to \R^n$ lipschitzienne. On sait donc qu'il existe $L > 0$ tel que~:
		\[\forall x, y \in \R^m : \norm{f(x) - f(y)} \leq L\norm{x-y}.\]
		Soit $\epsilon > 0$, et posons $\delta = \frac \epsilon L$. Supposons $\norm {x-y} < \delta$. On trouve donc~:
		\[\norm{f(x)-f(y)} \leq L\norm{y-y} < L\frac \epsilon L = \epsilon.\]
		\end{proof}

		\begin{déf} Une fonction de Lipschitz est dite \emph{contractante} si sa constante de Lipschitz est strictement inférieure à 1.
		\end{déf}

		\begin{thm}[Théorème du point fixe de Banach] Une application contractante $f : E \subset \R^m \to E$ admet un unique point fixe si $E$ est fermé.
		\end{thm}

		\begin{proof} Soit $f$ une telle contraction. On nomme $\alpha \in (0, 1)$ sa constante de Lipschitz. Montrons tout d'abord l'unicité du point fixe.
		Supposons qu'il existe deux points $x, y \in E$ tels que $F(x) = x$ et $F(y = y)$. On sait~:
		\[\norm {x-y} = \norm{f(x)-f(y)} \leq \alpha\norm{x-y},\]
		ce qui est une contradiction car $\alpha \lneqq 1$.

		Montrons maintenant que ce point fixe existe. On prend $x_0 \in E$ quelconque, et on définit la suite $(x_n)_{n \in \N^*} \subset E$ par
		$x_n = F(x_{n-1})$. On remarque que~:
		\[\norm{x_{n+1} - x_n} \leq \alpha\norm{x_n-x_{n-1}} \leq \alpha^2\norm{x_{n-1}-x_{n-2}} \leq \dotsb \leq \alpha^n\abs{F(x_0)-x_0}.\]
		On prend ensuite $n > m \in \N^*$ et on observe~:
		\[\norm{x_n-x_m} \leq \sum_{k=m+1}^n\norm{x_k-x_{k-1}} \leq \sum_{k=m+1}^n\alpha^{k-1}\norm{F(x_0)-x_0}.\]

		De plus, la suite des sommes partielles de $\alpha^k$ est convergente, et donc de Cauchy. On prend alors $\epsilon > 0$. On sait qu'il existe $N > 0$
		tel que~:
		\[\forall n \geq N : \sum_{k=m+1}^n\alpha^{k-1} < \frac \epsilon{\norm{F(0)-x_0}}.\]
		On trouve donc $\abs {x_n - x_m} < \epsilon$ pour tout $n > m > N$.

		Si $(x_n)$ est de Cauchy, alors elle est convergente. Appelons ce point $x$. De plus~:
		\[f(x) = f(\lim_{n \to +\infty}x_n) = \lim_{n \to +\infty}f(x_n) = \lim_{n \to + \infty}x_{n+1} = x.\]
		$x$ est donc bien un point fixe de $f$.
		\end{proof}

		\begin{lem}\label{lem:contractanteinjective} Si $g : \bar B(0, r) \to \R^n$ est une application contractante de constante $\alpha$ telle que $g(0)=0$,
		alors $I+g : \bar B(0, r) \to \R^n$ est injective et la boule $\bar B(0, (1-\alpha)r)$ est incluse dans son image.
		\end{lem}

		\begin{proof} Montrons que $I+g$ est injective : soient $x, y \in \bar B(0, r)$ tels que $x+g(x) = y+g(y)$. On peut réécrire cela par~:
		\[x-y = g(x)-g(y)\qquad\text{ et donc }\qquad\norm{x-y}=\norm{g(x)-g(y)}.\]
		Or $g$ est contractante. Dès lors, il faut $\norm {x-y}$ = 0, et donc $x=y$. $I+g$ est donc bien injective.

		Maintenant, montrons que la boule $\bar B(0, (1-\alpha)r)$ est incluse dans l'image de $I+g$. Pour cela, prenons $y \in \bar B(0, (1-\alpha)r)$ et
		montrons qu'il existe $x \in \bar B(0, r)$ tel que $x + g(x) = (I+g)(x) = y$. On cherche donc un point fixe de l'application~:
		\[x \mapsto y-g(x).\]

		On définit $F : \bar B(0, r) \to \R^n : x \mapsto y-g(x)$. On peut voir que $F$ est à valeurs dans $\bar B(0, r)$ car~:
		\[\norm {F(x)} \leq \norm y + \norm {g(x)} \leq \abs{(1-\alpha)r} + \alpha\abs r = r.\]
		De plus, on observe que $F$ est contractante car~:
		\[\norm {F(x) - F(\hat x)} = \norm {g(x) -g(\hat x)} \leq \alpha\norm {x-\hat x},\]
		où $\alpha \lneqq 1$. On peut donc appliquer le théorème de Banach qui garantit l'existence d'un point fixe. On a donc montré que la boule était
		contenue dans l'image de $I+g$.
		\end{proof}

	\subsection{Théorème de la fonction réciproque}
		\begin{lem}\label{lem:préimageouvertparcontinuestouvert} La préimage d'un ensemble de $U \subset \R^n$ ouvert par une fonction $f : \R^n \to \R^n$ est
		un ensemble ouvert de $V \subset \R^n$ si et seulement si $f$ est continue.
		\end{lem}

		\begin{thm}[Théorème de la fonction réciproque] Soit $f : E \subset \R^m \to \R^n$ de classe $C^1$ et soit $a \in E$. Si la matrice $J_f(a)$ est
		inversible, alors~:
		\begin{itemize}
			\item il existe $U, V \subset \R^n$ ouverts tels que $a \in U$ et $f(a) \in V$,
			\item et $f : U \to V$ est bijective.
		\end{itemize}

		De plus, $f^{-1}$ est bien définie, est dérivable en $f(a)$ et sa matrice jacobienne est donnée par~:
		\[J_{f^{-1}}(f(a)) = (J_f(a))^{-1}.\]
		\end{thm}

		\begin{proof} Montrons cela par étape, commençons par montrer la formule du jacobien de l'inverse, puis montrons que l'on peut se ramener au cas
		particulier $a=0$ et $J_f(0) = I$, et puis démontrons ce cas particulier.

		\paragraph{Démonstration de la formule du jacobien de l'inverse} On suppose $f$ différentiable en $f(a)$. Par la différentiation d'une composition, on
		trouve~:
		\[I = \left[\pd {x_i}{x_j}\right] = [\delta_{ij}] = J_I(a) = J_{f^{-1} \circ f}(a) = J_{f^{-1}}(f(a))J_f(a).\]
		On en déduit que $J_{f^{-1}}(f(a)) = (J_f(a))^{-1}$.

		\paragraph{Montrons que l'on peut se rapporter au cas particulier} On veut montrer qu'il est raisonnable de penser que l'on peut se ramener au cas où
		$a=0$, $f(0) = 0$, et $J_f(0) = I$.

		Soit $f$ respectant les conditions ci-dessus. On définit $\tilde f : E \to \R^n$ telle que~:
		\[\tilde f(x) = f(x)-f(a).\]
		On a alors $\tilde f(a) = 0$. En supposant le résultat vrai pour $\tilde f(a) = 0$,  il existe $U$ et $\tilde V$ ouverts tels que $a \in U$ et
		$\tilde f(a) \in \tilde V$ et $\tilde f : U \to \tilde V$ est une bijection. On pose $V = \tilde V - f(a) = \{v - f(a) \tq v \in \tilde V\}$.
		Dès lors $V$ est ouvert et $f$ met $U$ et $V$ en bijection. De plus, $f^{-1}$ est différentiable en $f(a)$ car $\tilde f$ est différentiable en $f(a)=0$.

		De manière similaire, on montre, avec $\bar f(x) = f(x+a)$ que l'on peut se ramener au cas $a=0$.

		Maintenant, on pose $\hat f$ définie par $\hat f(x) = (J_f(0))^{-1}f(x)$. Remarquons que l'application $x \mapsto (J_f(0))^{-1}$ est différentiable, que
		$\hat f(0) = 0$, et que $J_{\hat f}(0) = (J_f(0))^{-1}J_f(0) = I$.

		On peut donc dire qu'il existe $U, \hat V$ ouverts dans $\R^n$ tels que $0 \in U$ et $0 \in \hat V$ et $\hat f : U \to \hat V$ est une bijection.
		Également, $\hat f^{-1}$ est différentiable et son jacobien en $a=0$ est la matrice identité. On constate alors que $f$ est une bijection entre $U$ et
		$V$ défini par $J_f(0)\hat V$. Et également, $f$ est différentiable en $0$.

		\paragraph{Montrons le résultat pour les conditions énoncées ci-dessus} Maintenant que l'on a montré que le résultat généralisable s'il est vrai pour
		$a=0$, $f(a)=0$ et $J_f(0) = I$, démontrons qu'il l'est.

		On pose $g : E \to \R^n$ l'application $x \mapsto f(x) - x$ qui représente la \emph{distance} entre $f$ et $I$. On y voit bien que~:
		\[J_g(0) = J_f(0) - J_I(0) = 0.\]

		De plus, les dérivées partielles de $f$ sont continues, ce qui implique que les dérivées partielles de $g$ le sont également telles que~:
		\[\forall 1 \leq i, j \leq n : \pd {g_i}{x_j}(0) = 0.\]

		On en déduit qu'il existe une boule fermée $\bar B(0, r) \subset E$ telle que~:
		\[\forall \xi \in \bar B(0, r) : \abs{\pd {g_i}{x_j}(\xi)}^2 \leq \frac 1{4n^2}.\]
		Cela implique~:
		\[\forall \xi \in \bar B(0, r) : \abs{(\nabla g_i)(\xi)}^2 = \sum_{j=1}^n\abs{\pd {g_i}{x_j}(\xi)}^2 \leq \frac n{4n^2} = \frac 1{4n}.\]

		Par le théorème fondamental et l'inégalité de Cauchy-Schwartz, on peut exprimer pour $x, y \in \bar B(0, r)$~:
		\begin{align*}
			\abs{g_i(x) - g_j(x)} &= \abs{\int_0^1\od {}t\left(g_i\left(x + t(y-x)\right)\right)\dif t} = \abs{\int_0^1\scpr {(\nabla g_i)(x + t(y-x))}{y-x}} \\
			&\leq \int_0^1\abs{\scpr {(\nabla g_i)(x + t(y-x))}{y-x}}\dif t \leq \int_0^1\abs{(\nabla g_i)(x+t(y-x)}\abs{y-x}\dif t \\
			&\leq \frac 1{2\sqrt n}\abs{y-x}.
		\end{align*}

		Dès lors, on en conclut~:
		\[\norm {g(x)-g(y)}^2 = \sum_{i=1}^n\abs{g_i(x)-g_i(y)}^2 \leq \sum_{i=1}^n\left(\frac 1{2\sqrt n}\norm{y-x}\right)^2 = \frac n{4n}\norm{y-x}^2.\]

		On a donc défini une boule de rayon $r$ telle que pour tout $x, y$ dans cette boule, on ait~:
		\[\norm{g(x)-g(y)} \leq \frac 12\abs{x-y}.\]

		On en déduit que $g$ est une contraction de facteur $\alpha = \frac 12$. Par le lemme~\ref{lem:contractanteinjective}, on sait que $I+g$ est injective
		sur $\bar B(0, r)$ et que son image contient la boule $\bar B\left(0, \frac r2\right)$.

		On pose $V = B(0, r)$ et $U = f^{-1}(V)$, ce que l'on peut faire car $f^{-1}$ est bien définie (nous apprend le lemme). $f : U \to V$ est une bijection
		entre deux ouverts (par le lemme~\ref{lem:préimageouvertparcontinuestouvert}).

		Il reste à vérifier que la fonction inverse $f^{-1}$ est différentiable. Pour cela, calculons~:
		\[\lim_{x \to 0}\frac {\norm{f^{-1}(x) - f^{-1}(0) - (J_{f^{-1}}(0))^{-1}(x-0)}}{\norm x} = \lim_{x \to 0}\frac {\norm {f^{-1}(x)-x}}{\norm x}.\]

		Il faut vérifier que cette limite vaut 0. Il faut donc vérifier que pour toute suite $(x_n) \subset \R$ tendant vers 0 que~:
		\[\lim_{n \to +\infty}\frac {f^{-1}(x_n) - x_n}{\norm x_n} = 0.\]
		Puisque $f$ est bijective sur $U$ (dans $V$), on peut travailler sur $y_n \coloneqq f^{-1}(x_n)$. Or on sait $f$ différentiable en 0 de jacobien
		$J_f(0) = I$. On peut donc écrire~:
		\[\lim_{n \to +\infty}\frac {\norm {y_n - f(y_n)}}{\norm y_n} = 0.\]

		Puisque $\norm {g(x)} \leq \frac 12\norm x$ (car $g(0) = 0$), on trouve~:
		\[\frac 12\norm x\leq \norm {g(x)}\leq\frac 32\norm x.\]
		Cela veut dire que la suite $\left(\frac {\norm {y_n}}{\norm {x_n}}\right)_n$ est bornée. Dès lors~:
		\[\lim_{x \to 0}\frac {f^{-1}(x) - x}{\norm x} = \lim_{n \to +\infty}\frac {\norm{y_n - f(x_n)}}{\norm {y_n}}\frac {\norm {y_n}}{\norm {x_n}} = 0.\]
		En effet, le produit d'une suite bornée avec une suite tendant vers 0 tend vers 0.
		\end{proof}

		\begin{rmq} Si $f$ est de classe $C^k$, alors $f^{-1}$ est de classe $C^k$ également.
		\end{rmq}

	\subsection{Théorème de la fonction implicite}
		Une question qu'il est légitime de se poser est de savoir si l'ensemble~:
		\[\Gamma = \{x \in \R^n \tq g(x) = 0\},\]
		avec $g : \R^n \to \R$, est le graphe d'une fonction à $n-1$ variables.

		\begin{thm}[Théorème de la fonction implicite] Soit $g : E \subset \R^m \to \R$ une fonction de classe $C^1$ définie sur un ensemble ouvert. Soit $a \in E$ tel que~:
		\[\begin{cases}g(a) &= 0, \\\pd g{x_n} &\neq 0.\end{cases}\]
		Alors il existe~:
		\begin{enumerate}
			\item un ouvert $U \subset \R^{m-1} \tq (a_1, \dotsc, a_{n-1}) \in U$~;
			\item un ouvert $V \subset E \tq a \in V$~;
			\item une fonction $\varphi : U \to \R : (a_1, \dotsc, a_{n-1}) \mapsto a_n$,
		\end{enumerate}
		tels que~:
		\[\{x \in V \tq g(x) = 0\} = \{(a, \varphi(a)) \tq a \in U\}.\]

		De plus, $\varphi$ est différentiable en $(a_1, \dotsc, a_n)$ et~:
		\[\forall 1 \leq j < n : \frac {\partial \varphi}{\partial x_j}(a_1, \dotsc, a_{n-1}) = -\frac {\pd g{x_j}(a)}{\pd g{x_n}(a)}.\]
		\end{thm}

		\begin{rmq} La démonstration est omise dans ces notes.
		\end{rmq}

	\subsection{Règle des multiplicateurs de Lagrange}
		\begin{thm} Soient $U \subset \R^m$ un ouvert et $f, g : U \to \R$ tous deux de classe $C^1$. Soit $S = g^{-1}(0)$. Soit $p \in S$ tel que
		$(\nabla g)(p) \neq 0$. Si la restriction de $f$ à $S$ possède un extremum local en $p$, alors il existe $\lambda \in \R$ tel que~:
		\[(\nabla f)(p) = \lambda (\nabla g)(p).\]
		\end{thm}

		\begin{proof} On sait par hypothèse qu'au moins une des dérivées partielles de $g$ ne s'annule pas. On réorganise les variables de manière à avoir~:
		\[\pd g{x_n}(p) \neq 0.\]
		par le théorème de la fonction implicite, on sait qu'il existe $V_1, V_2$ deux ouverts et $\varphi : V_2 \to \R$ tels que~:
		\[(g^{-1} \eqqcolon S) \cap V_1 = \{(a, \varphi(a)) \tq a \in V_2\}.\]

		On prend $v \in \R^m$ tel que $\scpr {(\nabla g)(p)}v = 0$. Puisque $(\partial_n g)(p) \neq 0$, on peut exprimer~:
		\[v_n = -\frac {\sum_{i=1}^{n-1}v_i\pd g{x_i}(p)}{\pd g{x_n}}.\]

		Soit $\delta > 0$ tel que pour tout $t \in (-\delta, \delta)$, on ait $p + tv \in V_2$. Alors, on définit~:
		\[\gamma : (-\delta, \delta) \to \R^m : t \mapsto (p + tv, \varphi(p + tv)).\]
		On sait que $\varphi(p+tv)$ fait sens car $\delta$ est défini de telle manière que $p + tv$ soit dans $V_2$, le domaine de $\varphi$ pour tout
		$t \tq \abs t < \delta$. On sait exprimer la dérivée de cette courbe~:
		\[\od \gamma t(0) = \left(v_1, \dotsc, v_{n-1}, \od \varphi t(p + tv)\right)
		= \left(v_1, \dotsc, v_{n-1}, \sum_{k=1}^{n-1}v_k\pd \varphi{x_k}(p_1, \dotsc, p_{n-1})\right).\]

		Par le théorème de la fonction implicite, on peut écrire~:
		\[\pd \varphi{x_j}(p_1, \dotsc, p_{n-1}) = -\frac {\pd g{x_j}(p)}{\pd \varphi {x_n}(p)}.\]
		On peut dès lors exprimer la dérivée de $\gamma$ par~:
		\[\od \gamma t(0) = \left(v_1, \dotsc, v_{n-1}, -\sum_{k=1}v_k\frac {\pd g{x_k}(p)}{\pd g{x_n}(p)}\right) = (v_1, \dotsc, v_{n-1}, v_n) = v.\]

		On déduit de cela que $v$ est tangent à $\gamma$ en $p$, et donc par extension, $v$ est tangent à la surface $S$.

		On sait que $f$ admet un extremum en $p = \gamma(0)$. Donc on sait que $(f \circ \gamma) : (-\delta, \delta) \to \R$ admet un extremum en 0. Par les
		règles de dérivation, on trouve~:
		\[\od {(f \circ \gamma)}t(0) = \od ft(\gamma(0))\od \gamma t(0) = \sum_{k=1}^nv_k\pd f{x_k}(p) = \scpr v{(\nabla f)(p)}.\]

		Or on sait que $0$ est un extremum local de $(f \circ \gamma)$, donc on déduit $\scpr v{(\nabla f)(p)} = 0$. On a donc montré que pour tout vecteur $v$
		tel que $\scpr v{(\nabla g)(p)} = 0$, on a $\scpr v{(\nabla f)(p)} = 0$. Dès lors, les vecteurs $(\nabla g)(p)$ et $(\nabla f)(p)$ sont colinéaires.

		On note~:
		\begin{align*}
			\lambda &\coloneqq \frac {\scpr {(\nabla f)(p)}{(\nabla g)(p)}}{\norm{(\nabla g)(p)}^2}, \\
			\xi_1 &\coloneqq \lambda (\nabla g)(p), \\
			\xi_2 &\coloneqq (\nabla f)(p) - \xi_1.
		\end{align*}

		On remarque que $\scpr {\xi_1}{(\nabla g)(p)} = 0$. Donc on peut également dire $\scpr {\xi_2}{(\nabla f)(p)} = 0$, ce qui veut dire que~:
		\[0 = \scpr{\xi_2}{(\nabla f)(p) - \xi_1} + \scpr{\xi_2}{\xi_1} = \norm {\xi_2}^2 + \scpr{\xi_2}{\xi_1} = \norm {\xi_2}^2.\]
		On peut donc déduire $\xi_2 = 0$, et donc~:
		\[(\nabla f)(p) = \xi_1 = \lambda (\nabla g)(p).\]
		\end{proof}

\end{document}
