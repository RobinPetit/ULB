\documentclass{article}

\usepackage{palatino, eulervm}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fullpage}
\usepackage[bottom]{footmisc}
\usepackage[parfill]{parskip}
\usepackage{amsmath, amsthm, amsfonts, amssymb}
\usepackage{titlesec}  % reduce space between paragraphs
\usepackage{mathtools}
\usepackage{mathdots}
\usepackage{commath}
\usepackage{hyperref}
\usepackage{interval}

\makeatletter
\def\thm@space@setup{%
	\thm@preskip=.4cm
	\thm@postskip=\thm@preskip%
}
\makeatother

\title{Calcul différentiel et intégral}
\author{R. Petit}
\date{Année académique 2015 - 2016}

\DeclareMathOperator{\arctanh}{arctanh}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\Imf}{Im}
\DeclareMathOperator{\intr}{int}
\DeclareMathOperator{\Q}{\mathbb Q}
\DeclareMathOperator{\R}{\mathbb R}
\DeclareMathOperator{\N}{\mathbb N}
\DeclareMathOperator{\adh}{adh}
\DeclareMathOperator{\tq}{ t.q. }
\DeclareMathOperator{\Larea}{\mathcal L}
\DeclareMathOperator{\Uarea}{\mathcal U}
\DeclareMathOperator{\vol}{vol}

\newcommand{\scpr}[2]{{\left\langle#1, #2\right\rangle}}
\newcommand{\frrn}[2]{#1 : #2 \subseteq \R \to \R^n}
\newcommand{\frr}[2]{#1 : #2 \subseteq \R \to \R}
\newcommand{\frmr}[2]{#1 : #2 \subseteq \R^m \to \R}
\newcommand{\frmrn}[2]{#1 : #2 \subseteq \R^m \to \R^n}
\newcommand{\ab}{\interval ab}
\newcommand{\fabr}[1]{#1 : \ab \to \R}
\newcommand{\evf}[4]{\left[#1\left(#2\right)\right]_{#2=#3}^{#2=#4}}
\newcommand{\normfty}[1]{\norm {#1}_\infty}

\titlespacing\paragraph{0pt}{1pt plus 1pt minus 1pt}{4pt plus 1pt minus 1pt}
\titlespacing\subparagraph{0pt}{1pt plus 0pt minus 0pt}{1pt plus 1pt minus 1pt}

% amsthm
\newtheorem{thm}{Théorème}[section]
\newtheorem{prp}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollaire}
\newtheorem{lem}[thm]{Lemme}
\renewcommand{\proofname}{\it{Démonstration}}
\theoremstyle{definition}
\newtheorem{déf}[thm]{Définition}
\theoremstyle{remark}
\newtheorem*{rmq}{Remarque}

\begin{document}

\pagenumbering{Roman}
\maketitle
\tableofcontents
\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

\newpage

\section{Intuitions}
	\subsection{Les dérivées}
		\subsubsection{Définition}
			Alors que l'analyse nait au XVII$^e$ siècle, ce n'est que pendant le XIX$^e$ que les outils nécessaires à une
			expression rigoureuse ont été à disposition des mathématiciens. L'analyse a pour notion centrale celle de \textit{variation}.
			Intuitivement, la notion de variation instantanée d'une quantité $f(t)$ peut être décrite de la sorte~:
			\[V(t, \Delta t) = \frac {f(t + \Delta t) - f(t)}{\Delta t}\]

			pour une valeur $\Delta t$ étant \textit{de plus en petite}. On dit donc que $\Delta t$ \textit{tend vers 0}. Attention
			cependant car le manque de rigueur et le manque d'outils adaptés à la manipulation de données infinitésimales amènent à des
			paradoxes et des résultats illogiques voire inexplicables.

			Selon la définition de variation vue ci-dessus, nous pouvons exprimer la dérivée comme étant la variation instantanée d'une
			fonction $f : \R \to \R$ que nous notons $f'(t)$. Plus précisément, la variation \textit{instantanée} implique
			que $\Delta t$ soit \textit{infiniment petit}, ce qui s'écrit ainsi~:
			\[f'(x) = \lim_{\Delta t \to 0}\frac {f(t + \Delta t) - f(t)}{\Delta t}\]

			\begin{rmq} Pour des \textit{petites valeurs} de $\Delta t$, l'approximation suivante est admissible~:
			\[f(t + \Delta t) \simeq f(t) + f'(t)\Delta t\]
			\end{rmq}

		\subsubsection{Exemples}
			\paragraph{Fonction affine}
				Soit une fonction $f : \R \to \R : x \mapsto ax + b$. La dérivée de $f$ est l'expression suivante~:
				\[f'(x) = \lim_{h \to 0}\frac {f(x + h) - f(x)}{h} = \lim_{h \to 0}\frac {a(x + h) + b - (ax + b)}{h} =
				  \lim_{h \to 0}\frac {ax + ah + b - ax - b}{h} = \lim_{h \to 0}\frac {ah}{h} = a\]

				Il \textit{doit} sembler intuitif qu'étant donné que la fonction $f$ est une fonction dont le graphe est une droite, sa
				dérivée (donc sa variation instantanée) est constante sur tout son domaine du fait que la variation d'une fonction uniformément
				(dé)croissante est constante.

				Plus spécifiquement, si $a = 0$, la fonction est une fonction \textit{constante} : $f : \R \to \R : x \mapsto b$.
				De ce fait, sa dérivée doit être nulle car une fonction croissante n'a pas de variation (par définition). La quantification de
				cette variation doit donc être représentée par 0. Cela peut se montrer également aisément depuis la définition de la dérivée~:
				\[f'(x) = \lim_{h \to 0}\frac {f(x+h) - f(x)}{h} = \lim_{h \to 0}\frac {b - b}{h} = 0\]

			\paragraph{Fonction du second degré}
				Soit une fonction $f : \R \to \R : x \mapsto ax^2 + bx + c$. Sa dérivée peut être calculée ainsi~:

				\[
					\begin{aligned}
						  f'(x) &= \lim_{h \to 0}\frac {f(x+h) - f(x)}{h} = \lim_{h \to 0}\frac {a(x+h)^2 + b(x+h) + c - (ax^2 + bx + c)}{h} \\ &=
						  \lim_{h \to 0}\frac {ax^2 + 2axh + ah^2 + bx + bh + c- ax^2 - bx - c}{h} = \lim_{h \to 0}\frac {2axh + ah^2 + bh}{h} =
						  \lim_{h \to 0} 2ax + ah + b = 2ax + b
					\end{aligned}
				\]

				Ici, la dérivée est une fonction de $x$, ce qui indique que selon la valeur de $x$ à laquelle nous voulons évaluer la
				dérivée, la variation représentée est susceptible de différer.

			\paragraph{Fonction valeur absolue}
				Soit une fonction $f : \R \to \R : x \mapsto \left\{\begin{aligned}&\text{$+x$ si $x \ge 0$} \\&\text{$-x$ si $x < 0$}\end{aligned}\right.$.
				Afin de déterminer l'accroissement instantané de $f$ au point d'abscisse $x=0$, il faut repasser par la définition~:
				\[f'(a) = \lim_{h \to 0}V_f(a, h)\]

				Donc $f'(0)$ peut être déterminé de la manière suivante~:
				\[f'(0) = \lim_{h \to 0}V_f(0, h) = \lim_{h \to 0}\frac {f(0+h) - f(0)}{h} = \lim_{h \to 0}\frac {f(h)}{h}\]

				Les cas $h < 0$ et $h \ge 0$ doivent être traités séparément de par la définition de la fonction. Pour le premier cas,
				la dérivée est~:
				\[f'(0) = \lim_{h \to 0} \frac hh = 1\]

				Et pour le second cas,
				\[f'(0) = \lim_{h \to 0} \frac {-h}{h} = -1\]

				Le résultat est tout à fait cohérent par rapport au graphique de la fonction $x \mapsto |x|$ car cette fonction a deux
				accroissements différents~: l'un à gauche de 0, l'autre à droite de 0. Cependant, il est impossible d'exprimer la valeur
				de la dérivée de $f$ au point 0. On dit de $f$ qu'elle n'est pas dérivable en 0.

			\paragraph{Fonction exponentielle}
				Soit une fonction $f : \R \to \R : x \mapsto a^x$ avec $a \in \R_0^+$. À nouveau, pour définir sa
				dérivée, il nous faut repasser par la définition~:
				\[f'(x) = \lim_{h \to 0}\frac {a^{x+h} - a^x}{h} = \lim_{h \to 0}\frac {a^x(a^h - 1)}{h} = a^x\lim_{h \to 0}V(0, h) = a^xf'(0)\]

				Ce développement nous indique que pour connaitre la dérivée de $f$ au point $x$, il nous faut connaitre la dérivée de $f$
				au point $0$. Il est possible d'observer sur des esquisses de graphique que $f'(0)$ dépend de $a$ de manière croissante.
				Faire croître $a$ impliquera une croissance de $f'(0)$. Il existe cependant un nombre $\in \R$ tel que $f'(0) = 1$.
				Ce nombre est $e \simeq 2.718$, ce qui implique $f'(x) = e^xf'(0) = e^x = f(x)$. La fonction $x \mapsto e^x$ (et ses
				multiples) sont leur propre dérivée.

		\subsubsection{Dérivée d'ordre supérieur}
			Étant donné que la dérivée d'une fonction quelconque $f$ (en supposant qu'elle est dérivable) est également une fonction \\
			($f' : \R \to \R : x \mapsto f'(x)$), nous pouvons à nouveau dériver cette fonction (en supposant que la dérivée
			soit toujours dérivable). Il est alors question de \textit{dérivée seconde}, telle que $f''(x) = (f')'(x) = ((f)')'(x)$.
			Pour noter la $k^e$ dérivée, il existe la notation suivante~: $f^{(k)}$ (car répéter $k$ fois le symbole \textit{prime} (')
			est contre-productif en temps de lecture \textbf{et} d'écriture).

			Ces mêmes dérivées d'ordre supérieur à 1 ont leur importance et leur cohérence~: tant $f'(x)$ est la variation de la quantité
			$f(x)$, tant $f''(x)$ est la variation de $f'(x)$. La dérivée seconde représente donc l'accroissement de l'accroissement
			(ou la variation de la variation). Elle nous donne donc une information sur comment la variation évolue (en dynamique,
			si on représente la position d'un mobile par la fonction $x(t)$, $x'(t)$ représente la variation de la position, à savoir la
			vitesse (donc $x'(t) = v(t)$). Cependant, $x''(t)$ représente la variation de la vitesse, à savoir l'accélération, d'où
			$x''(t) = v'(t) = a(t)$).

			De plus, la dérivée seconde a une autre interprétation graphique~: si $f''(x) > 0$, nous savons que $f'(x)$ a une pente positive
			donc $f'(x)$ est croissante. Cela implique que $f(x)$ est croissante aussi mais de plus en plus croissante. Autrement dit,
			le graphe de $f(x)$ est \textit{concave} aux alentours de $(x, f(x))$. De manière similaire, lorsque $f''(x) < 0$, $f(x)$ est
			de moins en moins décroissante et donc le graphe de $f(x)$ est \textit{convexe} aux alentours de $(x, f(x))$.

		\subsubsection{Notation de Leibniz}
			En reprenant la définition de la variation donnée plus haut, nous pouvons définir $\delta f$ et $\delta x$ comme étant
			respectivement la variation de la quantité $f(x)$ et la variation de la quantité $x$. Nous avons donc~:
			\[V(x, h) = \frac {f(x+h) - f(x)}{h} = \frac {\delta f}{\delta x}\]

			Cela implique que nous ayons $\delta f = f(x + h) - f(x)$, ce qui est bien la variation de la quantité $f(x)$, et que nous
			ayons $\delta x = h = (x + h) - (x)$, ce qui est bien la variation de la quantité $x$.

			Or, nous avions défini la dérivée comme étant la variation instantanée, à savoir $f'(x) = \lim_{h \to 0}V(x, h)$, qui selon
			la notation de Leibniz correspond à $f'(x) = \lim_{h \to 0}\frac {\delta f}{\delta x}$. Leibniz a cependant instauré une
			seconde notation correspondant non plus à la variation comme la notation $\delta$ mais bien à la dérivée. Cette notation est~:
			\[\od{f}{x} = \lim_{h \to 0}\frac {\delta f}{\delta x}\]

			Attention cependant à ne pas utiliser cette valeur comme étant un quotient de nombres réels~: la notation $\od fx$ n'a de sens
			que lorsque la limite a été faite. Et comme $\lim_0 \frac {F(h)}{G(h)} \neq \frac {\lim_0 F(h)}{\lim_0 G(h)}$, nous ne pouvons
			pas séparer $\dif f$ et $\dif x$\footnote{Du moins pas dans le cas d'une dérivée.}.

			Leibniz a également eu besoin d'une notation pour la $k^e$ dérivée. Ayant considéré $\frac {\dif{}}{\dif x}$ comme étant
			une opération à réaliser $k$ fois, il a noté la $k^e$ dérivée de la sorte~:

			\[\od[k] fx\]

	\subsection{Règles de dérivation}
		\paragraph{Somme}
			Soient $f, g : \R \to \R$, deux fonction dérivables en $a$. De manière intuitive, nous pouvons dire que
			$(f+g)'(a) = f'(a) + g'(a)$ car l'accroissement de la fonction de somme est la somme des accroissements~: au point $a$,
			la fonction $(f+g)$ \textit{subit} un accroissement égal à l'accroissement de $f$ plus l'accroissement de $g$.

		\paragraph{Produit}
			Soient $f, g : \R \to \R$, deux fonction dérivables en $a$. Contrairement à ce que nous serions tentés de dire
			naïvement, nous ne pouvons pas définir $(fg)'(a) = f'(a)g'(a)$. C'est à Leibniz que l'on doit cette démonstration.

			Considérons un rectangle de dimensions $f(a)$ et $g(a)$. La quantité $(fg)(a)$ correspond à l'aire de ce rectangle.
			Si on passe de $a$ à $a+h$, nous obtenons un nouveau rectangle de dimensions $f(a+h)$ et $g(a+h)$. Selon l'approximation
			vue au point 1.1.1., nous savons que $f(a+h) \simeq f(a) + f'(a)h$. L'aire du nouveau rectangle est donc $A = f(a+h)g(a+h)
			\simeq (f(a) + f'(a)h)(g(a) + g'(a)h) = f(a)g(a) + f'(a)g(a)h + f(a)g'(a)h + f'(a)g'(a)h^2$.
			Rappelons tout de même que la dérivée ici est égale à~:
			\[\begin{aligned}(fg)'(a) &= \lim_{h \to 0}\frac{(fg)(a+h) - (fg)(a)}{h} = \lim_{h \to 0}\frac {f'(a)g(a)h + f(a)g'(a)h + f'(a)g'(a)h^2}{h} \\
									  &= \lim_{h \to 0}f'(a)g(a) + f(a)g'(a) + f'(a)g'(a)h = f'(a)g(a) + f(a)g'(a)\end{aligned}\]

			Nous avons donc la variation instantanée de la fonction $(fg)$ au point $a$~: $(fg)'(a) = f'(a)g(a) + f(a)g'(a)$.

		\paragraph{Composition}
			Soient $f, g : \R \to \R$, deux fonctions telles que $g$ est dérivable en $a$, et $f$ est dérivable en $g(a)$.
			Il est toujours possible de trouver la dérivée de la fonction composée $(f \circ g)(x) = f(g(x))$ à l'aide de l'approximation
			$f(a+h) \simeq f(a) + f'(a)h$ tant que $h$ est \textit{petit}. Notre dérivation devient donc~:
			\[
				\begin{aligned}
					(f \circ g)'(a) &= \lim_{h \to 0}\frac {(f \circ g)(a+h) - (f \circ g)(a)}{h} = \lim_{h \to 0}\frac {f(g(a+h)) - f(g(a))}{h}
					= \lim_{h \to 0}\frac {f(g(a) + g'(a)h) - f(g(a))}{h} \\
									&= \lim_{h \to 0}\frac {f(g(a)) + f'(g(a))g'(a)h - f(g(a))}{h} = f'(g(a))g'(a)
				\end{aligned}
			\]

		\paragraph{Réciproque}
			Soit $f : \R \to \R$. Si $f$ est bijective, nous pouvons définir sa fonction réciproque $f^{-1} : \R \to \R$.
			Si en plus, nous supposons que $f'(x) \neq 0 \forall x \in \R$, nous pouvons exprimer la dérivée de la fonction
			réciproque comme suit~:
			\[(f^{-1})'(f(a)) = \frac {1}{f'(a)}\]

			selon le développement suivant (nécessitant la dérivée d'une composée)~: en sachant que la dérivée de la fonction identité
			est 1 ($(x \mapsto x)'(x) = 1$) et que la composée d'une fonction $f$ avec sa réciproque ($f^{-1}$) correspond à la
			fonction identité, nous pouvons trouver la dérivée de la réciproque.
			\[
				\begin{aligned}
					id'(x) &= 1 \\
					(f^{-1} \circ f)'(x) &= 1 \\
					(f^{-1})'(f(x))f'(x) &= 1 \\
					(f^{-1})'(f(x)) &= \frac {1}{f'(x)}
				\end{aligned}
			\]

			Ou encore~:

			\[(f^{-1})'(a) = \frac {1}{f'((f^{-1})(a))}\]

		\begin{rmq} De manière graphique, ce résultat peut s'interpréter de la manière suivante~: la réciproque d'une fonction bijective est son image par
		symétrie orthogonale d'axe $y = x$. En traçant cet axe de symétrie, on peut observer qu'aux points $(a, f(a)) \in \Gamma_f$ et
		$(b, (f^{-1})(b)) \in \Gamma_{f^{-1}}$ tel que $b = f(a)$, nous avons un rapport entre les pentes des tangentes. Plus précisément, nous pouvons
		observer que la pente de la tangente au second point ($(f^{-1})'(b) = (f^{-1})'(f(a))$) correspond à l'inverse de la pente de la tangente au second
		point. Donc nous avons bien la même égalité~: $(f^{-1})'(f(a)) = \frac {1}{f'(a)}$. \end{rmq}

		\paragraph{Exemple des fonctions exponentielles et logarithmiques} Il a été vu plus haut que la fonction $x \mapsto \exp(x)$
		admettait pour dérivée elle-même. Nous avons également vu ci-dessus que $(f^{-1})'(a) = \frac {1}{f'((f^{-1})(a))}$.
		En étant tenté de trouver $f(x)$ telle que $f'(x) = f(x) \, \forall x \in \dom f$, nous aurions donc $(f^{-1})'(a) = \frac {1}{f((f^{-1})(a))} = \frac 1a$.
		Cependant, nous connaissons une fonction de ce genre~: la fonction exponentielle de base $e$. Sa fonction inverse, la fonction
		logarithmique de base $e$ a sa dérivée calculée de la sorte~:
		\[\log'(x) = \frac {1}{\exp(\log(x))} = \frac 1x\]

	\subsection{Les intégrales}
		\subsubsection{Définition}
			En analyse, il existe un outil permettant de calculer l'aire en dessous d'une courbe. Cet outil s'appelle \textit{intégrale}.
			Soit une fonction $f : \R \to \R$, nous définissons l'aire entre la courbe de $f$ et l'axe horizontal $y = 0$
			définie entre les droites $x = a$ et $x = b$ de la manière suivante~:
			\[A = \int_a^bf(x)\dif x\]

			\begin{rmq} Cette aire est dite \textit{signée}. C'est à dire que les parties \textit{en dessous} de l'axe $y = 0$ sont représentés par une valeur
			réelle négative alors que les parties \textit{au dessus} de l'axe $y = 0$ sont représentés par une valeur réelle positive. \end{rmq}

			Afin d'approximer l'aire que nous tentons de déterminer, nous découpons l'intervalle $[a, b]$ en $n$ \textit{morceaux} $[x_{i-1}, x_i] \, 1 \leq i \leq n$
			avec $x_i = a + (b-a)\frac in$. Ces \textit{arrêtes} nous permettent d'obtenir des rectangles de hauteur respective $f(x_{i-1})$.
			L'aire de chacun de ces rectangles est donc $b \times h = (x_i - x_{i-1})f(x_{i-1})$. L'approximation de l'aire recherchée (donc de
			l'intégrale) est la somme des aires de ces rectangles.
			\[\int_a^bf(x) \dif x \simeq \sum_{i=1}^n(x_i - x_{i-1})f(x_{i-1})\]

			Pour obtenir l'aire exacte, il faut faire tendre $n$ vers $+\infty$.

			\paragraph{Exemple de la fonction carrée}
				Si nous désirons trouver l'aire sous la courbe de $x \mapsto x^2$ sur $[0, t]$, nous la déterminons ainsi~:

				\[\begin{aligned}
					\int_0^tx^2\dif x &= \lim_{n \to \infty}\sum_{i=1}^n(x_i-x_{i-1})(x_{i-1})^2 = \lim_{n \to \infty}\sum_{i=1}^n\frac tn\left(t\frac {i-1}{n}\right)^2 =
					\lim_{n \to \infty}\frac {t^3}{n^3}\sum_{i=1}^n(i-1)^2 \\
									  &= \lim_{n \to \infty}\frac {t^3}{n^3}\frac {n(2n-1)(n-1)}{6} = \lim_{n \to \infty}\frac {t^3(2n-1)(n-1)}{6n^2} =
					t^3\lim_{n \to \infty}\frac {2n^2 + 1 - 3n}{n^2} \\
									  &= t^3\lim_{n \to \infty}\left(2 + \frac {1}{n^2} - \frac 3n\right) = t^3\frac 26 = \frac {t^3}{3}
				\end{aligned}\]

	\subsection{Théorème fondamental du calcul différentiel et intégral}
		\subsubsection{1ère version}
				En analyse, il existe un théorème (parfois appelé \textit{théorème fondamental de l'analyse}) qui définit la dérivation et l'intégration
				comme deux opérations inverses l'une de l'autre. Avant d'exprimer le théorème, tentons de donner un peu d'intuition au rapport
				entre dérivation et intégration.

				Soit $F(x)$, une fonction réelle définie par $F(x) = \int_a^xf(t)\dif t$ telle que $f$ est une fonction réelle continue. Admettons que
				$F(x)$ est dérivable sur l'intégralité de son domaine. Nous pouvons donc déterminer sa dérivée comme étant l'accroissement instantané~:
				\[F'(x) = \lim_{h \to 0}V_F(x, h) = \lim_{h \to 0}\frac {F(x+h) - F(x)}{h} = \lim_{h \to 0} \frac 1h\int_x^{x+h}f(t)\dif t\]

				Ce que représente $\int_x^{x+h}f(t)\dif t$ est l'aire en dessous de la courbe de $f$ entre les points d'abscisse $x$ et $x+h$.
				$h$ étant tout petit (tendant vers l'infini), nous pouvons approximer $f(t)$ constant entre $x$ et $x+h$. L'aire représentée par
				l'intégrale est donc $\simeq hf(x)$. Nous pouvons donc poursuivre le calcul de la dérivée de $F$~:
				\[F'(x) = \lim_{h \to 0} \frac 1h\int_x^{x+h}f(t)\dif t = \lim_{h \to 0} \frac 1hhf(x) = f(x)\]

				Nous avons donc $F(x) = \int_a^xf(t)\dif t$ et $f = F'(x)$. Nous pouvons dès à présent énoncer une première version du théorème~:

				\textit{Soit $f : \R \to \R$, une fonction continue sur $\R$, alors la fonction $F(x)$ définie par}
				\[F(x) = \int_a^xf(t)\dif t\]

				\textit{est dérivable sur l'intégralité de son domaine tel que $F'(x) = f(x)$.}

		\subsubsection{2nde version}
				Il existe une deuxième manière d'énoncer ce théorème. À nouveau, tentons de le trouver intuitivement. En ayant une fonction
				$f : \R \to \R$ dérivable en tout point de son domaine, nous pouvons considérer l'intégrale de $a$ en $b$ de $f'(x)$
				comme étant la somme des variations instantanées de $f$ sur l'intervalle $[a, b]$. Tentons maintenant de définir plus précisément
				ce que vaut cette intégrale.
				\[\int_a^bf'(t)\dif t = \lim_{n \to \infty}\sum_{i=1}^n\frac {b-a}{n}f'(x_{i-1})\]

				\paragraph{Rappel} Au point 1.1.1., nous avons vu l'approximation suivante~: $f(a+h) \simeq f(a) + f'(a)h$. En réorganisant cette
				approximation, nous avons $f'(a)h \simeq f(a+h) - f(a)$. Dans notre cas, si nous posons $h = \frac {b-a}{n}$, nous pouvons poursuivre
				notre intégrale.
				\[
					\begin{aligned}
						\int_a^bf'(t)\dif t &= \lim_{n \to \infty}\sum_{i=1}^nhf'(x_{i-1}) =  \lim_{n \to \infty}\sum_{i=1}^nf(x_{i-1} + h) - f(x_i) \\
											&=  \lim_{n \to \infty}f(x_n) - f(x_0) = f(b) - f(a)
					\end{aligned}
				\]

				Nous avons considéré cette intégrale comme étant la somme des variations instantanées de $f$ entre $a$ et $b$, et le résultat de
				ce développement est que la somme des variations instantanées est égale à la variation totale pour aller de $a$ à $b$.

				Le théorème peut donc être exprimé d'une deuxième manière~:

				\textit{Soit $f : \R \to \R$, une fonction réelle dérivable, alors}
				\[\int_a^bf'(t)\dif t = f(b) - f(a).\]

	\subsection{Primitives}
			En analyse, il est fréquent de devoir trouver $F$ tel que $F'(x) = f(x)$. C'est donc une recherche de fonction. Cependant, comme
			nous venons de le voir, trouver une fonction en connaissant sa dérivée revient à réaliser une intégrale (version 1 du théorème fondamental
			de l'analyse). Une solution $F$ satisfaisant $F'(x) = f(x)$ est appelée une \textit{primitive} de $f$. $\int f$ ou $\int f(t)\dif t$ sont
			les manières les plus courantes d'écrire \textit{primitive de $f$}\footnote{On parle parfois également d'intégrale indéfinie.}.
			Cependant, comme le laisse comprendre la seconde version du théorème fondamental de l'analyse, il existe une infinité de primitives,
			toutes définies à \textit{une constante près}. Nous généralisons donc «  la » primitive de $f$ en $\int f + C$, $C \in \R$.

	\subsection{Règles d'intégration}
			Tout comme il y a des règles de dérivation (point 1.1.5.), il existe des règles d'intégration.

			\paragraph{Produit}
				Tout comme $(fg)'(x) \neq f'(x)g'(x)$, $\int (fg)(x)\dif x \neq \int f(x)\dif x \int g(x) \dif x$. Pour réussir à intégrer un
				produit, il faut partir de la règle de Leibniz pour la dérivation~:
				\[
					\begin{aligned}
						(fg)'(x) &= f'(x)g(x) + f(x)g'(x) \\
						\int_a^b (fg)'(x) \dif x &= \int_a^b (f'(x)g(x) + f(x)g'(x)) \dif x \\
						[(fg)(x)]_a^b &= \int_a^b (f'(x)g(x) + f(x)g'(x)) \dif x \\
						[(fg)(x)]_a^b &= \int_a^b f'(x)g(x) \dif x + \int_a^b f(x)g'(x) \dif x \\
						\int_a^b f'(x)g(x) \dif x &= [(fg)(x)]_a^b - \int_a^b f(x)g'(x) \dif x
					\end{aligned}
				\]

				Il faut donc, pour pouvoir intégrer un produit, considérer un des deux facteurs comme étant une dérivée (qu'il faudra donc
				intégrer pour avancer).

			\paragraph{Composition}
				La règle \textit{d'intégration en chaine} est également appelée \textit{changement de variable}. Elle peut être exprimée comme
				suit. Soient $f$ et $g$, deux fonctions continûment dérivables, $g(\alpha) = a$ et $g(\beta) = b$, alors~:
				\[\int_a^bf(x)\dif x = \int_\alpha^\beta f(g(t))g'(t)\dif t.\]

				Cela veut dire qu'une intégration peut être résolue en \textit{transformant} l'intégrale de manière à faire apparaitre une
				composition. La justification de cette formule peut être donnée comme suit~:
				\[(F(g(t)))'(x) = F'(g(t))g'(t) = f(g(t))g'(t)\]

				Donc~:
				\[\int_\alpha^\beta f(g(t))g'(t) \dif t = F(g(\beta)) - F(g(\alpha)) = F(b) - F(a) = \int_a^bf(t)\dif t.\]

	\subsection{Les équation différentielles}
		Une équation différentielle est une équation dont l'inconnue est une fonction $f$ et dans laquelle les dérivées de $f$ apparaissent.

		\subsubsection{équation différentielle linéaire d'ordre 1}
			Comme le dit la pseudo-définition ci-dessus, une équadiff est une équation où l'inconnue est une fonction. Le cas de $F = \int_a^xf(t)\dif t$
			est un cas particulier d'une grande famille d'équadiffs que l'on appelle \textit{équation différentielle linéaire d'ordre 1}.
			Cette famille est caractérisée par la forme suivante~:
			\[f'(x) + p(x)f(x) = q(x).\]

			Une manière de résoudre une telle équation est de déterminer une fonction auxiliaire $a(x)$ que l'on va multiplier de part et d'autre
			de l'égalité~:
			\[a(x)f'(x) + a(x)p(x)f(x) = q(x)a(x)\]

			Le but de cette manipulation est de pouvoir faire ressortir $a(x)f'(x) + a'(x)f(x)$, ce qui est égal à $(af)'(x)$. Cependant, pour cela,
			il faut choisir $a(x)$ telle que $a'(x) = a(x)p(x)$. Il existe une solution~:
			\[a(x) : x \mapsto e^{\int p(x)\dif x}\]

			Maintenant que nous avons cette fonction, il suffit de résoudre $(af)'(x) = (aq)(x)$. Si $(aq)$ est continue, le théorème fondamental de
			l'analyse assure l'existence d'une primitive $b(x) = \int a(x)q(x)\dif x$. À présent, nous savons que si $(af)'(x) = (aq)(x)$, alors
			$(af)(x) = b(x)$, ou encore $f(x) = \frac {b(x)}{a(x)}$.

			Nous avons donc une solution à l'équation de départ~:
			\[f(x) = \frac {\int \left(e^{\int p(x)\dif x}\right)q(x)\dif x}{e^{\int p(x)\dif x}}.\]

		\subsubsection{Unicité de la solution et problème de Cauchy}
			Lorsque l'on \textit{transforme} le problème initial (équadiff linéaire d'ordre 1) en un problème conditionné (en précisant
			$f(x_0) = y_0$), nous limitons le nombre de solutions à 1. Si nous avons deux fonctions $f_1$ et $f_2$, solutions d'une équadiff linéaire
			d'ordre 1, et que nous définissons une autre fonction $g$ telle que $g(x) = (f_1 - f_2)(x)$, ladite fonction $g$ est une solution de
			l'équation suivante~:
			\[g'(x) + p(x)g'(x) = 0\]

			respectant, de plus $g(x_0) = 0$.

			Dans l'équation ci-dessus, le fait que $q(x) = 0 \forall x$ implique que $(ag)'(x) = 0 \forall x$ également, donc $(ag)(x) = C \forall x$.
			Autrement dit, $(ag)(x)$ est une fonction constante telle que $(ag)(x) = C \forall x$. Comme on sait que $g(x_0) = 0$, on sait que
			$e^{P(x_0)}g(x_0) = 0$ (où $P(x)$ est une primitive de $p(x)$), donc $C = 0$. Cependant, comme $(e^P)(x)$ ne peut s'annuler, il faut $g(x) = 0 \forall x$.

			Ce qui veut donc dire que $f_1$ et $f_2$, les deux fonctions solutions trouvées pour une équadiff linéaire d'ordre 1, sont les mêmes (vu que leur différence
			est la fonction nulle). Il existe donc \textbf{une et une seule} solution à l'équadiff linéaire d'ordre 1 conditionnée (problème de Cauchy).

		\subsubsection{équadiffs linéaire d'ordre 2 à coefficients constants}
			Une équadiff d'ordre 2 est sous la forme suivante~:
			\[\od[2]{}{x}f(x) + a\od{}{x}f(x) + bf(x) = 0\]

			avec deux paramètres réels $a, b \in \R$. Le terme \textit{linéaire} vient du fait que si $f_1, f_2 \in \R^{\R}$ sont deux solutions de
			cette équation, alors $c_1f_1 + c_2f_2$ en est également une (avec $C_1, C_2 \in \R$). Le fait que cette équation soit d'ordre 2 veut également
			dire que l'ensemble des solutions est un espace vectoriel de dimension 2.

			Pour résoudre une telle équation, il faut d'abord passer par ce que l'on appelle l'\textit{équation caractéristique}. Cette équation est la suivante~:
			\[\lambda^2 + a\lambda + c = 0.\]

			Cette équation étant du second degré, il faut séparer trois cas possibles~:
			\begin{itemize}
				\item l'équation admet deux solutions réelles distinctes $\lambda_1$ et $\lambda_2$~;
				\item l'équation admet une seule solution réelle $\lambda$~;
				\item l'équation n'admet aucune solution réelle.
			\end{itemize}

			Dans le premier cas (deux solutions distinctes), l'équation différentielle peut être réécrite sous la forme factorisée suivante~:
			\[\left(\od{}{x} - \lambda_1\right)\left(\od{}{x} - \lambda_2\right)f = 0.\]

			Les solutions $f_1(x) = e^{\lambda_1 x}$ et $f_2(x) = e^{\lambda_2 x}$ sont possible à \textit{deviner}. Le principe de linéarité exprimé juste au-dessus
			permet d'affirmer donc que $f(x) = C_1e^{\lambda_1 x} + C_2e^{\lambda_2 x}$ représente la famille des solutions paramétrées par $C_1$ et $C_2$.

			Dans le second cas (solution unique), l'équadiff peut être réécrite sous la forme factorisée suivante~:
			\[\left(\od{}{x} - \lambda\right)^2f = \od[2]{}{x}f + \lambda^2f - 2\lambda\od{}{x}f = 0.\]

			En posant $g(x) = \od{}{x}f(x) - \lambda f(x)$, nous avons $\od{}{x}g(x) = \od[2]{}{x}f(x) - \lambda\od{}{x}f(x)$. Cela nous permet de réécrire (encore une
			fois) l'équadiff sous la forme suivante~:
			\[\od[2]{}{x}f(x) - \lambda\od{}{x}f(x) - \lambda\left(\lambda\od{}{x}f(x) - \lambda f(x)\right) = \od{}{x}g(x) - \lambda(g(x)) = 0.\]

			La solution est $g(x) = Ke^{\lambda x}$. Or $g(x) = \od{}{x}f(x) - \lambda f(x)$. Donc il faut encore résoudre l'équation à l'aide de la méthode vue ci-dessus
			(ordre 1) afin de trouver la solution suivante. La solution finale est~:
			\[f(x) = \frac {\int \left(e^{\int p(x) \dif x}\right) q(x) \dif x}{e^{\int p(x) \dif x}}\]

			avec $p(x) = -\lambda$ et $q(x) = g(x) = K_1e^{\lambda x}$. D'où~:
			\[
				f(x) = \frac {\int \left(e^{\int -\lambda \dif x}\right) K_1e^{\lambda x} \dif x}{e^{\int -\lambda x \dif x}}
				     = \frac {\int K_2e^{-\lambda x}K_1e^{\lambda x} \dif x}{K_3e^{-\lambda x}}
				     = \left(K\int\dif x\right)e^{\lambda x}K_3^{-1} = K_3^{-1}K(x + C)e^{\lambda x} = (C_1x + C_2)e^{\lambda x}.
			\]

			Dans le dernier cas, l'équation caractéristique n'a pas de solution réelle. Il faut donc aller chercher du côté des nombres complexes. Les coefficients
			étant réels, les deux solutions complexes doivent être conjuguées l'une de l'autre (si $z = \alpha + \beta i$ et $\overline z = \alpha - \beta i$, alors
			$z\overline z = \alpha^2 + \beta^2 \in \R$ et $z + \overline z = 2\beta \in \R$). L'équadiff devient donc~:
			\[\left(\od{}{x} - z\right)\left(\od{}{x} - \overline z\right)f = 0.\]

			L'équation ressemble fortement à celle du premier cas, donc la solution générale est $f(x) = b_1e^{zx} + b_2e^{\overline z x}$ avec $b_1, b_2 \in \mathbb C$.
			Cependant, comme $\exp(zx) = \exp(\alpha x + i\beta x) = e^{\alpha x} \exp(i\beta x) = e^{\alpha x}(\cos(\beta x) + i\sin(\beta x))$ (et donc
			$\exp(\overline z) = e^{\alpha x}(\cos(\beta x) - i\sin(\beta x))$), en choisissant respectivement $b_1 = b_2 = \frac 12$ et $b_1 = -b_2 = -\frac i2$,
			on obtient respectivement $f(x) = e^{\alpha x}\cos(\beta x)$ et $f(x) = e^{\alpha x}\sin(\beta x)$. Nous avons donc deux solutions, et à nouveau, par
			linéarité, nous pouvons exprimer la famille des solutions paramétrée par $C_1, C_2 \in \R$~:
			\[f(x) = C_1e^{\alpha x}\cos(\beta x) + C_2e^{\alpha x}\sin(\beta x).\]

			Ici, notre solution ne fait plus intervenir quoi que ce soit de complexe ($\in \mathbb C$), tous les coefficients sont réels ($\alpha$ et $\beta$ sont des
			\textit{constantes} dépendantes de l'équation caractéristique).

		\subsubsection{équations de Newton}
			Après avoir étudié des équations linéaires, regardons une autre famille d'équations~: les \textbf{équations de Newton}. Cette famille est représentée
			par la forme suivante~:
			\[\od[2]{}{t}x(t) = f(x(t))\]

			avec $x : \R \to \R : t \mapsto x(t)$, l'inconnue et $f$, la fonction \textbf{continue} de \textit{force}. Une telle équation représente
			la position $x(t)$ en fonction du temps d'un mobile (de masse unitaire) soumis à une force $f$ dépendant de la position. Si on multiplie l'équation
			de part et d'autre par la quantité $x'(t)$, on obtient
			\[x^{(2)}(x)x'(t) - f(x(t))x'(t) = 0\]

			Ce que l'on peut intégrer afin d'avoir
			\[\frac 12(x'(t))^2 - F(x(t)) - K = 0,\]

			avec $F$, une primitive de $f$, ou encore
			\[(x'(t))^2 - 2F(x(t)) = E\]

			avec $E$, la constante d'intégration. Si l'équation différentielle est conditionnée (problème de Cauchy) telle que $x(0) = x_0$ et $x'(0) = v_0$, alors
			une solution pour cette équation est $x'(t) = \sqrt{E_0 + 2F(x(t))}$ avec $E_0 = v_0^2 - 2F(x_0)$.

				\paragraph{Exemple~: les équations de Fisher} Les équations de Fisher sont sous la forme suivante~: $u''(t) = (u - u^3)(t)$. La fonction $f$ de force
				est ici $f : \R \to \R : u(t) \mapsto u(t) - u^3(t)$, et a pour primitive $F : \R \to \R : \frac 12u^2(t) - \frac 14u^4(t) + C$.
				Pour des raisons de simplicité, ici, $C$ se verra attribuer la valeur $-\frac 14$. Donc $F(u(t)) = -\frac 14(u^2(t) - 1)^2$.

				Étant donné les solutions constantes à l'équation $u : t \mapsto K_u$ avec $K_u \in \{-1, 0, 1\}$, nous cherchons $u$ telle que~:
				\[\lim_{t \to \pm \infty}u(t) = \pm 1,\]
				et~:
				\[\lim_{t \to \pm\infty}u'(t) = 0.\]
				
				L'intégration première nous donne $u'(t)^2 = \frac 12(u^2(t) - 1)^2$, ou encore~:
				\[u'(t) = \frac {\sqrt{2}}{2}(u^2(t) - 1).\]

				Dans les intervalles de temps tels que $u'(t) > 0$ et $-1 \neq u(t) \neq 1$, nous avons~:

				\[\begin{aligned}
					\int_{t_0}^t \frac {1}{u^2 - 1}\dif u &= \int_{t_0}^t \frac {\sqrt{2}}{2} = \frac{\sqrt{2}}{2} (t - t_0) \\
					\arctanh(u(t)) - \arctanh(u(t_0)) &= \frac{\sqrt{2}}{2} (t - t_0)
				\end{aligned}\]

				Cependant, nous savons qu'$\exists t_0$ tel que $u(t_0) = 0$ vu que nous cherchons $-1 < u(t) < 1$. Supposons $t_0 = 0$, de manière à ce que
				l'équation devienne $\arctanh(u(t)) = \frac {\sqrt{2}}{2}t$ d'où $u(t) = \tanh(\frac t{\sqrt 2})$.

	\subsection{Problèmes et paradoxes}
		La majeure partie de l'analyse mathématique est basée sur la notion de limite, et surtout de limite infinie. Si cette notion est utilisée sans
		suffisamment de rigueur, un certain nombre de paradoxes peuvent apparaitre. Par exemple, soit la série $x, x^2, x^3, ...$ telle que
		$x_i = x^i$. Pour en connaitre la limite infinie, nous pouvons faire $L = \lim_{n\to\infty}x^n = \lim_{m\to\infty}x^{m+1} = x\lim_{m\to\infty}x^m = xL$.
		Nous avons donc $L = xL$, ou encore, $\forall x\neq1, L = 0$. Ce qui peut sembler contre-intuitif pour $x = 2$ ou $x = 3$ par exemple.
		Pareillement, si l'on désire mesurer l'hypoténuse d'un triangle $ABC$ avec $A = (0, 0), B = (1, 0), C = (0, 1)$, on peut approximer la
		longueur de l'hypoténuse comme la longueur d'un « escalier » de $N$ marches. Chaque marche est composée de deux segments de longueur $\frac 1N$.
		La longueur de l'escalier est donc $\frac {2N}{N}$, ou encore 2. Alors que le théorème de Pythagore nous dit que l'hypoténuse est de longueur
		$\sqrt 2$.

\newpage
\section{Les nombres réels}
	Le traitement de l'analyse peut être très puissant, cependant comme vu ci-dessus, un manque de rigueur peut amener à des contradictions voire à
	des résultats insensés. C'est pour cette raison qu'il faut instaurer cette rigueur dès les notions élémentaires telles que les nombres.

	\subsection{Axiomatique des nombres}
		\paragraph{Rappel} Les nombres sont organisés de la sorte~: $\N \subset \mathbb Z\subset \mathbb Q \subset \R$.

		Il faut savoir que chaque ensemble est défini selon des axiomes, et que c'est à partir de ces axiomes et de déductions logiques que sont
		construits les ensembles \textit{plus gros}. Ici, les axiomes correspondant aux ensembles $\N, \mathbb Z$ et $\mathbb Q$ sont considérés
		comme \textit{évidents} et seuls ceux de $\R$ sont explicités.

		\subsubsection{Axiomes de $\R$}
			Avant de citer les axiomes de $\R$, il faut introduire la notion de majorant et de minorant (pour l'axiome de complétude).

			Soit $A \subset \R$. On dit que $A$ est majoré $\iff \exists M \in \R | \forall a \in A, a \leq M$.

			De manière similaire, on dit que $A$ est minoré $\iff \exists m \in \R | \forall a \in A, a \geq m$.

			\begin{rmq} Le minorant/majorant ne doit pas nécessairement appartenir à $A$. \end{rmq}

			\begin{enumerate}
				\item $\R$ est un \textit{corps}
					\begin{itemize}
						\item $(\R, +)$ est un \textit{groupe commutatif}, donc la loi d'addition satisfait les conditions
							  suivantes~: associativité, commutativité, existence d'un élément neutre et existence d'un inverse~;
						\item $(\R \backslash \{0\}, .)$ est un \textit{groupe commutatif}~;
						\item La multiplication est distributive sur l'addition.
					\end{itemize}
				\item $(\R, \leq)$ est un corps entièrement ordonné
					\begin{itemize}
						\item la relation d'ordre $\leq$ satisfait les propriétés suivantes~: réflexivité, transitivité, antisymétrie
							  et ordre total~;
						\item $a \leq b \Rightarrow a + z \leq b + z \, \forall z \in \R$~;
						\item $a, b \leq 0 \Rightarrow ab \geq 0$.
					\end{itemize}
				\item $\R$ satisfait l'axiome de complétude. C'est cet axiome qui différencie grandement $\R$ de
					  $\mathbb Q$. Cet axiome de complétude dit ceci~:

					  \begin{itemize}
						\item $\forall A \subset \R$, si $A$ est non-vide et majoré, alors $A$ possède un majorant minimum appelé
							  \textit{supremum} de $A$ et noté $\sup A$.
						\item $\forall A \subset \R$, si $A$ est non-vide et minoré, alors $A$ possède un minorant maximum appelé
							  \textit{infimum} de $A$ et noté $\inf A$.
					  \end{itemize}

					  Et l'ensemble des rationnels $\mathbb Q$ ne respecte pas cet axiome.
			\end{enumerate}

		\subsubsection{Résultat de l'axiome~: les racines}
			Commençons par énoncer la propriété d'Archimède qui dit que $\forall y\in \R, \exists n \in \N \, | \, n > y$.
			La preuve de ce principe fait intervenir la notion de majorant vue ci-dessus.

			Soit $S = \{n \in \N \, | \, n \leq y\}$ avec $y \in \R$. Par définition, $y$ majore $S$. Il faut différencier les cas où
			$\#S = 0$ et $\#S > 0$. Dans le premier cas, $0 > y$, donc $n = 0$ est le nombre naturel recherché. Dans le second, posons $s = \sup S$.
			Comme $s \in S$, $s - 1$ n'est pas un majorant de $S$, $\Rightarrow \exists m \in S \, | \, s - 1 < m$, ou encore $m + 1 > s$. Or $s$
			majore $S$ donc $m + 1 \not \in S$. Si $m + 1 \not \in S$, alors $m+1 > y$. Donc $n = m + 1$ est le nombre naturel recherché.

			S'en suit le corollaire suivant~: $\forall y \in \R^+, \exists n \in \N \, | \, y > \frac 1n$. La preuve repose sur le lemme
			précédent~: soient $y \in \R^+, x = \frac 1y$. Le lemme d'Archimède dit qu'$\exists n \in \N \, | \, x < n$. Donc
			$\frac 1y < n$, ou encore $\frac 1n < y$. Il faut effectivement $y > 0$ car sinon lors de la dernière étape, nous avons $\frac 1n > y$,
			ce qui n'est pas possible pour $n \in \N$.

			Maintenant, intéressons-nous aux racines carrées et à l'affirmation de leur existence. Tentons de démontrer qu'
			$\exists x \in \R \, | \, x^2 = N \forall N \in \R$. Afin de démontrer ceci, procédons par l'absurde. Soient
			$S = \{y \in \R \, | \, y^2 \leq N\}$ et $x = \sup S$. Prouvons maintenant que $x^2 = N$.

			Si $x^2 < N$, posons $x_n = x + \frac 1n$ avec $n \in \N_0$. Donc
			$x_n^2 = x^2 + \frac {2x}{n} + \frac {1}{n^2} \leq x^2 + \frac {2x+1}{n} \, \forall n \in \mathbb N_0$. Comme $N - x^2 > 0$ et $2x + 1 > 0$ (vu
			que $x \geq 0$ du fait que $0 \in S$), la quantité $\frac {N - x^2}{2x + 1}$ est strictement positive également, donc
			$\exists n \in \N \, | \, \frac 1n \leq \frac {N - x^2}{2x + 1}$. Pour ce même $n$, nous avons $x_n^2 < N$ car
			$x_n^2 < x^2 + (2x+1)\frac 1n < x^2 + (2x+1)\frac {N - x^2}{2x + 1} = x^2 + N - x^2 = N$. Donc $x_n^2 \in S$, cependant $x_n = x + \frac 1n > x$,
			ce qui n'est pas possible.

			Inversement, si $x^2 > N$, on définit $x_n = x - \frac 1n$. D'où $x_n^2 >x^2 - \frac {2x+1}{x^2 - N}$. De plus, les quantités $x^2 - N$
			et $2x + 1$ sont toutes deux strictement positives, donc $\exists n \in \N \, | \, \frac 1n < \frac {x^2 - N}{2x + 1}$. Ce qui
			mène à $x_n^2 > N$. Or $x_n = x - \frac 1n < x$. Donc $x_n$ ne peut être un majorant de $S$. Cela implique qu'$\exists y \in S \, | \, y \geq x_n$,
			ou encore $y^2 \geq x_n^2 > N$, ce qui n'est pas possible car $y \in S \Rightarrow y^2 \leq N$.

			Donc si $x^2 \not < N$ et $x^2 \not > N$, alors $x^2 = N$. De plus, nous avons une définition d'une racine carrée (que l'on peut étendre à
			la racine $n^e$)~:
			\[x^\frac 1n = \sqrt[n] x = \sup \{y \in \R^+ \, | \, y^n \leq x\}.\]

			\begin{lem} Il s'en déduit que $\forall q = \frac mn \in \mathbb Q, x^q = (x^{\frac 1n})^m$. \end{lem}

			\begin{rmq} Ici, les puissances irrationnelles ne sont pas définies. Une telle définition viendra par la suite. \end{rmq}

	\subsection{Densité des rationnels}
		Nous avons vu l'ensemble $\R$ et sa partie rationnelle ($\mathbb Q$). Cependant, quelle est la proportion de ces nombres rationnels
		et donc quelle est la proportion de nombres irrationnels étant donné le résultat suivant~: $\forall x < y \in \R, \exists q \in \mathbb Q \, | \, x < q < y$ ?

		Commençons par prouver ce résultat. Séparons le problème en deux: le cas où $y - x > 1$ et le cas où $x < y$ quelconque. Dans le premier cas,
		$\exists n \in \N \, | \, n > x$ où $n$ est le plus petit entier plus grand que $x$. On en déduit que $n-1 \leq x$, ou encore
		$n \leq x + 1 < y$, ou encore $x < n < y$, prenons $q = n$. Dans le second cas, Archimède dit qu'$\exists m \in \N \, | \, m > \frac {1}{y - x}$.
		Cette inégalité peut se réécrire $my - mx > 1$, ce qui correspond au cas précédent. Prenons $q = \frac nm$, et nous avons $x < q < y$.

		Cela permet de se dire qu'il n'y a pas trop de points qui ont été ajoutés pour passer de $\mathbb Q$ à $\R$. Cependant, il faut savoir
		que $\#\R > \#\mathbb Q$, bien que ces deux ensembles soient infinis.

	\subsection{Inégalité triangulaire}
		L'inégalité triangulaire dit que $\forall x, y \in \R, |x + y| \leq |x| + |y|$. Pour le prouver, il faut savoir que $ab \geq 0 \Rightarrow |a + b| = |a| + |b|$.
		Cependant, quand $a \leq 0 \leq b$, alors $a - |b| = - |a| - |b| \leq a + b \leq |a| + b = |a| + |b|$. Or, pour avoir $x \leq -y$ et $x \geq y$,
		il faut $|x| \leq |y|$. Donc $|a + b| \leq ||a| + |b|| = |a| + |b|$.

		De manière similaire, on peut dire que $\forall x, y \in \R, |x - y| \geq |x| - |y|$. Il suffit pour le prouver de poser $x = a - b$ et $y = b$.
		De là, le lemme précédent s'applique de manière à ce que $|a| \leq |a - b| + |b|$, ce qui peut se réécrire comme suit~: $|a - b| \geq |a| - |b|$.

	\subsection{Autres corps}
		Avant tout, il faut définir ce qu'est un corps.

		L'ensemble $\mathbb K$ est un corps si, muni des opérations d'addition et de produit, il respecte les trois propriétés suivantes~:
		\begin{itemize}
			\item $(\mathbb K, +)$ est un groupe commutatif~;
			\item $(\mathbb K \backslash \{0\}, .)$ est un groupe commutatif~;
			\item le produit est distributif sur l'addition.
		\end{itemize}

		C'est principalement le corps des nombres réels qui sera étudié dans ce cours, cependant il en existe plein d'autres.

\newpage
\section{Les suites}
	Après avoir utilisé la notion de « limite » pour \textit{définir} les notions de dérivée et d'intégrale, il est nécessaire de définir précisément
	cette notion de limite (ou de \textit{convergence}).

	Une suite réelle (dans $\R$) est une liste infinie de $x_i \in \R$ indexés par $i \in \N$. Cette suite se note $(x_n)$, $(x_n)_n$, ou
	encore $(x_n)_{n \in \N}$.

	\subsection{Convergence et divergence}
		On dit d'une suite $(x_n)$ qu'elle converge en $a \in \R \iff \forall \epsilon > 0, \exists N \in \N \, | \, n \geq N \Rightarrow |x_n - a| < \epsilon$.
		Cela se note~:
		\[\lim_{n \to \infty}x_n = a.\]

		Il reste cependant à prouver qu'une suite convergente n'a qu'une seule limite. Pour ce faire, procédons par l'absurde. Supposons que
		$x_n \to a$ et $x_n \to b$ quand $n \to \infty$ et supposons que $a \neq b$. Selon la définition ci-dessus,
		$\exists N_1 \in \N \, | \, n \geq N_1 \Rightarrow |x_n - a| < \epsilon$ et
		$\exists N_2 \in \N \, | \, n \geq N_2 \Rightarrow |x_n - b| < \epsilon$. En prenant $\epsilon = \frac 12|a - b| > 0$ car $a \neq b$
		et $N = \max\{N_1, N_2\}$, l'inégalité triangulaire dit que $|a - x_N + x_N - b| \leq |a - x_N| + |x_N - b| < 2\epsilon$. Donc $|a - b| < |a - b|$.
		L'hypothèse disant $\epsilon > 0$ est fausse, ce qui implique $\epsilon = 0$, ou encore $a = b$.

		Pour définir une convergence en l'infini positif, on procède de la sorte~: $\lim_{n\to\infty} x_n = \infty \iff \forall K \in \R^+, \exists N \in \N \, | \, n \geq N \Rightarrow x_n > K$.
		De manière similaire, pour l'infini négatif~: $\lim_{n\to\infty}x_n = -\infty \iff \forall K \in \R^+, \exists N \in \N \, | \, n \geq N \Rightarrow x_n < K$.

		On dit d'une suite qui ne converge en aucun réel qu'elle est divergente ($\infty \not \in \R$ !).

		\subsubsection{Techniques de démonstration de divergence ou de convergence}
			Soit $(x_n)$, une suite convergente. Alors $(x_n)$ est bornée. Autrement dit, $\exists K \in \R^+ \, | \, \forall n \in \N, |x_n| \leq K$.

			Pour prouver ceci, il faut d'abord montrer que pour $a = \lim x_n$, $\forall \epsilon > 0, \exists N \in \N \, | \, n \geq N \Rightarrow |x_n - a| < \epsilon$.
			Donc $|x_n| = |x_n - a + a| \leq |x_n - a| + |a| < \epsilon + |a|$. Autrement dit, à partir de $n = N$, $(x_n)$ est borné par $\epsilon + |a|$.
			Il reste donc un nombre fini d'éléments à traiter. Donc $K = \max \{|x_0|, ..., |x_{N-1}|, \epsilon + |a|\}$ borne l'entièreté de $(x_n)$.

			De plus, les opérations sur suites sont définies telles que, pour $(x_n)$ et $(y_n)$ convergentes respectivement en $a$ et $b$~:

			\begin{itemize}
				\item $(x_n + y_n)$ est convergente en $a + b$~;
				\item $(x_ny_n)$ est convergente en $ab$~;
				\item $b \neq 0 \Rightarrow (\exists M \, | \, n \geq M \Rightarrow y_n \neq 0) \land (x_n/y_n)_{n \geq M}$ est convergente en $\frac ab$.
			\end{itemize}

			Ces assertions se démontrent comme suit.

			\paragraph{Somme de suites} Soit $\epsilon > 0$, $\exists N_1 \, | \, \forall n \geq N_1, |x_n - a| < \frac \epsilon2$. Pareillement pour
			$N_2 \ | \, \forall n \geq N_2, |y_n - b| < \frac \epsilon2$. En prenant $N = \max \{N_1, N_2\}$, on a $|(x_n + y_n) - (a + b)| \leq |x_n - a| + |y_n - b| < \epsilon$.

			\paragraph{Produit de suites} La suite $(x_ny_n)$ est bornée, donc $\exists K \, | \, |x_n| \leq K$. Donc $|x_ny_n - ab| = |x_n(y_n - b) - b(x_n - a)|$.
			Autrement dit, $|x_ny_n - ab| \leq |x_n||y_n - b| + |b||x_n - a| = K|y_n - b| + |b||y_n - b|$.
			Avec $\epsilon > 0$, $\exists N_1 \, | \, \forall n \geq N_1, |y_n - b| < \frac {\epsilon}{2K}$ et $\exists N_2 \, | \, \forall n \geq N_2, |x_n - a| l \frac {\epsilon}{|b| + 1}$
			(il faut mettre $|b| + 1$ dans le cas où $b = 0$. Avec $N = \max \{N_1, N_2\}$, $\forall n \geq N$, on a~:
			\[|x_ny_n - ab| \leq K|y_n - b| + |b||x_n - a| < \frac \epsilon2 + \frac \epsilon2 = \epsilon.\]

			\paragraph{Quotient de suites} En prouvant que $(\frac {1}{y_n}) \to \frac 1b$, le quotient découle du produit.
			Soit $\epsilon = \frac {|b|}{2} > 0$, $\exists M \, | \, n \geq M \Rightarrow |y_n - b| < \epsilon$. Donc, par l'inégalité triangulaire,
			$|y_n| \geq |b| - |y_n - b| > |b| - \epsilon = \frac {|b|}{2}$. La suite $(y_n)_{n \geq M}$ est bien définie.Maintenant prouvons sa
			convergence en $\frac 1b$~:
			\[\left|\frac {1}{y_n} - \frac 1b\right| = \left|\frac {b - y_n}{by_n}\right| \leq \frac {2}{|b|^2}|b - y_n|.\]

			Pour $\epsilon > 0$ fixé, $\exists N \geq M \, | \, \forall n \geq N, |y_n - b| < \frac {|b|^2}{2}\epsilon$.
			Donc $|\frac {1}{y_n} - \frac 1b| < \frac {2}{|b|^2}\frac {|b|^2}{2}\epsilon = \epsilon$.

			\paragraph{Exemple du quotient de polynômes de degré $k$} Soient $a_i, b_i \in \R$ avec $0 \leq i \leq k$. La suite $(x_n)$ définie par
			$x_n = \frac {\sum_{i = 0}^ka_in^i}{\sum_{i = 0}^kb_in^i}$ converge en $\frac {a_k}{b_k}$. Pour le prouver, il faut diviser le numérateur
			et le dénominateur par $n^k$. La suite devient $x_n = \frac {\sum_{i = 0}^ka_in^{i-k}}{\sum_{i = 0}^kb_in^{i-k}}$. Or $\frac {1}{n^K}$
			converge en 0 $\forall K > 0$. Donc $\lim_{n\to\infty}x_n = \lim_{n\to\infty}\frac {\sum_{i = 0}^ka_in^{i-k}}{\sum_{i = 0}^kb_in^{i-k}} = \frac {a_k}{b_k}$.

			\paragraph{Théorème du sandwich} Soient $(a_n)$ et $(b_n)$, deux suites réelles convergentes vers $l$. Si $(x_n)$ est une suite satisfaisant
			$a_n \leq x_n \leq b_n \forall n \geq N_0$, alors $x_n \to l$.

			Soit $\epsilon > 0$. Par définition, on sait~:
			\[\exists N_1, N_2 \, | \, (n \geq N_1 \Rightarrow |a_n - l| < \epsilon) \land (n \geq N_2 \Rightarrow |b_n - l| < \epsilon).\]
			
			Donc $\forall n \geq N = \max \{N_0, N_1, N_2\}, -\epsilon < a_n - l \leq x_n - l \leq b_n - l < \epsilon$. Autrement dit, $|x_n - l| < \epsilon$.

			Il en découle un corollaire disant que si $(y_n)$ est une suite bornée (pas forcément convergente) et $z_n \to 0$, alors $y_nz_n \to 0$. Pour le prouver,
			on sait qu'$\exists K > 0 \, | \, |y_n| < K$. Autrement dit, $-K < |y_n| < K$, d'où $-K|z_n| \leq |y_n||z_n| \leq K|z_n|$. Puisque $|y_n||z_n| = |y_nz_n|$ et
			$z_n \to 0$, alors $|y_nz_n| \to 0$.

			\paragraph{Règle de l'exponentielle} Cette règle dit que si $(a_n)$ est une suite réelle positive convergent en 0, alors $(a_n^p)$ avec $p \in \mathbb R$
			converge également en 0. Pour le prouver, il faut prendre $\epsilon > 0$ et $\epsilon' = \epsilon^{p^{-1}}$. Par définition,
			$\exists N \, | \, n \geq N \Rightarrow |a_n| < \epsilon'$. Cependant, $a_n \geq 0$, donc $0 \leq a_n < \epsilon' = \epsilon^{p^{-1}}$. D'où
			$0 \leq a_n^p < \epsilon$.

			\paragraph{Suites convergentes en 0} Les suites suivantes convergent en 0, la plupart se démontrent avec le binôme de Newton et/ou le théorème du sandwich.

			\begin{itemize}
				\item $(\frac {1}{n^p})$ avec $p > 0$~;
				\item $(c_n$) avec $|c| < 1$~;
				\item $(n^pc^n)$ avec $p \in \R, |c| < 1$~;
				\item $(\frac {c^n}{n!})$ avec $p \in \R$~;
				\item $\frac {n^p}{n!}$ avec $p \in \R$.
			\end{itemize}

			La première proposition se démontre avec la règle de l'exponentielle vu que $(n^{-1}) \to 0$. La seconde se démontre avec le théorème du sandwich en
			posant $c = \frac {1}{1+a}$ avec $a > 0$. La troisième se démontre de manière similaire. La quatrième se démontre avec le principe d'Archimède et le théorème du
			sandwich. La dernière se réécrit $\frac {n^p}{n!} = \frac {n^p}{2^n}\frac {2^n}{n!}$ et est donc un produit de deux suites convergentes en 0.

			Le procédé pour déterminer la convergence d'une suite est donc de trouver un maximum de suites convergentes en 0 puis d'appliquer les opérations sur les
			limites de suite. Regardons maintenant du côté des suites divergentes.

			Soit $x_n \to \infty$ et $(y_n) \subset \R$. S'il $\exists A \in \R \, | \, \forall n, y_n > A$, alors $x_n + y_n \to \infty$. Également, si $A > 0$,
			alors $x_ny_n \to \infty$. La première affirmation se démontre comme suit~: soit $K > 0$, $\exists N \, | \, \forall n \geq N, x_n > K - A$, donc
			$x_n + y_n > K$. Pour le produit, $\exists N \, | \, \forall n \geq N, x_n > \frac KA$ donc $x_ny_n > K$.

			\paragraph{Règle de la réciproque} Soit $(x_n)$ une suite réelle qui tend vers $\pm \infty$. Alors $\frac1{x_n} \to 0$. De plus, soit $(x_n)$, une
			suite réelle non-nulle. S'$\exists M \, | \, n \geq M \Rightarrow x_n > 0$, alors $x_n \to \infty$. Similairement, s'$\exists M \, | \, n \geq M
			\Rightarrow x_n < 0$, alors $x_n \to -\infty$. Pour démontrer la première affirmation, il faut avoir $\epsilon > 0$, donc $\frac 1\epsilon > 0$.
			On sait qu'$\exists N \, | \, n \geq N \Rightarrow x_n > \frac 1\epsilon$. ou encore $0 < \frac 1{x_n} < \epsilon$ (car $x_n > \frac 1\epsilon > 0$).
			Pour démontrer la seconde, il faut avoir $K > 0$. On sait qu'$\exists N \, | \, n \geq N \Rightarrow x_n < \frac 1K$. Ou encore $\frac 1{x_n} > K$.

			Soient $(x_n)$, une suite réelle et une suite strictement croissante $n_1 < n_2 < \ldots$. La suite $(x_{n_k})$ est une \textit{sous-suite} de $(x_n)$.

			Il en découle le lemme suivant~: Soit $(x_n)$, une suite réelle convergente en $a$. Alors toute sous-suite de $(x_n)$ converge également en $a$.
			Pour le démontrer, il faut repartir de la définition et donc, puisque $x_n \to a$, alors $\exists n \, | \, n \geq N \Rightarrow |x_n - a| < \epsilon$
			avec $\epsilon \in \R^+_0$. Or, comme $(n_k)$ est une série naturelle, $n_k \geq k \; \forall k$ (preuve par récurrence). Si $k \geq N$,
			alors $|x_{n_k} - a| < \epsilon$. Donc $x_{n_k} \to a$.

			On peut en déduire le corollaire suivant~: si $(x_n)$ a deux sous-suites convergentes mais ayant des limites différentes, alors $(x_n)$ ne converge pas.

		\subsubsection{Les suites monotones}
			Une suite est dite \textit{croissante} si $x_n \leq x_{n+1} \; \forall n$ et est dite \textit{décroissante} si $x_n \geq x_{n+1} \; \forall n$.
			Si une suite est soit croissante, soit décroissante, elle est dite \textit{monotone}. Sur base de cette définition, il existe un théorème important,
			celui de la convergence des suites monotones~:

			Soit $(x_n)$, une suite monotone bornée. $(x_n)$ est convergente telle que quand $(x_n)$ est croissante, $\lim x_n = \sup\{x_n \, | \, n \in \N\}$
			et quand $(x_n)$ est décroissante, $\lim x_n = \inf\{x_n \, | \, n \in \N\}$.

			Pour le démontrer, il faut définir $S = \{x_n \, | \, n \in \N\}$, borné par hypothèse. De là, il faut supposer $(x_n)$ soit croissante soit
			décroissante (la démonstration est similaire) et prouver que $a = \sup\{x_n \, | \, x \in \N\}$ si $x_n \to a$. Soit $\epsilon > 0$. Par définition,
			$a$ est le majorant minimal de $S$. Donc $a-\epsilon$ ne majore pas $S$, donc $\exists x_N > a-\epsilon$. Or, comme $(x_n)$ est croissante,
			$n \geq N \Rightarrow x_n \geq x_N > a-\epsilon$. Mais $x_n \leq a$ car $a$ majore $S$. Donc $a-\epsilon < x_n \leq a$. Ou encore $-\epsilon < x_n - a < 0$,
			d'où $|x_n - a| < \epsilon$.

			On sait donc qu'une suite monotone non bornée diverge en $\pm \infty$ et qu'une suite bornée converge en $\sup\{x_n \, | \, n \in \N\}$ ou en
			$\inf\{x_n \, | \, n \in \N\}$.

	\subsection{Théorème de Bolzano-Weierstrass}
		Regardons comment construire deux suites monotones à partir d'une suite quelconque. Soit une suite $(x_n)$, commençons par définir~:
		\[S_m := \{x_n | m \leq n\}.\]

		Si $(x_n)$ est majorée, alors $s_n = \sup S_n$ est fini pour tout $n$ et $(s_n)$ est une suite décroissante car $m \geq n \Rightarrow S_m \subseteq S_n \Rightarrow
		\sup S_m \leq \sup S_n$. De même, si $(x_n)$ est minorée, alors $i_n = \inf S_n$ est fini pour tout $n$ et $(i_n)$ est une suite croissante.

		\begin{déf} Soit $(x_n) \subseteq \R$.

		\begin{itemize}
			\item Si $(x_n)$ est majorée, alors $(s_n)$ est bien définie, et on écrit~:
			\[\limsup_{n\to\infty}x_n := \lim_{n\to\infty}s_n\]

			que l'on appelle la \textit{limite supérieure} de $(x_n)$.

			\item Si $(x_n)$ n'est pas majorée, alors on écrit $\limsup_{n\to\infty}x_n = \infty$.

			\item Si $(x_n)$ est minorée, alors $(i_n)$ est bien définie, et on écrit~:
			\[\liminf_{n\to\infty}x_n := \lim_{n\to\infty}i_n\]

			que l'on appelle la \textit{limite inférieure} de $(x_n)$.

			\item Si $(x_n)$ n'est pas minorée, alors on écrit $\liminf_{n\to\infty}x_n = -\infty$.

			\item Constatons donc que pour une suite $(x_n)$ bornée, $\limsup_{n\to\infty}x_n$ et $\liminf_{n\to\infty}x_n$ sont tous les deux finis.
		\end{itemize}
		\end{déf}

		\begin{thm}[Bolzano-Weierstrass]\label{thm:BolzWeieR} Soit $(x_n)$ une suite bornée. Alors il existe une sous-suite de $(x_n)$ qui converge en
		$\limsup_{n\to\infty}x_n$ et une autre qui converge en $\liminf_{n\to\infty}x_n$.

		De plus, pour une sous-suite convergente $(x_{n_k})$ quelconque, $\liminf_{n \to \infty}x_n \leq \lim_{k\to\infty}x_{n_k} \leq \limsup_{n\to\infty}x_n$.

		\[\liminf_{n\to\infty}x_n \leq \lim_{k\to\infty}x_{n_k} \leq \limsup_{n\to\infty}x_n.\]
		\end{thm}

		\begin{proof} Soit $(x_n)$ une suite bornée. Montrons qu'il existe un sous-suite convergent en $\limsup_{n\to\infty}x_n$. La démonstration
		pour la sous-suite convergent en $\liminf_{n\to\infty}x_n$ étant similaire. Définissons cette suite $(x_{n_k})$ terme à terme.

		Prenons $n_0 = 0$. C'est à dire que la sous-suite $(x_{n_k})$ débute en $x_0$. Regardons maintenant l'ensemble $S_{0+1} = \{x_1, x_2, \ldots\}$.
		Comme $s_1$ est le supremum de $S_1$, il doit exister $x_m \in S_1$ tel que $s_1-1 < x_m \leq s_1$. Prenons alors pour $n_1$ le plus petit $m$
		satisfaisant cette condition. Comme $\forall \alpha \in S_1, \alpha > x_0 = n_0$, il est évident que $n_1 > n_0$.

		Regardons maintenant l'ensemble $S_{1+n_1} = \{x_{1+n_1}, x_{2+n_1}, \ldots\}$. De manière similaire au point précédent, il est possible de trouver pour
		$n_2$ un $m$ tel que $x_m \in S_{1+n_1}$ et $s_{1+n_1}-\frac 12 < x_m \leq s_{1+n_1}$. Il est tout aussi évident que $n_2 > n_1$.

		En réitérant ce procédé $k$ fois, on obtient $n_0 < n_1 < n_2 <\ldots < n_k$ satisfaisant~: $s_{1+n_i} - \frac 1{i+1} < x_{n_{i+1}} \leq s_{1+n_i}$.

		On obtient donc au final une sous-suite $(x_{n_k})$ de $(x_n)$ telle que $s_{1+n_k} - \frac 1{k+1} < x_{n_{k+1}} \leq s_{1+n_k}$. De plus, la suite
		$(s_{1+k_k})$ est une sous-suite de $(s_n)$. On a donc $s_{1+n_k} \to \limsup_{n\to\infty}x_n$ et $s_{1+n_k} - \frac 1{k+1} \to \limsup_{n\to\infty}x_n$.
		Par le théorème du sandwich, on peut déterminer que $x_{n_k} \to \limsup_{n\to\infty}x_n$ également.

		De plus (en admettant l'existence de $(i_{n_k})$ car la preuve est similaire à celle de $(s_{n_k})$), on a~:
		\[\forall k \in \N, i_{n_k} \leq x_{n_k} \leq s_{n_k}.\]

		Et comme $(i_{n_k})$ et $(s_{n_k})$ sont des sous-suites d'une suite convergente, elles sont elles-mêmes convergentes. Donc par un résultat précédent,
		$\lim_{k\to\infty}i_{n_k} \leq \lim_{k\to\infty}x_{n_k} \leq \lim_{k\to\infty}s_{n_k}$, ou encore
		$\liminf_{n\to\infty}x_n \leq \lim_{k\to\infty}x_{n_k} \leq \limsup_{n\to\infty}x_n$. Ce qui prouve la deuxième partie du théorème. \end{proof}

		\begin{cor} On peut établir un corollaire de ce théorème~: une suite $(x_n) \subseteq \R$ converge si et seulement si~:
		\[\liminf_{n\to\infty}x_n = \limsup_{n\to\infty}x_n = L \in \R,\]
		en quel cas, $x_n \to L$. \end{cor}

		\begin{proof} La démonstration se fait par le théorème du sandwich dans un sens et par le théorème de Bolzano-Weierstrass dans l'autre sens. \end{proof}

		\begin{rmq} Nous avons donc maintenant trois résultats primordiaux sur la convergence des suites~:

		\begin{enumerate}
			\item Toute suite convergente est bornée~;
			\item toute suite monotone et bornée est convergente~;
			\item Toute suite bornée possède une sous-suite convergente.
		\end{enumerate}
		\end{rmq}

	\subsection{Le critère de Cauchy}

		Jusqu'ici, la notion de convergence fait intervenir \textit{directement} la notion de limite, ce qui sous-entend qu'il faut connaitre la limite avent de
		commencer la preuve de la convergence. Il existe donc une notion permettant de prouver la convergence sans expliciter la limite.

		\begin{déf} Une suite $(x_n)$ est dite \textit{de Cauchy} si $\forall \epsilon > 0, \exists N > 0 \ | \, m, n \geq N \Rightarrow |x_n - x_m| < \epsilon$.
		\end{déf}

		Cette définition n'implique pas uniquement qu'il faut que $|x_n - x_{n+1}|$ tende vers zéro mais bien qu'il faut que \textbf{pour tout} $m, n \geq N$,
		ces deux éléments tendent vers zéro.

		\begin{lem} Soit $(x_n) \subseteq \R$ une suite convergente. $(x_n)$ est de Cauchy. \end{lem}

		\begin{proof} Soit une suite $(x_n)$ convergente en $a \in \R$. Par définition,
		$\forall \epsilon > 0, \exists N > 0 | n \geq N \Rightarrow | x_n - a| < \epsilon$. Soit $\epsilon' = \frac \epsilon2$. Soit ce $N$ découlant de la
		définition de convergence. Prenons $m, n \geq N$. On a donc $|x_m - x_n| = |x_m - a + a - x_n| < |x_m - a | + |x_n - a| = \epsilon'$. \end{proof}

		\begin{thm}[Critère de Cauchy] Une suite $(x_n)$ converge si et seulement si elle est de Cauchy. \end{thm}

		\begin{proof} L'implication $\Rightarrow$ est donnée par le lemme précédent. Il faut encore prouver l'implication $\Leftarrow$.

		Commençons par montrer que $(x_n)$ est bornée et puis appliquons Bolzano-Weierstrass. Par hypothèse, $(x_n)$ est de Cauchy. Donc $\forall \epsilon > 0,
		\exists N > 0 \, | \, m, n \geq N \Rightarrow |x_n - x_m| < \epsilon$. Soit $\epsilon$, prenons ce $N$ qui découle de la définition de suite de Cauchy.
		On a donc $\forall n \geq N, |x_n| = |x_n + x_N - x_N| \leq |x_n - x_N| + |x_N| < \epsilon + |x_N|$. Construisons $K = \max(\{|x_0|, |x_1|, |x_2|, \ldots,
		\epsilon + |x_N|\})$. On a donc $|x_n| < K \forall n \geq N$, ce qui implique que $(x_n)$ est bornée.

		Appliquons maintenant le théorème de Bolzano-Weierstrass qui dit qu'il existe une sous-suite $(x_{n_k})_k$ qui converge en une valeur $a \in \R$ quand
		$k \to \infty$. Par la convergence, il existe $N_1$ tel que $\forall k \geq N_1, |x_{n_k} - a| < \frac \epsilon2$. Et par la définition de suite de Cauchy,
		il existe $N_2$ tel que $m, n \geq N_2 \Rightarrow |x_n - x_m| < \frac \epsilon2$. Soit $\delta$ tel que $\delta \geq N_1$ et $n_\delta \geq N_2$.
		On a alors $\forall n \geq N_2, |x_n - a| = |x_n - x_\delta + x_\delta - a| \leq |x_n - x_\delta| + |x_\delta - a| < \epsilon$.

		Il y a donc également convergence de la suite $(x_n)$ en $a \in \R$ quand $n \to \infty$. \end{proof}

\newpage
\section{Fonctions continues}
	\subsection{Limite d'une fonction en un point}
			\begin{déf} Soient $a, b \in \R$ tels que $a < b$. On note~:
				\[\begin{aligned}
					\interval {a}{b} &:= \{x \in \R \, | \, a \leq x \leq b\} \\
					]a, b] &:= \{y \in \R \, | \, a < x \leq b\} \\
					[a, b[ &:= \{x \in \R \, | \, a \leq x < b\} \\
					]a, b[ &:= \{x \in \R \, | \, a < x < b\} \\
					[a, \infty[ &:= \{x \in \R \, | \, a \leq x\} \\
					]a, \infty[ &:= \{x \in \R \, | \, a < x\} \\
					]-\infty, b] &:= \{x \in \R \, | \, x \leq b\} \\
					]-\infty, b[ &:= \{x \in \R \, | \, x < b\} \\
					]-\infty, \infty[ &:= \R.
				\end{aligned}\]

				Ces ensembles sont appelés \textit{intervalles}. \end{déf}

			\begin{déf} Soit $U \subseteq \R$. $U$ est dit ouvert $\iff \forall x \in U, \exists \epsilon > 0 \, | \, ]x-\epsilon, x+\epsilon[ \subseteq U$.
			$U$ est dit fermé $\iff \R \setminus U$ est ouvert. \end{déf}

			\begin{déf} Soient $A \subseteq \R$ et $a \in A$. $a$ est dit intérieur à $A$ s'$\exists \delta > 0 \, | \, ]a-\delta, a+\delta[ \subseteq A$.
			L'ensemble des points $a$ tels que $a$ est intérieur à $A$ est noté $\intr A$. \end{déf}

			\begin{rmq} $\forall A \subseteq \R$, $\intr A \subseteq A$. De plus, un ensemble peut être simultanément ouvert et fermé~: $]-\infty, \infty[$
			est ouvert car $\forall x \in R, \exists \epsilon > 0 \, | \, ]x-\epsilon, x+\epsilon[ \in \R$ et est fermé car
			$\R \setminus ]-\infty, \infty[ = \interval [open] 00$ est ouvert. \end{rmq}

			\begin{déf} Soit $a \in \R$. Un voisinage de $a$ est un ensemble contenant un intervalle de la forme $]c, d[$ avec $c < a < d$. \end{déf}

			\begin{déf} Soient $A \subseteq \R$ et $a \in \R$. $a$ est adhérent à $A$ si $\forall \delta > 0, ]a-\delta, a+\delta[ \cap A \neq \emptyset$.
			On note $\adh A$ l'ensemble des points adhérents à $A$. \end{déf}

			\begin{déf} Soient $f : U \subseteq \to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$. La limite de $f$ dans $B$ lorsque
			$x$ tend vers $a$ existe dans $\R$ et vaut $L \in \R$ si~:
			\[\forall \epsilon > 0, \exists \delta > 0 \, | \, \forall x \in (U \cap B), |x-a| < \delta \Rightarrow |f(x)-L| < \epsilon.\]

			Cela se note $f(x) \to L$ lorsque $x \to a$ dans $B$ ou~:
			\[\lim_{\underset{x \in B}{x \to a}} f(x) = L.\]
			\end{déf}

			\begin{rmq} Ici, $\epsilon$ permet de déterminer un voisinage autour de $L$, la limite, alors que $\delta$ permet de déterminer un voisinage autour
			de $a$. \end{rmq}

			\begin{thm}[Unicité de la limite] Soient $f : U \subseteq \R \to \R$, $B \subseteq \R$, $a \in \adh(B \cap U)$. Soient $L_1, L_2 \in \R$ tels que~:
			\[\begin{aligned}
				\lim_{\underset{x \in B}{x \to a}} f(x) &= L_1 \\
				\lim_{\underset{x \in B}{x \to a}} f(x) &= L_2.
			\end{aligned}\]

			Alors $L_1 = L_2$. \end{thm}

			\begin{proof} Soient $f : U \subseteq \R \to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$. Prenons $\epsilon = \frac {|L_1 - L_2|}3$.
			Par hypothèse, on sait qu'il existe $\delta_1 \, | \, \forall x \in U \cap B, |x-a| < \delta_1 \Rightarrow |f(x) - L_1| < \epsilon$ et
			$\delta_2 \, | \, \forall x \in U \cap B, |x-a| < \delta_2 \Rightarrow |f(x) - L_2| < \epsilon$.

			Soit $x_0 \in U \cap B$ tel que $|x_0 - a| < \min(\{L_1, L_2\})$. On sait alors que
			$f(x_0) \in ]L_1-\epsilon, L_1+\epsilon[$ et $f(x_0) \in ]L_2-\epsilon, L_2+\epsilon[$.
			Or, par choix de $\epsilon$, ces deux ensembles sont d'intersection vide. Il y a donc contradiction et $L_1 = L_2$. \end{proof}

			\begin{déf} La limite de $f : U \subseteq \R \to \R$ en $x \to a$ existe et vaut $L \in \R$ si la limite de $f$ en $x \to a$ dans $B = \R$
			existe et vaut $L$. On note cela~:
			\[\lim_{x \to a} f(x) = L \in \R.\]
			\end{déf}

		\subsubsection{Limites pointées, gauches et droites}

			Voyons dans quels contextes il est intéressant de manipuler le $B \subseteq \R$.

			\begin{déf} Soit $a \in \R$. Un voisinage de $a$ est pointé s'il contient un intervalle $]c, d[ \setminus \{a\}$. Un voisinage de $a$ est
			\textit{de droite} s'il contient un intervalle $[a, d[$ et peut être pointé si l'intervalle est sous la forme $]a, d[$. De manière similaire,
			un voisinage est \textit{de gauche} si l'intervalle est sous la forme $]c, a]$ et peut également être pointé. \end{déf}

			\begin{déf}[Définition des limites à gauche, à droite et pointées] Soit $f : U \to \R$ où $U$ est un intervalle contenant un voisinage pointé de $a$.
			La limite à gauche, respectivement à droite, respectivement pointée de $f$ en $x \to a$ existe et vaut $L \in \R$ si la limite de $f$ dans
			$B = ]-\infty, a[$, respectivement $]a, \infty[$, respectivement $\R \setminus \{a\}$ en $x \to a$ existe et vaut $L$.

			Cela se note respectivement~:
			\[\lim_{\underset{<}{x \to a}}f(x) = L, \;\;\lim_{\underset{>}{x \to a}} f(x) = L, \;\; \lim_{\underset{\neq}{x \to a}} f(x) = L.\]
			\end{déf}

			\begin{rmq} Pour pouvoir parler de limites de $f$ soit à gauche, soit à droite, soit pointée, il faut impérativement que $f$ soit définie dans les
			alentours de $a$. Plus précisément, il faut $a \in \adh(A \cap B)$ avec $B$ défini selon le cas. \end{rmq}

			\begin{lem} Soient $a \in \R$ et $f : U \to \R$ tels que $U$ définit un voisinage pointé de $a$. Alors $f$ possède une limite pointée en $a$ si et
			seulement si $f$ possède une limite à gauche en $a$ et une limite à droite en $a$ telles que $\lim_{x\to a^+}f(x) = \lim_{x \to a^-}f(x) = L \in \R$.
			Dans ce cas, on a $\lim_{\underset{\neq}{x \to a}} f(x) = L$. \end{lem}

			\begin{prp} Soient $f : U\to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$. La limite de $f$ dans $B$ en $x \to a$ existe et vaut
			$L \in \R$ si et seulement si $\forall A \subseteq B$, la limite de $f$ dans $A$ en $x \to a$ existe et vaut $L$. \end{prp}

			\begin{proof} Pour la condition suffisante ($\Leftarrow$), on sait que pour \textbf{tout} $A \subseteq B$, la limite existe. En prenant $A = B$,
			on sait que la limite existe également dans $B$ et vaut $L \in \R$.

			Pour la condition nécessaire ($\Rightarrow$), observons que
			$\forall \epsilon > 0, \exists \delta > 0 \, | \, x \in (B \cap U) \land (|x-a| < \delta) \Rightarrow (|f(x)-L| < \epsilon)$.
			On sait alors que pour $\epsilon \in \R_0^+$ fixé et pour $A \subseteq B$, il existe $\delta \in \R_0^+$ tel que $x \in A \Rightarrow |f(x) - L| < \epsilon$.
			\end{proof}

			\begin{prp} Soient $f : U \to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$. La limite de $f$ dans $B$ en $x \to a$ existe et vaut
			$L \in \R$ si et seulement si pour toute suite $(x_n)_{n \in \N} \subseteq B$ telle que $x_n \to a$, la suite
			$(f(x_n))_{n \in \mathbb N} \subseteq \R$ converge en $L$. \end{prp}

			\begin{proof} Soit $\epsilon > 0$. On sait par hypothèse qu'$\exists \delta > 0$ tel que
			$x \in (B\cap U) \land |x-a| < \delta \Rightarrow |f(x)-L| < \epsilon$. Soit $(x_n) \subseteq B$ telle que $x_n \to a$. On sait donc
			qu'$\exists N > 0$ tel que $n > N \Rightarrow |x_n - a| < \epsilon$. Si ceci est vrai pour tout $\epsilon$, ça l'est plus précisément pour $\delta$
			déterminé par l'hypothèse. On a donc $\forall \epsilon > 0, \exists N > 0 \, | \, n > N \Rightarrow |f(x_n) - L| < \epsilon$ ou encore
			$f(x_n) \to L$ pour $n \to \infty$.
			
			Montrons maintenant que si toutes les suites de $B$ convergentes en $a$ implique $f(x_n) \to L$, alors la limite de $f$ en $a$ existe et vaut $L$.
			Fonctionnons par l'absurde~: soit $(x_n) \subseteq B$ une suite dans $B$ telle que $x_n \to a$ quand $n \to \infty$ et $(f(x_n)) \to L \in \R$ mais
			alors que $f(x) \not \to L$ pour $x \to a$. On sait alors qu'$\exists \epsilon_0 > 0 \, | \, \forall \delta > 0$ il n'y a pas de convergence de
			$f(x)$ en $L$. Soit $\epsilon_0$. Prenons $\delta = \frac 1n$. On en déduit que $\forall n \geq 1$, il existe $x_n \in U$ tel que $|x_n-a| < \frac 1n$ et
			$|f(x_n) - L| \geq \epsilon_0$ donc tel que $x_n \to a$ mais $f(x_n) \not \to L$ lorsque $n \to \infty$. Or par hypothèse, on sait que
			$(f(x_n)) \to L$. Ce qui est une contradiction avec $|f(x_n) - L| \geq \epsilon_0$. Donc $f(x) \to L$ pour $x \to a$. \end{proof}

		\subsubsection{Règles de calcul de limite de fonctions}

			\begin{thm} Soient $f, g : U \subseteq \R \to \R$ et $a \in \R$ tels que $f$ et $g$ sont définies dans le voisinage de $a$. Soient $L_f$ et $L_g$ tels
			que $\lim_{x \to a}f(x) = L_f$ et $\lim_{x \to a}g(x) = L_g$. alors~:
			\[\begin{aligned}
				\lim_{x \to a}(f+g)(x) &= L_f + L_g \\
				\lim_{x \to a}(fg)(x) &= L_fL_g.
			\end{aligned}\]

			Si $L_g \neq 0$, la fonction $\frac fg(x)$ existe dans le voisinage de $a$ telle que~:
			\[\begin{aligned}\lim_{x \to a}\frac fg(x) = \frac {L_f}{L_g}.\end{aligned}\]
			\end{thm}

			\begin{proof} Soient $f, g : U \to \R$ convergents respectivement en $L_f$ et $L_g$ quand $x \to a$.

			\subparagraph{Addition} Montrons que la limite de la somme vaut la somme des limites.
			Soit $\epsilon > 0$. On sait par la définition des limites de $f$ et $g$ qu'il existe $\delta_1$ tel que $\forall x \in U, |x-a| < \delta_1
			\Rightarrow |f(x) - L_f| < \frac \epsilon2$
			et $\delta_2$ tel que $\forall x \in U, |x-a| < \delta_2 \Rightarrow |g(x)-L_g| < \frac \epsilon2$. Prenons alors $\delta = \min(\{\delta_1, \delta_2\})$.
			On sait dès lors que $\forall x \in U, |f(x) + g(x) - L_f - L_g| = |f(x)-L_f + g(x)-L_g| \leq |f(x)-L_f| + |g(x)-L_g| < \frac \epsilon2 + \frac \epsilon2 =
			\epsilon$.

			\subparagraph{Multiplication} Montrons que le produit des limite vaut le produit des limites. Montrons tout d'abord qu'il existe un voisinage autour de $a$
			tel que $f(x)$ est bornée.

			Soit $\epsilon = 1$. On sait alors qu'il existe $\delta_1$ tel que $\forall x \in U, |x-a| < \delta_1 \Rightarrow |f(x) - L_f| < 1$. Ce $\delta_1$
			définit un voisinage de $a$ où $|f(x)| < |L_f| + 1 \forall x$.

			Montrons ensuite que $\lim_{x \to a}(fg)(x) = L_fL_g$.

			Soit $\epsilon \in \R_0^+$. On sait qu'il existe $\delta_2$ tel que $\forall x \in U, |x-a| < \delta_2 \Rightarrow |f(x)-L_f| < \frac {\epsilon}{2(|L_g|+1)}$
			et $\delta_3$ tel que $\forall x \in U, |x-a| < \delta_3 \Rightarrow |g(x)-L_g| < \frac {\epsilon}{2|L_f|}$. Prenons alors
			$\delta = \min(\{\delta_i | i \in [3]\})$. On a alors $\forall x \in U$ tel que $|x-a| < \delta$~:
			\[\begin{aligned}
				|f(x)g(x) - L_fL_g| &= |f(x)(g(x) - L_g) + L_g(f(x) - L_f)| \leq |f(x)||g(x) - L_g| + |L_g||f(x) - L_f| \\
									&< (|L_f| + 1)\frac {\epsilon}{2(|L_f|+1)} + |L_g|\frac {\epsilon}{2|L_g|} = 2\frac {\epsilon}{2} = \epsilon.
			\end{aligned}\]

			\subparagraph{Quotient} Montrons que la limite du quotient vaut le quotient des limites. Commençons par montrer qu'il existe un voisinage de $a$ où
			$g(x) \neq 0$.

			Soit $\epsilon = \frac {|L_g|}2$. On sait alors qu'il existe $\delta_V$ tel que $\forall x \in U, |x-a| < \delta_V \Rightarrow |g(x)-L_g| < \frac {|L_g|}2$.
			Ou encore pour ce même $\delta_V$, $|g(x)| > \frac {|L_g|}2$. Donc $|g(x)|$ est strictement positif. Définissons alors $V := U \cap ]a-\delta_V, a+\delta_V[$,
			un voisinage de $a$ sur lequel la fonction $\frac fg(x)$ est bien définie.
			
			Prouvons maintenant que $\frac 1g(x) \to \frac 1{L_g}$ quand $x \to a$, et le résultat découlera de la proposition précédente.
			
			Soit $\epsilon > 0$. On sait qu'il existe $\delta > 0$ tel que $\forall x \in U, |x-a| < \delta \Rightarrow |g(x) - L_g| < \frac {|L_g|^2\epsilon}2$.
			On sait dès lors~:
			\[\left|\frac 1{g(x)} - \frac 1{L_g}\right| = \frac {|g(x)-L_g|}{|g(x)||L_g|} < \frac {|g(x)-L_g|}{\frac {|L_g|}{2}|L_g|}
			  = \frac {2|g(x)-L_g|}{|L_g|^2} < \frac {2(\frac {|L_g|^2\epsilon}{2})}{|L_g|^2} = \epsilon.\]
			
			\end{proof}
			  
			\begin{thm} le théorème du sandwich des suites a un homologue pour les fonctions~: soient $f, g, h : \R \to \R$ trois fonctions telles que
			$\forall x \in \R, f(x) \leq g(x) \leq h(x)$ avec $\lim_{x \to a}f(x) = \lim_{x \to a}h(x) = L \in \R$. Alors $\lim_{x \to a}g(x) = L$.
			\end{thm}
			
			\begin{proof} Définissons les suites $(f(x_n))_n, (g(x_n))_n$ et $(h(x_n))_n$. On sait que $\forall k \in {\R}^{\R}, \lim_{x \to a}k(x)$ existe
			et vaut $L_k$ si et seulement si pour toute suite $(x_n) \subseteq \R$ convergente en $a \in \R$, on a $k(x_n) \to L_k$ quand $n \to \infty$.
			
			On sait dès lors que $f(x_n) \to L$ et $h(x_n) \to L$. Par le théorème du sandwich, on sait que $g(x_n) \to L$ également. \end{proof}
			
			\begin{thm}[Conservation des inégalités] Soient $f, g : U \subseteq \R \to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$.
			Si $\exists \delta > 0$ tel que $\forall x \in ]a-\delta, a+\delta[ \cap U \cap B, f(x) \leq g(x)$ et $\lim_{x \to a}f(x)$ et $\lim_{x \to a}g(x)$
			existent dans $\R$, alors $\lim_{x \to a}f(x) \leq \lim{x \to a}g(x)$. \end{thm}
			
		\subsubsection{Limites infinies et limites à l'infini}
			
			\begin{déf} Soient $f, g : U \subseteq \R \to \R$, $B \subseteq \R$, $a \in \adh(U \cap B)$. La limite de $f$ dans $B$ en $a$ existe et vaut
			$+\infty$ si $\forall K > 0, \exists \delta > 0 \, | \, \forall x \in (U \cap B), |x-a| < \delta \Rightarrow f(x) > K$. Cela se note~:
			\[\lim_{x \to a}f(x) = \infty.\]
			
			De manière similaire, la limite de $f$ dans $B$ en $a$ vaut $-\infty$ si $\forall K > 0, \exists \delta > 0 \, | \, \forall x \in (U \cap B), |x-a| <
			\delta \Rightarrow f(x) < -K$.
			Ce la se note~:
			\[\lim_{x \to a}f(x) = -\infty.\]
			\end{déf}

			\begin{rmq} les cas particuliers où $B = \R, B = ]-\infty, a], B = [a, +\infty[, B = \R \setminus \{a\}$ donnent les définitions de limite
			standard infinie, limite à gauche, limite à droite et limite pointée infinies. \end{rmq}

			\begin{déf} Soit $f : U \subseteq \R \to \R$ où $U$ n'est pas majoré. La limite de $f$ en « l'infini » vaut $L \in \R$ si
			$\forall \epsilon > 0, \exists M > 0 \, | \, \forall x \in U, x > M \Rightarrow |f(x) - L| < \epsilon$. Cela se note~:
			\[\lim_{x \to \infty}f(x) = L.\]

			De manière similaire, si $U$ n'est pas minoré, la limite de $f$ en « moins l'infini » vaut $L \in \R$ si
			$\forall \epsilon > 0, \exists M > 0 \, | \, \forall x \in U, x < -M \Rightarrow |f(x) - L| < \epsilon$. Cela se note~:
			\[\lim_{x \to -\infty}f(x) = L.\]
			\end{déf}

	\subsection{Définition de la continuité}
		\begin{déf} Soient $a \in \R$, $f : I \subseteq \R \to \R$ une fonction définie sur un voisinage de $a$. La fonction $f$ est continue en $a$ si 

		\[\lim_{x \to a}f(x) = f(a).\]

		Ou encore si

		\[\forall \epsilon > 0, \exists \delta > 0 \tq \abs{x-a} < \delta \Rightarrow \abs{f(x) - f(a)} < \epsilon.\]

		Cette définition comporte trois points importants~: il faut que la limite de $f$ en $a$ existe, que $f$ soit définie en $a$ et que la fonction prenne la valeur
		de sa limite en $a$.
		\end{déf}

		\begin{prp} Venant de la définition de limite de fonction sur base des suites, il est possible d'exprimer la continuité d'une fonction en terme
		de suites. Soient $a \in \R$, $f : I \subseteq \R \to \R$ une fonction définie sur un voisinage de $a$. Alors la fonction $f$ est continue en
		$a$ si et seulement si pour toute suite $(x_n)$ convergente en $a$, la suite $(f(x_n))$ converge en $f(a)$. \end{prp}

		\begin{prp} Les règles de calcul concernant la continuité sont assez évidents. Soient $a \ni \R$, $f, g : I \subseteq \R \to \R$ deux fonctions
		définies sur un voisinage de$a$ et continues en $a$. Alors $f+g$ est également continue en $a$, $fg$ est également continue en $a$ et si
		$g(a) \neq 0$, alors $\frac fg$ est continue en $a$. De plus soit $h : V \subseteq \R \to \R$ une fonction définie sur un voisinage de $f(a)$
		et contenant $f(I)$, l'image de $f$. Si $h$ est continue en $f(a)$, alors $h \circ f : I \to \R$ est continue en $a$. \end{prp}

		\begin{proof} Les premières propositions viennent directement des règles sur les limites. Pour la composée, prenons $(x_n)$ une suite convergent en $a$.
		Par continuité de $f$, on sait que $f(x_n)$ converge en $f(a)$ et par continuité de $h$, on sait que $h(f(x_n))$ converge en $h(f(a))$. Dès lors, on sait que
		$h \circ f$ est continue en $a$. \end{proof}

		\begin{déf} Tout comme pour les limites, on parle de continuité à gauche (respectivement à droite) si la limite de $x$ tendant vers $a$ à gauche
		(respectivement à droite) vaut $f(a)$. \end{déf}

		\begin{déf} Une fonction $\fabr f$ est \textit{continue} si elle est continue en tout point $c \in ]a, b[$, continue à droite en
		$a$ et à gauche en $b$. \end{déf}
	
	\subsection{Théorème des bornes atteintes}

		\begin{thm} Soit $\fabr f$ une fonction continue. Alors $f$ est bornée \textbf{et} $f$ atteint ses bornes. Dès lors~:
		\[\exists m, M \in \ab \tq \forall x \in \ab : f(m) \leq f(x) \leq f(M).\]
		\end{thm}

		\begin{proof} Montrons d'abord que $f$ est bornée et puis montrons qu'elle atteint ses bornes.

		Supposons par l'absurde que $f$ n'est pas majorée. Dès lors, $\forall n \in \N : \exists x_n \in \ab \tq f(x_n) > n$. La suite $(x_n)$ est bornée car
		$\forall n : x_n \in \ab$. Par le théorème de Bolzano-Weierstrass, on sait qu'il existe $(x_{n_k})_k$ une sous-suite de $(x_n)$ convergeant en
		$x \in \R$. Par la continuité de $f$ en $x$ (hypothèse), la suite $(f(x_{n_k}))_k$ converge en $f(x)$. Or par construction de $(x_n)$, la suite $(f(x_{n_k}))_k$
		diverge vers $+\infty$. Il y a donc contradiction. Idem pour le minorant.

		Montrons maintenant que $f$ atteint ses bornes. Par l'absurde, supposons que $f$ n'atteint pas ses bornes. Prenons
		$M \coloneqq \sup\{f(x) \tq x \in \ab\}$ qui existe et est fini. Supposons que $\nexists c \in \ab \tq f(c) = M$. Posons alors~:
		\[g(x) \coloneqq \frac 1{M - f(x)}.\]

		$g$ est continue et donc bornée par $K \in \R$. Dès lors,
		\[\forall x \in \ab : \frac 1{M-f(x)} \leq K.\]

		Ce qui implique (du fait que $f(x) < A$ et $K > 0$) que $M-f(x) \geq \frac 1K$ ou encore $f(x) \leq M - \frac 1K$. Ce qui contredit le fait que $M$ est le
		majorant de $\{f(x) \tq x \in \ab\}$. Idem pour la borne inférieure. \end{proof}
	
	\subsection{Théorème de la valeur intermédiaire}
		
		\begin{thm} Soit $\fabr f$ une fonction continue. Soit $\gamma$ strictement entre $f(a)$ et $f(b)$.
		Alors $\exists c \in ]a, b[ \tq f(x) = \gamma$. \end{thm}

		\begin{proof} Supposons $f(a) < \gamma < f(b)$, le cas $f(b) < \gamma f(a)$ se montre de la même manière.

		Soit $S \coloneqq \{x \in \ab \tq f(x) < \gamma\} \subseteq \dom f$. Par définition de $S$, on sait que $a \in S$ et que $S$ est majoré par $b$.
		Il existe une suite $(x_n)$ dans $S$ telle que $x_n \to \sup S$. Donc $\forall n \in \N : x_n \in S$ donc $f(x_n) < \gamma$. De plus,
		$\lim_{n\to+\infty}f(x_n) = f(\sup S) \leq \gamma$. Notons $c \coloneqq \sup S$. On sait donc que $f(c) \leq \gamma$.

		Supposons par l'absurde $f(c) < \gamma$. Soit $\epsilon \coloneqq \gamma - f(x) > 0$. Par la continuité de $f$ en $c$, on sait
		qu'$\exists \delta > 0 \tq \abs{x-c} < \delta \Rightarrow \abs{f(x)-f(c)} < \epsilon = \gamma-f(c)$. On sait que $a < c < b$.

		Prenons $x \coloneqq c + \frac {\min\{\delta, b-c\}}2$. Alors $x \in \ab$, $\abs{x -c} < \delta$ et $x > c$. Donc $\abs{f(x) - f(c)} < \epsilon$,
		ou encore~:
		\[f(c) - \gamma < f(x) - f(c) < \gamma - f(c).\]

		On sait donc $f(x)-f(c) < \gamma-f(c)$ ce qui implique $f(x) < \gamma$. Ce qui veut dire que $x \in S$. Or, par construction de $x$, $x > c = \sup S$.
		Il y a une contradiction. Donc l'hypothèse $f(c) < \gamma$ est fausse. Dès lors, $f(c) = \gamma$. \end{proof}

		\paragraph{Exemple} On peut, par ce théorème, montrer que tout polynôme de degré $n$ impair admet au moins une racine réelle~: soit
		$P(x) \coloneqq \sum_{i=1}^{n}a_ix^i$. Prenons $Q(x) \coloneqq \frac {P(x)}{a_n}$. Dès lors, les limites en les infinis de $Q$ sont ces mêmes infinis~:
		$\lim_{x\to\pm\infty}Q(x) = \pm\infty$. On sait dès lors qu'il existe deux réels $a$ et $b$ tels que $Q(a) < 0$ et $Q(b) > 0$. Par le théorème, on sait qu'il
		existe un $c \in ]a, b[$ tel que $Q(c) = 0$. Si $Q(c) = 0$, alors $P(c) = 0$ par définition de $Q$.
	
	\subsection{Théorème de l'intervalle et de la réciproque}
		
		\begin{prp} Soit $I \subseteq \R$. Alors $I$ est un intervalle si et seulement si $\forall \alpha, \beta \in I : \interval \alpha\beta \subset I$.
		\end{prp}

		\begin{thm}[de l'intervalle] Si $f : I \to \R$ est continue et $I$ est un intervalle, alors $f(I)$ est un intervalle. De plus, si $I$ est fermé borné,
		$f(I)$ l'est aussi. \end{thm}

		\begin{déf} Soit $f : U \subseteq \R \to \R$ une fonction définie sur un ensemble $U$ $f$ est strictement croissante (respectivement strictement
		décroissante) si pour tout $x, y \in U : x < y \Rightarrow f(x) < f(y)$ (respectivement $x < y \Rightarrow f(x) > f(y)$). $f$ est simplement croissante
		(respectivement simplement décroissante) si pour tout $x, y \in U : x < y \Rightarrow f(x) \leq f(y)$ (respectivement $x < y \Rightarrow f(x) \geq f(y)$).
		De plus, $f$ est strictement monotone si elle est strictement croissante ou strictement décroissante et est simplement monotone si elle est simplement
		croissante ou simplement décroissante. \end{déf}

		\begin{thm}[de la réciproque] Soit $f : I \subseteq \R \to \R$ une fonction continue et strictement monotone définie sur un intervalle $I$. Alors
		$f$ est injective et sa réciproque $f^{-1} : f(I) \to I$ est aussi continue. \end{thm}

		\begin{proof} Soient $x, y \in I$ tels que $x \neq y$. Soit $f^{-1} : f(I) \to \R$ la réciproque de $f$. Supposons $f$ strictement croissante
		(le cas strictement décroissant est similaire). Soit $p \in \intr f(I)$ (on sait que $f(I)$ est un intervalle). Montrons que $f^{-1}$ est continue en
		$p$. Prenons $q \in I$ tel que $f(q) = p$. On sait que $q$ n'est pas une extrémité de $I$ car les extrémités de $I$ correspondent aux extrémités de
		$f(I)$ (par la monotonie stricte de $f$) et que $p$ est dans l'intérieur de $f(I)$. Prenons $\epsilon > 0$ tel que $]q-\epsilon, q+\epsilon[ \subset I$.
		Posons $c \coloneqq f(q-\epsilon)$ et $d \coloneqq f(q+\epsilon)$. On sait $c < p < d$ par la monotonie stricte de $f$. Soit
		$\delta \coloneqq \min(pc, d-p)$. Dès lors, $\delta > 0$ et $\interval {p-\delta}{p+\delta} \subset f(\interval {q-\epsilon}{q+\epsilon})$, ou encore
		$f^{-1}(\interval {p-\delta}{p+\delta}) \subset \interval {q-\epsilon}{q+\epsilon}$. Nous avons donc $\delta$ tel que
		$\abs{x-p} < \delta \Rightarrow \abs{f^{-1}(x)-f^{-1}(p)} < \epsilon$. Ce qui montre que $f^{-1}$, la réciproque de $f$ est continue en $p$. La continuité
		à droite et à gauche de $f^{-1}$ aux extrémités de $f(I)$ se montrent de la même manière. \end{proof}
	
	\subsection{Continuité uniforme}
		La notion de continuité de $f$ en $a$ est une notion locale. Il en existe une version globale~: la continuité uniforme.

		\begin{déf} Soit $f : U \subseteq \R \to \R$ une fonction définie sur un ensemble réel $U$. $f$ est uniformément continue sur $U$ si~:
		\[\forall \epsilon > 0 : \exists \delta > 0 \tq \forall x, y \in U : \abs{x-y} < \delta \Rightarrow \abs{f(x)-f(y)} < \epsilon.\]

		Cette définition englobe la continuité simple mais requiert en plus que la valeur de $\delta$ trouvée ne dépende pas du point $a$ choisi. Comme
		cette définition englobe l'autre, une fonction uniformément continue est également continue. \end{déf}

		\begin{déf} Deux suites $(x_n), (y_n) \subset \R$ sont équivalentes si $\abs{x_n-y_n} \to 0$ pour $n \to +\infty$. Donc si l'une de des suites
		converge en $L$, l'autre converge également en $L$. \end{déf}

		\begin{prp} Soit $f : U \subseteq \R \to \R$. La fonction $f$ est uniformément continue si et seulement si $\forall (x_n), (y_n) \subset U$
		équivalentes, les suites $(f(x_n)), (f(y_n))$ sont équivalentes. \end{prp}

		\begin{proof} Soit $f : U \to \R$ uniformément continue. Soient $(x_n), (y_n) \subset \R$ deux suites équivalentes. Soit $\epsilon > 0$. On sait
		qu'il existe $\delta$ tel que $\forall x, y \in U : \abs{x-y} < \delta \Rightarrow \abs{f(x)-f(y)} < \epsilon$ par l'uniforme continuité de $f$. De
		plus, par l'équivalence des suites, on sait qu'il existe $N \in \N$ tel que $\abs{x_n-y_n} < \delta$. On a donc
		$\abs{x_n-y_n} < \delta \Rightarrow \abs{f(x_n)-f(y_n)} < \epsilon$. Les deux suites sont bien équivalentes. Montrons maintenant que l'équivalence de
		$(f(x_n))$ et $(f(y_n))$ implique l'uniforme continuité de $f$.
		
		Supposons par l'absurde qu'il existe $\epsilon_0 > 0$ tel qu'il n'existe pas de $\delta_0 > 0$. En particulier, $\delta_0 = \frac 1n$ ne convient pas,
		quel que soit $n > 0$. Prenons donc $n > 0$. Alors il existe $x_n, y_n \in U$ tels que $\abs{x_n-y_n} < \frac 1n$ et $\abs{f(x_n)-f(y_n)} \geq \epsilon_0$.
		On a donc les suites $(x_n), (y_n)$ équivalentes mais pas les suites $(f(x_n))$ et $(f(y_n))$ ce qui est une contradiction avec l'hypothèse. \end{proof}

		\begin{thm} Si $\fabr f$ est une fonction continue, alors $f$ est uniformément continue. \end{thm}

		\begin{proof}[version 1] Reprenons le format de démonstration précédente~: supposons par l'absurde que $f$ n'est pas continue. Dès lors, on sait qu'il
		existe $\epsilon_0 > 0$ tel qu'il n'existe pas de $\delta$ satisfaisant la définition. Plus précisément, $\forall n > 0 : \exists x_n, y_n \in \ab$
		tels que $\abs{x_n-y_n} < \frac 1n$ et $\abs{f(x_n) - f(y_n)} \geq \epsilon_0$. Maintenant, prenons deux sous-suites $(x_{n_k})$ et $(y_{n_k})$. Puisque
		$(x_n)$ et $(y_n)$ sont des suites bornées, les sous-suites $(x_{n_k})$ et $(y_{n_k})$ sont convergentes par le théorème de Bolzano-Weierstrass. Comme
		$\abs{x_n-y_n} \to 0$, on sait que les sous-suites convergentes convergent vers la même valeur~: $\lim_{k\to+\infty}x_{n_k} = \lim_{k\to+\infty}y_{n_k}$.
		Posons $p \coloneqq \lim_{k\to+\infty}x_{n_k}$. Comme les suites considérées sont dans $\ab$, on sait que $p \in \ab$
		également. Or par la continuité de $f$, on sait que $f(x_{n_k}) \to f(p)$ et $f(y_{n_k}) \to f(p)$. Dès lors, $\abs{f(x_{n_k})-f(y_{n_k})} \to 0$.
		Or $\abs{f(x_n) - f(y_n)} \geq \epsilon_0 \forall n$. Il y a donc contradiction. \end{proof}

		\begin{lem} Soit $(x_n)$ une suite bornée. Si $\forall (x_{n_k})$ sous-suite de $(x_n) : x_{n_k} \to L$, alors $x_n \to L$. \end{lem}

		\begin{proof} Par Bolzano-Weierstrass, il existe une sous-suite $(x_{n_k})$ telle que $x_{n_k} \to \liminf (x_n) = L$ et une
		sous-suite $(x_{m_k})$ telle que $x_{m_k} \to \limsup (x_n) = L$. Si $\liminf(x_n) = \limsup (x_n) = L$, alors $x_n \to L$. \end{proof}

		\begin{proof}[version 2] Montrons que si $x_n - y_n \to 0$, alors $f(x_n) - f(y_n) \to 0$. Comme $f(x_n)-f(y_n)$ est bornée, prenons
		$f(x_{n_k})-f(y_{n_k})$, une sous-suite convergente et nommons la limite $L$. Par Bolzano-Weierstrass, on sait qu'il existe deux sous-suites
		$(x_{n_{k_l}})$ et $(y_{n_{k_l}})$ qui convergent (respectivement en $u$ et $v$). Alors on peut dire que $f(x_{n_{k_l}}) - f(x_{n_{k_l}}) \to f(u) - f(v) = L$.
		Or, comme $x_{n_{k_l}} - y_{n_{k_l}} \to 0$, on sait que $u = v$. Dès lors, on peut dire que $L = 0$. \end{proof}

		\begin{thm} Soit $f : ]a, b[ \to \R$ une fonction uniformément continue. Alors il existe un \textit{prolongement continu} $\fabr {\bar f}$ de $f$.
		\end{thm}

		\begin{proof} Prenons la fonction 
		\[\fabr {\bar f} : x \mapsto \left\{\begin{aligned}&\lim_{x \to a^+}f(x) &\text{ si } x = a\\&f(x) &\text{ si } x \in ]a, b[\\&\lim_{x \to b^-}f(x) &\text{ si } x = b\end{aligned}\right.\]

		Dès lors, la fonction $\bar f$ est uniformément continue. \end{proof}

		\begin{rmq} Soit $f : A \to \R$ une fonction uniformément continue. $\forall E \subset A$, $f_{|_E}$ est également uniformément continue. \end{rmq}
	
	\subsection{Fonctions à valeur vectorielle}

		\begin{déf} Soient $x = (x_1, \ldots, x_n), y = (y_1, \ldots, y_n) \in \R^n$. On définit la distance entre $x$ et $y$ par~:
		\[\norm{x-y} = \sum_{i=1}^n(x_i-y_i)^2.\]

		De plus, la \textit{norme} d'un vecteur $x$ est la distance entre lui-même et l'origine. Donc $\norm x = \sum_{i=1}^nx_i^2$. \end{déf}

		\begin{déf} Soient $x, y \in \R^n$. On définit le produit scalaire de $x$ et $y$ par~:
		\[\scpr xy = \sum_{i=1}^nx_iy_i.\]
		\end{déf}

		\begin{rmq} $\sqrt {\norm x} = \scpr xx$. \end{rmq}

		\begin{prp} Soient $x, y, z \in \R^n, \lambda, \mu \in \R$. Alors~:

		\begin{enumerate}
			\item $\scpr {\lambda x + \mu y}z = \lambda \scpr xz + \mu \scpr yz$~;
			\item $\scpr xy = \scpr yx$~;
			\item $\scpr xx \geq 0$ et $\scpr xx = 0 \Rightarrow x = 0$.
		\end{enumerate}
		\end{prp}

		\begin{thm}[Inégalité de Cauchy-Schwartz] Soient $x, y \in \R^n$. Alors $\abs {\scpr xy} \leq \norm x\norm y$ avec égalité si et seulement si $x$ et
		$y$ sont colinéaires. \end{thm}

		\begin{thm}[Inégalité triangulaire] Soient $x, y \in \R^n$. Alors~:
		\[\norm {x+y} \leq \norm x + \norm y.\]
		\end{thm}

		\begin{proof}
		\[\norm {x+y}^2 = \scpr {x+y}{x+y} = \scpr xx + 2\scpr xy + \scpr yy = \norm x^2 + 2\scpr xy + \norm y^2 \leq \norm x^2 + 2\norm x\norm y + \norm y^2 =
		(\norm x + \norm y)^2.\]
		\end{proof}

		\begin{déf}\label{bouleOuverte} Soient $r > 0$ et $a \in \R^n$. On définit la boule ouverte en $a$ de rayon $r$ par~:
		\[B(a, r) \coloneqq \{x \in \R^n : \norm {x-a} < r\}.\]
		\end{déf}

		\begin{déf} Soient $A \subset \R^n$ et $a \in \R^n$. $a \in \adh A$ si et seulement si $\forall \epsilon > 0 : B(a, \epsilon) \cap A \neq \emptyset$.
		\end{déf}

		\begin{déf} Soient $f : A \subset \R^m \to \R^n$, $a \in \adh A$ et $L \in \R^n$. On définit la limite de $f$ pour $x \to a$ par~:
		\[\forall \epsilon > 0 : \exists \delta > 0 \tq \forall x \in A : \norm {x-a} < \delta \Rightarrow \norm {f(x)-L} < \epsilon.\]
		\end{déf}

		\begin{rmq} \[\lim_{x \to a}f(x) = l \in \R^n \iff \forall \epsilon > 0 : \exists \delta > 0 \tq f(A \cap B(a, \delta)) \subseteq B(L, \epsilon).\]
		\end{rmq}

		\begin{déf} Soient $f : A \subset \R^m \to \R^n$, $a \in A$. $f$ est continue en $a$ si $\lim_{x \to a}f(x) = f(a)$. \end{déf}

		\begin{déf} Soit $f : A \subset \R^m \to \R^n$. Les composantes de $f$ sont les fonctions $f_i : A \subset \R^m \to \R^n$ telles que
		$f(x) = (f_1(x), f_2(x), \ldots, f_n(x))$. \end{déf}

		\begin{lem} Soit $f : A \subset \R \to \R^n$ une fonction définie sur un voisinage de $a \in \R$. La fonction $f$ possède une limite en $x \to a$ si
		et seulement si toutes les composantes $f_i : A \to \R^n$ possèdent une limite en $x \to a$. Alors~:
		\[\lim_{x \to a}f(x) = \left(\lim_{x \to a}f_1(x), \lim_{x \to a}f_2(x), \ldots, \lim_{x \to a}f_n(x)\right).\]
		\end{lem}

		\begin{proof} Montrons d'abord que $f(x) \to L \Rightarrow \forall i : f_i(x) \to L_i$. Montrons ensuite l'autre sens de l'implication.

		Soit $f : A \subset \R \to \R^n$ une fonction convergent en $L = (L_1, \ldots, L_n) \in \R^n$ en $x \to a$. Soit $\epsilon > 0$. On sait qu'il existe
		$\delta > 0$ tel que pour tout $x \in A$, $\abs{x-a} < \delta \Rightarrow \norm{f(x)-L} < \epsilon$. Dès lors~:
		\[\abs{f_i(x) - L_i} = \sqrt {(f_i(x) - L_i)^2} \leq \norm {f(x)-L} < \epsilon.\]

		Pour montrer l'implication dans l'autre sens, prenons $\epsilon > 0$. $\forall i \in \{1, \ldots, n\}~:
		\exists \delta_i \tq \abs{x-a} < \delta_i \Rightarrow \abs{f_i(x)-L_i} < \frac \epsilon{\sqrt n}$. On définit $\delta \coloneqq \min\{\{\delta_i\}_i\}$.
		Dès lors, si $\abs{x-a} < \delta$, on a~:
		\[\norm {f(x)-L} = \sqrt {\sum_{i=1}^n(f_i(x)-L_i)^2} < \sqrt {\sum_{i=1}^n\left(\frac \epsilon{\sqrt n}\right)^2} = \sqrt {n\frac {\epsilon^2}{n}} = \epsilon.\]
		\end{proof}

		\begin{lem} La fonction $f : A \subseteq \R \to \R^n$ est continue en $a$ si et seulement si toutes ses composantes sont continues en $a$. \end{lem}

		\begin{proof} Par le lemme précédent, la démonstration est triviale. \end{proof}

		\begin{lem} Soient $f, g : A \subseteq \R \to \R^n$ deux fonctions continues en $a \in A$. Alors les fonctions $\norm f, f+g$ et $fg$ sont également
		continues en $a$. \end{lem}

		\begin{rmq} Étant donné que la continuité d'une fonction à valeur vectorielle est équivalente à la continuité de ses composantes, les théorèmes sur
		les fonctions les fonctions réelles s'appliquent facilement aux fonctions à valeur vectorielle. \end{rmq}

		\begin{prp} Soit $\fabr f^n$ continue. Alors il existe $p, q \in \ab$ tels que $\forall x \in \ab~:
		\norm {f(q)} \leq \norm {f(x)} \leq \norm {f(p)}$. \end{prp}

		\begin{proof} Par le le lemme précédent, on sait que $\norm f$ est continue. En appliquant le théorème des bornes atteintes sur chaque composante,
		la proposition est démontrée. \end{proof}

\newpage
\section{Fonctions dérivables}\label{sec:foncdérivables}
	\subsection{Définitions}
	
		\begin{déf} Soit $f : I \to \R$ une fonction définie sur un intervalle ouvert $I$ contenant $a$. La fonction $f$ est dérivable en $a$ si la limite
		suivante existe~:
		\[\lim_{h \to 0}\frac {f(a+h)-f(a)}h.\]

		Si la limite existe, on la note $f'(a)$. Si la fonction $f$ est dérivable sur tout point $a$ de son domaine, $f$ est dérivable. On définit la fonction dérivée de
		$f$ par $f' : I \to \R : x \mapsto f'(x)$. \end{déf}

		\begin{déf} Il existe des classes de dérivabilité notées $C^k$ pour $k \in \N$. Si $f$ est continue, alors $f \in C^0$. De plus, si $f$ est dérivable
		et $f'$ est $C^k$, alors $f \in C^{k+1}$.  Et si $\forall k \in \N : f \in C^k$, alors on note $f \in C^\infty$. \end{déf}

		\begin{prp} Soit $f : A \subseteq \R \to \R$ dérivable en $a$. Alors $f$ est continue en $a$. \end{prp}

		\begin{proof}
		\[\lim_{x \to a}f(x) = \lim_{x \to a}f(a) + \lim_{x \to a}\frac {f(x)-f(a)}{x-a}(x-a) = f(a) + f'(a) \cdot 0 = f(a).\]
		\end{proof}

		\begin{thm}[Règles de calcul] Soient $f, g : I \subseteq \R \to \R$ deux fonctions définies sur un intervalle ouvert $I$ et dérivables en $a \in I$. Alors~:

		\begin{enumerate}
			\item $(f+g)'(a) = f'(a) + g'(a)$~;
			\item $(fg)'(a) = f'(a)g(a) + f(a)g'(a)$~;
			\item si $g(a) \neq 0$, alors $\left(\frac fg\right)'(a) = \frac {f'(a)g(a)-f(a)g'(a)}{g(a)^2}$.
		\end{enumerate}
		\end{thm}

		\begin{proof} Le premier point découle directement des règles de calcul sur la somme de limites. Le second point se montre en réécrivant la limite~:
		\[\lim_{x\to a}\frac {(fg)(x) - (fg)(a)}{x-a} = \lim_{x \to a}f(x)\frac {g(x)-g(a)}{x-a} + \lim_{x \to a}g(x)\frac {f(x)-f(a)}{x-a} = f'(a)g(a) + f(a)g'(a).\]

		Le dernier se montre d'abord par $\left(\frac 1g\right)'(a) = -\frac {g'(a)}{g(a)^2}$~:
		\[\lim_{x \to a}\frac {\frac 1g(x) - \frac 1g(a)}{x-a} = \lim_{x \to a}\frac {g(a)-g(x)}{(x-a)g(a)g(x)} = -\frac {g'(a)}{g(a)^2}.\]

		En utilisant le point 2 et cette propriété, le quotient est démontré. \end{proof}

		\begin{thm} Soient $f, g$ deux fonctions telles que $f$ est dérivable en $a$ et $g$ est dérivable en $f(a)$. Alors la composée $g \circ f$ est dérivable
		en $a$ et vaut~: \[g'(f(a))f'(a).\] \end{thm}

		\begin{proof} Soient les fonctions suivantes~:
		\[F(x) \coloneqq \left\{\begin{aligned}&\frac {f(x)-f(a)}{x-a} &\text{ si } x \neq 0 \\ &f\prime(a) &\text{ si } x = a\end{aligned}\right\},\]
		et~:
		\[G(x) \coloneqq \left\{\begin{aligned}&\frac {g(x)-g(f(a))}{x-f(a)} &\text{ si } x \neq f(a)\\&g\prime(f(a)) &\text{ si } x = f(a)\end{aligned}\right.\]

		Les fonctions $F$ et $G$ sont respectivement continues en $a$ et $f(a)$. De plus, $\forall x \in \dom f : f(x) = f(a) + (x-a)F(x)$ et
		$\forall x \in \dom g : g(x) = g(f(a)) + (x-f(a))G(x)$. Dès lors, on peut calculer $g \circ f$~:

		\begin{align*}
			(g \circ f)(x) &= g(f(x)) = g(f(a) + (x-a)F(x)) = g(f(a)) + (f(a) + (x-a)F(x) - f(a))G(f(a) + (x-a)F(x)) \\
		                   &= (g \circ f)(a) + (x-a)F(x)G(f(x)).
		\end{align*}

		On peut dès lors calculer la dérivée en faisant~:
		\[(g \circ f)'(a) = \lim_{x \to a}\frac {(g \circ f)(x) - (g \circ f)(a)}{x-a} = \lim_{x \to a}\frac {(x-a)F(x)G(f(x))}{x-a} = \lim_{x \to a}F(x)G(f(x)).\]

		Et puisque le produit de fonctions continues est toujours une fonction continue, par la continuité, cette valeur vaut $F(a)G(f(a)) = f'(a)g'(f(a))$.
		\end{proof}

		\begin{thm}[de la réciproque] Soit $f : I \to J$ une bijection continue réelle entre deux intervalles ouverts. Si $f$ est dérivable en $a \in I$ telle
		que $f'(a) \neq 0$, alors la réciproque $f^{-1}$ est dérivable en $f(a)$ et vaut~:
		\[\left(f^{-1}\right)'(f(a)) = \frac 1{f'(a)}.\]
		\end{thm}

		\begin{proof} Puisque $f$ est une bijection continue, elle est strictement monotone. Donc par un théorème précédent, on sait que $f^{-1} : J \to I$
		est également continue. Posons~:
		\[G(y) \coloneqq \frac {f^{-1}(y) - a}{y - f(a)}.\]

		Dès lors~:
		\[\lim_{y \to f(a)} G(y) = \lim_{y \to f(a))}G(f(f^{-1}(y))) = \lim_{x \to \lim_{y \to f(a)}f^{-1}(y)}\frac {x-a}{f(x)-f(a)} = \frac 1{f'(a)}.\]
		\end{proof}
	
	\subsection{Extrema}
		
		\begin{déf} Soient $f : U \subseteq \R \to \R$ et $a \in U$. Le point $a$ est un minimum local de $f$ si
		$\exists \epsilon > 0 \tq \forall x \in U : \abs{x-a} < \epsilon \Rightarrow f(a) \leq f(x)$. De même, $a$ et un maximum local si
		$\exists \epsilon > 0 \tq \forall x \in U : \abs{x-a} < \epsilon \Rightarrow f(a) \geq f(x)$. Si $a$ est un minimum local ou un maximum local, alors $a$ est
		un extremum local. \end{déf}

		\begin{prp} Soit $f : U \subseteq \R \to \R$ une fonction dérivable. Soit $a \in \intr U$ un extremum de $f$. Alors $f'(a) = 0$. \end{prp}

		\begin{proof} Montrons le cas où $a$ est un minimum local (le cas du maximum est identique). Par la définition du minimum, on sait qu'il existe
		$\epsilon$ tel que $\forall x \in U : \abs{x-a} < \epsilon \Rightarrow f(a) \leq f(x)$. Dès lors~:

		\begin{align*}
			&\forall x \in U \tq x < a : \frac {f(x)-f(a)}{x-a} \leq 0, \\
			&\forall x \in U \tq x > a : \frac {f(x)-f(a)}{x-a} \geq 0.
		\end{align*}

		Or, comme par hypothèse $f$ est dérivable en $a$, la limite pour $x \to a$ existe. Il faut donc $f'(a) = 0$. \end{proof}

		\begin{déf} Soit $f$ une fonction dérivable. Un point $a \in \dom f$ tel que $f'(a) = 0$ est appelé point critique. \end{déf}

	\subsection{Théorème de la moyenne}

		\begin{lem}[Théorème de Rolle] Soit $f : \ab \subseteq \R \to \R$ continue et dérivable sur $]a, b[$. Si $f(a) = f(b)$, alors
		$\exists c \in ]a, b[ \tq f'(c) = 0$. \end{lem}

		\begin{proof} La fonction $f$ est définie sur un intervalle fermé borné. Donc par le théorème des bornes atteintes, on sait qu'il existe $m, M \in \R$
		tels que $f(m)$ est le minimum de $f$ et $f(M)$ est le maximum de $f$. Si $f(m) = f(M)$, alors la fonction est constante. Alors prenons $c = \frac {a+b}2$.
		Sinon, si $m \neq a$ et $m \neq b$, prenons $c = m$ car $m$ est un extremum. Par la proposition précédente, $f'(c) = 0$. \end{proof}

		\begin{thm}[de la moyenne/eds accroissements finis] Soit $f : \ab \subseteq \R \to \R$ continue sur $\ab$ et dérivable sur $]a, b[$.
		Alors il existe $c \in ]a, b[$ tel que~:
		\[f'(c) = \frac {f(b)-f(a)}{b-a}.\]
		\end{thm}

		\begin{proof} Soit $G(x)$ une fonction continue sur $\ab$ et dérivable sur $]a, b[$ définie par~:
		\[G(x) \coloneqq f(x) - \frac {f(b)-f(a)}{b-a}x.\]

		Dès lors, $G(b)-G(a)=0$. Donc par le théorème de Rolle, on sait qu'il existe $c$ tel que $G'(c) = 0$. Or
		\[G'(x) = f'(x) - \frac {f(b)-f(a)}{b-a} = 0.\]

		On a donc bien $f'(c) = \frac {f(b)-f(a)}{b-a}$. \end{proof}

		\begin{prp} Soit $\fabr f$ continue sur $\ab$ et dérivable sur $]a, b[$.

		\begin{itemize}
			\item[$(i)$]   Si $\forall x \in ]a, b[ : f'(x) > 0$, alors $f$ est strictement croissante  sur $]a, b[$~;
			\item[$(ii)$]  Si $\forall x \in ]a, b[ : f'(x) < 0$, alors $f$ est strictement décroissante sur $]a, b[$~;
			\item[$(iii)$] Si $\forall x \in ]a, b[ : f'(x) = 0$, alors $f$ est constante sur $]a, b[$.
		\end{itemize}
		\end{prp}

		\begin{proof} Puisque $f$ est définie et continue sur un intervalle borné fermé, pour tout $x_1 < x_2 \in ]a, b[$, on sait que~:
		\[\exists c \in ]a, b[ \tq f'(c) = \frac {f(x_1)-f(x_2)}{x_1-x_2}.\]

		Donc si $f'(c) > 0$ (premier cas), il faut $f(x_2) > f(x_1)$, si $f'(c) < 0$ (second cas), il faut $f(x_2) < f(x_1)$ et si $f'(c) = 0$ (dernier cas), il faut
		$f(x_1 = f(x_2)$. On a donc $f$ soit strictement croissante, soit strictement décroissante soit constante. \end{proof}

		\begin{thm}[Comparatif de la moyenne] Soient $\fabr {f, g}$ continues sur $\ab$ et dérivables sur $]a, b[$. Si pour tout $x \in ]a, b[$, on a $g(x) \neq 0$,
		alors il existe $c \in ]a, b[$ tel que~:
		\[\frac {f'(c)}{g'(c)} = \frac {f(b)-f(a)}{g(b)-g(a)}.\]
		\end{thm}
	
	\subsection{Règle de l'Hospital}
		
		\begin{thm} Soient $f, g : \interval cd \to \R$ deux fonctions continues sur $\interval cd$ et dérivables sur $]c, d[$. Soit $a \in ]c, d[$.
		Si $f(a) = g(a) = 0$, alors~:
		\[\lim_{x \to a}\frac {f(x)}{g(x)} = \lim_{x \to a}\frac {f'(x)}{g'(x)}\]

		si cette limite existe. \end{thm}

		\begin{proof} Calculons d'abord la limite à droite, puis la limite à gauche. Soit $x \in ]a, d]$. Dès lors, par le théorème comparatif de la moyenne
		sur des fonctions $f$ et $g$ réduites au domaine $[a, x]$, on sait qu'il existe $\xi(x) \in ]a, x[$ tel que~:
		\[\frac {f'(\xi(x))}{g'(\xi(x))} = \frac {f(x)-f(a)}{g(x)-g(a)} = \frac {f(x)-0}{g(x)-0} = \frac {f(x)}{g(x)}.\]

		De plus, quand $x \to a^+$, il faut $\xi(x) \to a^+$ car $a < \xi(x) < x$. On sait donc que~:
		\[\lim_{x \to a^+}\frac {f(x)}{g(x)} = \lim_{\xi(x) \to a^+}\frac {f'(\xi(x))}{g'(\xi(x))}.\]

		De manière similaire, en prenant $x \in [c, a[$, on trouve $\xi(x) \in ]x, a[$ tel que~:
		\[\frac {f'(\xi(x))}{g'(\xi(x))} = \frac {f(x)-f(a)}{g(x)-g(a)} = \frac {f(x)}{g(x)}.\]

		À nouveau, quand $x \to a^-$, il faut $\xi(x) \to a^-$. Dès lors~:
		\[\lim_{x \to a^-}\frac {f(x)}{g(x)} = \lim_{\xi(x) \to a^-}\frac {f'(\xi(x))}{g'(\xi(x))}.\]

		Donc si les deux limites existent et sont égales, on a~:
		\[\lim_{x \to a}\frac {f(x)}{g(x)} = \lim_{x \to a}\frac {f'(x)}{g'(x)}.\]
		\end{proof}
		
		\begin{prp} Soient $f, g : \interval cd \to \R$ continues sur $\interval cd$ et dérivables sur $]c, d[$. Si $\lim_{x \to a^+}g(x) = \pm\infty$,
		$\forall x : x \in ]a, d[ \Rightarrow g'(x) \neq 0$, et si $\lim_{x \to a^+}\frac {f'(x)}{g'(x)} = L \in \overline \R$ alors~:

		\[\lim_{x \to a^+}\frac {f(x)}{g(x)} = L.\]
		\end{prp}

		\begin{proof} Soient $x < y \in ]a, d[$. Il existe $\xi \in ]x, y[$ tel que~:
		\[\frac {f(x)-f(y)}{g(x)-g(y)} = \frac {f'(\xi)}{g'(\xi)}.\]

		Comme $g(x) \to \pm \infty$ quand $x \to a$, on sait qu'il existe un voisinage de $a$ où $g(x) \neq 0$. De plus, en réécrivant~:
		\[f(x) = f(y) + (f(x)-f(y)) = f(y) + (g(x)-g(y))\frac {f(x)-f(y)}{g(x)-g(y)} = f(y) + (g(x)-g(y))\frac {f'(\xi)}{g'(\xi)},\]

		on peut trouver~:
		\[\frac {f(x)}{g(x)} = \frac 1{g(x)}\left[f(y) + (g(x)-g(y))\frac {f'(\xi)}{g'(\xi)}\right] =
		\frac {f(y)}{g(x)} + \left(1 - \frac {g(y)}{g(x)}\right)\frac {f'(\xi)}{g'(\xi)}.\]

		Soit $\epsilon > 0$. Prenons $\delta_1 > 0$ tel que $\forall \chi \in ]a, a+\delta_1[ : \abs{\frac {f'(\chi)}{g'(\chi)} - L} < \epsilon_1$.
		Fixons $y = a+\delta_1$. On a $\frac {g(y)}{g(x)} \to 0$ quand $x \to a$ car $g(y)$ est fixé et $g(x) \to +\infty$ (pareil pour $\frac {f(y)}{g(x)} \to 0$).
		Dès lors, il existe $\delta_2$ tel que $\forall x : x \in ]a, a+\delta_2[ \Rightarrow \abs{\frac {g(y)}{g(x)}} < \epsilon_2$ et
		$\abs{\frac {f(y)}{g(x)}} < \epsilon_3$.
		Prenons $(\epsilon_1, \epsilon_2, \epsilon_3) = \left(\frac \epsilon6, \min\left\{\frac \epsilon{3|L|+1}, \frac 12\right\}, \frac \epsilon3\right)$.
		Prenons $\delta \coloneqq \min \{\delta_1, \delta_2\}$. On a donc~:
		
		\begin{align*}
			\forall x : x \in ]a, a+\delta[ : \abs{\frac {f(x)}{g(x)} - L}
			&= \abs{\frac {f(y)}{g(x)} + \left(1 - \frac {g(y)}{g(x)}\right)\left(\frac {f'(\chi)}{g'(\chi)}-L+L\right) - L} \\
			&= \abs{\frac {f(y)}{g(x)} + \left(\frac {f'(\chi)}{g'(\chi)}-L\right)-\frac {g(y)}{g(x)}\left(\frac {f'(\chi)}{g'(\chi)}-L\right)+L-\frac {g(y)}{g(x)}L-L} \\
			&< \epsilon_3 + \epsilon_1 + \epsilon_2\epsilon_1 + \abs L\epsilon_2 \\
			&< \frac \epsilon3 + \frac \epsilon6 + \frac \epsilon6 + \frac \epsilon3 = \epsilon.
		\end{align*}
		\end{proof}
	
	\subsection{Dérivées de fonctions à valeur dans $\R^n$}
		
		\begin{déf} Soit $\frrn fA$. Soit $a \in \intr A$. $f$ est dérivable en $a$ si~:
		\[\lim_{h \to 0}\frac {f(a+h)-f(a)}h\]

		existe dans $\R^n$ cet élément. \end{déf}

		\begin{lem} La fonction $\frrn rA$ est dérivable en $a \in \intr A$ si et seulement si toutes ses composantes sont dérivables. En ce cas,
		$f'(a) = (f_1'(a), \ldots, f_n'(a))$. \end{lem}

		\begin{thm} Soient $f : \R \to \R$ dérivable en $a \in \R$ et $g : \R \to \R^n$ dérivable en $f(a)$. Alors $(g \circ f)'(a) = f'(a)g'(f(a))$. \end{thm}

		\begin{proof} Par le lemme précédent et la règle de dérivation de composée pour les fonctions réelles. \end{proof}

		\begin{thm} Soient $f : \R \to \R, g : \R^n \to \R, h : \R \to \R^n$ et $a \in \R$ tels que $f, g, h$ sont dérivables en $a$. Alors~:

		\begin{enumerate}
			\item $(fg)'(a) = f'(a)g(a) + f(a)g'(a)$~;
			\item $(\scpr gh)'(a) = \scpr {h'(a)}{g(a)} + \scpr {h(a)}{g'(a)}$~;
			\item $\left(\frac gf\right)'(a) = \frac {g'(a)f(a) - g(a)f'(a)}{f(a)^2}$~;
			\item $(g+h)'(a) = g'(a) + h'(a)$.
		\end{enumerate}
		\end{thm}
	
	\subsection{Dérivées de fonctions vectorielles}
		
		\begin{déf} Le graphe d'une fonction $\frmr fA$ est l'ensemble des couples~:
		\[\Gamma_f \coloneqq \{((x, f(x)) \in \R^n \times \R \cong \R^{n+1} \tq x \in A\}.\]
		\end{déf}

		\begin{déf} Soit $\frmr fA$. Soit $a \in \intr A$. La $j$ème dérivée partielle de $f$ (pour $1 \leq j < m$) est  donnée par~:
		\[\lim_{h \to 0}\frac {f(a_1, \ldots, a_j+h, \ldots, a_n) - f(a)}h = \lim_{h \to 0}\frac {f(a+he_j)-f(a)}h.\]

		si cette limite existe et se note $\pd f{x_j}(a)$ ou $(\partial_jf)(a)$. Si la limite n'existe pas, alors $f$ n'est pas dérivable en $a$.
		\end{déf}

		\begin{déf} On définit le gradient de $f : \R^m \to \R$ en $a$ par le vecteur des dérivées partielles~:
		\[(\nabla f)(a) = ((\partial_1 f)(a), \ldots, (\partial_m f)(a)) = ((\partial_i f)(a))_i \in \R^m.\]
		\end{déf}

		\begin{déf} $e_j$ est le $j$ème vecteur de la base canonique de $\R^n$. \end{déf}

		\begin{déf} Soit $f : \R^m \to \R$, $a \in \R^m$ et $v \in \R^m$. $f$ est dérivable en $a$ dans la direction $v$ si la limite suivante
		existe dans $\R$~:

		\[\lim_{h \to 0}\frac {f(a+hv)-f(a)}h.\]

		On note $(\partial_v f)(a)$ ce réel et on l'appelle dérivée directionnelle de la fonction $f$ dans la direction $v$ au point $a$. \end{déf}

		\begin{rmq} Comme $e_j$ est un vecteur de la base de $\R^m$, $(\partial_j f)(a) = (\partial_{e_j} f)(a)$ est une dérivée directionnelle. \end{rmq}

		\begin{déf} Soient $f : \R^m \to \R$, $a \in \R^m$. La fonction $f$ est différentiable s'il existe $u \in \R^m$ tel que~:

		\[\lim_{x \to a}\frac {f(x) - f(a) - \scpr u{x-a}}{\norm {x-a}} = 0 \in \R.\]
		\end{déf}

		\begin{prp} Soient $f : \R^m \to \R$, $a \in \R^m$. Si $f$ est différentiable en $a$, alors la fonction
		$\partial_\cdot f : \R^m \to \R : v \mapsto (\partial_v f)(a)$ est définie pour tout vecteur $v$ et est linéaire. De plus,
		$\forall v \in \R^m : (\partial_v f)(a) = \scpr {(\nabla f)(a)}v$. \end{prp}

		\begin{proof} Soit $x(t) \coloneqq a + tv$. Donc, $x(t) \to a$ si $t \to 0$. Dès lors~:

		\begin{align*}
			\lim_{t \to 0} \frac {f(a+tv)-(f(a) + \scpr u{tv})}{\abs t} &= 0 \\
			\lim_{t \to 0} \frac {f(a+tv)-f(a)}{\abs t} - \lim_{t \to 0}\frac {\scpr u{tv}}{\abs t} &= 0 \\
			(\partial_v f)(a) - \scpr uv \lim_{t \to 0}\frac tt &= 0 \\
			(\partial_v f)(a) &= \scpr uv.
		\end{align*}

		Dès lors, pour $v = e_j$, on a $(\partial_{e_j} f)(a) = (\partial_j f)(a) = \scpr uv = \scpr u{e_j} = u_j$. Donc
		$u = ((\partial_1 f)(a), \ldots, (\partial_m f)(a)) = (\nabla f)(a)$. \end{proof}

\newpage
\section{Intégrales de Riemann}
		\subsection{Définitions}
	
		\begin{déf} une partition de $\ab$ est la donnée $\{x_i \tq 0 \leq i \leq n\} \subset \ab$ telle que $a = x_0 < \ldots < x_n = b$. \end{déf}

		\begin{déf} Soit $\fabr f$ bornée. Soit $P = \{x_i\}_{i \in [n]}$  une partition de $\ab$. Pour tout $1 \leq i \leq n$, on définit~:

		\begin{align*}
			m_i &\coloneqq \inf \{f(x) \tq x \in \interval {x_{i-1}}{x_i}\}, \\
			M_i &\coloneqq \sup \{f(x) \tq x \in \interval {x_{i-1}}{x_i}\}.
		\end{align*}

		On définit ensuite~:
	
		\begin{align*}
			\mathcal L(f, P) &\coloneqq \sum_{i=1}^n(x_i-x_{i-1})m_i, \\
			\mathcal U(f, P) &\coloneqq \sum_{i=1}^n(x_i-x_{i-1})M_i.
		\end{align*}

		Qui sont respectivement l'aire signée de la somme des rectangles inférieurs et supérieurs. À partir de cela, on définit~:
	
		\begin{align*}
			\mathcal L(f) &\coloneqq \sup \{\mathcal L(f, P) \tq P \text{ est une partition de } \ab\}, \\
			\mathcal U(f) &\coloneqq \inf \{\mathcal U(f, P) \tq P \text{ est une partition de } \ab\}.
		\end{align*}
		\end{déf}

		\begin{déf} une fonction $\fabr f$ bornée est intégrale si $\mathcal U(f) = \mathcal L(f)$. On note cette valeur $\int_a^bf(x)\dif x$ ou encore
		$\int_a^b f$. \end{déf}

		\begin{lem} Si $\fabr f$ est bornée et $P$ est une partition de $\ab$, $y \in \ab$. On définit $P' \coloneqq P \cup \{y\}$. Alors~:
		\[\Larea(f, P) \leq \Larea(f, P) \leq \Uarea(f, P) \leq \Uarea(f, P').\]
		\end{lem}

		\begin{proof} Soit $r \in \{1, \ldots, n\}$ tel que $x_{r-1} < y < x_r$. On sait que $\Larea(f, P) = m_1(x_1-x_0) + \ldots + m_n(x_n-x_{n-1})$ et on
		sait que $\Larea(f, P') = m_1(x_1-x_0) + \ldots + \alpha(y-x_{r-1}) + \beta(x_r-y) + \ldots + m_n(x_n-x_{n-1})$ où
		$\alpha = \inf \{f(x) \tq x \in \interval {x_{r-1}}y\}$ et $\beta = \inf \{f(x) \tq x \in \interval y{x_r}\}$. Dès lors~:

		\begin{align*}
			\Larea(f, P')-\Larea(f, P) &= \alpha(y-x_{r-1}) + \beta(x_r-y) - m_r(x_r-x_{r-1}) = x_{r-1}(-\alpha+m_r)+ x_r(\beta-m_r) + \alpha y - \beta y + m_ry - m_ry \\
			                           &= (\alpha-m_r)(y-x_{r-1}) + (\beta-m_r)(x_r-y).
		\end{align*}

		Étant donné que $x_{r-1} < y < x_r$, on sait que les secondes parenthèses sont positives. De plus, comme les intervalles dont $\alpha$ et $\beta$ sont
		les minima sont inclus dans l'intervalle dont $m_r$ est le minimum, il est nécessaire que $\alpha \geq m_r$ et $\beta \geq m_r$. Les premières
		parenthèses sont dès lors également positives. Si $\Larea(f, P')-\Larea(f, P) \geq 0$, alors $\Larea(f, P') \geq \Larea(f, P)$. La partie pour les aires
		supérieures est identique. \end{proof}

		\begin{cor} Soient $P, P'$ deux partitions de $\ab$ telles que $P \subset P'$, alors $\Larea(f, P') \leq \Larea(f, P) \leq \Uarea(f, P) \leq \Uarea(f, P')$.
		\end{cor}

		\begin{proof} En écrivant $P' = P \cup \{y_1, \ldots, y_k\}$ et en appliquant $k$ fois le lemme précédent. \end{proof}

		\begin{lem} Soient $P$ et $P'$ deux partitions quelconques du même intervalle $\ab$. Alors~:
		\[\Larea(f, P) \leq \Uarea(f, P').\]
		\end{lem}

		\begin{proof} Soit $P'' = P \cup P'$. Dès lors, on sait $P'' \subset P'$ et $P'' \subset P$. Donc~:
		\[\Larea(f, P) \leq \Larea(f, P'') \leq \Uarea(f, P'') \leq \Uarea(f, P').\]
		\end{proof}

		\begin{prp} Soit $\fabr f$ bornée. Alors $\Larea(f) \leq \Uarea(f)$. \end{prp}

		\begin{proof} On sait que pour tout $P, P'$ partitions de $\ab$, $\Larea(f, P) \leq \Uarea(f, P')$. En particulier, en prenant le $\sup$
		à gauche et l'$\inf$ à droite, l'inégalité reste vraie. Donc~:
	
		\begin{align*}
			\sup \{\Larea(f, P) \tq P \text{ est une partition de } \ab\} &\leq \inf \{\Uarea(f, P) \tq P \text{ est une partition de } \ab\} \\
			\Larea(f) &\leq \Uarea(f)
		\end{align*}
		\end{proof}
	
	\subsection{Fonctions intégrables}
		\begin{prp}[Critère de Riemann] Soit $\fabr f$ bornée. Alors $f$ est intégrable si et seulement $\forall \epsilon > 0 : \exists P$ une partition
		de $\ab$ telle que $\Uarea(f, P) - \Larea(f, P) < \epsilon$. \end{prp}

		\begin{proof} Montrons d'abord l'implication $\Rightarrow$. On sait par hypothèse que $\Larea(f) = \Uarea(f)$. Soit $\epsilon > 0$. On sait
		qu'il existe $P_1, P_2$ partitions de $\ab$ tels que $\Uarea(f, P_1) < \Uarea(f)+\frac \epsilon2$ et $\Larea(f, P_2) > \Larea(f) + \frac \epsilon2$.
		Posons $P \coloneqq P_1 \cup P_2$. Dès lors~:
		\[
			\Larea(f) - \frac \epsilon2 \leq \Larea(f, P_2) \leq \Larea(f, P) \leq \Larea(f)
			  \stackrel{\text{par hypothèse}}=
			\Uarea(f) \leq \Uarea(f, P) \leq \Uarea(f, P_1) \leq \Uarea(f) + \frac \epsilon2.
		\]

		On sait donc $\Uarea(f, P) < \Uarea(f, P_1) < \Uarea(f)+\frac \epsilon2$ et $-\Larea(f, P) < -\Larea(f, P_2) < -\Larea(f)+\frac \epsilon2$. Donc~:
		\[\Uarea(f, P) - \Larea(f, P) < \Uarea(f)+\frac \epsilon2-\Larea(f)+\frac \epsilon2 = \epsilon.\]

		Montrons maintenant l'autre sens de l'implication $\Leftarrow$. Soit $\epsilon > 0$. On sait qu'il existe $P$ une partition de $\ab$ telle que
		$\Uarea(f, P) - \Larea(f, P) < \epsilon$. De plus, on sait $0 \leq \Uarea(f)-\Larea(f) \leq \Uarea(f, P)-\Larea(f, P) < \epsilon$. On a donc une
		quantité $\Uarea(f)-\Larea(f)$ ne dépendant pas de $\epsilon$ mais étant plus petite qu'$\epsilon$ pour tout $\epsilon > 0$. Il faut dès lors
		$\Uarea(f)-\Larea(f) = 0$. \end{proof}

		\begin{thm} Si $\fabr f$ est continue, alors $f$ est intégrable. \end{thm}

		\begin{proof} Soit $\epsilon > 0$. Si $f$ est continue, alors elle est uniformément continue. Donc il existe $\delta$ tel que~:
		\[\forall x, y : \abs{x-y} < \delta \Rightarrow \abs{f(x)-f(y)} < \frac \epsilon{b-a}.\]

		On construit $P$ telle que $\forall 1 \leq 1 \leq n : x_i-x_{i-1} < \delta$. Pour tout $i$, on sait qu'il existe $x_*$ et $x^*$ tels que
		$M_i = f(x^*)$ et $m_i = f(x_*)$ par le théorème des bornes atteintes. Donc~:
		\[\Uarea(f, P)-\Larea(f, P) = \sum_{i=1}^n(M_i-m_i)(x_i-x_{i-1}) = \sum_{i=1}^n(f(x^*)-f(x_*))(x_i-x_{i-1}) \leq \frac \epsilon{b-a}\sum_{i=1}^n(x_i-x_{i-1})
		= \frac \epsilon{b-a}(b-a) = \epsilon.\]
		\end{proof}

	\subsection{Propriétés des intégrales}
		
		\begin{déf} Soit $\fabr f$ intégrable. On définit~:
		\[\int_a^bf(x)\dif x = -\int_b^af(x)\dif x.\]
		\end{déf}

		\begin{prp} Soient $\fabr {f, g}$ intégrables. Soient $\alpha, \beta \in \R$ et $c \in \ab$. Alors~:

		\begin{enumerate}
			\item $\int_a^b\left(\alpha f(x) + \beta g(x)\right)\dif x = \alpha \int_a^bf(x)\dif x + \beta\int_a^bg(x)\dif x$~;
			\item $\int_a^bf(x)\dif x = \int_a^cf(x)\dif x + \int_c^bf(x)\dif x$~;
			\item Si $\forall x \in \ab : f(x) \leq g(x)$, alors $\int_a^bf(x)\dif x \leq \int_a^bg(x)\dif x$~;
			\item $\abs f$ est intégrable et $\abs{\int_a^bf(x)\dif x} \leq \abs{\int_a^b\abs{f(x)}\dif x}$.
		\end{enumerate}
		\end{prp}
	
	\subsection{Théorème fondamental de l'analyse}

		\begin{thm}[Théorème fondamental du calcul et intégral] Soit $\fabr f$ continue (donc intégrable). Alors la fonction $\fabr F : x \mapsto \int_a^xf$
		est l'unique primitive de $f$ sur $\ab$ qui s'annule en $a$. \end{thm}

		\begin{proof} Soit $c \in \ab$. Soit $\epsilon > 0$. Par la continuité(uniforme) de $f$ en $c$, il existe $\delta$ tel que
		$\abs{x-c}<\delta \Rightarrow \abs{f(x)-f(c)} < \epsilon$. De plus, notons que~:

		\begin{align*}
			\abs{\frac {F(x)-F(c)}{x-c}-f(c)} = \abs{\frac {\int_a^xf - \int_a^cf}{x-c}-f(x)} = \abs{\frac {\int_c^xf - f(c)(x-c)}{x-c}}
			= \abs{\frac {\abs{\int_c^x(f(t)-f(c))\dif t}}{x-c}} \leq \frac 1{\abs{x-c}}\abs{\int_c^x\abs{f(t)-f(c)}\dif t}
		\end{align*}

		Et si $\abs{x-c} < \delta$, alors $\forall t \in \interval cx : \abs{t-c} < \delta $. Et donc $\abs{f(t)-f(c)} < \epsilon$. Donc~:
		\[\frac 1{\abs{x-c}}\abs{\int_c^x\abs{f(t)-f(c)}\dif t} < \frac 1{\abs{x-c}}\abs{\int_c^x\epsilon\dif t} = \epsilon.\]

		On a donc montré que pour $x \to c$, on a $\frac {F(x)-F(c)}{x-c} = F'(c) \to f(c)$.

		Montrons maintenant que $F$ est l'\textbf{unique} primitive s'annulant en $a$. Soit $\fabr G$ telle que $G' = f$ et $G(a) = 0$. Montrons que $G = F$~:
		\[(G-F)'=(f-f)' = 0.\]

		On sait donc que $G-F$ est une fonction constante. Et comme $G(a) = F(a) = 0$, on sait que $\forall x \in \ab : (G-F)(x) = 0$. \end{proof}

		\begin{cor} Soit $\fabr f$ intégrable. Sois $F \coloneqq \int f$. On a~:
		\[\int_a^b f(x)\dif x = F(b) - F(a) = \evf Fxab.\]
		\end{cor}

		\begin{proof} Les fonctions suivantes~:

		\begin{align*}
			x \mapsto \int_a^xf(t)\dif t, \\
			x \mapsto F(x) - F(a),
		\end{align*}

		sont deux primitives de $f$ s'annulant en $a$ et donc sont égales. \end{proof}

		\begin{prp} Soient $\fabr {f, g} \in C^1$. Alors~:

		\[\int_a^bf(x)g'(x)\dif x = \evf {(fg)}xab - \int_a^bf'(x)g(x)\dif x.\]
		\end{prp}

		\begin{proof} Soit $h(x) \coloneqq (fg)'(x) = f'(x)g(x) + f(x)g'(x)$. Par le théorème fondamental, on sait~:
		\[\int_a^bh'(x)\dif x = \int_a^b(f'(x)g(x) + f(x)g'(x))\dif x = \evf hxab.\]

		Ou encore~:
		\[\int_a^bf'(x)g(x)\dif x = \evf hxab - \int_a^bf(x)g'(x)\dif x.\]
		\end{proof}

		\begin{prp} Soient $\fabr f$ continue et $\fabr g \in C^1$. Posons $\alpha \coloneqq g(a), \beta \coloneqq g(b)$. Alors~:
		\[\int_a^bf(x)\dif x = \int_\alpha^\beta(f \circ g)(t)g'(t)\dif t.\]
		\end{prp}

		\begin{proof} Soient $F(x) \coloneqq \int_a^xf(t)\dif t$ et $G(t) \coloneqq (F \circ g)(t)$. Dès lors, on sait que $G'(t) = (f \circ g)(t)g'(t)$.
		De plus, l'intégration bornée donne~:
		\[\int_\alpha^\beta (f \circ g)(t)g'(t)\dif t = \int_\alpha^\beta G'(t)\dif t = \evf Gt\alpha\beta = (F \circ g)(\beta)-(F \circ g)(\alpha) = \int_a^b f(x)\dif x.\]
		\end{proof}

	\subsection{Les intégrales impropres}

		\begin{déf} Soit $f : [a, +\infty[ \to \R$ une fonction bornée et intégrable sur tout $\interval ab$ pour $b > a$. Alors si la limite suivante existe~:
		\[\lim_{b \to +\infty}\int_a^bf(x)\dif x,\]

		on dit que $\int_a^{+\infty} f(x)\dif x$ converge. On appelle une telle limite une intégrale impropre. Si la limite n'existe pas, on dit que $\int_a^\infty$
		diverge. De manière similaire, soit $f : ]-\infty, b] \to \R$ une fonction bornée et intégrable sur tout $\interval ab$ pour $a < b$.
		Alors si la limite suivante existe~:
		\[\lim_{a \to -\infty}\int_a^bf(x)\dif x,\]

		on dit que $\int_{-\infty}^bf(x)\dif x$ converge. Pour une fonction $f : \R \to \R$, bornée, si les deux intégrales impropres suivantes existent~:

		\[\int_{-\infty}^0f(x)\dif x, \\
		\int_0^{+\infty}f(x)\dif x,\]

		alors on définit l'intégrale suivante~:

		\[\int_{-\infty}^{+\infty}f(x)\dif x = \int_{-\infty}^0f(x)\dif x + \int_0^{+\infty}f(x)\dif x.\]
		\end{déf}

		\begin{déf} De manière similaire, pour des fonctions non bornées, on a les définitions suivantes. Soit $f : ]a, b] \to \R$. Si $f$ est intégrable
		sur tout $\interval cb$ avec $c \in ]a, b]$, alors on définit~:
		\[\int_a^bf(x)\dif x = \lim_{c \to a}\int_c^bf(x)\dif x.\]

		De même, soit $f : [a, b[ \to \R$ intégrable sur tout $\interval ac$ pour $c \in [a, b[$. On définit alors~:
		\[\int_a^bf(x)\dif x = \lim_{c \to b}\int_a^cf(x)\dif x.\]

		Et finalement soit $f : ]a, b[ \to \R$. Si les deux intégrales impropres existent, on définit~:

		\[\int_a^bf(x)\dif x = \int_a^{\frac {a+b}2}f(x)\dif x + \int_{\frac {a+b}2}^bf(x)\dif x.\]
		\end{déf}

		\begin{prp}[Critère de comparaison] Soient $f, g : [a, +\infty[ \to \R$ intégrables sur $\interval ab$ pour tout $b > a$. Si
		$\forall x > a : 0 \leq f(x) \leq g(x)$ et $\int_a^{+\infty}g(x)\dif x$ converge, alors $\int_a^{+\infty}f(x)\dif x$ converge également telle que~:
		\[\int_a^{+\infty}f(x)\dif x \leq \int_a^{+\infty}g(x)\dif x.\]
		\end{prp}

		\begin{proof} Comme $f(x) \leq g(x)$ pour tout $x > a$, on sait que pour tout $b > a$, on a~:
		\[\int_a^bf(x)\dif x \leq \int_a^bg(x)\dif x.\]

		De plus, les fonctions suivantes sont croissantes car $f$ et $g$ sont toujours positives~:

		\begin{align*}
			&F : b \mapsto \int_a^bf(x)\dif x, \\
			&G : b \mapsto \int_a^bg(x)\dif x.
		\end{align*}

		Dès lors, $F$ est majorée et bornée par $\int_a^{+\infty}g(x)\dif x$. La limite de $F(b)$ pour $b \to +\infty$ existe. \end{proof}

	\subsection{Longueur de courbes}
		
		\begin{déf} Une courbe différentiable est une application $\gamma \ab \subseteq \R \to \R^n$ de classe $C^1$ telle que
		$\forall t : t \in \ab \Rightarrow \gamma'(t) \neq 0 \in \R^n$. \end{déf}

		\begin{déf} L'ensemble $C \coloneqq \Imf \gamma$ est la courbe différentiable associée à $\gamma$. \end{déf}

		\begin{déf} Soit $\gamma : \ab \to \R^n$ une courbe différentiable. Si $\gamma(a) = \gamma(b)$, alors $\gamma$ est un lacet. Et si $\gamma$
		est un lacet tel que $\gamma'(a) = \gamma'(b)$ (dérivées respectivement « à droite » et « à gauche »), alors $\gamma$ est un lacet différentiable.
		\end{déf}

		\begin{déf} Si $\gamma$ est une courbe injective sur $]a, b[$, alors $\gamma$ est dite simple. \end{déf}

		\begin{déf} Soit $\gamma$ une courbe paramétrée simple et différentiable. On définit sa longueur par~:
		\[L(\gamma) \coloneqq \int_a^b\norm{\gamma'(t)}\dif t.\]
		\end{déf}

		\begin{prp} Soient $f : \ab \to \interval cd \in C^1$ bijective, $\gamma : \ab \to \R^n$ et $\eta : \interval cd \to \R^n$ deux courbes
		paramétrées différentiables. Supposons que $\gamma = (\eta \circ f)$ (que $\gamma$ est une reparamétrisation de $\eta$). Alors
		$L(\gamma) = L(\eta)$. \end{prp}

		\begin{proof} Supposons $f$ croissante. Dès lors (en posant $u \coloneqq f(t)$)~:

		\begin{align*}
			L(\gamma) &= L(\eta \circ f) = \int_a^b\norm{(\eta \circ f)'(t)}\dif t = \int_a^b\norm{(\eta' \circ f)(t)f'(t)}\dif t =
			\int_a^b\norm{(\eta' \circ f)(t)}\abs{f'(t)}\dif t \\
			          &= \int_a^b\norm{(\eta' \circ f)(t)}f'(t)\dif t = \int_{f(a)}^{f(b)}\norm{\eta'(u)}\dif u = \int_c^d\norm{\eta'(u)}\dif u = L(\eta).
		\end{align*}
		
		Si $f$ était décroissant, il aurait fallu mettre un $-$ en sortant $f'(t)$ de la valeur absolue mais les bornes d'intégration $f(a)$ et $f(b)$ auraient
		respectivement donné $d$ et $c$. Donc en rentrant le moins dans les bornes d'intégration, on obtient le même résultat. \end{proof}

		\begin{déf} Soit $\gamma : \ab \to \R^n$ une courbe paramétrée différentiable telle que $\forall t : \norm{\gamma'(t)} = 1$. Alors $\gamma$
		est une paramétrisation par longueur. \end{déf}

		\begin{rmq} La longueur d'une telle courbe est $b-a$ et la distance entre deux points $p, q$ quelconques est $\abs{p-q}$. \end{rmq}

		\begin{lem} Toute courbe paramétrée différentiable possède une paramétrisation par longueur. \end{lem}

		\begin{proof} Soit $\gamma : \ab \to \R^n$ une courbe paramétrée différentiable. Définissons $\ell : \ab \to \R^+ : t \mapsto \int_a^t\gamma'(u)\dif u$.
		$\ell$ est une fonction strictement croissante et une bijection $\in C^1$. Soit $\eta = (\gamma \circ \ell^{-1})$. On a~:
		\[\norm{\eta'(t)} = \norm{\gamma((\ell^{-1})(t))(\ell^{-1})'(t)} = \norm {\gamma'((\ell^{-1})(t))} \frac 1{\abs{\ell'((\ell^{-1})(t))}}.\]

		Et comme, par définition, $\ell'(t) = \norm{\gamma'(t)}$, on a $\norm{\eta'(t)} = 1$. \end{proof}

	\subsection{Intégrales curvilignes}
		
		\begin{déf} Soient $f : E \subseteq \R^n \to \R$ et $\gamma : \ab \to \R^n$ avec $\Imf \gamma \subseteq E$. On définit l'intégrale de $f$ le
		long de la courbe $\gamma$ par~:
		\[\in_\gamma f\dif s \coloneqq \int_a^bf(\gamma(t))\norm{f'(t)}\dif t.\]
		\end{déf}

		\begin{déf} Un champ de vecteurs est une application $f : E \subseteq \R^n \to \R^n$. \end{déf}

		\begin{déf} Soient $f : E \subseteq \R^n \to \R^n$ un champ de vecteurs continu et $\gamma : \ab \to \R^n$ une courbe paramétrée différentiable.
		On définit le travail de $f$ le long de $\gamma$ par~:
		\[\int_\gamma\scpr f{\dif s} \coloneqq \int_a^b\scpr {f(\gamma(t))}{\gamma'(t)}\dif t.\]
		\end{déf}

		\begin{prp} Soient $\gamma$ et $\eta$ deux paramétrisations d'une même courbe. Alors $\int_\gamma\scpr f{\dif s} = \pm\int_\eta\scpr f{\dif s}$.
		Le signe moins peut apparaitre si $\gamma$ et $\eta$ dont d'orientation opposée. \end{prp}

	\subsection{Champs conservatifs}
		
		\begin{déf} Soit $f : E \subseteq \R^n \to \R^n$ un champ de vecteurs. $f$ est conservatif si il existe $F : E \to \R$ tel que $f = \nabla F$.
		\end{déf}

		\begin{prp} Soit $f : E \subseteq \R^n \to \R^n$ un champ conservatif. Alors $\int_\gamma\scpr f{\dif s}$ ne dépend que des extrémités de
		$\gamma$. \end{prp}

		\begin{proof} Soit $F = \nabla f$.
		\[\int_\gamma\scpr f{\dif s} = \int_a^b\scpr {\nabla F(\gamma(t))}{\gamma'(t)}\dif t = \int_a^b(F(\gamma(t)))'(t)\dif t = F(\gamma(b))-F(\gamma(a)).\]
		\end{proof}

\newpage
\section{Fonctions continues à plusieurs variables}
	\subsection{Introduction}
		\begin{rmq} C'est d'abord l'espace euclidien $\R^n$ qui sera étudié ici. \end{rmq}

		\begin{déf} Soit $f : \R^m \to \R^n$ une fonction. On dit que $f$ est une fonction à $m$ variables et à valeur vectorielle. \end{déf}

		\paragraph{Rappel} Soit $U \subseteq \R^m$. On dit que $U$ est un voisinage de $a \in \R^m$ s'il existe $\delta > 0$ tel que pour tout $x \in \R^m$,
		si $\norm{x-a} < \delta$, alors $x \in U$.

	\subsection{Norme et distance dans $\R^n$}
		Le choix de la norme euclidienne pour exprimer la distance dans $\R^n$ est arbitraire. Mais une fonction de distance $d : \R^n \times \R^n \to \R$ doit
		respecter certaines propriétés.

		\begin{déf} Soit $X$ un ensemble. Une \textbf{distance} sur $X$ est une application $d : X \times X \to \R$ qui respecte les propriétés suivantes
		$\forall x, y \in \R^n$~:

		\begin{enumerate}
			\item $d(x, y) \geq 0$~;
			\item $d(x, y) = 0 \iff x = y$~;
			\item $d(x, y) = d(y, x)$~;
			\item $\forall z \in \R^n : d(x, z) \leq d(x, y) + d(y, z)$.
		\end{enumerate}
		\end{déf}

		\begin{déf} Une \textbf{norme} sur $\R^n$ est une application $\norm \cdot : \R^n \to \R$ respectant les propriétés suivantes
		$\forall x, y \in \R^n, \lambda \in \R$~:

		\begin{enumerate}
			\item $\norm x \geq 0$~;
			\item $\norm {\lambda x} = \abs \lambda \norm x$~;
			\item $\norm x = 0 \iff x = 0$~;
			\item $\norm {x+y} \leq \norm x + \norm y$.
		\end{enumerate}
		\end{déf}

		\begin{prp} La définition d'une norme sur $\R^n$ implique l'existence d'une distance $d : \R^n \times \R^n \to \R : (x, y) \mapsto \norm {x-y}$.
		\end{prp}

		\begin{proof} Soient $x, y, z \in \R^n$. Montrons les propriétés nécessaires pour une distance~:
		
		\begin{enumerate}
			\item $d(x, y) = \norm {x-y} \geq 0$~;
			\item $0 = d(x, y) = \norm {x-y} \iff x-y = 0 \iff x = y$~;
			\item $d(x, y) = \norm {x-y} = \norm{(-1)(y-x)} = \abs {-1}\norm {y-x} = \norm {y-x} = d(y, x)$~;
			\item $d(x, z) = \norm {x-z} = \norm {(x-y)+(y-z)} \leq \norm {x-y} + \norm {y-z} = d(x, y) + d(y, z)$.
		\end{enumerate}
		\end{proof}

		\begin{lem} Soient $x, y \in \R^n$. Alors $\abs {\scpr xy} \leq \sqrt {\scpr xx}\sqrt {\scpr yy}$. \end{lem}

		\begin{proof} Tout d'abord, supposons $y = 0$. On a alors~:
		\[\abs {\scpr x0} = 0 = \sqrt{\scpr xx}\sqrt {\scpr 00}.\]
		L'inégalité large est donc vérifiée puisque les deux membres sont égaux.

		On sait que $\forall \lambda \in \R : 0 \leq \scpr {x - \lambda y}{x - \lambda y} = \scpr xx - \lambda \scpr yx - \lambda(\scpr xy - \lambda \scpr yy)$.
		Supposons maintenant $y \neq 0$. Prenons $\lambda$ tel que $\scpr xy - \lambda \scpr yy = 0$ (c.-à-d. $\lambda = \frac {\scpr xy}{\scpr yy}$).
		On a donc~:
		\[0 \leq \scpr xx - \lambda yx = \scpr xx - \frac {\scpr xy}{\scpr yy}\scpr yx = \scpr xx - \frac {\scpr xy^2}{\scpr yy}.\]
		\end{proof}

	\subsection{Convergence des suites dans $\R^n$}
		\begin{déf}[Convergence de suite dans $\R^n$] Soit $(x_n)_n \subset \R^n$. $(x_n)_n$ converge en $x \in \R^n$ si~:
		\[\forall \epsilon > 0 : \exists K(\epsilon) \in \N \tq \forall k > K(\epsilon) : \norm {x_k - x} < \epsilon,\]
		ce que l'on écrit~:
		\[\lim_{k \to +\infty}\norm{x_k - x} = 0.\]
		\end{déf}

		\begin{déf} Soit $(x_k) \subset \R^n$ une suite. $(x_{k\,i})_k$ désigne la suite de la $i$ème composante de $(x_k)$. \end{déf}

		\begin{prp}\label{convergenceRnComposantes} La suite $(x_n)_n \subset \R^n$ converge en $x \in \R^n$ si et seulement si chaque suite de composante
		$(x_{k\,i})_k \subset \R$ converge en $x_i$. \end{prp}

		\begin{proof} Dans le sens direct, on suppose $x_k \to x \in \R^n$. Prenons $1 \leq i \leq n$, alors~:
		\[\abs {x_{k\,i} - x_i} \leq \norm {x_k - x} \to 0.\]
		Dans le sens indirect, on suppose que $\forall i \in \{1, \dotsc, n\} : x_{k\,i}$. Alors~:
		\[\norm {x_k - x}^2 \coloneqq \sum_{i=1}^n(x_{k\,i} - x_i)^2 \to 0.\]
		\end{proof}

		\begin{cor} Soit $(x_k)_k \subset \R^n$. Pour tout $1 \leq p, q \leq n-1$, la suite $(x_k)_k \coloneqq (y_k, z_k)_k \subset \R^p \times \R^q$
		converge si et seulement si $(y_k)$ converge dans $\R^p$ et $qz_k)$ converge dans $\R^q$. \end{cor}

		\begin{proof} Trivial par la proposition~\ref{convergenceRnComposantes}. \end{proof}

		\begin{déf} Une suite $(x_k)_k \subset \R^n$ est dite de Cauchy si $\forall 1 \leq i \leq n$, la suite $(x_{k\,i})_k \subset \R$ est de Cauchy. \end{déf}

		\begin{thm} La suite $(x_k)_k \subset \R^n$ converge si et seulement si elle est de Cauchy. \end{thm}

		\begin{lem}[Inégalité de Young] \[\forall a, b \in \R^+, p, q \in (0, +\infty) \tq p+q=pq : \frac {a^p}p + \frac {b^q}q \geq ab.\] \end{lem}

		\begin{proof} Par les propriétés de la fonction $\log$ (concavité), on a~:
		\[\log(ab) = \log\left(a^{\frac pp}\right) + \log\left(b^{\frac qq}\right) = \frac 1p\log\left(a^p\right) + \frac 1q\log\left(b^q\right) \leq \log\left(\frac {a^p}p + \frac {b^q}q\right).\]
		La fonction $\exp$ est croissante, donc on peut composer les deux membres de l'inégalité avec $\exp$, et on obtient bien l'inégalité de Young.
		\end{proof}

		\begin{déf} Soit $x \in \R^n$ un vecteur. On définit la norme $\norm \cdot_p : \R^n \to \R$ telle que~:
		\[\norm x_p = \left(\sum_{k=1}^n\abs{x_k}^p\right)^{\frac 1p}.\]
		\end{déf}

		\begin{thm}[Inégalité de Minkowski\footnote{Généralisation de l'inégalité de Cauchy-Schwartz en dimension $n \geq 1$}]
		Soient $p, q \in [1, +\infty)$ tels que $p+q = pq$. Alors $\forall x, y \in \R^n :$
		\[\abs {\sum_{i=1}^nx_iy_i} \leq \sum_{i=1}^n\abs{x_iy_i} \leq \left(\sum_{i=1}^n\abs {x_i}^p\right)^{\frac 1p} \left(\sum_{i=1}^n\abs {y_i}^q\right)^{\frac 1q}.\]
		\end{thm}

		\begin{proof} Par Young, on a le cas où $\sum_{k = 1}^n\abs {x_k}^p = \sum_{k=1}^n\abs{y_k}^q = 1$~:
		\[\abs{x_k}\abs{y_k} \leq \frac {\abs{x_k}^p}p + \frac {\abs {y_k}^q}q.\]
		En sommant sur $k$ de $1$ à $n$, on obtient~:
		\[\sum_{k = 1}^n\abs{x_k}\abs{y_k} \leq \frac 1p\sum_{k=1}^n\abs{x_k}^p + \frac 1q\sum_{k=1}^n\abs{y_k}^q = \frac 1p + \frac 1q = 1
		= \left(\sum_{k=1}^n\abs {x_k}^p\right) \left(\sum_{k=1}^n\abs{y_k}^q\right).\]

		Si $\norm x_p \neq 1$ ou $\norm y_q \neq 1$, alors on renormalise les vecteurs avec $x' \coloneqq \frac x{\norm x_p}$, $y' \coloneqq \frac y{\norm y_q}$.
		On a alors~:
		\[\frac 1{\norm x_p\norm y_q}\sum_{k=1}^n\abs{x_ky_k} = \sum_{k=1}^n\abs{x'_ky'_k} \leq 1,\]
		ou encore~:
		\[\sum_{k=1}^n\abs{x_ky_k} \leq \norm x_p\norm y_q.\]
		\end{proof}

		\begin{déf} On définit $\normfty \cdot \coloneqq \lim_{p \to +\infty} \norm \cdot_p$. Ce qui donne, pour $x \in \R^n$~:
		\[\normfty x = \max_{k=1, \dotsc, n}\abs{x_k}.\]
		\end{déf}

		\begin{déf}\label{equivNormes} Les normes $\norm \cdot_a$ et $\norm \cdot_b$ sont dites équivalentes s'il existe deux constantes $C_1, C_2 \in \R_0^+$
		telles que~:
		\[\forall x \in \R^n : C_1\norm x_b \leq \norm x_a \leq C_2\norm x_b.\]
		\end{déf}

		\begin{rmq} On observe alors que~:
		\[(\norm x_p)^p = \sum_{k=1}^n\abs{x_k}^p \leq n(\max_k\abs{x_k})^p \leq n\sum_{k=1}^n\abs{x_k}^p.\]
		En prenant la racine $p$ème des inégalités, on obtient~:
		\[\norm x_p \leq \sqrt[p]n \normfty x \leq \sqrt[p]n \norm x_p.\]

		Selon la définition~\ref{equivNormes}, on remarque que pour tout $p$, les normes $\norm \cdot_p$ et $\normfty \cdot$ sont équivalentes. On remarque
		donc que la notion de convergence (définie par $\norm \cdot_2$, la norme euclidienne) aurait pu être définie selon une norme $\norm \cdot_p$
		quelconque.\footnote{Pour être complet, toutes les normes sur $\R^n$ sont équivalentes, mais ce résultat dépasse le contenu de ce cours.}
		\end{rmq}

	\subsection{Limite d'une fonction en un point}
		\begin{déf} Soit $f : \R^m \to \R^n$. Si $m > 1$, on dit que $f$ est une fonction à plusieurs variables réelles, et si $n > 1$, on dit que $f$ est à
		valeurs vectorielles. \end{déf}

		\begin{déf}[Adéhrence en dimension $n > 1$] Soit $A \subseteq \R^n$. Un point $a \in \R^n$ est dit \textbf{adhérent} à $A$ si
		$\forall \delta > 0 : \exists x \in A \tq \norm {x-a} < \delta$.
		
		L'ensemble des points adhérents à $A$ s'appelle l'\emph{adhérence} de $A$ et se note $\adh A$. \end{déf}

		\begin{lem} Soient $A \subseteq \R^n$ et $a \in \R^n$. $a \in \adh A$ si et seulement si $\exists (x_k) \subset A \tq x_k \to a$. \end{lem}

		\begin{proof} Pour le sens indirect, soit $(x_k) \subset A$ une suite convergente en $a \in \R^n$.
		Soit $\delta > 0$. On sait qu'il existe $N(\delta) \in \N^*$ tel que $\forall n \geq N(\delta) : \norm {x_n - a} < \delta$. Donc $a \in \adh A$.

		Pour le sens direct, on prend $a \in \adh A$. On sait donc que pour tout $\delta > 0$, il existe $x(\delta) \in A \tq \norm {x(\delta) - a} < \delta$.
		On construit alors une suite $(x_k) \subset A$ telle que $x_k \coloneqq x(1/k)$ pour tout $k \geq 1$. Dès lors, soit $\delta > 0$. On sait qu'il existe
		$N = \left\lceil\frac 1\delta\right\rceil$ tel que $\forall n > N : \norm{x_n - a} < \delta$. Donc $(x_k) \subset A$ par définition, et on vient de
		montrer que $x_k \to a$. \end{proof}

		\begin{déf} La boule ouverte a été définie au~\ref{bouleOuverte}~; on définit également la boule fermée de centre $a$ et de rayon $r$ par~:
		\[\bar B(a, r) \coloneqq \{x \in \R^n \tq \norm{x-a} \leq r\} = B(a, r) \cup \{x \in \R^n \tq \norm{x-a} = r\}.\]
		\end{déf}

		\begin{rmq} L'adhérence d'un ensemble $A$ peut également se définir à l'aide de la boule ouverte~:
		\[\adh A = \{x \in \R^n \tq \forall r > 0 : B(x, r) \cup A \neq \emptyset\}.\]
		\end{rmq}

		\begin{rmq} La boule fermée $\bar B(a, r)$ est l'adhérence de la boule ouverte $B(a, r)$. \end{rmq}

		\begin{déf}[Intérieur en dimension $n > 1$] Soit $A \subset \R^n$. Un élément $a \in \R^n$ est dit \emph{intérieur} à $A$ si
		$\exists \delta > 0 \tq \forall x \in \R^n : \norm {x-a} < \delta \Rightarrow x \in A$. \end{déf}

		\begin{rmq} Géométriquement, on dit que l'intérieur $\intr A$ est l'ensemble~:
		\[\intr A \coloneqq \{x \in \R^n \tq \exists r > 0 \tq B(x, r) \subseteq A\}.\]
		\end{rmq}

		\begin{déf} La frontière de $A$ est l'ensemble $\adh A \setminus \intr A$. On le note $\partial A$. \end{déf}

		\begin{déf} Un ensemble $A \subseteq \R^n$ est~:
		\begin{itemize}
			\item \emph{ouvert} si et seulement si $A = \intr A$~;
			\item \emph{fermé} si et seulement si $A = \adh A$.
		\end{itemize}
		\end{déf}

		\begin{déf}[Limite d'une fonction multivariée à valeur vectorielle]\label{def:limfrmrn} Soient $f : U \subseteq \R^m \to \R^n$, $B \subset \R^m$, et
		$a \in \adh(U \cap B)$. La limite de $f$ dans $B$ pour $x$ tendant vers $a$ existe et vaut $L \in \R^n$ si~:
		\[\forall \epsilon > 0 : \exists \delta > 0 \tq \forall x \in \adh(U \cap B) : \norm{x-a} < \delta \Rightarrow \norm{f(x)-L} < \epsilon.\]
		On écrit cela~:
		\[\lim_{\stackrel B{x \to a}}f(x) = L\qquad\qquad\text{ ou }\qquad\qquad\lim_{\stackrel {x \to a}{x \in B}}f(x) = L.\]
		\end{déf}

		\begin{rmq} Dans la définition~\ref{def:limfrmrn}, $\epsilon$ sert à déterminer une boule ouverte $B(L, \epsilon)$ et $\delta$ sert à déterminer une
		boule ouverte $B(a, \delta)$ tels que~:
		\[f\left(B(a, \delta) \cap (B \cap U)\right) \subset B(L, \epsilon).\]
		\end{rmq}

		\begin{déf} La limite de $f : \R^m \to \R^n$ en $x$ tendant vers $a \in \R^m$ existe et vaut $L \in \R^n$ si et seulement si la limite de $f$ dans
		$B = \R^m$ existe et vaut $L$ pour $x$ tendant vers $a$. On note cela~:
		\[\lim_{x \to a}f(x) = L.\]
		\end{déf}

		\begin{déf} Soient $f : \R^m \to \R^n$, et $a \in \adh\left(\dom f \setminus \{a\}\right)$. Alors la limite pointée~:
		\[\lim_{\stackrel {x \to a}{x \neq a}}f(x) = L\]
		est vérifiée si la limite de $f$ dans $B = \R^m \setminus \{a\}$ existe et vaut $L$.
		\end{déf}

		\begin{prp} Soient $f : U \subset \R^m \to \R^n$, $B \subset \R^m$, et $a \in \adh (B \cap U)$. La limite de $f$ dans $B$ pour $x \to a$ existe et vaut
		$L$ si et seulement si $\forall A \subseteq B : $ la limite de $f$ dans $A$ de $x \to a$ existe et vaut $L$.
		\end{prp}

		\begin{prp}\label{prp:limssisuitesconvergent} Soient $f : \R^m \to \R^n$, et $a \in \adh(\dom f)$. Alors~:
		\[\lim_{x \to a}f(x) = L \iff \forall (x_n)_n \subset \dom f : x_n \to a \Rightarrow (f(x_n))_n \to L.\]
		\end{prp}

		\begin{proof} Supposons d'abord que $f(x) \to L$ quand $x \to a$.

		Soit $\epsilon > 0$. On sait qu'il existe $\delta > 0$ tel que~:
		\[\forall x \in \R^m : \norm{x-a} < \delta \Rightarrow \norm{f(x)-L} < \epsilon.\]
		Soit $(x_k) \subset \R^m$ une suite convergente en $a$. On sait qu'il existe $N > 0$ tel que~:
		\[\forall n > N : \norm{x_n-a} < \delta.\]
		Dès lors~:
		\[\norm{x_n-a} < \delta \Rightarrow \norm{f(x_n)-L} < \epsilon.\]

		Supposons ensuite que pour toute suite $(x_k) \subset \R^m$ convergente en $a$, on ait $(f(x_k))_k \subseteq \R^n$ convergente en $L$.
		Supposons par l'absurde que $f(x) \not \to L$ pour $x \to a$. On sait alors qu'il existe $\epsilon_0$ tel que~:
		\[\forall \delta_0 > 0 : \exists x \in \R^m \tq \norm{x_n-a} < \delta \land \norm{f(x_n)-L} \geq \epsilon_0.\]
		En particulier, pour $\delta_0 = \frac 1n$, on trouve~:
		\[\forall n \geq 1 : \exists x_n \in \R^m \tq \norm{x_n-a} < \delta_0 = \frac 1n \land \norm{f(x_n)-L} \geq \epsilon_0.\]
		Or, par hypothèse, o na supposé que pour toute suite $(x_k) \subset \R^m \tq x_k \to a$, la suite $(f(x_k))_k$ convergeait en $L$.
		Ce qui est une contradiction, et donc $f(x) \to L$.
		\end{proof}

	\subsection{Continuité d'une fonction}
		\begin{déf} Soient $f : \R^m \to \R^n$, et $a \in \dom f$. On dit que $f$ est continue en $a$ si~:
		\[\lim_{x \to a}f(x) = f(a),\]
		c'est-à-dire~:
		\[\forall \epsilon > 0 : \exists \delta >0 \tq \forall x \in \dom f : \norm {x-a} < \delta \Rightarrow \norm {f(x)-f(a)} < \epsilon.\]
		\end{déf}

		\begin{déf} Une fonction $f : \R^m \to \R^n$ est dite continue si elle est continue en tout point de son domaine. \end{déf}

		\begin{prp} Soient $f : \R^m \to \R^p$ et $g : \R^p \to \R^n$ deux fonctions continues. Alors $(g \circ f)$ est également continue. \end{prp}

		\begin{proof} Soit $\epsilon > 0$. On sait (par la continuité de $g$) qu'il existe $\delta_g > 0$ tel que~:
		\[\forall y \in \R^p : \norm{y - f(a)} < \delta_g \Rightarrow \norm{g(y) - g(f(a))} < \epsilon.\]
		On sait également, par continuité de $f$ qu'il existe $\delta_f > 0$ tel que~:
		\[\forall x \in \R^m : \norm{x - a} < \delta_f \Rightarrow \norm{f(x)-f(a)} < \delta_g.\]
		Dès lors, on trouve~:
		\[\forall x \in \R^m : \norm{x-a} < \delta_f \Rightarrow \norm{f(x)-f(a)} < \delta_g \Rightarrow \norm{g(f(x)) - g(f(a))} < \epsilon.\]
		\end{proof}

		\subsubsection{Fonctions à valeur réelle}
		\begin{déf} On appelle la fonction $\pi_j : \R^m \to \R$ définie par~:
		\[\pi_j((x_1, \dotsc, x_m)) = x_j\]
		la $j$ème fonction de projection.
		\end{déf}

		\begin{lem} La fonction de projection $\pi_j$ est une fonction continue. \end{lem}

		\begin{proof} Soient $\epsilon > 0$, et $a \in \R^m$. Prenons $\delta = \epsilon$. En supposant $\norm{x-a} < \delta$, on trouve~:
		\[\epsilon = \delta > \norm{x-a} = \sqrt {(x_j-a_j)^2 + \sum_{j \neq i}(x_i-a_i)^2} \geq \sqrt {(x_j-a_j)^2} = \abs {\pi_j(x)-\pi_j(a)}.\]
		On a bien $\abs {\pi_j(x)-\pi_j(a)} < \epsilon$.
		\end{proof}

		\begin{prp}\label{prp:continuitérmparsuites} Soient $f : \R^m \to \R$, et $a \in \R^m$. Alors $f$ est continue en $a$ si et seulement si~:
		\[\forall (x_k)_k \subset \R : x_k \to a \Rightarrow (f(x_k))_k \to f(a).\]
		\end{prp}

		\begin{proof} Corollaire direct de la proposition~\ref{prp:limssisuitesconvergent}. \end{proof}

		\begin{rmq} Ce résultat nous permet d'écrire~:
		\[\lim_{k \to +\infty}f(x_k) = f\left(\lim_{k \to +\infty}x_k\right)\]
		pour $f$ continue, ce qui est assez pratique pour les règles de calcul.
		\end{rmq}

		\begin{cor} Soit $f : \R^m \to \R$. S'il existe deux suites $(x_k), (y_k) \subset \R^m$ convergentes en $a$ telles que~:
		\[\lim_{k \to +\infty}f(x_k) \neq \lim_{k \to +\infty}f(y_k),\]
		alors $f$ n'est pas continue. \end{cor}

		\begin{rmq} Ce corollaire est en réalité la contraposée de la proposition~\ref{prp:continuitérmparsuites}. \end{rmq}

		\subsubsection{Fonctions à valeur vectorielle}
		\begin{thm} Une fonction $f = (f_1, \dotsc, f_n) : \R^m \to \R^n$ est continue en $a \in \R^m$ si et seulement si les $n$ fonctions $f_i$ sont
		continues en $a$.
		\end{thm}

		\begin{proof} Si $f$ est continue, alors $\forall i \in \{1, \dotsc, n\} : f_i \coloneqq (\pi_i \circ f)$ est continue par composée de fonctions
		continues.

		Si $f_i$ est continue pour tout $i$, on calcule~:
		\[\lim_{x \to a}\norm{f(x)-f(a)} \coloneqq \lim_{x \to a}\sqrt{\sum_{i=1}^n\abs{f_i(x)-f_i(a)}^2}
		= \sqrt {\sum_{i=1}^n\lim_{x \to a}\abs{f_i(x)-f_i(a)}^2} = 0,\]
		ce qui est équivalent à $\lim_{x \to a}f(x) = f(a)$.
		\end{proof}

		\begin{rmq} Pour déterminer la continuité de $f$, on peut déterminer la continuité de ses composantes. On ne peut cependant pas déterminer la continuité
		des restrictions de la fonction selon les différents arguments !
		\end{rmq}
	
	\subsection{Valeur intermédiaire et ensembles connexes par arcs}
		\begin{déf} Soit $E \subset \R^n$. On dit que l'ensemble $E$ est \emph{connexe par arcs} s'il existe $\gamma : [0, 1] \to E$ une application continue
		telle que $\gamma(0) = p$ et $\gamma(1) = q$ pour tout $p, q \in E$.

		$\gamma$ est appelé un \emph{chemine de $p$ à $q$}.
		\end{déf}

		\begin{prp} L'ensemble des ensembles connexes par arcs de $\R$ sont les intervalles. \end{prp}

		\begin{proof} Montrons que les intervalles sont des ensembles connexes. Soit $I = [p, q]$ un intervalle. Alors $\gamma : t \mapsto tq + (1-t)p$ est
		un chemin de $p$ à $q$.

		Soit $I$ un ensemble connexe par arcs. S'il y a un chemin de $p$ à $q$ dans $I$, alors $\forall r \in \R \tq p < r < q$, il faut $r \ni I$.
		\end{proof}

		\begin{thm}[Image continue d'un ensemble connexe par arcs]\label{thm:imageconnexepararcs} Soit $f : E \subset \R^m \to \R^n$ où $E$ est un ensemble
		connexe par arcs. Si $f$ est continue sur $E$, alors $f(E)$ est connexe par arcs.
		\end{thm}

		\begin{proof} Prenons $p, q \in f(E)$. Par définition, on sait qu'il existe $a, b \in E$ tels que $f(a) = p$ et $f(b) = q$. Soit $\gamma$ un chemin
		de $a$ à $b$ dans $E$. Par continuité de $\gamma$ on sait que $(f \circ \gamma)$ est continue et représente un chemin de $p$ à $q$.
		\end{proof}

		\begin{cor} Soit $f : E \subset \R^m \to \R$ une fonction continue où $E$ est un ensemble connexe par arcs. Si $\alpha < \beta$ sont deux valeurs
		prises par $f$, alors $\forall c \in (\alpha, \beta) : \exists m \in E \tq f(m) = c$.
		\end{cor}

		\begin{proof} Corollaire direct du théorème~\ref{thm:imageconnexepararcs} : $f(E)$ est un intervalle. \end{proof}

		\begin{prp} Il n'existe pas de bijection continue $f : \R^m \to \R$ pour $m > 1$. \end{prp}

		\begin{proof} Supposons par l'absurde qu'il existe $f : \R^n \to \R$ continue et bijective. Prenons $g : \R^m \setminus \{0\} \to \R \setminus \{x\}$,
		la restriction de $f$ à $\R \setminus \{x\}$ où $x = f(0) \in \R^m$. On observe que $\R^m \setminus \{0\}$ est un ensemble connexe par arcs alors
		que $\R \setminus \{x\}$ n'est pas un intervalle, ce qui contredit le théorème~\ref{thm:imageconnexepararcs}.
		\end{proof}

		\begin{rmq} Pour être complet, il est possible de déterminer une bijection de $\R^n$ dans $\R$ mais elle ne peut être continue. Ces ensembles ont donc
		la même cardinalité. Il existe un résultat similaire disant qu'il n'existe pas de bijection continue $\R^m \to \R^n$ pour $n \neq m$, mais qui dépasse
		le contenu de ce cours.
		\end{rmq}

	\subsection{Bornes atteintes et ensembles compacts}
		\begin{thm}[Théorème de Bolzano-Weierstrass dans $\R^n$]\label{thm:BolzWeierRn} Dans $\R^m$, toute suite bornée $(x_k)_k$ contient une sous-suite
		convergente.
		\end{thm}

		\begin{proof} On note $x_{k\,i}$ la $i$ème composante du vecteur $x_k$. Comme $x_k$ est bornée par hypothèse, on sait que pour tout $i$, la suite
		$(x_{k_i})_k$ est également bornée. Par le théorème~\ref{thm:BolzWeieR} de Bolzano-Weierstrass, on sait que chaque composante de la suite contient
		une sous-suite convergente.

		On nomme $(x_{k_{l^{(1)}}\,1})_k$ une sous-suite convergente de $(x_{k\,1})_k$. On s'intéresse à la suite $(x_{k_{l^{(1)}}})_k$. On prend également une
		sous-suite convergente de $(x_{k\,2})_k$ que l'on appelle $(x_{k_{m^{(2)}}\,2})_k$. On prend alors les éléments communs entre $(x_{k_{l^{(1)}}})_k$
		et $(x_{m^{(2)}})_k$ et on appelle cette suite $(x_{k_{l^{(2)}}})$. On répète cet argument au total $m$ fois afin d'obtenir une suite
		$(x_{k_{l^{(m)}}})$ qui est l'intersection entre tous les $(x_{k_{m^{(i)}}})$. Cette suite est une sous-suite de $(x_k)_k$ est est convergente en
		toutes ses composantes par construction.
		\end{proof}

		\begin{prp} Un ensemble $E \subset \R^m$ est fermé et borné si et seulement si~:
		\[\forall (x_k)_k \subset E : \exists (x_{k_l})_l \text{ convergente en }e \in E.\]
		\end{prp}

		\begin{proof} Supposons d'abord que $E$ est fermé et borné. Par Bolzano-Weierstrass, on sait que toute suite admet une sous-suite convergente en une
		valeur $e$ adhérente à $E$. Or puisque $E$ est fermé, on sait que $e \in E$.

		Supposons maintenant que toute suite dans $E$ admet une sous-suite convergente. Prenons $x \in \adh E$. Soit $(x_k)_k \subset E$, une suite convergente
		en $x$. On sait donc qu'il existe une sous-suite de $(x_k)$ qui converge dans $E$ par hypothèse, et par unicité de la limite, on sait que la sous-suite
		converge en $x$. Donc $x \in E$, et donc $\adh E \subset E$, ou encore, $E$ est fermé.

		On montre que $E$ est borné par l'absurde en trouvant $y_i \in E \setminus B(y_{i-1}, R_i)$, qui montre que la suite $(y_i)_i$ n'admet pas de sous-suite
		convergente, ce qui est une contradiction.
		\end{proof}

		\begin{déf} Un ensemble $E \subset \R^m$ est dit \emph{compact} si toute suite $(x_k) \subset E$ admet une sous-suite convergente dans $E$. \end{déf}

		\begin{thm}[Théorème des bornes atteintes]\label{thm:bornesatteintesrn} Soit $\frmr fE$ une fonction continue. Si $E$ est un ensemble fermé borné,
		alors $f(E) \subseteq \R$ est un ensemble fermé borné. En particulier, $f$ est bornée et atteint ses bornes.
		\end{thm}

		\begin{proof} Notons $m, M$ respectivement les quantités $\inf_{x \in E}f(x)$ et $\sup_{x \in E}f(x)$. Montrons que $f$ est bornée, et donc supposons par
		l'absurde que $m = -\infty$ ou $M = +\infty$.  On trouve donc une suite $(x_k) \subset E$ telle que $\abs {f(x_k)} \to +\infty$ (ou encore
		$\forall k \in \N : \abs{f(x_k)} \geq k$). Puisque $E$ est un ensemble fermé borné, on peut extraire $(x_{k_n})$ une sous-suite de $(x_k)$ convergente
		en $x \in E$. Par continuité de $f$, on sait que $f(x_{k_n})$ est également une suite convergente en $f(x)$. La fonction $f$ est donc bornée, ce qui
		contredit l'hypothèse.

		Montrons ensuite que $m \in f(E)$. (L'argument pour montrer $M \in f(E)$ est identique.) Par définition de l'infimum, il existe $(x_k) \subset E$ une
		suite telle que $f(x_k)$ est convergente en $m$. Puisque $E$ est fermé borné, on peut extraire une sous-suite $(x_{k_n})$ telle que $x_{k_n} \to x \in E$.
		On sait alors $f(x_{k_n}) \to f(x)$ par continuité de $f$, et donc $m \in f(E)$.

		Montrons maintenant que $f(E)$ est fermé. Prenons $c \in \adh f(E)$. On sait qu'il existe une suite $(x_k) \subset f(E)$ convergente en $c$. Si $x_k$
		est dans $E$, alors il existe $z_k \in E$ tel que $x_k = f(z_k)$. La suite $(z_k) \subset E$ possède une sous-suite $(z_{k_n})$ convergente en $x \in E$.
		Par continuité de $f$, on sait que $f(z_k) \to f(x) \in f(E)$. Or, par unicité de la limite, on a $c = f(x) \in f(E)$.
		\end{proof}

	\subsection{continuité uniforme}
		\begin{déf} Soit $\frmr fE$ une fonction. Si~:
		\[\forall \epsilon > 0 : \exists \delta > 0 \tq \forall x, y \in E : \norm{x-y} < \delta \Rightarrow \abs{f(x)-f(y)} < \epsilon,\]
		alors on dit que que $f$ est \emph{uniformément continue}.
		\end{déf}

		\begin{déf} Une fonction $\frmr fE$ est dite \emph{Lipschitzienne} si~:
		\[\exists \alpha > 0 \tq \forall x, y \in E : \norm{f(x)-f(y)} \leq \alpha\norm{x-y}.\]
		\end{déf}

		\begin{lem} Une fonction Lipschitzienne est uniformément continue. \end{lem}

		\begin{proof} Soit $\frmr fE$ une fonction Lipschitzienne. Soit $\epsilon > 0$. Prenons $\delta = \frac \epsilon\alpha$ où $\alpha$ est le paramètre
		lipschitzien de $f$. Supposons que $\norm{x-y} < \delta$, on trouve alors~:
		\[\abs {f(x)-f(y)} \leq \alpha\norm{x-y} < \alpha\delta = \epsilon.\]
		\end{proof}

		\begin{rmq} On remarque également que l'uniforme continuité implique la continuité. \end{rmq}

		\begin{prp} Soit $\frmr fE$ une fonction continue définie sur $E$ fermé borné. Alors $f$ est uniformément continue. \end{prp}

		\begin{proof} Par Bolzano-Weierstrass. \end{proof}

\newpage
\section{Fonctions différentiables à plusieurs variables}
	\subsection{Rappels}
		On appelle la \emph{différentielle} en $a$ l'application $(\partial_vf)(a)$ qui envoie $v$ sur $\scpr {(\nabla f)(a)}v$, où
		$(\nabla f)(a)$ est le gradient de $f$ au point $a$, donné par le vecteur $\left((\partial_if)(a)\right)_i$.

		\begin{déf} Soit $\frmrn fE$ et soient $a \in \intr E, v \in \R^m$. On dit que \emph{$f$ est dérivable au point $a$ dans la direction $v$} si~:
		\[\forall i \in \{1, \dotsb, n\} : (\partial_if)(a) \coloneqq \lim_{h \to 0}\frac {f_i(a+hv)-f_i(a)}h\qquad\text{existe}.\]
		Dans ce cas, on note~:
		\[(\partial_vf)(a) \coloneqq \lim_{h \to 0}\frac {f(a+hv)-f(a)}h = \left((\partial_vf_i)(a)\right)_i.\]
		\end{déf}

		La définition de différentiabilité des fonctions réelles a été vue à la section~\ref{sec:foncdérivables}. $\frmr fE$ est différentiable en $x \in \R^m$
		s'il existe $u \in \R^m$ tel que~:
		\[\lim_{x \to a}\frac {f(x)-f(a)-\scpr u{x-a}}{\abs {x-a}} = 0.\]

		Si un tel vecteur $u$ existe, il faut que que ce vecteur $u$ soit le gradient $(\nabla f)(a)$ de $f$. On peut dès lors déterminer les différentes
		dérivées directionnelles d'une fonction $\frmr fE$ en connaissant son gradient~:
		\[(\partial_vf)(a) = \scpr {(\nabla f)(a)}v.\]

		On extrapole cette formule dans le cas de fonctions à valeur vectorielle~:
		\[(\partial_vf)(a) = \left((\partial_vf_i)(a)\right)_i = \left(\scpr{(\nabla f_i)(a)}v\right)_i.\]

		Cette formule peut s'écrire de manière matricielle~:
		\[(\partial_vf_i)(a) =
		\begin{pmatrix}
			\pd {f_1}{x_1} & \pd {f_1}{x_2} & \ldots & \pd {f_1}{x_m} \\
			\pd {f_2}{x_1} & \pd {f_2}{x_2} & \ldots & \pd {f_2}{x_m} \\
			     \vdots    &      \vdots    & \ddots &     \vdots     \\
			\pd {f_n}{x_1} & \pd {f_n}{x_m} & \ldots & \pd {f_n}{x_m}
		\end{pmatrix}
		\begin{pmatrix}
			v_1 \\ v_2 \\ \vdots \\ v_m
		\end{pmatrix}.\]

		\begin{déf} La matrice $J_f(a) \coloneqq \left[\pd {f_i}{x_j}\right]$ est appelée la \emph{matrice Jacobienne} de $\frmrn fE$ au point $a$. \end{déf}

		\begin{rmq} La matrice jacobienne (ou le jacobien) est une généralisation du gradient : $J_f(a)$ est une matrice à $n$ lignes si $f$ est à valeurs dans
		$\R^n$. Donc si $n = 1$ (cas de fonction à valeur réelle), alors la matrice ne contient qu'une seule ligne et est donc le vecteur gradient.
		\end{rmq}

	\subsection{Différentiabilité de fonctions à valeur vectorielle}
		\begin{déf} Soient $\frmrn fE$ et $a \in \intr E$. Ont dit que $L : \R^m \to \R^n$ est une \emph{approximation linéaire de $f$ en $a$} si~:
		\[\lim_{x \to a}\frac {\norm {f(x)-f(a)-L(x-a)}}{\norm {x-a}} = 0.\]
		\end{déf}

		\begin{déf} Soient $\frmrn fE$ et $a \in \intr E$. On dit que $f$ est \emph{différentiable au point $a$} s'il existe une approximation linéaire de $f$
		en $a$. Cette application linéaire est appelé la \emph{différentielle} de $f$ en $a$ et se note $(Df)(a)$ ou $(\dif f)(a)$.
		\end{déf}

		\begin{lem} Soient $\frmrn fE$ et $a \in \intr E$. Soient $L_1, L_2$ deux approximations linéaires de $f$ autour de $a$. Alors $L_1 = L_2$. \end{lem}

		\begin{proof} On observe que la somme de deux applications linéaires reste une application linéaire. De plus, on trouve~:
		\begin{align*}
			\frac {\norm {(L_1-L_2)(x-a)}}{\norm{x-a}} &= \frac {\norm {(L_1-L_2)(x-a) + f(a) - f(x) - f(a) + f(x)}}{\norm {x-a}} \\
			&\leq \frac {\norm{-L_1(x-a)-f(a)+f(x)}}{\norm {x-a}} + \frac {\norm {f(x)-f(a)-L_2(x-a)}}{\norm {x-a}}.
		\end{align*}

		Par définition, en faisant tendre $x$ vers $a$ dans ces deux termes, on obtient 0. On a~:
		\[0 \leq \frac {\norm{(L_1-L_2)(x-a)}}{\norm{x-a}} \leq \frac {\norm{-L_1(x-a)-f(a)+f(x)}}{\norm {x-a}} + \frac {\norm {f(x)-f(a)-L_2(x-a)}}{\norm {x-a}}.\]
		Par le théorème du sandwich, on trouve que~:
		\[\lim_{x \to a}\frac {\norm{(L_1-L_2)(x-a)}}{\norm{x-a}} = 0.\]

		Soit $v \in \R^m$ tel que $v \neq 0$. On considère la suite $(x_k) \subset \R^m$ telle que $x_k = a + \frac vk$. Par hypothèse, $a \in \intr E$. Dès
		lors, il existe un voisinage de $a$ entièrement inclus dans $E$. On peut donc trouver $K$ tel que $\forall k > K : x_k \in E$. En faisant tendre $k$
		vers $+\infty$, on retrouve la même limite que juste au-dessus~:
		\[\lim_{k \to +\infty}\frac {\norm{(L_1-L_2)(x_k-a)}}{\norm{x_k-a}} = 0.\]

		Or, par linéarité, on trouve~:
		\[0 = \lim_{k \to +\infty}\frac {\norm{(L_1-L_2)(a+\frac vk-a)}}{\norm{a+\frac vk-a}}
		= \lim_{k \to +\infty}\frac {\frac 1k}{\frac 1k}\frac {\norm{(L_1-L_2)(v)}}{\norm v}.\]

		On trouve effectivement $0 = (L_1-L_2)(v)$ pour tout $v$.
		\end{proof}

		\begin{déf} Soient $f : \R^m \to \R^n$. On dit que $f$ est un petit $o$ de $\norm h$ si~:
		\[\lim_{h \to 0}\frac {\norm {f(h)}}{\norm h} = 0.\]
		Ce la se note $f(h) = o(\norm h)$.
		\end{déf}

		\begin{thm} Soient $\frmrn fE$ et $a \in \intr E$. La fonction $f$ est différentiable en $a$ si et seulement si toutes les composantes $f_i$ pour
		$1 \leq i \leq n$ sont différentiables en $a$. De plus, on détermine $(\dif f)(a) = J_f(a)$ où $J_f(a)$ est la matrice jacobienne.\footnote{
		Pour être entièrement rigoureux, on dit que $(\dif f)(a)$ est \emph{déterminée} par $J_f(a)$ car $(\dif f)(a)$ est une application linéaire et toute
		application linéaire est sous la forme $v \mapsto av$ où $a$ est une matrice. Dans un tel cas, on dit que la matrice $a$ détermine (ou définit)
		l'\emph{application linéaire associée}.}
		\end{thm}

		\begin{proof} Supposons d'abord que $f$ est différentiable en $a$. On sait alors qu'il existe $L$, une application linéaire telle que~:
		\[f(x) = f(a) + L(x-a) + o(\norm{x-a}).\]
		On note $U = [U_{i\,j}]$ la matrice associée à l'application linéaire $L$. Soit $1 \leq k \leq n$. Alors~:
		\begin{align*}
			\lim_{x \to a}\frac {\abs {f_k(x)-f_k(a) - \scpr {U_{k\,\cdot}}{x-a}}}{\norm{x-a}}
			&\leq \lim_{x \to a} \frac 1{\norm{x-a}}\sqrt{\sum_{i=1}^n\abs {f_i(x)-f_i(a)-\scpr{U_{i\,\cdot}}{x-a}}^2} \\
			&= \lim_{x \to a}\frac 1{\norm{x-a}}\norm{\left[f_i(x)-f_i(a)-\sum_{j=1}^m(U_{i\,j}(x_j-a_j))\right]_i} \\
			&= \lim_{x \to a}\frac 1{\norm{x-a}}\norm{[f_i(x)]_i - [f_i(a)]_i - \left[\sum_{j=1}^mU_{i\,j}(x_j-a_j)\right]_i} \\
			&= \lim_{x \to a}\frac 1{\norm{x-a}}\norm{f(x)-f(a)-L(x-a)}.
		\end{align*}

		Or ce dernier membre tend vers 0 par définition de la différentiabilité. Par le théorème du sandwich, en sachant que~:
		\[0 \leq \frac {\abs {f_k(x)-f_k(a) - \scpr {U_{k\,\cdot}}{x-a}}}{\norm{x-a}},\]
		on trouve que~:
		\[\lim_{x \to a}\frac {\abs{f_k(x)-f_k(a)-\scpr{U_{k\,\cdot}}{x-a}}}{\norm{x-a}} = 0,\]
		et donc la composante $f_k$ est différentiable en $a$.

		Supposons maintenant que $f_k$ est différentiable pour tout $1 \leq k \leq n$ et montrons que $f$ est différentiable. On sait que~:
		\[\forall 1 \leq i \leq n : \lim_{x \to a}\frac {f(x)-f(a)-\scpr{U_i}{x-a}}{\norm{x-a}} = 0.\]
		On définit ensuite $U = [U_{i\,j}]$ où $U_{i\,j} \coloneqq (U_i)_j$. Si $L$ est l'application linéaire telle que $L(v) = Uv$, on trouve~:
		\begin{align*}
			\lim_{x \to a}\frac {\norm{f(x)-f(a)-L(x-a)}^2}{\norm{x-a}^2}
			&= \lim_{x \to a}\frac 1{\norm{x-a}^2}\sum_{i=1}^n\abs {f_i(x)-f_i(a)-\scpr{U_{i\,\cdot}}{x-a}}^2 \\
			&= \sum_{i=1}^n\lim_{x \to a}\norm{\frac {\abs{f_i(x)-f_i(a) - \scpr{U_{i\,\cdot}}{x-a}}}{\norm{x-a}}}^2 \\
			&= \sum_{i=1}^n0^2 = 0.
		\end{align*}
		Ce qui veut dire que $f$ est différentiable.

		La seconde partie du théorème est de dire que la différentielle de $f$ est l'application associée à la matrice jacobienne $J_f(a)$.
		Afin de montrer cela, on a vu dans le cas $n=1$ qu'il était obligatoire d'avoir $U_i = (\nabla f_i)(a)$. Par définition de $U$ (mettre les vecteurs
		$U_i$ en lignes pour en faire une matrice), on a bien~:
		\[U_{i\,j} = (U_i)_j = \left((\nabla f_i)(a)\right)_j = \pd {f_i}{x_j}(a).\]
		\end{proof}

		\begin{prp}\label{prp:diffexistanceC} Soient $\frmrn fE$ et $a \in \intr E$. Si $f$ est différentiable en $a$, alors il existe $C, R \in  \R_0^+$ tels
		que~:
		\[\forall x \in B(a, r) : \norm{f(x)-f(a)} \leq \norm{x-a}.\]
		En particulier, $f$ est continue en $a$.
		\end{prp}

	\subsection{Condition suffisante de différentiabilité}
		\begin{déf} Soit $\frmrn fE$ où $E$ est ouvert. Si les dérivées partielles de $f$ existent en tout point $a \in E$ et qu'elles définissent des fonctions
		continues, on dit que $f$ est de classe $C^1$.
		\end{déf}

		\begin{thm}\label{thm:diffssipdcont} Soient $\frmrn fE$ et $a \in \intr E$. S'il existe $\delta > 0$ tel que~:
		\[\forall x \in B(a, \delta), j \in \{1, \dotsc, m\} : \pd f{x_j} : \intr E \to \R\qquad\text{ existe et est continue en $a$},\]
		alors $f$ est différentiable en $a$.
		\end{thm}

		\begin{proof} Considérons le cas où $n=1$. On sait alors revenir au cas $n \gneqq 1$ par le théorème qui dit qu'une fonction $f$ est différentiable si
		et seulement si ses composantes le sont.

		Soient $a \in \intr E, \epsilon > 0$. Par hypothèse, toutes les dérivées partielles sont continues en $a$. Donc, pour tout $1 \leq j \leq m$, il existe
		$\delta_j > 0$ (et $\delta_j < r$\footnote{Cette hypothèse nous permet que les dérivées partielles soient définies sur $B(a, \delta_j) \subset B(a, r)$
		sans souci.}) tel que pour tout $x \in B(a, \delta_j)$, on ait~:
		\[\abs{\pd f{x_j}(x) - \pd f{x_j}(a)} \leq \frac \epsilon m.\]
		On pose $\delta \coloneqq \min_{j = 1, 2, \dotsc, m}\delta_j$. Ainsi, $\forall x \in B(a, \delta)$, l'inéquation précédente est vérifiée.

		Soit $\beta \coloneqq \{e_1, \dotsb, e_m\}$ la base canonique de $\R^m$. On pose la fonction~:
		\[h_1 : [0, 1] \to \R : t \mapsto f(a + t(x-a)_1e_1).\]

		L'évaluation $h_1'(s)$ au point $s \in [0, 1]$ représente la dérivée partielle de $f$ par rapport à $x_1$ au point $a + s(x-a)_1e_1$. Par le théorème de
		la moyenne, on sait qu'il existe $t_1 \in (0, 1)$ tel que~:
		\[f(a + (x-a)_1e_1) - f(a) = h_1(1) - h_1(0) = h_1'(t_1)(x-a)_1 = \pd {}{x_1}f(a + t(x-a)_1e_1)(x-a)_1.\]

		Par choix de $\delta$, on sait que~:
		\[\abs{\pd {}{x_1}f(a + t_1(x-a)_1e_1) - \pd {}{x_1}f(a)} \leq \frac \epsilon m.\]

		En multipliant de part et d'autre par $\abs {(x-a)_1}$, on trouve~:
		\[\abs {f(a + t(x-a)e_1) - f(a) - \pd {}{x_j}f(a)(x-a)_1} \leq \frac \epsilon m\abs {(x-a)_1} \leq \frac \epsilon m \abs {x-a}.\]

		En reproduisant l'argument pour $h_2, \dotsc, h_m$ et en appliquant à nouveau le théorème de la moyenne, on trouve les inégalités suivantes~:
		\[\forall i \in \{1, \dotsc, m\} : \abs{f\left(a+ \sum_{j=1}^i(x-a)_je_j\right) - f\left(a + \sum_{j=1}^{i-1}(x-a)_je_j\right) - \pd f{x_i}(a)}
		\leq \frac \epsilon m \norm{x-a}\]

		En les sommant, on obtient~:
		\[\abs{f(x)-f(a) - \sum_{j=1}^m\pd f{x_j}(a)(x-a)_j} = \abs {f\left(a + \sum_{k=1}^m(x-a)_ke_k\right)-f(a) - \sum_{j=1}^m\pd f{x_k}(a)(x-a)_j}
		\leq \epsilon\norm{x-a},\]
		ce qui prouve que la limite tend vers 0. La fonction $f$ est donc différentiable en $a$.
		\end{proof}

		\begin{rmq} Il est habituellement plus simple d'établir les dérivées partielles et leur continuité que de définir la différentiabilité sur base de la
		définition \emph{brute}. C'est pour cela que le théorème~\ref{thm:diffssipdcont} s'avère très utile dans beaucoup de situations.
		\end{rmq}

	\subsection{Différentiation de fonction composées}
		On a vu que pour $f : \R \to \R^n$ et $g : \R^n \to \R$, la dérivée de la composée était donnée par~:
		\[(g \circ f)(a) = \scpr {(\nabla g)(f(a))}{f'(a)}.\]

		Soient maintenant deux fonctions $f : \R^m \to \R^n$ et $g : \R^m \to \R^p$. Leur composée $(g \circ f) : \R^m \to \R^p$ est bien définie. Supposons
		que $f$ est différentiable en $a$ et a $(\dif f)(a)$ pour différentielle et que $g$ est différentiable en $f(a)$ et a $(\dif g)(f(a))$ pour
		différentielle.

		\begin{thm} Soient $f : A \subset \R^m \to \R^n$ et $g : B \subset \R^n \to \R^p$.Si $f(A) \subseteq B$, $a \in \intr A$, $f(a) \in \intr B$, et si
		$f$ et $g$ sont différentiables respectivement en $a$ et $f(a)$, alors $(g \circ f)$ est différentiable en $a$ et sa différentielle est donnée par~:
		\[(\dif (g \circ f))(a) : \R^m \to \R^p : v \mapsto \left[(\dif g)(f(a)) \circ (\dif f)(a)\right]v = J_g(f(a))J_f(a)v,\]
		où $J_g$ et $J_f$ représentent respectivement la matrice jacobienne de $g$ et de $f$.
		\end{thm}

		\begin{proof} Étant donné que l'on peut réduire la différentiabilité d'une fonction vectorielle à la différentiabilité de ses composantes,
		concentrons-nous sur le cas $p=1$. On pose~:
		\[Q(x) \coloneqq \frac {(\dif g)(f(a))\left(f(x)-f(a)-(\dif f)(a)(x-a)\right)}{\norm {x-a}}
		= \sum_{k=1}^n\pd g{x_k}(f(a))\frac{f_k(x)-f_k(a)-(\dif f_k)(a)(x-a)}{\norm{x-a}}.\]
		Par différentiabilité de $f$, on peut dire que $f_k$ est différentiable en $a$ pour tout $k$. Donc en passant à la limite, on obtient~:
		\[\lim_{x \to a}Q(x) = \sum_{k=1}^n\pd g{x_k}(f(a))\lim_{x \to a}\frac {f_k(x)-f_k(a)-(\dif f_k)(a)(x-a)}{\norm{x-a}} = 0.\]
		Si $f(x) = f(a)$, alors $P(x) = 0$. Sinon, on pose~:
		\begin{align*}
			R(x) &\coloneqq \frac {g(f(x)) - g(f(a)) - (\dif f)(a)(f(x)-f(a))}{\norm{f(x)-f(a)}} \\
			S(x) &\coloneqq \frac {\norm{f(x)-f(a)}}{\norm{x-a}}.
		\end{align*}

		On peut alors réécrire $P(x) = (RS)(x)$. Par la proposition~\ref{prp:diffexistanceC}, on sait que $f(x) \to f(a)$ et par différentiabilité de $g$, on
		sait que $R(x) \to 0$ pour $x \to a$. On sait donc également que $P(x) \to 0$ pour $x \to a$. De là, on trouve~:
		\[0 = \lim_{x \to a}P(x)-Q(x) = \lim_{x \to a}\frac {g(f(x)) - g(f(a)) - (\dif g)(f(a))((\dif f)(a)(x-a)}{\norm {x-a}},\]
		ou encore $(g \circ f)$ est différentiable en $a$ et a $(\dif g)(f(a)) \circ (\dif f)(a)$ pour différentielle.
		\end{proof}

	\subsection{Changement de variables}
		\begin{thm} Soient $U, V \subseteq \R^n$. Soient $f : U \to V$ et $g : V \to \R$ deux fonction admettant des dérivées partielles continues.
		Alors la fonction définie par $h : U \to \R : x \mapsto (g \circ f)(x)$ admet des dérivées partielles définies par~:
		\[\pd h{y_j}(a) = \sum_{i=1}^n\pd g{x_i}(f(a))\pd {f_i}{y_j}(a).\]
		\end{thm}

		\begin{proof} Par le théorème~\ref{thm:diffssipdcont}, on sait que les fonctions $f$ et $g$ sont différentiables. On sait également que sa
		différentielle est donnée par~:
		\[(\dif h)(a) = (\dif g)(f(a)) \circ (\dif f)(a).\]
		On peut exprimer ses dérivées partielles (qui existent puisque $h$ est différentiable) comme~:
		\[\pd h{y_j} = (\dif h)(a)(e_j),\]
		où $e_j$ est un vecteur de la base canonique de $\R^n$.
		On peut alors exprimer la différentielle de $f$ par la différentielle de ses composantes~:
		\[(\dif f)(a)(e_j) = \left[\pd {f_i}{x_j}(a)\right]_i \in \R^n.\]

		Également, on peut écrire pour tout $u, b \in \R^n$~:
		\[(\dif g)(b)(u) = \sum_{i=1}^n\pd g{x_i}(b)u_i.\]
		En posant $u \coloneqq (\dif f)(a)(e_j)$ et $b \coloneqq f(a)$, cela revient à écrire~:
		\[(\dif g)(f(a))((\dif f)(a)(e_j)) = \sum_{i=1}^n\pd g{x_i}(f(a))\pd {f_i}{y_j}(a).\]
		On remarque que~:
		\[(\dif g)(f(a))((\dif f)(a)(e_j)) = \pd h{y_j}.\]
		\end{proof}

		\begin{rmq} En changeant ainsi les variables, on passe de $x_k$ qui sont des variables indépendantes à une collection de fonctions $f_k$ qui sont liées
		aux variables $x_k$.
		\end{rmq}

\newpage
\section{Intégration de fonctions à plusieurs variables}
	\subsection{Introduction}
		\begin{rmq} Il existe deux théories principales de l'intégration : l'intégration de Riemann et l'intégration de Lebesgue. L'intégration de Lebesgue est
		intuitivement \emph{meilleure} que l'intégration de Riemann au sens que si $f$ est une fonction intégrable au sens de Riemann, alors elle l'est également
		au sens de Lebesgue et la valeur de l'intégrale concorde alors qu'il existe des fonctions\footnote{Dont la fonction de Dirichlet définie par
		$f(x) = 1$ si $x \in \Q$ et $f(x) = 0$ si $x \not \in \Q$.} qui sont intégrables au sens de Lebesgue et qui ne le sont pas au sens de Riemann.

		L'intégration de Riemann est cependant plus simple à définir et à comprendre, Lebesgue dépasse donc le cadre de ce cours.
		\end{rmq}

	\subsection{Intégrale de Darboux sur un rectangle}
		\begin{déf} On définit un \emph{rectangle} $R$ dans $\R^n$ par~:
		\[R = \prod_{i=1}^n[a_i, b_i] = \{x \in \R^n \tq \forall 1 \leq i \leq n : a_i \leq x_i \leq b_i\}.\]
		\end{déf}

		\begin{déf} Une partition d'un intervalle $I = [a, b] \subset \R$ est un choix de $k$ nombres $t_i \in I$ tels que~:
		\[a = t_0 < t_1 < \dotsb < t_{k-1} < t_k = b.\]

		Une partition rectangulaire d'un rectangle $R = \prod_k[a_k, b_k] \subset \R^n$ est une partition de chaque intervalle de $R$.
		Cela revient à choisir $t_{i\,j}$ tels que~:
		\begin{align*}
			a_1 &= t_{1\,0} < t_{1\,1} < \dotsb < t_{1\,k_1-1} < t_{1\,k_1} = b_1 \\
			a_2 &= t_{2\,0} < t_{2\,1} < \dotsb < t_{2\,k_2-1} < t_{2\,k_2} = b_2 \\
			\vdots \\
			a_n &= t_{n\,0} < t_{n\,1} < \dotsb < t_{n\,k_n-1} < t_{n\,k_n} = b_n.
		\end{align*}
		\end{déf}

		\begin{déf} On définit $R_j$ pour $1 \leq j \leq p$ (où $p$ est le nombre de \emph{sous-rectangles} de la partition $\mathcal P$) comme étant le $j$ème
		rectangle. On définit ensuite es valeurs maximales et minimales atteintes par rectangle par~:
		\begin{align*}
			m_j &\coloneqq \inf\{f(x) \tq x \in R_j\} \\
			M_j &\coloneqq \sup\{f(x) \tq x \in R_j\}.
		\end{align*}
		\end{déf}

		\begin{déf} Soit $f : R \subset \R^n \to \R$ une fonction définie sur un rectangle de $\R^n$, et $\mathcal P$ une partition rectangulaire de $R$. Les
		sommes~:
		\begin{align*}
			\underline S(f, \mathcal P) &\coloneqq \sum_jm_j\vol(R_j), \\
			\overline  S(f, \mathcal P) &\coloneqq \sum_jM_j\vol(R_j)
		\end{align*}
		s'appellent respectivement la \emph{somme inférieure} et la \emph{somme supérieure} de Darboux de $f$ par rapport à $\mathcal P$.
		\end{déf}

		\begin{déf} On définit les intégrales respectivement \emph{inférieure} et \emph{supérieure} de Darboux de $f$ par~:
		\begin{align*}
			\underline \int_R f(x)\dif x &\coloneqq \sup_{\mathcal P} \underline S(f, \mathcal P), \\
			\overline  \int_R f(x)\dif x &\coloneqq \inf_{\mathcal P} \overline  S(f, \mathcal P).
		\end{align*}
		\end{déf}

		\begin{rmq} Il suit directement de la définition que pour toute partition $\mathcal P$, on a~:
		\[\underline S(f, \mathcal P) \leq \overline S(f, \mathcal P),\]
		et donc, en particulier~:
		\[\underline \int_R f(x)\dif x \leq \overline \int_R f(x)\dif x.\]
		\end{rmq}

		\begin{déf} On dit que la fonction $f : R \subset \R^m \to \R$ est intégrable au sens de Riemann si les intégrales inférieure et supérieure de
		Darboux sont égales~:
		\[\underline \int_R f(x)\dif x = \overline \int_R f(x)\dif x.\]
		Dans ce cas, on définit l'\emph{intégrale de Riemann} de la fonction $f$ par~:
		\[\int_R f(x)\dif x,\]
		qui vaut cette valeur commune.
		\end{déf}

		\begin{déf} Soit $f : R \subset \R^m \to \R^n$. $f$ est dite intégrable sur $R$ si toutes ses composantes sont intégrables, et on définit son intégrale
		par le vecteur correspondant aux intégrales des composantes~:
		\[\int_R f(x)\dif x = \left[\int_R f_i(x)\dif x\right]_i \in \R^n.\]
		\end{déf}

		\begin{lem}\label{lem:intégrablesiepsilonP(epsilon)} Soit $f : R \to \R$ bornée. Si pour tout $\epsilon > 0$, il existe $\mathcal P(\epsilon)$ une
		partition rectangulaire de $R$ telle que~:
		\[\overline S(f, \mathcal P(\epsilon)) - \underline S(f, \mathcal P(\epsilon)) \leq \epsilon,\]
		alors $f$ est intégrable au sens de Riemann.
		\end{lem}

		\begin{rmq} À nouveau, et pour la plupart des résultats qui suivront, le fait que la définition d'une intégrale d'une fonction à valeur vectorielle est
		définie par le vecteur dont les composantes sont les intégrales des composantes implique que les résultats basés sur une fonction
		$f : R \subset \R^m \to \R$ peuvent être étendus aux fonctions $f : R \subset \R^m \to \R^n$.
		\end{rmq}

		\begin{proof} Supposons par l'absurde que le lemme soit faux et donc que sous ces hypothèses, il existe $f$ non intégrable au sens de Riemann\footnote{
		À compter d'ici, le terme \emph{intégrable} fera toujours référence à la notion d'\emph{intégrable au sens de Riemann}.}. On peut alors prendre~:
		\[\epsilon = \inf_{\mathcal P}\overline S(f, \mathcal P) - \sup_{\mathcal P}S(f, \mathcal P) \gneqq 0.\]
		On sait par les hypothèses qu'il existe $\mathcal P(\epsilon)$ une partition de $R$ telle que~:
		\[\overline S(f, \mathcal P(\epsilon)) \leq \underline S(f, \mathcal P(\epsilon)) + \epsilon
		= \underline S(f, \mathcal P(\epsilon)) + \inf_{\mathcal P}\overline S(f, \mathcal P) - \sup_{\mathcal P}\underline S(f, \mathcal P).\]
		De plus, par définition, on sait que~:
		\[\inf_{\mathcal P}\overline S(f, \mathcal P) \leq \overline S(f, \mathcal P(\epsilon))
		\lneqq \underline S(f, \mathcal P(\epsilon)) + \inf_{\mathcal P}\overline S(f, \mathcal P) - \sup_{\mathcal P}\underline S(f, \mathcal P).\]
		En soustrayant $\inf_{\mathcal P}$ de part et d'autre, on obtient~:
		\[\sup_{\mathcal P}\underline S(f, \mathcal P) < \underline S(f, \mathcal P(\epsilon)),\]
		ce qui est une contradiction, par définition du supremum.
		\end{proof}

		\begin{prp} Si $R \subset \R^m$ est un rectangle et $f : E \to \R$ est continue, alors $f$ est intégrable. \end{prp}
		
		\begin{proof} Puisque $E$ est un rectangle, il est fermé borné, et donc $f$ est bornée également (par sa continuité). $f$ est également uniformément
		continue (puisque continue sur un ensemble fermé borné). Dès lors, pour tout $\epsilon > 0$, il existe $\delta > 0$ tel que~:
		\[\forall x, x' \in R : \norm{x-x'} < \delta \Rightarrow \abs{f(x)-f(x')} < \frac \epsilon{\vol(R)}.\]
		En choisissant $\mathcal P$ de manière à ce que chaque $R_j$ soit contenu dans une boule ouverte de diamètre $\delta$. On trouve donc, pour tout
		$R_j$ dans $\mathcal P$~:
		\[M_j(f) - m_j(f) = \sup_{R_j}f(x) - \inf_{R_j}f(x) < \frac \epsilon{\vol(R)}.\]

		On a donc construit une partition $\mathcal P$ telle que~:
		\[\overline S(f, \mathcal P) - \underline S(f, \mathcal P) = \sum_j\left(M_j(f)-m_j(f)\right)\vol(R) < \frac \epsilon{\vol(R)}\vol(R) = \epsilon.\]
		Dès lors, par le lemme~\ref{lem:intégrablesiepsilonP(epsilon)}, sait que $f$ est intégrable.
		\end{proof}

		\begin{prp}[Propriétés des intégrales]
		\begin{enumerate}
			\item Soient $f, g : R \subset \R^m \to \R$ intégrables et $\alpha, \eta \in \R$. Alors $(\alpha f + \beta g)$ est intégrable telle que~:
				\[\int_R (\alpha f + \beta g)(x)\dif x = \alpha \int_R f(x)\dif x + \beta \int_R g(x)\dif x.\]
			\item Soient $f, g : R \subset \R^m \to \R$ intégrables telles que pour tout $x \in R$, on a $f(x) \geq g(x)$, alors~:
				\[\int_R f(x)\dif x \geq \int_R g(x)\dif x.\]
			\item Soit $f : R \subset \R^m \to \R$ intégrable. Alors $\abs{f(x)}$ est également intégrable telle que~:
				\[\int_R \abs{f(x)}\dif x \geq \abs{\int_R f(x)\dif x}.\]
			\item Soit $R \subset \R^m$ un rectangle et soient $P, S$ tels que $R = P \cup S$ où $P$ et $S$ sont également des rectangles n'ayant pour seule
				intersection éventuelle leurs bords. Soit $f : R \to \R$. Alors $f$ est intégrable, si et seulement si $f$  réduit à $P$ et $f$ réduit à $S$
				sont intégrables. De plus~:
				\[\int_R f(x)\dif x = \int_P f(x)\dif x + \int_S f(x)\dif x.\]
		\end{enumerate}
		\end{prp}

		\begin{proof} Les démonstrations sont similaires à celles concernant les fonctions univariées. \end{proof}

		\begin{déf} Soit $E$ un ensemble quelconque (discret, infini dénombrable, infini indénombrable, etc.) Alors on définit la \emph{fonction caractéristique}
		de $E$\footnote{Également appelée \emph{fonction} indicatrice de l'ensemble $E$.} par~:
		\[1_E : x \mapsto \begin{cases}1 &\text{ si }x \in E\\0 &\text{ sinon}\end{cases}.\]
		\end{déf}

		\begin{rmq} Cette fonction indicatrice permet, entre autres, de définir une intégrale sur un ensemble qui n'est pas un rectangle. En effet, soit $S$
		un sous-ensemble de $\R^m$. Si $S$ n'est pas un rectangle, o peut déterminer un rectangle $R$ tel que $S \subset R$. Dans ce cas, on intègre la fonction
		$(1_Sf)(x)$ sur le rectangle $S$. La fonction s'annulant sur tous les points de $R \setminus S$ c'est bien l'aire de $f$ sur $S$ qui est obtenue.

		Attention tout de même~: la fonction $(1_Sf)(x)$ n'est (à priori) pas continue.
		\end{rmq}

	\subsection{Critère de Lebesgue}

\end{document}
