\documentclass{article}

\usepackage{times}
\usepackage[cm]{fullpage}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{commath}
\usepackage[bottom]{footmisc}
\usepackage[latin1]{inputenc}
\usepackage[parfill]{parskip}
\usepackage{titlesec}  % reduce space between paragraphs
\usepackage{interval}

\DeclareMathOperator{\arctanh}{arctanh}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\R}{\mathbb R}
\DeclareMathOperator{\intr}{int}
\DeclareMathOperator{\adh}{adh}

\titlespacing\paragraph{0pt}{1pt plus 1pt minus 1pt}{4pt plus 1pt minus 1pt}
\titlespacing\subparagraph{0pt}{1pt plus 0pt minus 0pt}{1pt plus 1pt minus 1pt}

\title{Calcul différentiel et intégral}
\author{R. Petit}

\begin{document}

\pagenumbering{Roman}
\maketitle
\tableofcontents
\pagebreak
\clearpage
\setcounter{page}{1}
\pagenumbering{arabic}

\newpage

\section{Intuitions}
    \subsection{Les dérivées}
        \subsubsection{Définition}
            Alors que l'analyse nait au XVII$^e$ siècle, ce n'est que pendant le XIX$^e$ que les outils nécessaires à une
            expression rigoureuse ont été à disposition des mathématiciens. L'analyse a pour notion centrale celle de \textit{variation}.
            Intuitivement, la notion de variation instantanée d'une quantité $f(t)$ peut être décrite de la sorte :

            \[V(t, \Delta t) = \frac {f(t + \Delta t) - f(t)}{\Delta t}\]

            pour une valeur $\Delta t$ étant \textit{de plus en petite}. On dit donc que $\Delta t$ \textit{tend vers 0}. Attention
            cependant car le manque de rigueur et le manque d'outils adaptés à la manipulation de données infinitésimales amènent à des
            paradoxes et des résultats illogiques voire inexplicables.

            Selon la définition de variation vue ci-dessus, nous pouvons exprimer la dérivée comme étant la variation instantanée d'une
            fonction $f : \R \to \R$ que nous notons $f'(t)$. Plus précisément, la variation \textit{instantanée} implique
            que $\Delta t$ soit \textit{infiniment petit}, ce qui s'écrit ainsi :

            \[f'(x) = \lim_{\Delta t \to 0}\frac {f(t + \Delta t) - f(t)}{\Delta t}\]

            \paragraph{Remarque}
                Pour des \textit{petites valeurs} de $\Delta t$, l'approximation suivante est admissible :

                \[f(t + \Delta t) \simeq f(t) + f'(t)\Delta t\]

        \subsubsection{Exemples}
            \paragraph{Fonction affine}
                Soit une fonction $f : \R \to \R : x \mapsto ax + b$. La dérivée de $f$ est l'expression suivante :

                \[f'(x) = \lim_{h \to 0}\frac {f(x + h) - f(x)}{h} = \lim_{h \to 0}\frac {a(x + h) + b - (ax + b)}{h} =
                  \lim_{h \to 0}\frac {ax + ah + b - ax - b}{h} = \lim_{h \to 0}\frac {ah}{h} = a\]

                Il \textit{doit} sembler intuitif qu'étant donné que la fonction $f$ est une fonction dont le graphe est une droite, sa
                dérivée (donc sa variation instantanée) est constante sur tout son domaine du fait que la variation d'une fonction uniformément
                (dé)croissante est constante.

                Plus spécifiquement, si $a = 0$, la fonction est une fonction \textit{constante} : $f : \R \to \R : x \mapsto b$.
                De ce fait, sa dérivée doit être nulle car une fonction croissante n'a pas de variation (par définition). La quantification de
                cette variation doit donc être représentée par 0. Cela peut se montrer également aisément depuis la définition de la dérivée :

                \[f'(x) = \lim_{h \to 0}\frac {f(x+h) - f(x)}{h} = \lim_{h \to 0}\frac {b - b}{h} = 0\]

            \paragraph{Fonction du second degré}
                Soit une fonction $f : \R \to \R : x \mapsto ax^2 + bx + c$. Sa dérivée peut être calculée ainsi :

                \[\begin{aligned}
                      f'(x) &= \lim_{h \to 0}\frac {f(x+h) - f(x)}{h} = \lim_{h \to 0}\frac {a(x+h)^2 + b(x+h) + c - (ax^2 + bx + c)}{h} \\ &=
                      \lim_{h \to 0}\frac {ax^2 + 2axh + ah^2 + bx + bh + c- ax^2 - bx - c}{h} = \lim_{h \to 0}\frac {2axh + ah^2 + bh}{h} =
                      \lim_{h \to 0} 2ax + ah + b = 2ax + b
                \end{aligned}\]

                Ici, la dérivée est une fonction de $x$, ce qui indique que selon la valeur de $x$ à laquelle nous voulons évaluer la
                dérivée, la variation représentée est susceptible de différer.

            \paragraph{Fonction valeur absolue}
                Soit une fonction $f : \R \to \R : x \mapsto \left\{\begin{aligned}&\text{$+x$ si $x \ge 0$} \\&\text{$-x$ si $x < 0$}\end{aligned}\right.$.
                Afin de déterminer l'accroissement instantané de $f$ au point d'abscisse $x=0$, il faut repasser par la définition :

                \[f'(a) = \lim_{h \to 0}V_f(a, h)\]

                Donc $f'(0)$ peut être déterminé de la manière suivante :

                \[f'(0) = \lim_{h \to 0}V_f(0, h) = \lim_{h \to 0}\frac {f(0+h) - f(0)}{h} = \lim_{h \to 0}\frac {f(h)}{h}\]

                Les cas $h < 0$ et $h \ge 0$ doivent être traités séparément de par la définition de la fonction. Pour le premier cas,
                la dérivée est :

                \[f'(0) = \lim_{h \to 0} \frac hh = 1\]

                Et pour le second cas,

                \[f'(0) = \lim_{h \to 0} \frac {-h}{h} = -1\]

                Le résultat est tout à fait cohérent par rapport au graphique de la fonction $x \mapsto |x|$ car cette fonction a deux
                accroissements différents : l'un à gauche de 0, l'autre à droite de 0. Cependant, il est impossible d'exprimer la valeur
                de la dérivée de $f$ au point 0. On dit de $f$ qu'elle n'est pas dérivable en 0.

            \paragraph{Fonction exponentielle}
                Soit une fonction $f : \R \to \R : x \mapsto a^x$ avec $a \in \R_0^+$. À nouveau, pour définir sa
                dérivée, il nous faut repasser par la définition :

                \[f'(x) = \lim_{h \to 0}\frac {a^{x+h} - a^x}{h} = \lim_{h \to 0}\frac {a^x(a^h - 1)}{h} = a^x\lim_{h \to 0}V(0, h) = a^xf'(0)\]

                Ce développement nous indique que pour connaitre la dérivée de $f$ au point $x$, il nous faut connaitre la dérivée de $f$
                au point $0$. Il est possible d'observer sur des esquisses de graphique que $f'(0)$ dépend de $a$ de manière croissante.
                Faire croître $a$ impliquera une croissance de $f'(0)$. Il existe cependant un nombre $\in \R$ tel que $f'(0) = 1$.
                Ce nombre est $e \simeq 2.718$, ce qui implique $f'(x) = e^xf'(0) = e^x = f(x)$. La fonction $x \mapsto e^x$ (et ses
                multiples) sont leur propre dérivée.

        \subsubsection{Dérivée d'ordre supérieur}
            Étant donné que la dérivée d'une fonction quelconque $f$ (en supposant qu'elle est dérivable) est également une fonction \\
            ($f' : \R \to \R : x \mapsto f'(x)$), nous pouvons à nouveau dériver cette fonction (en supposant que la dérivée
            soit toujours dérivable). Il est alors question de \textit{dérivée seconde}, telle que $f''(x) = (f')'(x) = ((f)')'(x)$.
            Pour noter la $k^e$ dérivée, il existe la notation suivante : $f^{(k)}$ (car répéter $k$ fois le symbole \textit{prime} (')
            est contre-productif en temps de lecture \textbf{et} d'écriture).

            Ces mêmes dérivées d'ordre supérieur à 1 ont leur importance et leur cohérence : tant $f'(x)$ est la variation de la quantité
            $f(x)$, tant $f''(x)$ est la variation de $f'(x)$. La dérivée seconde représente donc l'accroissement de l'accroissement
            (ou la variation de la variation). Elle nous donne donc une information sur comment la variation évolue (en dynamique,
            si on représente la position d'un mobile par la fonction $x(t)$, $x'(t)$ représente la variation de la position, à savoir la
            vitesse (donc $x'(t) = v(t)$). Cependant, $x''(t)$ représente la variation de la vitesse, à savoir l'accélération, d'où
            $x''(t) = v'(t) = a(t)$).

            De plus, la dérivée seconde a une autre interprétation graphique : si $f''(x) > 0$, nous savons que $f'(x)$ a une pente positive
            donc $f'(x)$ est croissante. Cela implique que $f(x)$ est croissante aussi mais de plus en plus croissante. Autrement dit,
            le graphe de $f(x)$ est \textit{concave} aux alentours de $(x, f(x))$. De manière similaire, lorsque $f''(x) < 0$, $f(x)$ est
            de moins en moins décroissante et donc le graphe de $f(x)$ est \textit{convexe} aux alentours de $(x, f(x))$.

        \subsubsection{Notation de Leibniz}
            En reprenant la définition de la variation donnée plus haut, nous pouvons définir $\delta f$ et $\delta x$ comme étant
            respectivement la variation de la quantité $f(x)$ et la variation de la quantité $x$. Nous avons donc :

            \[V(x, h) = \frac {f(x+h) - f(x)}{h} = \frac {\delta f}{\delta x}\]

            Cela implique que nous ayons $\delta f = f(x + h) - f(x)$, ce qui est bien la variation de la quantité $f(x)$, et que nous
            ayons $\delta x = h = (x + h) - (x)$, ce qui est bien la variation de la quantité $x$.

            Or, nous avions défini la dérivée comme étant la variation instantanée, à savoir $f'(x) = \lim_{h \to 0}V(x, h)$, qui selon
            la notation de Leibniz correspond à $f'(x) = \lim_{h \to 0}\frac {\delta f}{\delta x}$. Leibniz a cependant instauré une
            seconde notation correspondant non plus à la variation comme la notation $\delta$ mais bien à la dérivée. Cette notation est :

            \[\od{f}{x} = \lim_{h \to 0}\frac {\delta f}{\delta x}\]

            Attention cependant à ne pas utiliser cette valeur comme étant un quotient de nombres réels : la notation $\od fx$ n'a de sens
            que lorsque la limite a été faite. Et comme $\lim_0 \frac {F(h)}{G(h)} \neq \frac {\lim_0 F(h)}{\lim_0 G(h)}$, nous ne pouvons
            pas séparer $\dif f$ et $\dif x$\footnote{Du moins pas dans le cas d'une dérivée.}.

            Leibniz a également eu besoin d'une notation pour la $k^e$ dérivée. Ayant considéré $\frac {\dif{}}{\dif x}$ comme étant
            une opération à réaliser $k$ fois, il a noté la $k^e$ dérivée de la sorte :

            \[\od[k] fx\]

    \subsection{Règles de dérivation}
        \paragraph{Somme}
            Soient $f, g : \R \to \R$, deux fonction dérivables en $a$. De manière intuitive, nous pouvons dire que
            $(f+g)'(a) = f'(a) + g'(a)$ car l'accroissement de la fonction de somme est la somme des accroissements : au point $a$,
            la fonction $(f+g)$ \textit{subit} un accroissement égal à l'accroissement de $f$ plus l'accroissement de $g$.

        \paragraph{Produit}
            Soient $f, g : \R \to \R$, deux fonction dérivables en $a$. Contrairement à ce que nous serions tentés de dire
            naïvement, nous ne pouvons pas définir $(fg)'(a) = f'(a)g'(a)$. C'est à Leibniz que l'on doit cette démonstration.

            Considérons un rectangle de dimensions $f(a)$ et $g(a)$. La quantité $(fg)(a)$ correspond à l'aire de ce rectangle.
            Si on passe de $a$ à $a+h$, nous obtenons un nouveau rectangle de dimensions $f(a+h)$ et $g(a+h)$. Selon l'approximation
            vue au point 1.1.1., nous savons que $f(a+h) \simeq f(a) + f'(a)h$. L'aire du nouveau rectangle est donc $A = f(a+h)g(a+h)
            \simeq (f(a) + f'(a)h)(g(a) + g'(a)h) = f(a)g(a) + f'(a)g(a)h + f(a)g'(a)h + f'(a)g'(a)h^2$.
            Rappelons tout de même que la dérivée ici est égale à :

            \[\begin{aligned}(fg)'(a) &= \lim_{h \to 0}\frac{(fg)(a+h) - (fg)(a)}{h} = \lim_{h \to 0}\frac {f'(a)g(a)h + f(a)g'(a)h + f'(a)g'(a)h^2}{h} \\
                                      &= \lim_{h \to 0}f'(a)g(a) + f(a)g'(a) + f'(a)g'(a)h = f'(a)g(a) + f(a)g'(a)\end{aligned}\]

            Nous avons donc la variation instantanée de la fonction $(fg)$ au point $a$ : $(fg)'(a) = f'(a)g(a) + f(a)g'(a)$.

        \paragraph{Composition}
            Soient $f, g : \R \to \R$, deux fonctions telles que $g$ est dérivable en $a$, et $f$ est dérivable en $g(a)$.
            Il est toujours possible de trouver la dérivée de la fonction composée $(f \circ g)(x) = f(g(x))$ à l'aide de l'approximation
            $f(a+h) \simeq f(a) + f'(a)h$ tant que $h$ est \textit{petit}. Notre dérivation devient donc :

            \[\begin{aligned}
                (f \circ g)'(a) &= \lim_{h \to 0}\frac {(f \circ g)(a+h) - (f \circ g)(a)}{h} = \lim_{h \to 0}\frac {f(g(a+h)) - f(g(a))}{h} = \lim_{h \to 0}\frac {f(g(a) + g'(a)h) - f(g(a))}{h} \\
                                &= \lim_{h \to 0}\frac {f(g(a)) + f'(g(a))g'(a)h - f(g(a))}{h} = f'(g(a))g'(a)
            \end{aligned}\]

        \paragraph{Réciproque}
            Soit $f : \R \to \R$. Si $f$ est bijective, nous pouvons définir sa fonction réciproque $f^{-1} : \R \to \R$.
            Si en plus, nous supposons que $f'(x) \neq 0 \forall x \in \R$, nous pouvons exprimer la dérivée de la fonction
            réciproque comme suit :

            \[(f^{-1})'(f(a)) = \frac {1}{f'(a)}\]

            selon le développement suivant (nécessitant la dérivée d'une composée) : en sachant que la dérivée de la fonction identité
            est 1 ($(x \mapsto x)'(x) = 1$) et que la composée d'une fonction $f$ avec sa réciproque ($f^{-1}$) correspond à la
            fonction identité, nous pouvons trouver la dérivée de la réciproque.

            \[\begin{aligned}
                id'(x) &= 1 \\
                (f^{-1} \circ f)'(x) &= 1 \\
                (f^{-1})'(f(x))f'(x) &= 1 \\
                (f^{-1})'(f(x)) &= \frac {1}{f'(x)}
            \end{aligned}\]

            Ou encore :

            \[(f^{-1})'(a) = \frac {1}{f'((f^{-1})(a))}\]

        \paragraph{Remarque} De manière graphique, ce résultat peut s'interpréter de la manière suivante : la réciproque d'une
            fonction bijective est son image par symétrie orthogonale d'axe $y = x$. En traçant cet axe de symétrie, on peut observer
            qu'aux points $(a, f(a)) \in \Gamma_f$ et $(b, (f^{-1})(b)) \in \Gamma_{f^{-1}}$ tel que $b = f(a)$, nous avons un rapport
            entre les pentes des tangentes. Plus précisément, nous pouvons observer que la pente de la tangente au second point
            ($(f^{-1})'(b) = (f^{-1})'(f(a))$) correspond à l'inverse de la pente de la tangente au second point. Donc nous avons bien
            la même égalité : $(f^{-1})'(f(a)) = \frac {1}{f'(a)}$.

        \paragraph{Exemple des fonctions exponentielles et logarithmiques} Il a été vu plus haut que la fonction $x \mapsto \exp(x)$
        admettait pour dérivée elle-même. Nous avons également vu ci-dessus que $(f^{-1})'(a) = \frac {1}{f'((f^{-1})(a))}$.
        En étant tenté de trouver $f(x)$ telle que $f'(x) = f(x) \, \forall x \in \dom f$, nous aurions donc $(f^{-1})'(a) = \frac {1}{f((f^{-1})(a))} = \frac 1a$.
        Cependant, nous connaissons une fonction de ce genre : la fonction exponentielle de base $e$. Sa fonction inverse, la fonction
        logarithmique de base $e$ a sa dérivée calculée de la sorte :

        \[\log'(x) = \frac {1}{\exp(\log(x))} = \frac 1x\]

    \subsection{Les intégrales}
        \subsubsection{Définition}
            En analyse, il existe un outil permettant de calculer l'aire en dessous d'une courbe. Cet outil s'appelle \textit{intégrale}.
            Soit une fonction $f : \R \to \R$, nous définissons l'aire entre la courbe de $f$ et l'axe horizontal $y = 0$
            définie entre les droites $x = a$ et $x = b$ de la manière suivante :

            \[A = \int_a^bf(x)\dif x\]

            \paragraph{Attention} Cette aire est dite \textit{signée}. C'est à dire que les parties \textit{en dessous} de l'axe $y = 0$
            sont représentés par une valeur réelle négative alors que les parties \textit{au dessus} de l'axe $y = 0$ sont représentés par
            une valeur réelle positive.

            Afin d'approximer l'aire que nous tentons de déterminer, nous découpons l'intervalle $[a, b]$ en $n$ \textit{morceaux} $[x_{i-1}, x_i] \, 1 \leq i \leq n$
            avec $x_i = a + (b-a)\frac in$. Ces \textit{arrêtes} nous permettent d'obtenir des rectangles de hauteur respective $f(x_{i-1})$.
            L'aire de chacun de ces rectangles est donc $b \times h = (x_i - x_{i-1})f(x_{i-1})$. L'approximation de l'aire recherchée (donc de
            l'intégrale) est la somme des aires de ces rectangles.

            \[\int_a^bf(x) \dif x \simeq \sum_{i=1}^n(x_i - x_{i-1})f(x_{i-1})\]

            Pour obtenir l'aire exacte, il faut faire tendre $n$ vers $+\infty$.

            \paragraph{Exemple de la fonction carrée}
                Si nous désirons trouver l'aire sous la courbe de $x \mapsto x^2$ sur $[0, t]$, nous la déterminons ainsi :

                \[\begin{aligned}
                    \int_0^tx^2\dif x &= \lim_{n \to \infty}\sum_{i=1}^n(x_i-x_{i-1})(x_{i-1})^2 = \lim_{n \to \infty}\sum_{i=1}^n\frac tn\left(t\frac {i-1}{n}\right)^2 = \lim_{n \to \infty}\frac {t^3}{n^3}\sum_{i=1}^n(i-1)^2 \\
                                      &= \lim_{n \to \infty}\frac {t^3}{n^3}\frac {n(2n-1)(n-1)}{6} = \lim_{n \to \infty}\frac {t^3(2n-1)(n-1)}{6n^2} = t^3\lim_{n \to \infty}\frac {2n^2 + 1 - 3n}{n^2} \\
                                      &= t^3\lim_{n \to \infty}\left(2 + \frac {1}{n^2} - \frac 3n\right) = t^3\frac 26 = \frac {t^3}{3}
                \end{aligned}\]

        \subsection{Théorème fondamental du calcul différentiel et intégral}
            \subsubsection{1ère version}
                En analyse, il existe un théorème (parfois appelé \textit{théorème fondamental de l'analyse} qui définit la dérivation et l'intégration
                comme deux opérations inverses l'une de l'autre. Avant d'exprimer le théorème, tentons de donner un peu d'intuition au rapport
                entre dérivation et intégration.

                Soit $F(x)$, une fonction réelle définie par $F(x) = \int_a^xf(t)\dif t$ telle que $f$ est une fonction réelle continue. Admettons que
                $F(x)$ est dérivable sur l'intégralité de son domaine. Nous pouvons donc déterminer sa dérivée comme étant l'accroissement instantané :

                \[F'(x) = \lim_{h \to 0}V_F(x, h) = \lim_{h \to 0}\frac {F(x+h) - F(x)}{h} = \lim_{h \to 0} \frac 1h\int_x^{x+h}f(t)\dif t\]

                Ce que représente $\int_x^{x+h}f(t)\dif t$ est l'aire en dessous de la courbe de $f$ entre les points d'abscisse $x$ et $x+h$.
                $h$ étant tout petit (tendant vers l'infini), nous pouvons approximer $f(t)$ constant entre $x$ et $x+h$. L'aire représentée par
                l'intégrale est donc $\simeq hf(x)$. Nous pouvons donc poursuivre le calcul de la dérivée de $F$ :

                \[F'(x) = \lim_{h \to 0} \frac 1h\int_x^{x+h}f(t)\dif t = \lim_{h \to 0} \frac 1hhf(x) = f(x)\]

                Nous avons donc $F(x) = \int_a^xf(t)\dif t$ et $f = F'(x)$. Nous pouvons dès à présent énoncer une première version du théorème :

                \textit{Soit $f : \R \to \R$, une fonction continue sur $\R$, alors la fonction $F(x)$ définie par}

                \[F(x) = \int_a^xf(t)\dif t\]

                \textit{est dérivable sur l'intégralité de son domaine tel que $F'(x) = f(x)$.}

            \subsubsection{2nde version}
                Il existe une deuxième manière d'énoncer ce théorème. À nouveau, tentons de le trouver intuitivement. En ayant une fonction
                $f : \R \to \R$ dérivable en tout point de son domaine, nous pouvons considérer l'intégrale de $a$ en $b$ de $f'(x)$
                comme étant la somme des variations instantanées de $f$ sur l'intervalle $[a, b]$. Tentons maintenant de définir plus précisément
                ce que vaut cette intégrale.

                \[\int_a^bf'(t)\dif t = \lim_{n \to \infty}\sum_{i=1}^n\frac {b-a}{n}f'(x_{i-1})\]

                \paragraph{Rappel} Au point 1.1.1., nous avons vu l'approximation suivante : $f(a+h) \simeq f(a) + f'(a)h$. En réorganisant cette
                approximation, nous avons $f'(a)h \simeq f(a+h) - f(a)$. Dans notre cas, si nous posons $h = \frac {b-a}{n}$, nous pouvons poursuivre
                notre intégrale.

                \[\begin{aligned}
                    \int_a^bf'(t)\dif t &= \lim_{n \to \infty}\sum_{i=1}^nhf'(x_{i-1}) =  \lim_{n \to \infty}\sum_{i=1}^nf(x_{i-1} + h) - f(x_i) \\
                                        &=  \lim_{n \to \infty}f(x_n) - f(x_0) = f(b) - f(a)
                \end{aligned}\]

                Nous avons considéré cette intégrale comme étant la somme des variations instantanées de $f$ entre $a$ et $b$, et le résultat de
                ce développement est que la somme des variations instantanées est égale à la variation totale pour aller de $a$ à $b$.

                Le théorème peut donc être exprimé d'une deuxième manière :

                \textit{Soit $f : \R \to \R$, une fonction réelle dérivable, alors}

                \[\int_a^bf'(t)\dif t = f(b) - f(a).\]

        \subsection{Primitives}
            En analyse, il est fréquent de devoir trouver $F$ tel que $F'(x) = f(x)$. C'est donc une recherche de fonction. Cependant, comme
            nous venons de le voir, trouver une fonction en connaissant sa dérivée revient à réaliser une intégrale (version 1 du théorème fondamental
            de l'analyse). Une solution $F$ satisfaisant $F'(x) = f(x)$ est appelée une \textit{primitive} de $f$. $\int f$ ou $\int f(t)\dif t$ sont
            les manières les plus courantes d'écrire \textit{primitive de $f$}\footnote{On parle parfois également d'intégrale indéfinie.}.
            Cependant, comme le laisse comprendre la seconde version du théorème fondamental de l'analyse, il existe une infinité de primitives,
            toutes définies à \textit{une constante près}. Nous généralisons donc «  la » primitive de $f$ en $\int f + C$, $C \in \R$.

        \subsection{Règles d'intégration}
            Tout comme il y a des règles de dérivation (point 1.1.5.), il existe des règles d'intégration.

            \paragraph{Produit}
                Tout comme $(fg)'(x) \neq f'(x)g'(x)$, $\int (fg)(x)\dif x \neq \int f(x)\dif x \int g(x) \dif x$. Pour réussir à intégrer un
                produit, il faut partir de la règle de Leibniz pour la dérivation :

                \[\begin{aligned}
                    (fg)'(x) &= f'(x)g(x) + f(x)g'(x) \\
                    \int_a^b (fg)'(x) \dif x &= \int_a^b (f'(x)g(x) + f(x)g'(x)) \dif x \\
                    [(fg)(x)]_a^b &= \int_a^b (f'(x)g(x) + f(x)g'(x)) \dif x \\
                    [(fg)(x)]_a^b &= \int_a^b f'(x)g(x) \dif x + \int_a^b f(x)g'(x) \dif x \\
                    \int_a^b f'(x)g(x) \dif x &= [(fg)(x)]_a^b - \int_a^b f(x)g'(x) \dif x
                \end{aligned}\]

                Il faut donc, pour pouvoir intégrer un produit, considérer un des deux facteurs comme étant une dérivée (qu'il faudra donc
                intégrer pour avancer).

            \paragraph{Composition}
                La règle \textit{d'intégration en chaine} est également appelée \textit{changement de variable}. Elle peut être exprimée comme
                suit. Soient $f$ et $g$, deux fonctions continûment dérivables, $g(\alpha) = a$ et $g(\beta) = b$, alors :

                \[\int_a^bf(x)\dif x = \int_\alpha^\beta f(g(t))g'(t)\dif t.\]

                Cela veut dire qu'une intégration peut être résolue en \textit{transformant} l'intégrale de manière à faire apparaitre une
                composition. La justification de cette formule peut être donnée comme suit :

                \[(F(g(t)))'(x) = F'(g(t))g'(t) = f(g(t))g'(t)\]

                Donc

                \[\int_\alpha^\beta f(g(t))g'(t) \dif t = F(g(\beta)) - F(g(\alpha)) = F(b) - F(a) = \int_a^bf(t)\dif t.\]

    \subsection{Les équation différentielles}
        Une équation différentielle est une équation dont l'inconnue est une fonction $f$ et dans laquelle les dérivées de $f$ apparaissent.

        \subsubsection{équation différentielle linéaire d'ordre 1}
            Comme le dit la pseudo-définition ci-dessus, une équadiff est une équation où l'inconnue est une fonction. Le cas de $F = \int_a^xf(t)\dif t$
            est un cas particulier d'une grande famille d'équadiffs que l'on appelle \textit{équation différentielle linéaire d'ordre 1}.
            Cette famille est caractérisée par la forme suivante :

            \[f'(x) + p(x)f(x) = q(x).\]

            Une manière de résoudre une telle équation est de déterminer une fonction auxiliaire $a(x)$ que l'on va multiplier de part et d'autre
            de l'égalité :

            \[a(x)f'(x) + a(x)p(x)f(x) = q(x)a(x)\]

            Le but de cette manipulation est de pouvoir faire ressortir $a(x)f'(x) + a'(x)f(x)$, ce qui est égal à $(af)'(x)$. Cependant, pour cela,
            il faut choisir $a(x)$ telle que $a'(x) = a(x)p(x)$. Il existe une solution :

            \[a(x) : x \mapsto e^{\int p(x)\dif x}\]

            Maintenant que nous avons cette fonction, il suffit de résoudre $(af)'(x) = (aq)(x)$. Si $(aq)$ est continue, le théorème fondamental de
            l'analyse assure l'existence d'une primitive $b(x) = \int a(x)q(x)\dif x$. À présent, nous savons que si $(af)'(x) = (aq)(x)$, alors
            $(af)(x) = b(x)$, ou encore $f(x) = \frac {b(x)}{a(x)}$.

            Nous avons donc une solution à l'équation de départ :

            \[f(x) = \frac {\int \left(e^{\int p(x)\dif x}\right)q(x)\dif x}{e^{\int p(x)\dif x}}.\]

        \subsubsection{Unicité de la solution et problème de Cauchy}
            Lorsque l'on \textit{transforme} le problème initial (équadiff linéaire d'ordre 1) en un problème conditionné (en précisant
            $f(x_0) = y_0$), nous limitons le nombre de solutions à 1. Si nous avons deux fonctions $f_1$ et $f_2$, solutions d'une équadiff linéaire
            d'ordre 1, et que nous définissons une autre fonction $g$ telle que $g(x) = (f_1 - f_2)(x)$, ladite fonction $g$ est une solution de
            l'équation suivante :

            \[g'(x) + p(x)g'(x) = 0\]

            respectant, de plus $g(x_0) = 0$.

            Dans l'équation ci-dessus, le fait que $q(x) = 0 \forall x$ implique que $(ag)'(x) = 0 \forall x$ également, donc $(ag)(x) = C \forall x$.
            Autrement dit, $(ag)(x)$ est une fonction constante telle que $(ag)(x) = C \forall x$. Comme on sait que $g(x_0) = 0$, on sait que
            $e^{P(x_0)}g(x_0) = 0$ (où $P(x)$ est une primitive de $p(x)$), donc $C = 0$. Cependant, comme $(e^P)(x)$ ne peut s'annuler, il faut $g(x) = 0 \forall x$.

            Ce qui veut donc dire que $f_1$ et $f_2$, les deux fonctions solutions trouvées pour une équadiff linéaire d'ordre 1, sont les mêmes (vu que leur différence
            est la fonction nulle). Il existe donc \textbf{une et une seule} solution à l'équadiff linéaire d'ordre 1 conditionnée (problème de Cauchy).

        \subsubsection{équadiffs linéaire d'ordre 2 à coefficients constants}
            Une équadiff d'ordre 2 est sous la forme suivante :

            \[\od[2]{}{x}f(x) + a\od{}{x}f(x) + bf(x) = 0\]

            avec deux paramètres réels $a, b \in \R$. Le terme \textit{linéaire} vient du fait que si $f_1, f_2 \in \R^{\R}$ sont deux solutions de
            cette équation, alors $c_1f_1 + c_2f_2$ en est également une (avec $C_1, C_2 \in \R$). Le fait que cette équation soit d'ordre 2 veut également
            dire que l'ensemble des solutions est un espace vectoriel de dimension 2.

            Pour résoudre une telle équation, il faut d'abord passer par ce que l'on appelle l'\textit{équation caractéristique}. Cette équation est la suivante :

            \[\lambda^2 + a\lambda + c = 0.\]

            Cette équation étant du second degré, il faut séparer trois cas possibles :
            \begin{itemize}
                \item l'équation admet deux solutions réelles distinctes $\lambda_1$ et $\lambda_2$ ;
                \item l'équation admet une seule solution réelle $\lambda$ ;
                \item l'équation n'admet aucune solution réelle.
            \end{itemize}

            Dans le premier cas (deux solutions distinctes), l'équation différentielle peut être réécrite sous la forme factorisée suivante :

            \[\left(\od{}{x} - \lambda_1\right)\left(\od{}{x} - \lambda_2\right)f = 0.\]

            Les solutions $f_1(x) = e^{\lambda_1 x}$ et $f_2(x) = e^{\lambda_2 x}$ sont possible à \textit{deviner}. le principe de linéarité exprimé juste au-dessus
            permet d'affirmer donc que $f(x) = C_1e^{\lambda_1 x} + C_2e^{\lambda_2 x}$ représente la famille des solutions paramétrées par $C_1$ et $C_2$.

            Dans le second cas (solution unique), l'équadiff peut être réécrite sous la forme factorisée suivante :

            \[\left(\od{}{x} - \lambda\right)^2f = \od[2]{}{x}f + \lambda^2f - 2\lambda\od{}{x}f = 0.\]

            En posant $g(x) = \od{}{x}f(x) - \lambda f(x)$, nous avons $\od{}{x}g(x) = \od[2]{}{x}f(x) - \lambda\od{}{x}f(x)$. Cela nous permet de réécrire (encore une
            fois l'équadiff sous la forme suivante :

            \[\od[2]{}{x}f(x) - \lambda\od{}{x}f(x) - \lambda\left(\lambda\od{}{x}f(x) - \lambda f(x)\right) = \od{}{x}g(x) - \lambda(g(x)) = 0.\]

            La solution est $g(x) = Ke^{\lambda x}$. Or $g(x) = \od{}{x}f(x) - \lambda f(x)$. Donc il faut encore résoudre l'équation à l'aide de la méthode vue ci-dessus
            (ordre 1) afin de trouver la solution suivante. La solution finale est :

            \[f(x) = \frac {\int \left(e^{\int p(x) \dif x}\right) q(x) \dif x}{e^{\int p(x) \dif x}}\]

            avec $p(x) = -\lambda$ et $q(x) = g(x) = K_1e^{\lambda x}$. D'où :

            \[f(x) = \frac {\int \left(e^{\int -\lambda \dif x}\right) K_1e^{\lambda x} \dif x}{e^{\int -\lambda x \dif x}} = \frac {\int K_2e^{-\lambda x}K_1e^{\lambda x} \dif x}{K_3e^{-\lambda x}} =
                     \left(K\int\dif x\right)e^{\lambda x}K_3^{-1} = K_3^{-1}K(x + C)e^{\lambda x} = (C_1x + C_2)e^{\lambda x}.\]

            Dans le dernier cas, l'équation caractéristique n'a pas de solution réelle. Il faut donc aller chercher du côté des nombres complexes. Les coefficients
            étant réels, les deux solutions complexes doivent être conjuguées l'une de l'autre (si $z = \alpha + \beta i$ et $\overline z = \alpha - \beta i$, alors
            $z\overline z = \alpha^2 + \beta^2 \in \R$ et $z + \overline z = 2\beta \in \R$). L'équadiff devient donc :

            \[\left(\od{}{x} - z\right)\left(\od{}{x} - \overline z\right)f = 0.\]

            L'équation ressemble fortement à celle du premier cas, donc la solution générale est $f(x) = b_1e^{zx} + b_2e^{\overline z x}$ avec $b_1, b_2 \in \mathbb C$.
            Cependant, comme $\exp(zx) = \exp(\alpha x + i\beta x) = e^{\alpha x} \exp(i\beta x) = e^{\alpha x}(\cos(\beta x) + i\sin(\beta x))$ (et donc
            $\exp(\overline z) = e^{\alpha x}(\cos(\beta x) - i\sin(\beta x))$), en choisissant respectivement $b_1 = b_2 = \frac 12$ et $b_1 = -b_2 = -\frac i2$,
            on obtient respectivement $f(x) = e^{\alpha x}\cos(\beta x)$ et $f(x) = e^{\alpha x}\sin(\beta x)$. Nous avons donc deux solutions, et à nouveau, par
            linéarité, nous pouvons exprimer la famille des solutions paramétrée par $C_1, C_2 \in \R$ :

            \[f(x) = C_1e^{\alpha x}\cos(\beta x) + C_2e^{\alpha x}\sin(\beta x).\]

            Ici, notre solution ne fait plus intervenir quoi que ce soit de complexe ($\in \mathbb C$), tous les coefficients sont réels ($\alpha$ et $\beta$ sont des
            \textit{constantes} dépendantes de l'équation caractéristique).

        \subsubsection{équations de Newton}
            Après avoir étudié des équations linéaires, regardons une autre famille d'équations : les \textbf{équations de Newton}. Cette famille est représentée
            par la forme suivante :

            \[\od[2]{}{t}x(t) = f(x(t))\]

            avec $x : \R \to \R : t \mapsto x(t)$, l'inconnue et $f$, la fonction \textbf{continue} de \textit{force}. Une telle équation représente
            la position $x(t)$ en fonction du temps d'un mobile (de masse unitaire) soumis à une force $f$ dépendant de la position. Si on multiplie l'équation
            de part et d'autre par la quantité $x'(t)$, on obtient

            \[x^{(2)}(x)x'(t) - f(x(t))x'(t) = 0\]

            Ce que l'on peut intégrer afin d'avoir

            \[\frac 12(x'(t))^2 - F(x(t)) - K = 0,\]

            avec $F$, une primitive de $f$, ou encore

            \[(x'(t))^2 - 2F(x(t)) = E\]

            avec $E$, la constante d'intégration. Si l'équation différentielle est conditionnée (problème de Cauchy) telle que $x(0) = x_0$ et $x'(0) = v_0$, alors
            une solution pour cette équation est $x'(t) = \sqrt{E_0 + 2F(x(t))}$ avec $E_0 = v_0^2 - 2F(x_0)$.

                \paragraph{Exemple : les équations de Fisher} Les équations de Fisher sont sous la forme suivante : $u''(t) = (u - u^3)(t)$. La fonction $f$ de force
                est ici $f : \R \to \R : u(t) \mapsto u(t) - u^3(t)$, et a pour primitive $F : \R \to \R : \frac 12u^2(t) - \frac 14u^4(t) + C$.
                Pour des raisons de simplicité, ici, $C$ se verra attribuer la valeur $-\frac 14$. Donc $F(u(t)) = -\frac 14(u^2(t) - 1)^2$.

                Étant donné les solutions constantes à l'équation $u : t \mapsto K_u$ avec $K_u \in \{-1, 0, 1\}$, nous cherchons $u$ telle que \\
                $\lim_{t \to \pm \infty}u(t) = \pm 1$ et $\lim_{t \to \pm\infty}u'(t) = 0$. L'intégration première nous donne $u'(t)^2 = \frac 12(u^2(t) - 1)^2$,
                ou encore \\$u'(t) = \frac {\sqrt{2}}{2}(u^2(t) - 1)$.

                Dans les intervalles de temps tels que $u'(t) > 0$ et $-1 \neq u(t) \neq 1$, nous avons :

                \[\begin{aligned}
                    \int_{t_0}^t \frac {1}{u^2 - 1}\dif u &= \int_{t_0}^t \frac {\sqrt{2}}{2} = \frac{\sqrt{2}}{2} (t - t_0) \\
                    \arctanh(u(t)) - \arctanh(u(t_0)) &= \frac{\sqrt{2}}{2} (t - t_0)
                \end{aligned}\]

                Cependant, nous savons qu'$\exists t_0$ tel que $u(t_0) = 0$ vu que nous cherchons $-1 < u(t) < 1$. Supposons $t_0 = 0$, de manière à ce que
                l'équation devienne $\arctanh(u(t)) = \frac {\sqrt{2}}{2}t$ d'où $u(t) = \tanh(\frac t{\sqrt 2})$.

    \subsection{Problèmes et paradoxes}
        La majeure partie de l'analyse mathématique est basée sur la notion de limite, et surtout de limite infinie. Si cette notion est utilisée sans
        suffisamment de rigueur, un certain nombre de paradoxes peuvent apparaitre. Par exemple, soit la série $x, x^2, x^3, ...$ telle que
        $x_i = x^i$. Pour en connaitre la limite infinie, nous pouvons faire $L = \lim_{n\to\infty}x^n = \lim_{m\to\infty}x^{m+1} = x\lim_{m\to\infty}x^m = xL$.
        Nous avons donc $L = xL$, ou encore, $\forall x\neq1, L = 0$. Ce qui peut sembler contre-intuitif pour $x = 2$ ou $x = 3$ par exemple.
        Pareillement, si l'on désire mesurer l'hypoténuse d'un triangle $ABC$ avec $A = (0, 0), B = (1, 0), C = (0, 1)$, on peut approximer la
        longueur de l'hypoténuse comme la longueur d'un « escalier » de $N$ marches. Chaque marche est composée de deux segments de longueur $\frac 1N$.
        La longueur de l'escalier est donc $\frac {2N}{N}$, ou encore 2. Alors que le théorème de Pythagore nous dit que l'hypoténuse est de longueur
        $\sqrt 2$.

\section{Les nombres réels}
    Le traitement de l'analyse peut être très puissant, cependant comme vu ci-dessus, un manque de rigueur peut amener à des contradictions voire à
    des résultats insensés. C'est pour cette raison qu'il faut instaurer cette rigueur dès les notions élémentaires telles que les nombres.

    \subsection{Axiomatique des nombres}
        \paragraph{Rappel} Les nombres sont organisés de la sorte : $\mathbb N \subset \mathbb Z\subset \mathbb Q \subset \R$.

        Il faut savoir que chaque ensemble est défini selon des axiomes, et que c'est à partir de ces axiomes et de déductions logiques que sont
        construits les ensembles \textit{plus gros}. Ici, les axiomes correspondant aux ensembles $\mathbb N, \mathbb Z$ et $\mathbb Q$ sont considérés
        comme \textit{évidents} et seuls ceux de $\R$ sont explicités.

        \subsubsection{Axiomes de $\R$}
            Avant de citer les axiomes de $\R$, il faut introduire la notion de majorant et de minorant (pour l'axiome de complétude).

            Soit $A \subset \R$. On dit que $A$ est majoré $\iff \exists M \in \R | \forall a \in A, a \leq M$.

            De manière similaire, on dit que $A$ est minoré $\iff \exists m \in \R | \forall a \in A, a \geq m$.

            \paragraph{Remarque} Le minorant/majorant ne doit pas nécessairement appartenir à $A$.

            \begin{enumerate}
                \item $\R$ est un \textit{corps}
                    \begin{itemize}
                        \item $(\R, +)$ est un \textit{groupe commutatif}, donc la loi d'addition satisfait les conditions
                              suivantes : associativité, commutativité, existence d'un élément neutre et existence d'un inverse ;
                        \item $(\R \backslash \{0\}, .)$ est un \textit{groupe commutatif} ;
                        \item La multiplication est distributive sur l'addition.
                    \end{itemize}
                \item $(\R, \leq)$ est un corps entièrement ordonné
                    \begin{itemize}
                        \item la relation d'ordre $\leq$ satisfait les propriétés suivantes : réflexivité, transitivité, antisymétrie
                              et ordre total ;
                        \item $a \leq b \Rightarrow a + z \leq b + z \, \forall z \in \R$ ;
                        \item $a, b \leq 0 \Rightarrow ab \geq 0$.
                    \end{itemize}
                \item $\R$ satisfait l'axiome de complétude. C'est cet axiome qui différencie grandement $\R$ de
                      $\mathbb Q$. Cet axiome de complétude dit ceci :

                      \begin{itemize}
                        \item $\forall A \subset \R$, si $A$ est non-vide et majoré, alors $A$ possède un majorant minimum appelé
                              \textit{supremum} de $A$ et noté $\sup A$.
                        \item $\forall A \subset \R$, si $A$ est non-vide et minoré, alors $A$ possède un minorant maximum appelé
                              \textit{infimum} de $A$ et noté $\inf A$.
                      \end{itemize}

                      Et l'ensemble des rationnels $\mathbb Q$ ne respecte pas cet axiome.
            \end{enumerate}

        \subsubsection{Résultat de l'axiome : les racines}
            Commençons par énoncer la propriété d'Archimède qui dit que $\forall y\in \R, \exists n \in \mathbb N \, | \, n > y$.
            La preuve de ce principe fait intervenir la notion de majorant vue ci-dessus.

            Soit $S = \{n \in \mathbb N \, | \, n \leq y\}$ avec $y \in \R$. Par définition, $y$ majore $S$. Il faut différencier les cas où
            $\#S = 0$ et $\#S > 0$. Dans le premier cas, $0 > y$, donc $n = 0$ est le nombre naturel recherché. Dans le second, posons $s = \sup S$.
            Comme $s \in S$, $s - 1$ n'est pas un majorant de $S$, $\Rightarrow \exists m \in S \, | \, s - 1 < m$, ou encore $m + 1 > s$. Or $s$
            majore $S$ donc $m + 1 \not \in S$. Si $m + 1 \not \in S$, alors $m+1 > y$. Donc $n = m + 1$ est le nombre naturel recherché.

            S'en suit le corollaire suivant : $\forall y \in \R^+, \exists n \in \mathbb N \, | \, y > \frac 1n$. La preuve repose sur le lemme
            précédent : soient $y \in \R^+, x = \frac 1y$. Le lemme d'Archimède dit qu'$\exists n \in \mathbb N \, | \, x < n$. Donc
            $\frac 1y < n$, ou encore $\frac 1n < y$. Il faut effectivement $y > 0$ car sinon lors de la dernière étape, nous avons $\frac 1n > y$,
            ce qui n'est pas possible pour $n \in \mathbb N$.

            Maintenant, intéressons-nous aux racines carrées et à l'affirmation de leur existence. Tentons de démontrer qu'
            $\exists x \in \R \, | \, x^2 = N \forall N \in \R$. Afin de démontrer ceci, procédons par l'absurde. Soient
            $S = \{y \in \R \, | \, y^2 \leq N\}$ et $x = \sup S$. Prouvons maintenant que $x^2 = N$.

            Si $x^2 < N$, posons $x_n = x + \frac 1n$ avec $n \in \mathbb N_0$. Donc $x_n^2 = x^2 + \frac {2x}{n} + \frac {1}{n^2} \leq x^2 + \frac {2x+1}{n} \, \forall n \in \mathbb N_0$.
            Comme $N - x^2 > 0$ et $2x + 1 > 0$ (vu que $x \geq 0$ du fait que $0 \in S$), la quantité $\frac {N - x^2}{2x + 1}$ est strictement
            positive également, donc $\exists n \in \mathbb N \, | \, \frac 1n \leq \frac {N - x^2}{2x + 1}$. Pour ce même $n$, nous avons $x_n^2 < N$
            car $x_n^2 < x^2 + (2x+1)\frac 1n < x^2 + (2x+1)\frac {N - x^2}{2x + 1} = x^2 + N - x^2 = N$. Donc $x_n^2 \in S$, cependant $x_n = x + \frac 1n > x$,
            ce qui n'est pas possible.

            Inversement, si $x^2 > N$, on définit $x_n = x - \frac 1n$. D'où $x_n^2 >x^2 - \frac {2x+1}{x^2 - N}$. De plus, les quantités $x^2 - N$
            et $2x + 1$ sont toutes deux strictement positives, donc $\exists n \in \mathbb N \, | \, \frac 1n < \frac {x^2 - N}{2x + 1}$. Ce qui
            mène à $x_n^2 > N$. Or $x_n = x - \frac 1n < x$. Donc $x_n$ ne peut être un majorant de $S$. Cela implique qu'$\exists y \in S \, | \, y \geq x_n$,
            ou encore $y^2 \geq x_n^2 > N$, ce qui n'est pas possible car $y \in S \Rightarrow y^2 \leq N$.

            Donc si $x^2 \not < N$ et $x^2 \not > N$, alors $x^2 = N$. De plus, nous avons une définition d'une racine carrée (que l'on peut étendre à
            la racine $n^e$) :

            \[x^\frac 1n = \sqrt[n] x = \sup \{y \in \R^+ \, | \, y^n \leq x\}.\]

            \paragraph{Lemme} Il s'en déduit que $\forall q = \frac mn \in \mathbb Q, x^q = (x^{\frac 1n})^m$.

            \paragraph{Remarque} Ici, les puissances irrationnelles ne sont pas définies. Une telle définition viendra par la suite.

    \subsection{Densité des rationnels}
        Nous avons vu l'ensemble $\R$ et sa partie rationnelle ($\mathbb Q$). Cependant, quelle est la proportion de ces nombres rationnels
        et donc quelle est la proportion de nombres irrationnels étant donné le résultat suivant : $\forall x < y \in \R, \exists q \in \mathbb Q \, | \, x < q < y$ ?

        Commençons par prouver ce résultat. Séparons le problème en deux: le cas où $y - x > 1$ et le cas où $x < y$ quelconque. Dans le premier cas,
        $\exists n \in \mathbb N \, | \, n > x$ où $n$ est le plus petit entier plus grand que $x$. On en déduit que $n-1 \leq x$, ou encore
        $n \leq x + 1 < y$, ou encore $x < n < y$, prenons $q = n$. Dans le second cas, Archimède dit qu'$\exists m \in \mathbb N \, | \, m > \frac {1}{y - x}$.
        Cette inégalité peut se réécrire $my - mx > 1$, ce qui correspond au cas précédent. Prenons $q = \frac nm$, et nous avons $x < q < y$.

        Cela permet de se dire qu'il n'y a pas trop de points qui ont été ajoutés pour passer de $\mathbb Q$ à $\R$. Cependant, il faut savoir
        que $\#\R > \#\mathbb Q$, bien que ces deux ensembles soient infinis.

    \subsection{Inégalité triangulaire}
        L'inégalité triangulaire dit que $\forall x, y \in \R, |x + y| \leq |x| + |y|$. Pour le prouver, il faut savoir que $ab \geq 0 \Rightarrow |a + b| = |a| + |b|$.
        Cependant, quand $a \leq 0 \leq b$, alors $a - |b| = - |a| - |b| \leq a + b \leq |a| + b = |a| + |b|$. Or, pour avoir $x \leq -y$ et $x \geq y$,
        il faut $|x| \leq |y|$. Donc $|a + b| \leq ||a| + |b|| = |a| + |b|$.

        De manière similaire, on peut dire que $\forall x, y \in \R, |x - y| \geq |x| - |y|$. Il suffit pour le prouver de poser $x = a - b$ et $y = b$.
        De là, le lemme précédent s'applique de manière à ce que $|a| \leq |a - b| + |b|$, ce qui peut se réécrire comme suit : $|a - b| \geq |a| - |b|$.

    \subsection{Autres corps}
        Avant tout, il faut définir ce qu'est un corps.

        L'ensemble $\mathbb K$ est un corps si, muni des opérations d'addition et de produit, il respecte les trois propriétés suivantes :
        \begin{itemize}
            \item $(\mathbb K, +)$ est un groupe commutatif ;
            \item $(\mathbb K \backslash \{0\}, .)$ est un groupe commutatif ;
            \item le produit est distributif sur l'addition.
        \end{itemize}

        C'est principalement le corps des nombres réels qui sera étudié dans ce cours, cependant il en existe plein d'autres.

\section{Les séries}
    Après avoir utilisé la notion de « limite » pour \textit{définir} les notions de dérivée et d'intégrale, il est nécessaire de définir précisément
    cette notion de limite (ou de \textit{convergence}).

    Une suite réelle (dans $\R$) est une liste infinie de $x_i \in \R$ indexés par $i \in \mathbb N$. Cette suite se note $(x_n)$, $(x_n)_n$, ou
    encore $(x_n)_{n \in \mathbb N}$.

    \subsection{Convergence et divergence}
        On dit d'une suite $(x_n)$ qu'elle converge en $a \in \R \iff \forall \epsilon > 0, \exists N \in \mathbb N \, | \, n \geq N \Rightarrow |x_n - a| < \epsilon$.
        Cela se note

        \[\lim_{n \to \infty}x_n = a.\]

        Il reste cependant à prouver qu'une suite convergente n'a qu'une seule limite. Pour ce faire, procédons par l'absurde. Supposons que
        $x_n \to a$ et $x_n \to b$ quand $n \to \infty$ et supposons que $a \neq b$. Selon la définition ci-dessus,
        $\exists N_1 \in \mathbb N \, | \, n \geq N_1 \Rightarrow |x_n - a| < \epsilon$ et
        $\exists N_2 \in \mathbb N \, | \, n \geq N_2 \Rightarrow |x_n - b| < \epsilon$. En prenant $\epsilon = \frac 12|a - b| > 0$ car $a \neq b$
        et $N = \max\{N_1, N_2\}$, l'inégalité triangulaire dit que $|a - x_N + x_N - b| \leq |a - x_N| + |x_N - b| < 2\epsilon$. Donc $|a - b| < |a - b|$.
        L'hypothèse disant $\epsilon > 0$ est fausse, ce qui implique $\epsilon = 0$, ou encore $a = b$.

        Pour définir une convergence en l'infini positif, on procède de la sorte : $\lim_{n\to\infty} x_n = \infty \iff \forall K \in \R^+, \exists N \in \mathbb N \, | \, n \geq N \Rightarrow x_n > K$.
        De manière similaire, pour l'infini négatif : $\lim_{n\to\infty}x_n = -\infty \iff \forall K \in \R^+, \exists N \in \mathbb N \, | \, n \geq N \Rightarrow x_n < K$.

        On dit d'une suite qui ne converge en aucun réel qu'elle est divergente ($\infty \not \in \R$ !).

        \subsubsection{Techniques de démonstration de divergence ou de convergence}
            Soit $(x_n)$, une suite convergente. Alors $(x_n)$ est bornée. Autrement dit, $\exists K \in \R^+ \, | \, \forall n \in \mathbb N, |x_n| \leq K$.

            Pour prouver ceci, il faut d'abord montrer que pour $a = \lim x_n$, $\forall \epsilon > 0, \exists N \in \mathbb N \, | \, n \geq N \Rightarrow |x_n - a| < \epsilon$.
            Donc $|x_n| = |x_n - a + a| \leq |x_n + a| + |a| < \epsilon + |a|$. Autrement dit, à partir de $n = N$, $(x_n)$ est borné par $\epsilon + |a|$.
            Il reste donc un nombre fini d'éléments à traiter. Donc $K = \max \{|x_0|, ..., |x_{N-1}|, \epsilon + |a|\}$ borne l'entièreté de $(x_n)$.

            De plus, les opérations sur suites sont définies telles que, pour $(x_n)$ et $(y_n)$ convergentes respectivement en $a$ et $b$ :

            \begin{itemize}
                \item $(x_n + y_n)$ est convergente en $a + b$ ;
                \item $(x_ny_n)$ est convergente en $ab$ ;
                \item $b \neq 0 \Rightarrow (\exists M \, | \, n \geq M \Rightarrow y_n \neq 0) \land (x_n/y_n)_{n \geq M}$ est convergente en $\frac ab$.
            \end{itemize}

            Ces assertions se démontrent comme suit.

            \paragraph{Somme de suites} Soit $\epsilon > 0$, $\exists N_1 \, | \, \forall n \geq N_1, |x_n - a| < \frac \epsilon2$. Pareillement pour
            $N_2 \ | \, \forall n \geq N_2, |y_n - b| < \frac \epsilon2$. En prenant $N = \max \{N_1, N_2\}$, on a $|(x_n + y_n) - (a + b)| \leq |x_n - a| + |y_n - b| < \epsilon$.

            \paragraph{Produit de suites} La suite $(x_ny_n)$ est bornée, donc $\exists K \, | \, |x_n| \leq K$. Donc $|x_ny_n - ab| = |x_n(y_n - b) - b(x_n - a)|$.
            Autrement dit, $|x_ny_n - ab| \leq |x_n||y_n - b| + |b||x_n - a| = K|y_n - b| + |b||y_n - b|$.
            Avec $\epsilon > 0$, $\exists N_1 \, | \, \forall n \geq N_1, |y_n - b| < \frac {\epsilon}{2K}$ et $\exists N_2 \, | \, \forall n \geq N_2, |x_n - a| l \frac {\epsilon}{|b| + 1}$
            (il faut mettre $|b| + 1$ dans le cas où $b = 0$. Avec $N = \max \{N_1, N_2\}$, $\forall n \geq N$, on a :

            \[|x_ny_n - ab| \leq K|y_n - b| + |b||x_n - a| < \frac \epsilon2 + \frac \epsilon2 = \epsilon.\]

            \paragraph{Quotient de suites} En prouvant que $(\frac {1}{y_n}) \to \frac 1b$, le quotient découle du produit.
            Soit $\epsilon = \frac {|b|}{2} > 0$, $\exists M \, | \, n \geq M \Rightarrow |y_n - b| < \epsilon$. Donc, par l'inégalité triangulaire,
            $|y_n| \geq |b| - |y_n - b| > |b| - \epsilon = \frac {|b|}{2}$. La suite $(y_n)_{n \geq M}$ est bien définie.Maintenant prouvons sa
            convergence en $\frac 1b$ :

            \[\left|\frac {1}{y_n} - \frac 1b\right| = \left|\frac {b - y_n}{by_n}\right| \leq \frac {2}{|b|^2}|b - y_n|.\]

            Pour $\epsilon > 0$ fixé, $\exists N \geq M \, | \, \forall n \geq N, |y_n - b| < \frac {|b|^2}{2}\epsilon$. Donc $|\frac {1}{y_n} - \frac 1b| < \frac {2}{|b|^2}\frac {|b|^2}{2}\epsilon = \epsilon$.

            \paragraph{Exemple du quotient de polynômes de degré $k$} Soient $a_i, b_i \in \R$ avec $0 \leq i \leq k$. La suite $(x_n)$ définie par
            $x_n = \frac {\sum_{i = 0}^ka_in^i}{\sum_{i = 0}^kb_in^i}$ converge en $\frac {a_k}{b_k}$. Pour le prouver, il faut diviser le numérateur
            et le dénominateur par $n^k$. La suite devient $x_n = \frac {\sum_{i = 0}^ka_in^{i-k}}{\sum_{i = 0}^kb_in^{i-k}}$. Or $\frac {1}{n^K}$
            converge en 0 $\forall K > 0$. Donc $\lim_{n\to\infty}x_n = \lim_{n\to\infty}\frac {\sum_{i = 0}^ka_in^{i-k}}{\sum_{i = 0}^kb_in^{i-k}} = \frac {a_k}{b_k}$.

            \paragraph{Théorème du sandwich} Soient $(a_n)$ et $(b_n)$, deux suites réelles convergentes vers $l$. Si $(x_n)$ est une suite satisfaisant
            $a_n \leq x_n \leq b_n \forall n \geq N_0$, alors $x_n \to l$.

            Soit $\epsilon > 0$. Par définition, $\exists N_1, N_2 \, | \, (n \geq N_1 \Rightarrow |a_n - l| < \epsilon) \land (n \geq N_2 \Rightarrow |b_n - l| < \epsilon)$.
            Donc $\forall n \geq N = \max \{N_0, N_1, N_2\}, -\epsilon < a_n - l \leq x_n - l \leq b_n - l < \epsilon$. Autrement dit, $|x_n - l| < \epsilon$.

            Il en découle un corollaire disant que si $(y_n)$ est une suite bornée (pas forcément convergente) et $z_n \to 0$, alors $y_nz_n \to 0$. Pour le prouver,
            on sait qu'$\exists K > 0 \, | \, |y_n| < K$. Autrement dit, $-K < |y_n| < K$, d'où $-K|z_n| \leq |y_n||z_n| \leq K|z_n|$. Puisque $|y_n||z_n| = |y_nz_n|$ et
            $z_n \to 0$, alors $|y_nz_n| \to 0$.

            \paragraph{Règle de l'exponentielle} Cette règle dit que si $(a_n)$ est une suite réelle positive convergent en 0, alors $(a_n^p)$ avec $p \in \mathbb R$
            converge également en 0. Pour le prouver, il faut prendre $\epsilon > 0$ et $\epsilon' = \epsilon^{p^{-1}}$. Par définition, $\exists N \, | \, n \geq N \Rightarrow |a_n| < \epsilon'$.
            Cependant, $a_n \geq 0$, donc $0 \leq a_n < \epsilon' = \epsilon^{p^{-1}}$. D'où $0 \leq a_n^p < \epsilon$.

            \paragraph{Suites convergentes en 0} Les suites suivantes convergent en 0, la plupart se démontrent avec le binôme de newton et/ou le théorème du sandwich.

            \begin{itemize}
                \item $(\frac {1}{n^p})$ avec $p > 0$ ;
                \item $(c_n$) avec $|c| < 1$ ;
                \item $(n^pc^n)$ avec $p \in \R, |c| < 1$ ;
                \item $(\frac {c^n}{n!})$ avec $p \in \R$ ;
                \item $\frac {n^p}{n!}$ avec $p \in \R$.
            \end{itemize}

            La première proposition se démontre avec la règle de l'exponentielle vu que $(n^{-1}) \to 0$. La seconde se démontre avec le théorème du sandwich en
            posant $c = \frac {1}{1+a}$ avec $a > 0$. La troisième se démontre de manière similaire. La quatrième se démontre avec le principe d'Archimède et le théorème du
            sandwich. La dernière se réécrit $\frac {n^p}{n!} = \frac {n^p}{2^n}\frac {2^n}{n!}$ et est donc un produit de deux suites convergentes en 0.

            Le procédé pour déterminer la convergence d'une suite est donc de trouver un maximum de suites convergentes en 0 puis d'appliquer les opérations sur les
            limites de suite. Regardons maintenant du côté des suites divergentes.

            Soit $x_n \to \infty$ et $(y_n) \subset \R$. S'il $\exists A \in \R \, | \, \forall n, y_n > A$, alors $x_n + y_n \to \infty$. Également, si $A > 0$,
            alors $x_ny_n \to \infty$. La première affirmation se démontre comme suit : soit $K > 0$, $\exists N \, | \, \forall n \geq N, x_n > K - A$, donc
            $x_n + y_n > K$. Pour le produit, $\exists N \, | \, \forall n \geq N, x_n > \frac KA$ donc $x_ny_n > K$.

            \paragraph{Règle de la réciproque} Soit $(x_n)$ une suite réelle qui tend vers $\pm \infty$. Alors $\frac1{x_n} \to 0$. De plus, soit $(x_n)$, une
            suite réelle non-nulle. S'$\exists M \, | \, n \geq M \Rightarrow x_n > 0$, alors $x_n \to \infty$. Similairement, s'$\exists M \, | \, n \geq M
            \Rightarrow x_n < 0$, alors $x_n \to -\infty$. Pour démontrer la première affirmation, il faut avoir $\epsilon > 0$, donc $\frac 1\epsilon > 0$.
            On sait qu'$\exists N \, | \, n \geq N \Rightarrow x_n > \frac 1\epsilon$. ou encore $0 < \frac 1{x_n} < \epsilon$ (car $x_n > \frac 1\epsilon > 0$).
            Pour démontrer la seconde, il faut avoir $K > 0$. On sait qu'$\exists N \, | \, n \geq N \Rightarrow x_n < \frac 1K$. Ou encore $\frac 1{x_n} > K$.

            Soient $(x_n)$, une suite réelle et une suite strictement croissante $n_1 < n_2 < \ldots$. La suite $(x_{n_k})$ est une \textit{sous-suite} de $(x_n)$.

            Il en découle le lemme suivant : Soit $(x_n)$, une suite réelle convergente en $a$. Alors tout sous-suite de $(x_n)$ converge également en $a$.
            Pour le démontrer, il faut repartir de la définition et donc, puisque $x_n \to a$, alors $\exists n \, | \, n \geq N \Rightarrow |x_n - a| < \epsilon$
            avec $\epsilon \in \R^+_0$. Or, comme $(n_k)$ est une série naturelle, $n_k \geq k \; \forall k$ (preuve par récurrence). Si $k \geq N$,
            alors $|x_{n_k} - a| < \epsilon$. Donc $x_{n_k} \to a$.

            On peut en déduire le corollaire suivant : si $(x_n)$ a deux sous-suites convergentes mais ayant des limites différentes, alors $(x_n)$ ne converge pas.

        \subsubsection{Les suites monotones}
            Une suite est dite \textit{croissante} si $x_n \leq x_{n+1} \; \forall n$ et est dite \textit{décroissante} si $x_n \geq x_{n+1} \; \forall n$.
            Si une suite est soit croissante, soit décroissante, elle est dite \textit{monotone}. Sur base de cette définition, il existe un théorème important,
            celui de la convergence des suites monotones :

            Soit $(x_n)$, une suite monotone bornée. $(x_n)$ est convergente telle que quand $(x_n)$ est croissante, $\lim x_n = \sup\{x_n \, | \, n \in \mathbb N\}$
            et quand $(x_n)$ est décroissante, $\lim x_n = \inf\{x_n \, | \, n \in \mathbb N\}$.

            Pour le démontrer, il faut définir $S = \{x_n \, | \, n \in \mathbb N\}$, borné par hypothèse. De là, il faut supposer $(x_n)$ soit croissante soit
            décroissante (la démonstration est similaire) et prouver que $a = \sup\{x_n \, | \, x \in \mathbb N\}$ si $x_n \to a$. Soit $\epsilon > 0$. Par définition,
            $a$ est le majorant minimal de $S$. Donc $a-\epsilon$ ne majore pas $S$, donc $\exists x_N > a-\epsilon$. Or, comme $(x_n)$ est croissante,
            $n \geq N \Rightarrow x_n \geq x_N > a-\epsilon$. Mais $x_n \leq a$ car $a$ majore $S$. Donc $a-\epsilon < x_n \leq a$. Ou encore $-\epsilon < x_n - a < 0$,
            d'où $|x_n - a| < \epsilon$.

            On sait donc qu'une suite monotone non bornée diverge en $\pm \infty$ et qu'une suite bornée converge en $\sup\{x_n \, | \, n \in \mathbb N\}$ ou en
            $\inf\{x_n \, | \, n \in \mathbb N\}$.

        \subsection{Théorème de Bolzano-Weierstrass}
            Regardons comment construire deux suites monotones à partir d'une suite quelconque. Soit une suite $(x_n)$, commençons par définir

            \[S_m := \{x_n | m \geq n\}.\]

            Si $(x_n)$, alors $s_n = \sup S_n$ est fini pour tout $n$ et $(s_n)$ est une suite décroissante car $m \geq n \Rightarrow S_m \subseteq S_n \Rightarrow
            \sup S_m \leq \sup S_n$. De même, si $(x_n)$ est minorée, alors $i_n = \inf S_n$ est fini pour tout $n$ et $(i_n)$ est une suite croissante.

            \paragraph{Définition} Soit $(x_n) \subseteq \R$.

            \begin{itemize}
                \item Si $(x_n)$ est majorée, alors $(s_n)$ est bien définie, et on écrit :

                \[\limsup_{n\to\infty}x_n := \lim_{n\to\infty}s_n\]

                que l'on appelle la \textit{limite supérieure} de $(x_n)$.

                \item Si $(x_n)$ n'est pasmajorée, alors on écrit $\limsup_{n\to\infty}x_n = \infty$.

                \item Si $(x_n)$ est minorée, alors $(i_n)$ est ien définie, et on écrit :

                \[\liminf_{n\to\infty}x_n := \lim_{n\to\infty}i_n\]

                que l'on appelle la \textit{limite inférieure} de $(x_n)$.

                \item Si $(x_n)$ n'est pas minorée, alors on écrit $\liminf_{n\to\infty}x_n = -\infty$.

                \item Constatons donc que pour une suite $(x_n)$ bornée, $\limsup_{n\to\infty}x_n$ et $\liminf_{n\to\infty}x_n$ sont tous les deux finis.
            \end{itemize}

            \paragraph{Théorème (de Bolzano-Weierstrass)} Soit $(x_n)$ une suite bornée. Alors il existe une sous-suite de $(x_n)$ qui converge en
            $\limsup_{n\to\infty}x_n$ et une autre qui converge en $\liminf_{n\to\infty}x_n$.

            De plus, pour une sous-suite convergente $(x_{n_k})$ quelconque, $\liminf_{n\not\infty}x_n \leq \lim_{k\to\infty}x_{n_k} \leq \limsup_{n\to\infty}x_n$.

            \[\liminf_{n\to\infty}x_n \leq \lim_{k\to\infty}x_{n_k} \leq \limsup_{n\to\infty}x_n.\]

            \paragraph{Démonstration} Soit $(x_n)$ une suite bornée. Montrons qu'il existe un sous-suite convergent en $\limsup_{n\to\infty}x_n$. La démonstration
            pour la sous-suite convergent en $\liminf_{n\to\infty}x_n$ étant similaire. Définissons cette suite $(x_{n_k})$ terme à terme.

            Prenons $n_0 = 0$. C'est à dire que la sous-suite $(x_{n_k})$ débute en $x_0$. Regardons maintenant l'ensemble $S_{0+1} = \{x_1, x_2, \ldots\}$.
            Comme $s_1$ est le suprémum de $S_1$, il doit exister $x_m \in S_1$ tel que $s_1-1 < x_m \leq s_1$. Prenons alors pour $n_1$ le plus petit $m$
            satisfaisant cette condition. Comme $\forall \alpha \in S_1, \alpha > x_0 = n_0$, il est évident que $n_1 > n_0$.

            Regardons maintenant l'ensemble $S_{1+n_1} = \{x_{1+n_1}, x_{2+n_1}, \ldots\}$. De manière similaire au point précédent, il est possible de trouver pour
            $n_2$ un $m$ tel que $x_m \in S_{1+n_1}$ et $s_{1+n_1}-\frac 12 < x_m \leq s_{1+n_1}$. Il est tout aussi évident que $n_2 > n_1$.

            En réitérant ce procédé $k$ fois, on obtient $n_0 < n_1 < n_2 <\ldots < n_k$ satisfaisant : $s_{1+n_i} - \frac 1{i+1} < x_{n_{i+1}} \leq s_{1+n_i}$.

            On obtient donc au final une sous-suite $(x_{n_k})$ de $(x_n)$ telle que $s_{1+n_k} - \frac 1{k+1} < x_{n_{k+1}} \leq s_{1+n_k}$. De plus, la suite
            $(s_{1+k_k})$ est une sous-suite de $(s_n)$. On a donc $s_{1+n_k} \to \limsup_{n\to\infty}x_n$ et $s_{1+n_k} - \frac 1{k+1} \to \limsup_{n\to\infty}x_n$.
            Par le théorème du sandwich, on peut déterminer que $x_{n_k} \to \limsup_{n\to\infty}x_n$ également.

            De plus (en admettant l'existence de $(i_{n_k})$ car la preuve est similaire à celle de $(s_{n_k})$), on a :

            \[\forall k \in \mathbb N, i_{n_k} \leq x_{n_k} \leq s_{n_k}.\]

            Et comme $(i_{n_k})$ et $(s_{n_k})$ sont des sous-suites d'une suite convergente, elles sont elles-mêmes convergentes. Donc par un résultat précédent,
            $\lim_{k\to\infty}i_{n_k} \leq \lim_{k\to\infty}x_{n_k} \leq \lim_{k\to\infty}s_{n_k}$, ou encore
            $\liminf_{n\to\infty}x_n \leq \lim_{k\to\infty}x_{n_k} \leq \limsup_{n\to\infty}x_n$. Ce qui prouve la deuxième partie du théorème.

            \paragraph{Corollaire} On peut établir un corollaire de ce théorème : une suite $(x_n) \subseteq \R$ converge si et seulement si

            \[\liminf_{n\to\infty}x_n = \limsup_{n\to\infty}x_n = L \in \R,\]

            en quel cas, $x_n \to L$.

            \paragraph{Démonstration} La démonstration se fait par le théorème du sandwich dans un sens et par le théorème de Bolzano-Weierstrass dans l'autre sens.

            \paragraph{Remarque} Nous avons donc maintenant trois résultats primordiaux sur la convergence des suites :

            \begin{enumerate}
                \item Toute suite convergente est bornée ;
                \item toute suite monotone et bornée est convergente ;
                \item Toute suite bornée possède une sous-suite convergente.
            \end{enumerate}

        \subsection{Le critère de Cauchy}

            Jusqu'ici, la notion de convergence fait intervenir \textit{directement} la notion de limite, ce qui sous-entend qu'il faut connaitre la limite avent de
            commencer la preuve de la convergence. Il existe donc une notion permettant de prouver la convergence sans expliciter la limite.

            \paragraph{Définition} Une suite $(x_n)$ est dite \textit{de Cauchy} si $\forall \epsilon > 0, \exists N > 0 \ | \, m, n \geq N \Rightarrow |x_n - x_m| < \epsilon$.

            Cette définition n'implique pas uniquement qu'il faut que $|x_n - x_{n+1}|$ tende vers zéro mais bien qu'il faut que \textbf{pour tout} $m, n \geq N$,
            ces deux éléments tendent vers zéro.

            \paragraph{Lemme} Soit $(x_n) \subseteq \R$ une suite convergente. $(x_n)$ est de Cauchy.

            \paragraph{Démonstration} Soit une suite $(x_n)$ convergeant en $a \in \R$. Par définition,
            $\forall \epsilon > 0, \exists N > 0 | n \geq N \Rightarrow | x_n - a < \epsilon|$. Soit $\epsilon' = \frac \epsilon2$. Soit ce $N$ découlant de la
            définition de convergence. Prenons $m, n \geq N$. On a donc $|x_m - x_n| = |x_m - a + a - x_n| < |x_m - a | + |x_n - a| = \epsilon'$.

            \paragraph{Théorème (critère de Cauchy)} Une suite $(x_n)$ converge si et seulement si elle est de Cauchy.

            \paragraph{Démonstration} L'implication $\Rightarrow$ est donnée par le lemme précédent. Il faut encore prouver l'implication $\Leftarrow$.

            Commençons par montrer que $(x_n)$ est bornée et puis appliquons Bolzano-Weierstrass. Par hypothèse, $(x_n)$ est de Cauchy. Donc $\forall \epsilon > 0,
            \exists N > 0 \, | \, m, n \geq N \Rightarrow |x_n - x_m| < \epsilon$. Soit $\epsilon$, prenons ce $N$ qui découle de la définition de suite de Cauchy.
            On a donc $\forall n \geq N, |x_n| = |x_n + x_N - x_N| \leq |x_n - x_N| + |x_N| < \epsilon + |x_N|$. Construisons $K = \max(\{|x_0|, |x_1|, |x_2|, \ldots,
            \epsilon + |x_N|\})$. On a donc $|x_n| < K \forall n \geq N$, ce qui implique que $(x_n)$ est bornée.

            Appliquons maintenant le théorème de Bolzano-Weierstrass qui dit qu'il existe une sous-suite $(x_{n_k})_k$ qui converge en une valeur $a \in \R$ quand
            $k \to \infty$. Par la convergence, il existe $N_1$ tel que $\forall k \geq N_1, |x_{n_k} - a| < \frac \epsilon2$. Et par la définition desuite de Cauchy,
            il existe $N_2$ tel que $m, n \geq N_2 \Rightarrow |x_n - x_m| < \frac \epsilon2$. Soit $\delta$ tel que $\delta \geq N_1$ et $n_\delta \geq N_2$.
            On a alors $\forall n \geq N_2, |x_n - a| = |x_n - x_\delta + x_\delta - a| \leq |x_n - x_\delta| + |x_\delta - a| < \epsilon$.

            Il y a donc également convergence de la suite $(x_n)$ en $a \in \R$ quand $n \to \infty$.

    \section{Fonctions continues}
        \subsection{Limite d'une fonction en un point}
            \paragraph{Def} Soient $a, b \in \R$ tels que $a < b$. On note :

                \[\begin{aligned}
                    \interval {a}{b} &:= \{x \in \R \, | \, a \leq x \leq b\} \\
                    ]a, b] &:= \{y \in \R \, | \, a < x \leq b\} \\
                    [a, b[ &:= \{x \in \R \, | \, a \leq x < b\} \\
                    ]a, b[ &:= \{x \in \R \, | \, a < x < b\} \\
                    [a, \infty[ &:= \{x \in \R \, | \, a \leq x\} \\
                    ]a, \infty[ &:= \{x \in \R \, | \, a < x\} \\
                    ]-\infty, b] &:= \{x \in \R \, | \, x \leq b\} \\
                    ]-\infty, b[ &:= \{x \in \R \, | \, x < b\} \\
                    ]-\infty, \infty[ &:= \R.
                \end{aligned}\]

                Ces ensembles sont appelés \textit{intervalles}.

            \paragraph{Def} Soit $U \subseteq \R$. $U$ est dit ouvert $\iff \forall x \in U, \exists \epsilon > 0 \, | \, ]x-\epsilon, x+\epsilon[ \in U$.
            $U$ est dit fermé $\iff \R \setminus U$ est ouvert.

            \paragraph{Def} Soient $A \subseteq \R$ et $a \in A$. $a$ est dit intérieur à $A$ s'$\exists \delta > 0 \, | \, ]a-\delta, a+\delta[ \subseteq A$. L'ensemble des points $a$ tels que $a$ est intérieur à $A$ est noté $\intr A$.

            \paragraph{Remarque} $\forall A \subseteq \R$, $\intr A \subseteq A$. De plus, un ensemble peut être simultanément ouvert et fermé : $]-\infty, \infty[$
            est ouvert car $\forall x \in R, \exists \epsilon > 0 \, | \, ]x-\epsilon, x+\epsilon[ \in \R$ et est fermé car $\R \setminus ]-\infty, \infty[ = \interval [open] 00$ est ouvert.

            \paragraph{Def} Soit $a \in \R$. Un voisinage de $a$ est un ensemble contenant un intervalle de la forme $]c, d[$ avec $c < a < d$.

            \paragraph{Def} Soient $A \subseteq R$ et $a \in \R$. $a$ est adhérent à $A$ si $\forall \delta > 0, ]a-\delta, a+\delta[ \cap A \neq \emptyset$. On note $\adh A$
            l'ensemble des points adhérents à $A$.

            \paragraph{Définition de la limite} Soient $f : U \subseteq \to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$. La limite de $f$ dans $B$ lorsque $x$ tend vers $a$
            existe dans $\R$ et vaut $L \in \R$ si $\forall \epsilon > 0, \exists \delta > 0 \, | \, \forall x \in (U \cap B), |x-a| < \delta \Rightarrow |f(x)-L| < \epsilon$.

            Cela se note $f(x) \to L$ lorsque $x \to a$ dans $B$ ou :

            \[\lim_{\underset{x \in B}{x \to a}} f(x) = L.\]

            \paragraph{Remarque} Ici, $\epsilon$ permet de déterminer un voisinage autour de $L$, la limite, alors que $\delta$ permet de déterminer un voisinage autour de $a$.

            \paragraph{Théorème} Soient $f : U \subseteq \R \to \R$, $B \subseteq \R$, $a \in \adh(B \cap U)$. Soient $L_1, L_2 \in \R$ tels que :

            \[\begin{aligned}
                \lim_{\underset{x \in B}{x \to a}} f(x) &= L_1 \\
                \lim_{\underset{x \in B}{x \to a}} f(x) &= L_2.
            \end{aligned}\]

            \paragraph{Démonstration} Soient $f : U \subseteq \R \to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$. Prenons $\epsilon = \frac {|L_1 - L_2|}3$.
            Par hypothèse, on sait qu'il existe $\delta_1 \, | \, \forall x \in U \cap B, |x-a| < \delta_1 \Rightarrow |f(x) - L_1| < \epsilon$ et
            $\delta_2 \, | \, \forall x \in U \cap B, |x-a| < \delta_2 \Rightarrow |f(x) - L_2| < \epsilon$.

            Soit $x_0 \in U \cap B$ tel que $|x_0 - a| < \min(\{L_1, L_2\})$. On sait alors que $f(x_0) \in ]L_1-\epsilon, L_1+\epsilon[$ et $f(x_0) \in ]L_2-\epsilon, L_2+\epsilon[$.
            Or, par choix de $\epsilon$, ces deux ensembles sont d'intersection vide. Il y a donc contradiction et $L_1 = L_2$.

            \paragraph{Def} La limite de $f : U \subseteq \R \to \R$ en $x \to a$ existe et vaut $L \in \R$ si la limite de $f$ en $x \to a$ dans $B = \R$ existe et vaut $L$.
            On note cela :

            \[\lim_{x \to a} f(x) = L \in \R.\]

            \subsubsection{Limites pointées, gauches et droites}

            Voyons dans quels contextes il est intéressant de manipuler le $B \subseteq \R$.

            \paragraph{Def} Soit $a \in \R$. Un voisinage de $a$ est pointé s'il contient un intervalle $]c, d[ \setminus \{a\}$. Un voisinage de $a$ est
            \textit{de droite} s'il contient un intervalle $[a, d[$ et peut être pointé si l'intervalle est sous la forme $]a, d[$. De manière similaire,
            un voisinage est \textit{de gauche} si l'intervalle est sous la forme $]c, a]$ et peut également être pointé.

            \paragraph{Definition des limites à gauche, à droite et pointées} Soit $f : U \to \R$ où $U$ est un intervalle contenant un voisinage pointé de $a$. La limite à
            gauche, respectivement à droite, respectivement pointée de $f$ en $x \to a$ existe et vaut $L \in \R$ si la limite de $f$ dans $B = ]-\infty, a[$, respectivement
            $]a, \infty[$, respectivement $\R \setminus \{a\}$ en $x \to a$ existe et vaut $L$.

            Cela se note respectivement :

            \[\lim_{\underset{<}{x \to a}}f(x) = L, \;\;\lim_{\underset{>}{x \to a}} f(x) = L, \;\; \lim_{\underset{\neq}{x \to a}} f(x) = L.\]

            \paragraph{Remarque} Pour pouvoir parler de limites de $f$ soit à gauhe, soit à droite, soit pointée, il faut impérativement que $f$ soit définie dans les
            alentours de $a$. Plus précisément, il faut $a \in \adh(A \cap B)$ avec $B$ défini selon le cas.

            \paragraph{Lemme} Soient $a \in \R$ et $f : U \to \R$ tels que $U$ définit un voisinage pointé de $a$. Alors $f$ possède une limite pointée en $a$ si et
            seulement si $f$ possède une limite à gauche en $a$ et une limite à droite en $a$ telles que $\lim_{x\to a^+}f(x) = \lim_{x \to a^-}f(x) = L \in \R$.
            Dans ce cas, on a $\lim_{\underset{\neq}{x \to a}} f(x) = L$.

            \paragraph{Proposition} Soient $f : U\to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$. La limite de $f$ dans $B$ en $x \to a$ existe et vaut $L \in \R$ si et
            seulement si $\forall A \subseteq B$, la limite de $f$ dans $A$ en $x \to a$ existe et vaut $L$.

            \paragraph{Démonstration} Pour la condition suffisante ($\Leftarrow$), on sait que pour \textbf{tout} $A \subseteq B$, la limite existe. En prenant $A = B$,
            on sait que la limite existe également dans $B$ et vaut $L \in \R$.

            Pour la condition nécessaire ($\Rightarrow$), observons que
            $\forall \epsilon > 0, \exists \delta > 0 \, | \, x \in (B \cap U) \land |x-a| < \delta \Rightarrow |f(x)-L| < \epsilon$.
            On sait alors que pour $\epsilon \in \R_0^+$ fixé et pour $A \subseteq B$, il existe $\delta \in \R_0^+$ tel que $x \in A \Rightarrow |f(x) - L| < \epsilon$.

            \paragraph{Proposition} Soient $f : U \to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$. La limite de $f$ dans $B$ en $x \to a$ existe et vaut $L \in \R$
            si et seulement si pour toute suite $(x_n)_{n \in \mathbb N} \subseteq B$ telle que $x_n \to a$, la suite $(f(x_n))_{n \in \mathbb N} \subseteq \R$ converge en $L$.

            \paragraph{Démonstration} Soit $\epsilon > 0$. On sait par hypothèse qu'$\exists \delta > 0$ tel que
            $x \in (B\cap U) \land |x-a| < \delta \Rightarrow |f(x)-L| < \epsilon$. Soit $(x_n) \subseteq B$ telle que $x_n \to a$. On sait donc qu'$\exists N > 0$ tel que
            $n > N \Rightarrow |x_n - a| < \epsilon$. Si ceci est vrai pour tout $\epsilon$, ça l'est plus précisément pour $\delta$ déterminé par l'hypothèse.
            On a donc $\forall \epsilon > 0, \exists N > 0 \, | \, n > N \Rightarrow |f(x_n) - L| < \epsilon$ ou encore $f(x_n) \to L$ pour $n \to \infty$.

            Montrons maintenant que si toutes les suites de $B$ convergentes en $a$ implique $f(x_n) \to L$, alors la limite de $f$ en $a$ existe et vaut $L$.
            Fonctionnons par l'absurde : soit $(x_n) \subseteq B$ une suite dans $B$ telle que $x_n \to a$ quand $n \to \infty$ et $(f(x_n)) \to L \in \R$ mais alors que
            $f(x) \not \to L$ pour $x \to a$. On sait alors qu'$\exists \epsilon_0 > 0 \, | \, \forall \delta > 0$ il n'y a pas de convergence de $f(x)$ en $L$.
            Soit $\epsilon_0$. Prenons $\delta = \frac 1n$. On en déduit que $\forall n \geq 1$, il existe $x_n \in U$ tel que $|x_n-a| < \frac 1n$ et
            $|f(x_n) - L| \geq \epsilon_0$ donc tel que $x_n \to a$ mais $f(x_n) \not \to L$ lorsque $n \to \infty$. Or par hypothèse, on sait que
            $(f(x_n)) \to L$. Ce qui est une contradiction avec $|f(x_n) - L| \geq \epsilon_0$. Donc $f(x) \to L$ pour $x \to a$.

            \subsubsection{Règles de calcul de limite de fonctions}

            \paragraph{Théorème} Soient $f, g : U \subseteq \R \to \R$ et $a \in \R$ tels que $f$ et $g$ sont définies dans le voisinage de $a$. Soient $L_f$ et $L_g$ tels
            que $\lim_x{x \to a}f(x) = L_f$ et $\lim_{x \to a}g(x) = L_g$. Alors :

            \[\begin{aligned}
                \lim_{x \to a}(f+g)(x) &= L_f + L_g \\
                \lim_{x \to a}(fg)(x) &= L_fL_g.
            \end{aligned}\]

                Si $L_g \neq 0$, la fonction $\frac fg(x)$ existe dans le voisinage de $a$ telle que :

            \[\begin{aligned}\lim_{x \to a}\frac fg(x) = \frac {L_f}{L_g}.\end{aligned}\]

            \paragraph{Démonstration} Soient $f, g : U \to \R$ convergents respectivement en $L_f$ et $L_g$ quand $x \to a$.

            \subparagraph{Addition} Montrons que la limite de la somme vaut la somme des limites.
            Soit $\epsilon > 0$. On sait par la définition des limites de $f$ et $g$ qu'il existe $\delta_1$ tel que $\forall x \in U, |x-a| < \delta_1 \Rightarrow |f(x) - L_f| < \frac \epsilon2$
            et $\delta_2$ tel que $\forall x \in U, |x-a| < \delta_2 \Rightarrow |g(x)-L_g| < \frac \epsilon2$. Prenons alors $\delta = \min(\{\delta_1, \delta_2\})$.
            On sait dès lors que $\forall x \in U, |f(x) + g(x) - L_f - L_g| = |f(x)-L_f + g(x)-L_g| \leq |f(x)-L_f| + |g(x)-L_g| < \frac \epsilon2 + \frac \epsilon2 = \epsilon$.

            \subparagraph{Multiplication} Montrons que le produit des limite vaut le produit des limites. Montrons tout d'abord qu'il existe un voisinage autour de $a$ tel que $f(x)$ est bornée.

            Soit $\epsilon = 1$. On sait alors qu'il existe $\delta_1$ tel que $\forall x \in U, |x-a| < \delta_1 \Rightarrow |f(x) - L_f| < 1$. Ce $\delta_1$
            définit un voisinage de $a$ où $|f(x)| < |L_f| + 1 \forall x$.

            Montrons ensuite que $\lim_{x \to a}(fg)(x) = L_fL_g$.

            Soit $\epsilon \in \R_0^+$. On sait qu'il existe $\delta_2$ tel que $\forall x \in U, |x-a| < \delta_2 \Rightarrow |f(x)-L_f| < \frac {\epsilon}{2(|L_g|+1)}$
            et $\delta_3$ tel que $\forall x \in U, |x-a| < \delta_3 \Rightarrow |g(x)-L_g| < \frac {\epsilon}{2|L_f|}$. Prenons alors $\delta = \min(\{\delta_i | i \in [3]\})$.
            On a alors $\forall x \in U$ tel que $|x-a| < \delta$ :

            \[\begin{aligned}
                |f(x)g(x) - L_fL_g| &= |f(x)(g(x) - L_g) + L_g(f(x) - L_f)| \leq |f(x)||g(x) - L_g| + |L_g||f(x) - L_f| \\
                                    &< (|L_f| + 1)\frac {\epsilon}{2(|L_f|+1)} + |L_g|\frac {\epsilon}{2|L_g|} = 2\frac {\epsilon}{2} = \epsilon.
            \end{aligned}\]

            \subparagraph{Quotient} Montrons que la limite du quotient vaut le quotient des limites. Commençons par montrer qu'il existe un voisinage de $a$ où $g(x) \neq 0$.

            Soit $\epsilon = \frac {|L_g|}2$. On sait alors qu'il existe $\delta_V$ tel que $\forall x \in U, |x-a| < \delta_V \Rightarrow |g(x)-L_g| < \frac {|L_g|}2$.
            Ou encore pour ce même $\delta_V$, $|g(x)| > \frac {|L_g|}2$. Donc $|g(x)|$ est strictement positif. Définissons alors $V := U \cap ]a-\delta_V, a+\delta_V[$,
            un voisinage de $a$ sur lequel la fonction $\frac fg(x)$ est bien définie.
            
            Prouvons maintenant que $\frac 1g(x) \to \frac 1{L_g}$ quand $x \to a$, et le résultat découlera de la proposition précédente.
            
            Soit $\epsilon > 0$. On sait qu'il existe $\delta > 0$ tel que $\forall x \in U, |x-a| < \delta \Rightarrow |g(x) - L_g| < \frac {|L_g|^2\epsilon}2$.
            On sait dès lors :
            
            \[\left|\frac 1{g(x)} - \frac 1{L_g}\right| = \frac {|g(x)-L_g|}{|g(x)||L_g|} < \frac {|g(x)-L_g|}{\frac {|L_g|}{2}|L_g|}
              = \frac {2|g(x)-L_g|}{|L_g|^2} < \frac {2(\frac {|L_g|^2\epsilon}{2})}{|L_g|^2} = \epsilon.\]
              
            \paragraph{Théorème} le théorème du sandwich des suites a un hoologue pour les fonctions : Soient $f, g, h : \R \to \R$ trois fonctions telles que
            $\forall x \in \R, f(x) \leq g(x) \leq h(x)$ avec $\lim_{x \to a}f(x) = \lim_{x \to a}h(x) = L \in \R$. Alors $\lim_{x \to a}g(x) = L$.
            
            \paragraph{Démonstration} Définissons les suites $(f(x_n))_n, (g(x_n))_n$ et $(h(x_n))_n$. On sait que $\forall k \in {\R}^{\R}, \lim_{x \to a}k(x)$ existe
            et vaut $L_k$ si et seulement si pour toute suite $(x_n) \subseteq \R$ convergente en $a \in \R$, on a $k(x_n) \to L_k$ quand $n \to \infty$.
            
            On sait dès lors que $f(x_n) \to L$ et $h(x_n) \to L$. Par le théorème du sandwich, on sait que $g(x_n) \to L$ également.
            
            \paragraph{Théorème (conservation des inégalités)} Soient $f, g : U \subseteq \R \to \R$, $B \subseteq \R$ et $a \in \adh(U \cap B)$.
            Si $\exists \delta > 0$ tel que $\forall x \in ]a-\delta, a+\delta[ \cap U \cap B, f(x) \leq g(x)$ et $\lim_{x \to a}f(x)$ et $\lim_{x \to a}g(x)$
            existent dans $\R$, alors $\lim_{x \to a}f(x) \leq \lim{x \to a}g(x)$.
            
            \subsubsection{Limites infinies et limites à l'infini}
            
            \paragraph{Définition} Soient $f, g : U \subseteq \R \to \R$, $B \subseteq \R$, $a \in \adh(U \cap B)$. La limite de $f$ dans $B$ en $a$ existe et vaut
            $+\infty$ si $\forall K > 0, \exists \delta > 0 \, | \, \forall x \in (U \cap B), |x-a| < \delta \Rightarrow f(x) > K$. Cela se note :
            
            \[\lim_{x \to a}f(x) = \infty.\]
            
            De manière similaire, la limite de $f$ dans $B$ en $a$ vaut $-\infty$ si $\forall K > 0, \exists \delta > 0 \, | \, \forall x \in (U \cap B), |x-a| < \delta \Rightarrow f(x) < -K$.
            Ce la se note :
            
            \[\lim_{x \to a}f(x) = -\infty.\]
            
            \paragraph{Remarque} les cas particuliers où $B = \R, B = ]-\infty, a], B = [a, +\infty[, B = \R \setminus \{a\}$ donnent les définitions de limite
            standard infinie, limite à gauche, limite à droite et limite pointée infinies.
            
            \paragraph{Définition} Soit $f : U \subseteq \R \to \R$ où $U$ n'est pas majoré. La limite de $f$ en « l'infini » vaut $L \in \R$ si
            $\forall \epsilon > 0, \exists M > 0 \, | \, \forall x \in U, x > M \Rightarrow |f(x) - L| < \epsilon$. Cela se note :
            
            \[\lim_{x \to \infty}f(x) = L.\]
            
            De manière similaire, si $U$ n'est pas minoré, la limite de $f$ en « moins l'infini » vaut $L \in \R$ si
            $\forall \epsilon > 0, \exists M > 0 \, | \, \forall x \in U, x < -M \Rightarrow |f(x) - L| < \epsilon$. Cela se note :
            
            \[\lim_{x \to -\infty}f(x) = L.\]
            
            

\end{document}
