\documentclass{report}

\usepackage{commath}
\usepackage{hyperref}
\usepackage[francais]{babel}
\usepackage{mathtools}
\usepackage{eulervm, palatino}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[parfill]{parskip}
\usepackage{fullpage}
\usepackage{amsmath, amsthm,amssymb, amsfonts}
\usepackage{stmaryrd}
\usepackage{ulem}

\title{Statistiques mathématiques}
\author{R. Petit}
\date{année académique 2016 - 2017}

\DeclareMathOperator{\tq}{\text{ t.q. }}
\DeclareMathOperator{\Unif}{Unif}  % uniform law
\DeclareMathOperator{\Bern}{Bern}  % Bernoulli law
\DeclareMathOperator{\Bin}{Bin}  % Binomial law
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\MAE}{MAE}
\DeclareMathOperator{\Jac}{Jac}  % Jacobian
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\intr}{int}  % interior of set

\DeclareMathOperator*{\argmax}{arg\,max}

\renewcommand{\P}{\mathbb P}
\newcommand{\E}{\mathbb E}

\newcommand{\minfty}{{-\infty}}
\newcommand{\pinfty}{{+\infty}}
\newcommand{\cvgp}{\xrightarrow[n \to \pinfty]{\P}}
\newcommand{\cvgLr}{\xrightarrow[n \to \pinfty]{L_r}}
\newcommand{\cvgd}{\xrightarrow[n \to \pinfty]{\mathcal D}}
\newcommand{\ps}{{\text{p.s.}}}
\newcommand{\cvgps}{\xrightarrow[n \to \pinfty]\ps}
\newcommand{\distreq}{\overset {\mathcal D}=}
\newcommand{\statmod}[4]{\left(#1^{#4}, #2\left(#1^{#4}\right), #3^{\left(#4\right)}\right)}
\newcommand{\probspace}[3]{\left(#1, #2, #3\right)}
\newcommand{\Nms}{\mathcal N(\mu, \sigma^2)}
\newcommand{\Nzo}{\mathcal N(0, 1)}
\newcommand{\vvp}[2]{\begin{pmatrix}#1 \\ #2\end{pmatrix}}

\newcommand{\N}{\mathbb N}
\newcommand{\Ns}{\N^{*}}
\newcommand{\R}{\mathbb R}
\newcommand{\Rp}{{\mathbb R^+}}
\newcommand{\Rm}{{\mathbb R^-}}

\newcommand{\Brl}{\mathcal B}  % \sigma-algebra of borelians
\newcommand{\charfun}[1]{I_{\left[#1\right]}}
\newcommand{\intint}[2]{\left\llbracket#1, #2\right\rrbracket}

\newcommand{\n}{{(n)}}
\newcommand{\Xn}{{X^\n}}
\newcommand{\Tn}{{T^\n}}
\newcommand{\TnXn}{{\Tn(\Xn)}}

\newtheorem{thm}{Théorème}[chapter]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prp}[thm]{Proposition}
\theoremstyle{definition}
\newtheorem{déf}[thm]{Définition}
\theoremstyle{remark}
\newtheorem*{rmq}{Remarque}
\newtheorem{ex}{Exemple}[chapter]

\NoAutoSpaceBeforeFDP

\begin{document}
\pagenumbering{Roman}
\maketitle
\tableofcontents
\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

\chapter*{Introduction}
	En probabilités, une variable aléatoire $X$ donnée est entièrement définie par sa loi. On peut l'exprimer par la fonction de répartition $F^X$ ou par la
	fonction de densité $f^X = \od {}xF^X$. Ces fonctions permettent de déterminer~:
	\[\P[a \leq X \leq b] = \int_a^bf^X(x)\dif x = F^X(b) - F^X(a).\]
	Ou encore~:
	\[\E[X] = \int_\minfty^\pinfty xf^X(x)\dif x.\]

	Cependant, les fonctions $f^X$ et $F^X$ ne sont jamais connues précisément. Elles peuvent être approchées par des modélisations, mais les modèles ne sont
	jamais exacts. En probabilités, on cherche donc les observations sur base de la loi qui est connue, alors qu'en statistiques, on cherche à retrouver la loi
	sur base de $n$ observations $X_1, \ldots, X_n$.

	Nous allons nous intéresser à des \textit{modèles statistiques} sous la forme $\statmod \R\Brl{\mathcal P}n$ où~:
	\[\mathcal P^{(n)} = \left\{\P^{(n)}\right\} = \left\{\P^{(n)}_\theta \tq \theta \in \Theta \subset \R^k\right\},\]
	et donc les $P^{(i)}$ sont chacun une loi possible pour $(X_1, \ldots, X_n)$.

	Ces modèles sont dits \textit{paramétriques} car les différentes lois sont les mêmes au paramètre $\theta$ près. Nous n'étudierons que des modèles
	paramétriques où $\Theta$ est un espace de dimension $d \in \N$ finie.

	\begin{ex} Soient $X_1,\ldots, X_n$ des variables aléatoires iid (indépendantes et identiquement distribuées).
	\begin{itemize}
		\item Si les $X_i$ sont de loi normale $\Nms$, alors le paramètre $\theta$ est donné par ~:
		      \[\theta = \vvp \mu{\sigma^2} \in \Theta = \R \times \R^+ \subset \R^2~;\]
		\item si les $X_i$ sont de loi uniforme $\Unif(0, \theta)$, le paramètre $\theta$ est donné par $\theta \in \Theta = \R_0^+ \subset \R$~;
		\item si les $X_i$ sont de loi $\Bern(p)$, le paramètre $\theta$ est donné par $\theta = p \in \Theta = [0, 1] \subset \R$.
	\end{itemize}
	\end{ex}

	\begin{rmq} Une loi normale $\Nms$ est déraisonnable car les valeurs observables ne vont empiriquement pas vers les infinis alors que la distribution le
	permet théoriquement mais n'est pas \textbf{complètement} déraisonnable car ces probabilités sont négligeables grâce à l'exponentielle de $(-x^2)$ dans la
	formule de la densité.
	\end{rmq}

\chapter{Théorie de l'échantillonnage}
	\section{Terminologie et définitions}
		\begin{déf} On appelle \textit{modèle d'échantillonnage} un modèle d'observations iid.  \end{déf}

		\begin{déf} Soit un modèle statistique $\statmod \R\Brl{\mathcal P}n$ où $\mathcal P^{(n)} =
		\left\{\P_\theta^{(n)} \tq \theta \in \Theta \subset \R^k\right\}$. On note ici $\P^{(n)}_\theta$ une loi possible pour $(X_1, \ldots, X_n)$ et $\P_\theta$
		une loi possible pour $X_i$ avec $i$ fixé. On dit alors que $\P^{(n)}_\theta$ est déterminé par $\P_\theta$.
		\end{déf}

		\begin{rmq} Ici, deux visions vont s'opposer et se compléter~: la vision \textit{population} qui est associée à $P_\theta$ et la version
		\textit{échantillonage} (ou \textit{empirique}), qui, elle, est associée à $P_\theta^{(n)}$.
		\end{rmq}

		\begin{déf} On définit la fonction indicatrice $I_{[\cdot]}$ qui vaut 1 quand l'expression entre crochets est vraie et 0 sinon. \end{déf}

		\begin{déf} Soit $X_1, \ldots, X_n$ une suite de $n$ observations. On définit la \textit{$i$eme statistique d'ordre} par
		$X_{(i)} = X_k \tq \abs {\{X_j \tq X_j < X_k, 1 \leq j \leq n\}} = i$. On définit également la \textit{statistique d'ordre} par $\left(X_{(i)}\right)_i$.
		\end{déf}

		\begin{déf} On définit les fonctions de répartitions comme suit~:
		\begin{itemize}
			\item la fonction de répartition population~:
			      \[F_\theta(x) = P_\theta[X_i \leq x]~;\]
			\item la fonction de répartition empirique~:
			      \[F_n(x) = \frac 1n\sum_{i=1}^nI_{[X_i \leq x]}.\]
		\end{itemize}
		\end{déf}

		\begin{rmq} La fonction $F_n$ empirique est une fonction en escaliers. Elle fait des sauts de hauteur $\frac 1n$, et est telle que~:
		\[\lim_{x \to \pinfty}F_n(x) = 1\qquad\qquad \text{ et } \qquad\qquad \lim_{x \to \minfty}F_n(x) = 0.\]

		On peut également remarquer que $F_n(X_{(i)}) = \frac in$. En effet, par définition de $X_{(i)}$, il y a exactement $i$ observations inférieures à
		$X_{(i)}$. Dès lors, la fonction indicatrice donnera $i$ fois la valeur 1 et $(n-i)$ fois la valeur 0. La somme donc donc $i$ et la fonction donne
		$\frac in$. \end{rmq}

		\begin{déf} On appelle \textit{statistique} toute fonction mesurable faisant intervenir \textbf{uniquement} des observations. \end{déf}

		\begin{ex} Par exemple $F_n$ est une statistique car seules les valeurs $X_i$ sont utilisée, mais $F_\theta$ n'est pas une statistique car la valeur
		du paramètre $\theta$ apparait et n'est pas une observation. \end{ex}

		\begin{rmq} Une statistique peut être à valeur scalaire ($X_{(i)}$ par exemple), à valeur vectorielle ($\left(X_{(i)}\right)_{1 \leq i \leq n}$ par
		exemple), à valeur ensembliste ($[X_i \pm \overline X]$ avec $i$ fixé par exemple), ou encore à valeur fonctionnelle ($F_n$ par exemple). \end{rmq}

		\begin{rmq} L'objectif est de pouvoir approximer la loi régissant les populations ($F_\theta$) à l'aide de la loi observée empiriquement. Par la loi
		des grands nombres, on a~:
		\[F_n(x) \xrightarrow[n \to \pinfty]{p.s. \text{ par } \P_\theta} F_\theta(x).\]
		\end{rmq}

		\begin{thm}[Théorème de Glivenko-Cantelli] Si $F_n$ et $F_\theta$ sont repsectivement une fonction de répartition empirique et de population, alors~:
		\[\sup_{x \in \R}\abs {F_n(x) - F_\theta(x)} \cvgps 0\]
		\end{thm}

	\section{Moments}
		\begin{déf}[Moments pour populations] On définit $\mu_r'(\theta)$ le \textit{moment \textbf{non}-centré} d'ordre $r$ avec $r \in \Ns$ par~:
		\[\mu_r'(\theta) \coloneqq E_\theta[X_1^r].\]

		On définit également $\mu_r(\theta)$, le \textit{moment \textbf{centré}} d'ordre $r$ avec $r \in \Ns$ par~:
		\[\mu_r(\theta) \coloneqq E_\theta\left[\left(X_1 - \mu_r'(\theta)\right)^r\right].\]
		\end{déf}

		\begin{déf}[Moments pour échantillon] On définit $m_r'$, le \textit{moment \textbf{non}-centré} d'ordre $r$ avec $r \in \Ns$ par~:
		\[m_r' \coloneqq \frac 1n\sum_{i=1}^n X_i^r.\]

		On définit également le \textit{moment \textbf{centré}} d'ordre $r$ avec $r \in \Ns$ par~:
		\[m_r \coloneqq \frac 1n\sum_{i=1}^n\left(X_i - m_r'\right)^r.\]
		\end{déf}

		\begin{rmq} La loi des grands nombres dit que~:
		\[m_r' \cvgps \mu_r'(\theta),\]
		mais on ne peut pas dire que~:
		\[m_r \cvgps \mu_r(\theta).\]

		Ce n'est donc pas possible car pour $m_r'$, il y a une somme de variables iid alors que pour $m_r$, les variables sommées ne sont pas iid (mais
		dépendent toutes de tous les $X_i$).

		En réalité, il y a convergence, mais on ne peut pas l'exprimer de manière triviale par la loi des grands nombres. \end{rmq}

		\subsection{Indicateurs}
			On peut observer que $\mu_1'(\theta) = \E_\theta[X_1]$. Pareil pour $m_1' = \overline X$. Le moment d'ordre 1 est donc un indice de position.
			On a alors $\mu \coloneqq \mu_1(\theta) = \E[(X - \E[X_1])] = \E[X_1] - \E[X_1] = 0$. Cette valeur n'est donc pas intéressante. Par contre~:
			\[\mu_2(\theta) = \E\left[(X_1 - \E[X_1])^2\right] \eqqcolon \Var(X) \qquad\qquad \text{ si }
				\qquad\qquad m_2 = \frac 1n\sum_{i=1}^n\left(X_i - \overline X\right)^2 \eqqcolon s^2.\]
			Le moment d'ordre 2 est donc un indice de dispersion.

			\begin{déf} On appelle le \textit{coefficient d'asymétrie de Fisher} la quantité~:
			\[\gamma_1 \coloneqq \mu_3(\theta) \cdot \left(\mu_2(\theta)\right)^{-\frac 32}.\]
			\end{déf}

			\begin{rmq} Le dénominateur $\mu_2(\theta)^{\frac 32}$ apparait afin de rendre invariant le coefficient d'asymétrie de Fisher aux transformations
			affines. \end{rmq}

			\begin{déf} Le coefficient d'asymétrie de Fisher \textit{empirique} est donné par~:
			\[m_3 \cdot m_2^{-\frac 32}.\]
			\end{déf}

			\begin{déf} On appelle \textit{coefficient d'applatissement de Fisher} la quantité~:
			\[\gamma_2 \coloneqq \mu_4(\theta) \cdot \left(\mu_2(\theta)\right)^{-2} - 3.\]
			\end{déf}

			\begin{déf} Le coefficient d'aplatissement de Fisher \textit{empirique} est donné par~:
			\[m_4 \cdot m_2^{-2} - 3.\]
			\end{déf}

			\begin{rmq} Si $\gamma_2 \gneqq 0$, c'est que les événements extrêmes sont de plus haute probabilité et si $\gamma_2 \lneqq 0$, c'est que les
			événements extrêmes sont de moins haute probabilité.

			À nouveau, le dénominateur y a été ajouté afin de rendre le coefficient invariant aux transformations affines. Et le terme $-3$ sert à annuler le
			coefficient d'aplatissement de Fisher pour une normale $\Nms$. \end{rmq}

	\section{Quantile}
		\begin{déf} Si $F_\theta$ est inversible, alors on définit $x_\alpha(\theta) \coloneqq F_\theta^{-1}(\alpha)$, et on appelle $x_\alpha(\theta)$ un
		\textit{quantile}. \end{déf}

		\begin{rmq} Il faut cependant faire attention car on peut avoir le cas de $F_\theta$ discontinue où on choisit $\alpha = F_\theta^{-1}$(point de
		discontinuité) ou alors le cas de $F_\theta$ admettant un plateau et où on choisit $\alpha$ sur le plateau. \end{rmq}

		\begin{déf} On définit alors~:
		\[x_\alpha(\theta) \coloneqq \inf\left\{x \in \R \tq F_\theta(x) \geq \alpha\right\}.\]
		\end{déf}

		\begin{rmq} On donne les noms de \textit{médiane}, \textit{quartile}, \textit{décile}, \textit{percentile} pour $\alpha$ valant, avec $k$ entier,
		respectivement $\frac 12$, $\frac k4$ avec $k < 4$, $\frac k{10}$ avec $k < 10$, et $\frac k{100}$ avec $k < 100$.
		\end{rmq}

		\begin{déf} Pour les échantillons, on définit le \textit{quantile empirique d'ordre $\alpha$} par~:
		\[x_\alpha^{(n)} \coloneqq \inf\{x \in \R \tq F_n(x) \geq \alpha\}.\]
		\end{déf}

		\begin{rmq} On peut également définir des indices de position, dispersion, asymétrie, aplatissement, etc. sur les quantiles plutôt que sur les moments.
		Ils auront des propriétés différentes et une robustesse différente aux valeurs aberrantes.
		\end{rmq}

		\begin{déf} La loi échantillonnée de $T(X^{(n)})$ est la loi déterminée par~:
		\[\P_\theta^{(n)}\left[T(X^{(n)}) \in B\right] = \P_\theta^{(n)}\left[\left\{x^{(n)} \in \mathcal X^{(n)} \tq T(x^{(n)}) \in B\right\}\right], B \in \Brl(\R^m).\]
		\end{déf}

		\begin{ex}[Bernoulli] $X^{(n)} = (X_1, \ldots, X_n)$ où les $X_i$ sont iid $\Bern(p)$. On a alors~: $T(X^{(n)}) = \sum_{i=1}^nX_i$, sous
		$\P_\theta^{(n)}$, est de loi $\Bin(n, p)$.
		\end{ex}

		\begin{ex}[Normale] $X^{(n)} = (X_1, \ldots, X_n)$ où les $X_i$ sont iid $\Nms$ et où
		$\theta = \begin{pmatrix}\mu \\ \sigma^2\end{pmatrix} \in \Theta = \R \times \R^+_0 \subset \R^2$.
		La statistique $T_1(X^{(n)}) = \sum_{i=1}^nX_i$, sous $P_\theta^{(n)}$, est de loi $\mathcal N(n\mu, n\sigma^2)$.

		La statistique $T_2(X^{(n)}) = \frac 1n\sum_{i=1}^nX_i$, sous $\P_\theta^{(n)}$, est de loi $\mathcal N(\mu, \frac {\sigma^2}n)$.
		\end{ex}

		\begin{ex}[Uniforme] $X^{(n)} = (X_1, \ldots, X_n)$ où les $X_i$ sont iid $\Unif(0, \theta)$, pour $\theta \in \Theta = \R^+_0 \subset \R$.
		On a donc $f_\theta^{X_i}(x) = \theta^{-1}\charfun {0 \leq x \leq \theta}$. Et donc~:
		\[F_\theta^{X_i}(x) = \begin{cases}0 &\text{ si } x < 0 \\\frac x\theta &\text{ si } 0 \leq x \leq \theta \\1 &\text{ sinon}\end{cases}.\]

		La statistique $T(X^{(n)}) = X_{(n)} = \max_{1 \leq k \leq n}\{X_k\}$ a pour fonction de répartition, sous $P_\theta^{(n)}$~:
		\[F_\theta^{(n)}(x) = \P[X_{(n)} \leq x] = \P[X_1 \leq x, X_2 \leq x, \ldots, X_n \leq x].\]

		La seconde forme est plus agréable car on a une intersection d'événements indépendants. Donc~:
		\[F_\theta^{(n)}(x) = \prod_{i=1}^n\P[X_i \leq x] = \prod_{i=1}^nF_\theta^{X_i}(x) =
			\begin{cases}0 &\text{ si } x < 0 \\\left(\frac x\theta\right)^n &\text{ si } 0 \leq x \leq \theta \\0 &\text{ sinon}\end{cases}.\]

		On a alors la fonction de densité~:
		\begin{align*}
			f_\theta^{X_{(n)}}(x) &= \od {}xF_\theta^{X_{(n)}}\sVert[3]_x =
			\begin{cases}0 &\text{ si } x < 0 \\\frac {nx^{n-1}}{\theta^n}\charfun{0 \leq x \leq \theta} &\text{ si } 0\leq x \leq \theta \\0 &\text{ sinon}\end{cases} \\
			&= \frac {nx^{n-1}}{\theta^n}\charfun{0 \leq x \leq \theta}.
		\end{align*}
		\end{ex}

		\begin{rmq} La loi échantillonnée n'est pas toujours possible à déterminer exactement analytiquement. Dans ce cas, on donne~:
		\begin{itemize}
			\item[$(i)$] les/des moments de la loi échantillonnée exacte~;
			\item[$(ii)$] la loi échantillonnée asymptotique.
		\end{itemize}

		Et pour de grandes valeurs de $n$, la loi asymptotique donne une assez bonne approximation de la loi exacte.
		\end{rmq}

		\begin{rmq} Ici, les termes \textit{exact} et \textit{asymptotique} s'opposent~: on parle d'objet \textit{exact} lorsque l'objet est connu pour $n$ fixé,
		et d'objet \textit{asymptotique} lorsque l'objet n'est connu que pour $n \to \pinfty$.
		\end{rmq}

		\begin{ex} Voici un cas où on ne peut exprimer de loi exacte mais où il est possible d'exprimer une loi asymptotique. Soit $X^{(n)} = (X_1, \ldots, X_n)$
		où les $X_i$ sont iid $F$ avec la fonction $F$ telle que $\Var_F(X_i) = \sigma^2 < \pinfty$ et donc $E_F(X_i) = \mu < \pinfty$. On peut dès lors
		appliquer le théorème central limite (TCL)~:
		\[\sqrt n(\overline X^{(n)} - \mu) \cvgd \mathcal N(0, \sigma^2).\]

		Pour $n \gg$, on peut alors dire~:
		\[\overline X^{(n)} \approx \mathcal N\left(\mu, \frac {\sigma^2}n\right),\]
		où le symbole $\approx$ se lit \textit{est à peu près de même loi}.

		On en conclut donc qu'avec $n$ suffisamment grand, on peut approximer $\overline X^{(n)}$, même sans connaitre sa loi exacte.
		\end{ex}

	\subsection{Lemme de Fisher}
		\begin{déf} La variable aléatoire $Q$ est de loi $\chi^2$ (chi-carrée) à $k (\in \N^*)$ degrés de liberté lorsque~:
		\[Q \distreq \sum_{i=1}^kZ_i^2,\]
		où les $Z_i$ sont iid $\Nzo$ et où «~$\distreq$~» veut dire \textit{a la même distribution que}. Cela se note~:
		\[Q \sim \chi^2_k\]
		\end{déf}
		\begin{rmq} Si $Q \sim \chi^2_k$, alors~:
		\[f^Q(x) = \frac 1{2^{\frac k2}\Gamma\left(\frac k2\right)}x^{\frac k2-1}\exp\left(-\frac x2\right)\charfun{x > 0},\]
		où $\Gamma$ est la fonction Gamma d'Euler définie par~:
		\[\Gamma(x) = \int_0^\pinfty t^{x-1}\exp(-t)\dif t.\]

		De plus, $\Var(Q) = 2k$, et $\E(Q) = k$.

		On peut également noter que les $\chi^2$ sont stables par la somme~: si $Q_1 \sim \chi^2_{k_1}$ et $Q_2 \sim \chi^2_{k_2}$, alors~:
		\[Q_1 + Q_2 \sim \chi^2_{k_1+k_2}.\]
		\end{rmq}

		\begin{lem}\label{lem:preFisher} Soit $W = (W_1, \ldots, W_k)$ un vecteur de variables aléatoires, où $f^W$ : $\R^k \to \R^+$ est la fonction de
		densité du vecteur $W$. Alors~:
		\begin{enumerate}
			\item $\P[W \in B] = \int_Bf^W(x)\dif x$~;
			\item si $V = AW + b$ où $A$ est une matrice $k \times k$ inversible, alors~:
				\[f^V(v) = \abs {\det A^{-1}}f^W\left(A^{-1}(v-b)\right).\]
		\end{enumerate}
		\end{lem}

		\begin{thm}[Lemme de Fisher] Soient $X_1, \ldots, X_n$ iid $\Nms$ où $n \geq 2$. Alors~:
		\begin{enumerate}
			\item[$(i)$]   $\overline X \sim \mathcal N(\mu, \frac {\sigma^2}n)$~;
			\item[$(ii)$]  $\frac {ns^2}{\sigma^2} \sim \chi^2_{n-1}$~;
			\item[$(iii)$] $\overline X \sqcup s^2$.
		\end{enumerate}
		\end{thm}

		\begin{proof} Posons $Z_i \coloneqq \frac {X_i - \mu}\sigma$ pour $i \in \intint 1n$. Puisque les $X_i$ sont iid, les $Z_i$ le sont également (même
		transformation appliquée à tous les $X_i$ et chaque $Z_i$ ne fait intervenir que le $X_i$ correspondant). Notons que~:
		\[\overline X = \frac 1n\sum_{i=1}^nX_i = \frac 1n\sum_{i=1}^n\left(\sigma Z_i + \mu\right) = \sigma\overline Z + \mu,\]
		où $\overline Z$ est la moyenne empirique des $Z_i$. Notons également que~:
		\[ns^2 = \sum_{i=1}^n(X_i - \overline X)^2 = \sum_{i=1}^n\left(\left(\sigma Z_i + \mu\right) - \left(\sigma\overline Z + \mu\right)\right)^2
			= \sigma^2\sum_{i=1}^n\left(Z_i - \overline Z\right)^2 = n\sigma^2s_Z^2.\]

		Il nous faut alors montrer que $\overline Z \sim \Nzo$ et $ns_Z^2 \sim \chi^2_{n-1}$, avec $\overline Z \sqcup s_Z^2$.

		Pour cela, on sait que le vecteur $Z^{(n)} = (Z_1, \ldots, Z_n)$ a pour densité~:
		\[f^{Z^{(n)}}(z^{(n)}) = \prod_{i=1}^nf^{Z_i}(z_i) = \prod_{i=1}^n\left(\frac 1{\sqrt {2\pi}}\exp\left(\frac {z_i^2}2\right)\right)
			= \left(\frac 1{\sqrt {2\pi}}\right)^n\exp\left(-\sum_{i=1}^n\frac {z_i^2}2\right)
			= \left(\frac 1{\sqrt {2\pi}}\right)^n\exp\left(-\frac 12\norm {z^{(n)}}^2\right).\]

		Soit $O$ une matrice orthogonale de dimension $n \times n$ telle que $\forall j \in \intint 1n$ : $O_{1j} = \frac 1{\sqrt n}$. On pose alors~:
		\[(Y_1, \ldots, Y_n) = Y^{(n)} = OZ^{(n)}.\]

		Puisque la matrice $O$ est orthogonale, on sait que $O^{-1}$ existe et que $\abs {\det O} = \abs {\det O^{-1}} = 1$. Par le lemme~\ref{lem:preFisher},
		on peut dire~:
		\[f^{Y^{(n)}}(y^{(n)}) = \abs {\det O^{-1}}f^{Z^{(n)}}\left(O^{-1}y^{(n)}\right)
			= \left(\frac 1{\sqrt {2\pi}}\right)^n\exp\left(-\frac 12\norm {O^{-1}y^{(n)}}\right)
			= \left(\frac 1{\sqrt {2\pi}}\right)^n\exp\left(-\frac 12\norm {y^{(n)}}\right).\]

		On a donc $f^{Y^{(n)}} = f^{Z^{(n)}}$, ce qui implique que les $Y_i$ sont iid $\Nzo$.

		En particulier, $Y_1 = (Y^{(n)})_1 = (OZ^{(n})_1 = \sum_{i=1}^nO_{1i}Z_i = \sum_{i=1}^n\frac {Z_i}{\sqrt n} = \sqrt n\overline Z \sim \Nzo$.
		On peut alors en déduire que $\overline Z \sim \mathcal N(0, n^{-1})$.

		Montrons alors que $ns_Z^2 \sim \chi^2_{n-1}$~:
		\[ns_Z^2 = \sum_{i=1}^n(Z_i - \overline Z)^2 = \sum_{i=1}^nZ_i^2 - n(\overline Z)^2 = \norm {Z^{(n)}}^2 - (\sqrt n\overline Z)^2 = \norm{Y^{(n)}} - Y_1^2
			= \sum_{i=2}^nY_i^2.\]

		Or, les $Y_i$ sont $\Nzo$. On a alors bien $ns_Z^2 \sim \chi^2_{n-1}$ (car la somme sur $i$ commence à $2$, il y a donc $(n-1)$ variables sommées).

		De plus, puisque les $Y_i$ sont indépendantes deux à deux, que $\overline Z$ ne dépend que de $Y_1$ et que $ns_Z^2$ ne dépend pas de $Y_1$, on sait que
		$\overline Z \sqcup ns_Z^2$.
		\end{proof}

\chapter{Estimation ponctuelle}
	\section{Introduction}
		Considérons toujours un modèle statistique $\statmod \R\Brl{\mathcal P}n$ avec $\theta$ le paramètre vectoriel $\in \Theta \subset \R^k$.

		\begin{déf} Soit $g : \Theta \to \R^k$. Une statistique est appelée \textit{estimateur de $g(\theta)$} lorsqu'elle est à valeurs dans $g(\Theta)$.
		\end{déf}

		\begin{déf} Soit $\theta \in \Theta \subset \R^k$. Si $g : \Theta \to \R^m : \theta \mapsto (\theta_{\varphi(1)}, \ldots, \theta_{\varphi(m)})$, on
		appelle les paramètres $\theta_{\varphi(i)}$ les paramètres \textit{d'intérêt}, et on appelle les autres paramètres les paramètres \textit{de nuisance}.
		\end{déf}

		\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Nms$. On sait $\theta = (\mu, \sigma^2) \in \Theta = \R \times \Rp_0 \subset \R^2$.
		Soit $g : \Theta \to \R : \theta \mapsto \mu$. $\mu$ est le paramètre d'intérêt et $\sigma^2$ est le paramètre de nuisance.
		\end{ex}

		\begin{rmq} Ne pas connaître le paramètre de nuisance induit une \textit{nuisance} pour déterminer le paramètre d'intérêt.
		\end{rmq}

	\section{Critères d'estimation}
		\begin{rmq} Afin de définir les estimateurs convergents, il faut définir la notion de convergence, or il n'existe pas une manière canonique de la définir.
		Il existe donc plusieurs définitions de convergences différentes.
		\end{rmq}

		\subsection{Définitions de convergence}
			Soient $Z^{(n)} = (Z_1, \ldots, Z_n)$ définis pour $n \geq 1$ et sur $\probspace \Omega{\mathcal F}\P$.

			\begin{déf} On dit que $Z^{(n)}$ converge \textit{presque sûrement} (ou \textit{stochastiquement}) vers $Z$ lorsque~:
			\[\P\left[\left\{\omega \in \Omega \tq Z^{(n)} \xrightarrow[n \to \pinfty]{} Z(\omega)\right\}\right] = 1.\]
			Cela se note~:
			\[Z^{(n)} \cvgps Z.\]
			\end{déf}

			\begin{déf} On dit que $Z^{(n)}$ converge \textit{en probabilités} vers $Z$ lorsque~:
			\[\forall \varepsilon > 0 : \P\left[\abs {Z^{(n)} - Z} > \varepsilon\right] \xrightarrow[n \to \pinfty]{} 0.\]
			Cela se note~:
			\[Z^{(n)} \cvgp Z.\]
			\end{déf}

			\begin{déf} On dit que $Z^{(n)}$ converge \textit{en $L_r$} pour $r \in \N^*$ vers $Z$ lorsque~:
			\[\E\left[\abs {Z^{(n)} - Z}^r\right] \xrightarrow[n \to \pinfty]{} 0.\]
			Cela se note~:
			\[Z^{(n)} \cvgLr Z.\]
			\end{déf}

			\begin{rmq} Lorsque $r=2$, on parle de convergence en moyenne quadratique.
			\end{rmq}

			\begin{déf} On dit que $Z^{(n)}$ converge \textit{en loi} (ou \textit{en distribution}) vers $Z$ lorsque~:
			\[\forall z \text{ point de continuité de }F^Z : F^{Z^{(n)}}(z) \xrightarrow[n \to \pinfty]{} F^Z(z).\]
			Cela se note~:
			\[Z^{(n)} \cvgd Z.\]
			\end{déf}

			\begin{rmq} Ces définitions sont faites pour des variables aléatoires réelles mais peuvent être étendues à $\R^n$ en appliquant la convergence composante
			par composante.
			\end{rmq}

		\subsection{Résultats élémentaires sur les convergences}
			\begin{prp} Les convergences sont induites mutuellement par les assertions suivantes~:
			\begin{enumerate}
				\item si $Z^{(n)} \cvgps Z$, alors $Z^{(n)} \cvgp Z$~;
				\item si $Z^{(n)} \cvgp Z$, alors $Z^{(n)} \cvgd Z$~;
				\item si $Z^{(n)} \cvgLr Z$, alors $Z^{(n)} \cvgp Z$.
			\end{enumerate}
			\end{prp}

			\begin{thm} Les convergences presque sûre, en probabilités, et en loi sont stables par transformations continues.
			\end{thm}

			\begin{thm}\label{thm:propconvg} Notons $\to$ une convergence soit presque sûre, soit en probabilités. Si $Z^{(n)} \to Z$, et $Y^{(n)} \to Y$, alors~:
			\begin{itemize}
				\item[$(i)$]   $Z^{(n)} + Y^{(n)} \to Z+Y$~;
				\item[$(ii)$]  $Z^{(n)} \cdot Y^{(n)} \to Z \cdot Y$~;
				\item[$(iii)$] si $\P[Y^{(n)} = 0] = 0$, alors $\frac {Z^{(n)}}{Y^{(n)}} \to \frac ZY$.
			\end{itemize}
			\end{thm}

			\begin{lem}[Lemme de Slutzky] Si $Z^{(n)} \cvgd Z$, et $Y^{(n)} \cvgd c \neq 0$, alors~:
			\begin{itemize}
				\item[$(i)$]   $Z^{(n)} + Y^{(n)} \cvgd Z+c$~;
				\item[$(ii)$]  $Z^{(n)} \cdot Y^{(n)} \cvgd Z \cdot c$~;
				\item[$(iii)$] $\frac {Z^{(n)}}{Y^{(n)}} \cvgd \frac Zc$.
			\end{itemize}
			\end{lem}

			\begin{thm}[Loi forte des grands nombres] Soient $Z_1, Z_2, \ldots$ iid avec $\E\left[\abs {Z_1}\right] < \pinfty$. Alors~:
			\[\overline Z^{(n)} = \frac 1n\sum_{k=1}^nZ_k \cvgps \mu = \E[Z_1].\]
			\end{thm}

			\begin{thm}[Loi faible des grands nombres] Soient $Z_1, Z_2, \ldots$ iid avec $\E\left[\abs {Z_1}\right] < \pinfty$. Alors~:
			\[\overline Z^{(n)} = \frac 1n\sum_{k=1}^nZ_k \cvgp \mu = \E[Z_1].\]
			\end{thm}

			\begin{thm}[Théorème central limite (TCL)] Soient $Z_1, Z_2, \ldots$ iid, avec $\E[Z_1^2] < \pinfty$. Alors~:
			\[\sqrt n\left(Z^{(n)} - \mu\right) \cvgd W,\]
			où~:
			\[W \sim \mathcal N(0, \sigma^2),\]
			avec $\sigma^2 = \Var(Z_1)$.
			\end{thm}

		\subsection{Estimateurs convergents}
			\begin{déf} Un estimateur $T^{(n)}(X^{(n)})$ de $g(\theta)$ est dit \textit{faiblement convergent} lorsque~:
			\[\forall \theta \in \Theta : T^{(n)}(X^{(n)}) \cvgp g(\theta) \qquad \text{ sur }\P_\theta^{(n)}.\]
			\end{déf}

			\begin{déf} Un estimateur $T^{(n)}(X^{(n)})$ de $g(\theta)$ est dit \textit{fortement convergent} lorsque~:
			\[\forall \theta \in \Theta : T^{(n)}(X^{(n)}) \cvgps g(\theta)\qquad \text{ sur }\P_\theta^{(n)}.\]
			\end{déf}

			\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Nms$. Prenons $g(\theta) = \mu$ et $T^{(n)}(X^{(n)}) = \overline X$. On a bien~:
			\[T^{(n)}(X^{(n)}) = \overline X^{(n)} \cvgps \mu = \E[X_1] \qquad \text{ sur }\P_{\mu, \sigma^2}^{(n)}.\]
			$T^{(n)}(X^{(n)})$ est donc un estimateur fortement convergent.

			Prenons maintenant $T_2^{(n)}(X^{(n)}) = s^2$. On ne peut pas appliquer la loi des grands nombres car les variables aléatoires $(X_i - \overline X)^2$
			sommées ne sont pas indépendantes. On a alors~:
			\begin{align*}
				T_2^{(n)}(X^{(n)}) &= s^2 = \frac 1n\sum_{i=1}^n(X_i-\overline X)^2
					= \frac 1n\sum_{i=1}^n\left((X_i - \mu)^2 + (\mu - \overline X)^2 - 2(X_i - \overline X)(\overline X - X_i)\right) \\
				&= \frac 1n\sum_{i=1}^n(X_i - \mu)^2 - (\overline X - \mu)^2,
			\end{align*}
			où $\frac 1n\sum_{i=1}^n(X_i-\mu)^2 \cvgps \E\left[(X_i - \mu)^2\right] = \sigma^2$, par la loi forte des grands nombres,
			et $(\overline X - \mu) \cvgps 0$. Donc, par le théorème~\ref{thm:propconvg}, on a $T_2^{(n)}(X^{(n)}) \cvgps \sigma^2$
			\end{ex}

			\begin{rmq} On a également~:
			\[\frac 1{n-1}\sum_{i=1}^n(X_i-\overline X)^2 = \frac n{n-1}s^2 \cvgps 1 \cdot \sigma^2.\]
			\end{rmq}

			\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Unif(0, \theta)$, avec $\theta \in \Theta \subset \R$. On veut estimer $g(\theta) = \theta$. L'estimateur
			$T^{(n)}(X^{(n)}) = \overline X$ n'est pas un estimateur convergent car~:
			\[\overline X \cvgps \E_\theta(X_1) = \frac \theta2 \neq \theta \qquad \text{ sur }\P_\theta^{(n)}.\]
			Par contre, si on prend $T_2^{(n)}(X^{(n)}) = 2\overline X$, on a~:
			\[2\overline X \cvgps 2\E_\theta(X_1) = 2\frac \theta2 = \theta \qquad \text{ sur }\P_\theta^{(n)}.\]

			Si on prend $T_3^{(n)}(X^{(n)}) = X_{(n)}$, à savoir l'observation maximale, on a~:
			\[F_\theta^{X_{(n)}}(x) = \begin{cases}0 &\text{ si }x < 0 \\\frac {x^n}{\theta^n} &\text{ si } 0 \leq x \leq \theta \\1 &\text{ sinon}\end{cases}.\]
			Posons donc $\varepsilon > 0$. On calcule~:
			\begin{align*}
				\P_\theta^{(n)}\left[\abs {X_{(n)} - \theta} > \varepsilon\right] &= \P_\theta^{(n)}\left[X_{(n)} \leq \theta - \varepsilon\right]
					+ \P_\theta^{(n)}\left[X_{(n)} \geq \theta + \varepsilon\right] = \P_\theta^{(n)}\left[X_{(n)} \leq \theta - \varepsilon\right]
					= F_\theta^{X_{(n)}}(\theta - \varepsilon) \\
				&=\begin{cases}0 &\text{ si }\varepsilon \geq \theta \\\left(\frac {\theta-\varepsilon}{\theta}\right)^n &\text{ si } 0 < \varepsilon < \theta\end{cases}
					\longrightarrow 0
			\end{align*}
			on a alors convergence en probabilité de $X_{(n)}$ vers $\theta$. On en déduit que $T_3^{(n)}(X^{(n)})$ est un estimateur faiblement convergent.
			\end{ex}

			\begin{rmq} L'estimateur $\frac {n+1}nX_{(n)}$ est également faiblement convergent.
			\end{rmq}

			\begin{rmq} Il n'est pas toujours possible de s'en sortir en invoquant le TCL ou la loi des grands nombres pour déterminer la convergence d'un estimateur.
			Prenons par exemple $X_1, \ldots, X_n$ iid de densité~:
			\[f^X(x) = \frac 1{\pi(1 + (x-\theta)^2)}.\]
			On a effectivement $\E[X_1] = \pinfty$. En réalité~:
			\[\lnot\left(\overline X^{(n)} \cvgp \theta \qquad \text{ sur } P_\theta^{(n)}\right),\]
			mais bien~:
			\[\overline X^{(n)} \cvgd X_1\]
			\end{rmq}

	\section{Estimateur exhaustif}
		\begin{déf} Soit $T^{(n)}(X^{(n)})$, une statistique. On la dit \textit{exhaustive} lorsque~:
		\[\forall B \in \Brl(\R^n) : \forall t \in T^{(n)}(\R^n) : \P_\theta^{(n)}\left[X^{(n)} \in B | T^{(n)}(X^{(n)}) = t\right] \text{ ne dépend pas de } \theta.\]
		\end{déf}

		\begin{rmq} Puisque l'on travaille sur des modèles paramétriques $\statmod \R\Brl{\mathcal P}n$, il existe toujours un $B \in \Brl(\R^n)$ tel que
		$\P_\theta[X^{(n)} \in B]$ dépende de $\theta$.
		\end{rmq}

		\begin{rmq} On peut en comprendre qu'une statistique est exhaustive si la valeur prise par $T^{(n)}(X^{(n)})$ donne toutes les informations contenues par
		$X^{(n)}$ sur $\theta$.
		\end{rmq}

		\begin{ex} La statistique identité $x^{(n)} \mapsto x^{(n)}$ est une statistique exhaustive car~:
		\[\P\left[X^{(n)} \in B | X^{(n)} = x^{(n)}\right] =  \begin{cases}1 &\text{ si } x^{(n)} \in B \\0 &\text{ sinon}\end{cases}.\]
		\end{ex}

		\begin{ex} Prenons $X_1, \ldots, X_n$ iid $\Bern(p)$ avec la statistique $T^{(n)}(X^{(n)}) = \sum_{i=1}^nX_i$. Pour évaluer la probabilité~:
		\[\P_p^{(n)}\left[X^{(n)} \in \{x^{(n)}\} | \sum_{i=1}^nX_i = t\right],\]
		on est en présence d'une binomiale. Dès lors, si $\sum_{i=1}^nX_i \neq t$, alors la probabilité est nulle. Sinon, la probabilité est $\frac 1{\binom nt}$ car
		il y a $\binom nt$ moyens d'avoir $n$ observations dont $t$ valant 1 et $n-t$ valant 0. Ces probabilités ne dépendent donc pas de $\theta$, la statistique
		$T^{(n)}(X^{(n)}) = \sum_{i=1}^nX_i$ est donc une statistique exhaustive.
		\end{ex}

		\begin{rmq} Si $T^{(n)}(X^{(n)})$ est une statistique bijective, alors elle est exhaustive. Cependant, les estimateurs intéressants sont ceux qui «~réduisent~»
		l'information de manière à ce qu'elles soient plus facilement analysables.
		\end{rmq}

		\begin{déf} Soit $X^{(n)} = (X_1, \ldots, X_n)$. On appelle la \textit{fonction de vraisemblance de $X^{(n)}$} la fonction~:
		\[L_\theta^{(n)} : \R^n \to \R : x^{(n)} \mapsto \begin{cases}\P[X^{(n)}
			= x^{(n)}] &\text{ si $X^{(n)}$ est de loi discrète} \\f_\theta^{X^{(n)}}(x^{(n)} &\text{ sinon}\end{cases}.\]
		\end{déf}

		\begin{rmq} Dans le cas de variables $X_1, \ldots, X_n$ iid, la fonction de vraisemblance correspond toujours à un produit~:
		\[L_\theta^{(n)}(X^{(n)}) = \P[X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n] \overset \sqcup= \prod_{i=1}^n\P[X_i = x_i].\]
		\end{rmq}

		\begin{thm}[Critère de factorisation de Neymann-Fisher] Dans un modèle paramétrique $\statmod \R\Brl{\mathcal P}n$, une statistique $T^{(n)}(X^{(n)})$ est
		exhaustive \textbf{si et seulement si} pour tout $\theta \in \Theta$, la fonction de vraisemblance $L_\theta^{(n)}(X^{(n)})$ est factorisable sous la forme~:
		\[L_\theta^{(n)}(X^{(n)}) = \left(g_\theta \circ T^{(n)}\right)(X^\n) h(X^\n),\]
		et ce $\P_\theta^{(n)}$-presque sûrement.
		\end{thm}

		\begin{rmq} Dans cette factorisation, la fonction $h$ ne peut dépendre de $\theta$, et seule la fonction $g_\theta$ peut en dépendre, mais uniquement par
		l'intermédiaire de $T^{(n)}$.
		\end{rmq}

		\begin{ex} En reprenant l'exemple d'au-dessus~: $X_1, \ldots, X_n$ iid $\Bern(p)$ et $T^{(n)}(X^{(n)}) = \sum_{i=1}^nX_i$, on a~:
		\[L_\theta^{(n)}(x^{(n)}) = \prod_{i=1}^np^{x_i}(1-p)^{1-x_i} = p^{\sum_{i=1}^nx_i}(1-p)^{n - \sum_{i=1}^nx_i}.\]
		Dès lors, en posant $g_\theta(x) = p^x(1-p)^{n-x}$ et $h(x^{(n)}) = 1$, on a bien une factorisation de Neymann-Fisher, ce qui implique que la statistique
		est exhaustive.
		\end{ex}

		\begin{rmq} Pour chaque statistique exhaustive, il en existe une infinité définies à bijection près. En effet, si $T(X^{(n)})$ est une statistique exhaustive
		et si $H$ est une fonction bijective quelconque, alors~:
		\[L_\theta^{(n)}(x^{(n)}) = g_\theta(T(x^{(n)})h(x^{(n)}) = (g_\theta \circ H^{-1} \circ H \circ T)(x^{(n)})h(x^{(n)}).\]
		La fonction $H \circ T$ est donc également une statistique exhaustive.
		\end{rmq}

		\begin{rmq} Le critère précédent peut également donner une manière de \textit{deviner} des statistiques exhaustives. Prenons par exemple
		$X^{(n)} = (X_1, \ldots, X_n)$ iid $\Nms$. Si $\theta = (\mu, \sigma^2) \in \Theta \subset \R^2$, on peut écrire~:
		\begin{align*}
			L_\theta^{(n)}(x^{(n)}) &= \prod_{i=1}^nf^{X_i}_\theta(x_i) = \prod_{i=1}^n\frac 1{\sqrt {2\pi}\sigma}\exp\left(-\frac {(x-\mu)^2}{2\sigma^2}\right)
				= \left(\frac 1{2\pi\sigma^2}\right)^{\frac n2}\exp\left(-\sum_{i=1}^n\frac {(x_i-\mu)^2}{2\sigma^2}\right) \\
			&= \left(\frac 1{2\pi\sigma^2}\right)^{\frac n2}\exp\left(-\frac 2{2\sigma^2}\sum_{i=1}^nx_i^2 - \frac {n\mu}{2\sigma^2} + \frac \mu{\sigma^2}\sum_{i=1}^nx_i\right).
		\end{align*}

		Dès lors, en prenant $T(X^{(n)}) = \left(\sum_{i=1}^nx_i, \sum_{i=1}^nx_i^2\right)$, on a bien une statistique exhaustive. Alors, de même, on peut dire que
		$\left(\frac 1n\sum_{i=1}^nx_i, \frac 1n\sum_{i=1}^nx_i^2\right)$ est un estimateur exhaustif (composition avec une bijection). On peut également dire que
		$\left(\frac 1n\sum_{i=1}^nx_i, \frac 1n\sum_{i=1}^nx_i^2 - \left(\frac 1n\sum_{i=1}^nx_i\right)^2\right)$ est un estimateur exhaustif (même argument). Or
		ce dernier vecteur correspond à $(\overline X, s^2)$.

		En prenant cette fois $X^{(n)} = (X_1, \ldots, X_n)$ iid $\Unif(0, \theta)$, on peut à nouveau construire des statistiques exhaustives~:
		\[L_\theta^{(n)}(x^{(n)}) = \prod_{i=1}^nf^{X_i}_\theta(x_i) = \prod_{i=1}^n\frac 1\theta\charfun{0 \leq x_i \leq \theta}
			= \frac 1{\theta^n}\charfun{0 \leq x_1, \ldots, x_n \leq \theta}.\]
		Cela garantit bien que $x^{(n)}$ est une statistique exhaustive. Mais de plus, par commutativité du produit~:
		\[L_\theta^{(n)}(x^{(n)}) = \prod_{i=1}^nf^{X_{(i)}}_\theta(x_i) = \frac 1{\theta^n}\charfun{0 \leq x_{(1)}, \ldots, x_{(n)} \leq \theta)}.\]
		On a donc que la statistique d'ordre est une statistique exhaustive. Or, la condition $0 \leq x_{(1)}, \ldots, x_{(n)} \leq \theta$ revient à la condition
		$0 \leq x_{(1)}, x_{(n)} \leq \theta$. La statistique $(x_{(1)}, x_{(n)})$ est donc également une statistique exhaustive. Pour aller plus loin, décomposant
		la fonction caractéristique $\charfun {0 \leq x_{(1)}, x_{(n)} \leq \theta)}$ en $\charfun {0 \leq x_{(1)}}\charfun{x_{(n)} \leq \theta}$, on peut poser
		$h(x^{(n)}) = \charfun{0 \leq x_{(1)}}$, ce qui amène à une nouvelle statistique exhaustive~: $x^\n \mapsto x_{(n)}$.

		À chaque étape du raisonnement, la statistique exhaustive contient \textit{de moins en moins d'information} générale mais conserve l'information sur $\theta$
		qui est donc en quelque sorte contenue dans $x_{(n)}$.
		\end{rmq}

		\subsection{Estimateurs non biaisés}
			En se situant toujours dans un modèle statistique $\statmod \R\Brl{\mathcal P}n$, on veut estimer $g(\theta)$, avec $g : \Theta \to \R^m$.

			\begin{déf} Un estimateur $T^{(n)}(X^{(n)})$ de $g(\theta)$ est dit \textit{non biaisé} lorsque~:
			\[\forall \theta \in \Theta : \E_\theta[T^{(n)}(X^{(n)})] = g(\theta).\]
			\end{déf}

			\begin{déf} Un estimateur $T^{(n)}(X^{(n)})$ de $g(\theta)$ est dit \textit{asymptotiquement non biaisé} lorsque~:
			\[\forall \theta \in \Theta : \E_\theta[T^{(n)}(X^{(n)})] \xrightarrow[n \to \pinfty]{} g(\theta).\]
			\end{déf}

			\begin{déf} Le \textit{biais} d'un estimateur $T^{(n)}(X^ {(n)})$ est la quantité~:
			\[b^{(n)}_\theta = b^{(n)}(\theta) = \E_\theta[T^{(n)}(X^{(n)})] - g(\theta).\]
			\end{déf}

			\begin{rmq} On remarque donc qu'un estimateur non biaisé a un biais de 0 et qu'un estimateur asymptotiquement non biaisé a un biais qui tend vers 0 pour
			$n$ tendant vers $\pinfty$.
			\end{rmq}

			\begin{ex} $X_1, \ldots, X_n$ iid $\Unif(0, \theta)$, avec $\theta \in \Theta = \Rp_0 \subset \R$.
			\begin{itemize}
				\item Si $T^{(n)}(X^{(n)}) = 2\overline X$, on a~:
				\[\E_\theta^{(n)}\left[2\overline X\right] = \frac 2n\E_\theta^{(n)}\left[\sum_{i=1}^nX_i\right] = \frac 2n\sum_{i=2}^n\E_\theta[X_i]
					= \frac 2n\frac {n\theta}2 = \theta.\]
				\item Si $T^{(n)}(X^{(n)}) = X_{(n)}$, on a~:
				\[\E_\theta^{(n)}[X_{(n)}] = \int_\R xf_\theta^{X_{(n)}}(x)\dif x = \int_0^\theta x\frac {nx^{n-1}}{\theta^n}\dif x
					= \frac n{\theta^n}\int_0^\theta x^n\dif x = \frac n{\theta^n}\left[\frac {x^{n+1}}{n+1}\right]_0^\theta = \frac n{n+1}\theta.\]
				On en déduit que $T^{(n)}(X^{(n)})$ est biaisé car il \textit{vise en moyenne trop à gauche} et a un biais de $-\frac \theta{n+1}$. Il est cependant
				asymptotiquement non biaisé car $\frac n{n+1} \xrightarrow[n \to \pinfty]{} 1$.
			\end{itemize}
			\end{ex}

			\begin{rmq} Si le biais d'un estimateur est négatif, alors c'est que l'estimateur sous-estime en moyenne, alors que si le biais est positif, c'est que
			l'estimateur surestime.
			\end{rmq}

			\begin{rmq} On peut tout de même dire que l'estimateur $\frac {n+1}nX_{(n)}$ est non biaisé car~:
			\[\E_\theta^{(n)}\left[\frac {n+1}nX_{(n)}\right] = \frac {n+1}n\E_\theta^{(n)}[X_{(n)}] = \frac {n+1}n\frac n{n+1}\theta = \theta.\]
			\end{rmq}

			\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Nms$, avec $\theta = (\mu, \sigma^2)$.
			\begin{itemize}
				\item pour tout $\theta \in \Theta$, on a~: $\E_\theta[\overline X] = \frac 1n\sum_{i=1}^n\E(X_i) = \frac {n\mu}n = \mu$. $\overline X$ est donc un
				estimateur sans biais de $\mu$~;
				\item $\E_\theta[s^2] = \E_\theta[X_1^2] - \E_\theta[\overline X^2]$. On remarque que pour toute variable aléatoire $Z$, on a~:
				\[\Var(Z) = \E[Z^2] - \E[Z]^2 \qquad \iff \qquad \E[Z^2] = \Var(Z) + \E[Z]^2.\]
				On peut donc remplacer dans la formule de l'espérance de $s^2$, et on obtient~:
				\[\E_\theta[s^2] = \Var_\theta(X_1) + \E(X_1)^2 - \Var_\theta(\overline X) - \E(\overline X)^2.\]
				Or on sait $\E_\theta(X_1) = \E_\theta(\overline X) = \mu$. On a donc~:
				\[\E_\theta[s^2] = \Var_\theta(X_1) - \Var_\theta(\overline X) = \sigma^2 - \frac 1{n^2}\Var_\theta\left(\sum_{i=1}^nX_i\right)
					= \sigma^2 - \frac 1{n^2}\sum_{i=1}^n\Var_\theta(X_i) = \sigma^2 - \frac {n\sigma^2}{n^2} = \frac {n-1}n\sigma^2.\]

				On en conclut que l'estimateur $s^2$ est biaisé pour $\sigma^2$ et de biais $\frac {-\sigma^2}n \xrightarrow[n \to \pinfty]{} 0$.

				Notons alors $S^2 \coloneqq \frac {n}{n-1}s^2$. On a que $\E_\theta[S^2] = \sigma^2$, et donc $S^2$ est un estimateur sans biais.
			\end{itemize}
			\end{ex}

			\begin{rmq} Le non biais est une propriété fragile. Soient $X_1, \ldots, X_n$ iid $\Nms$ où l'on veut estimer $g(\theta) = \sigma$ (et pas $\sigma^2$).

			Que vaut $\E_\theta[S]$~? On sait $\Var_\theta(S) = \E_\theta[S^2] - \E_\theta[S]^2 = \sigma^2 - \E_\theta[S]^2$. Or $\Var_\theta(S) \gneqq 0$, et donc
			$\sigma^2 \gneqq \E_\theta[S]^2$. On en déduit $\E_\theta[S] \lneqq \sigma$.
			\end{rmq}

			\begin{rmq} De plus, il n'existe pas toujours d'estimateur sans biais. Soit $X_1 \sim \Bern(p)$. On veut estimer $g(p) = p^2 \in [0, 1]$. L'estimateur
			$T(X^{(n)})$ est entièrement déterminé par $T(0)$ et $T(1)$. Imposons donc pour tout $p \in [0, 1] : \E_\theta[T(X_1)] = p^2$. On a donc~:
			\[p^2 = \E_\theta[T(X_1)] = T(0)\P[X_1 = 0] + T(1)\P[X_1 = 1] = T(0)(1-p) + T(1)p.\]
			En réarrangeant cette équation du second degré, on obtient~:
			\[\forall p \in [0, 1] : p^2 + (T(0) - T(1))p - T(0) = 0.\]
			Or une telle équation ne peut avoir que 2 racines tout au plus, et ici, une infinité non-dénombrable est requise. Il n'existe donc pas de telle fonction $T$,
			et donc par extension, il n'existe pas d'estimateur sans biais de $p^2$.
			\end{rmq}

			\begin{ex} Prenons $X_1, \ldots, X_n$ iid $\mathcal N(\mu, 1)$, $g(\theta) = g(\mu) = \mu$. Prenons $\TnXn = \overline \Xn  + Y$, où $Y = \pm 10^9$, avec
			probabilité $\frac 12$ pour chaque. On a alors~:
			\[\E_\mu[\TnXn] = \E_\mu[\overline \Xn] + \E_\mu[Y] = \mu + 0 = \mu.\]
			L'estimateur $\TnXn$ est donc sans biais, mais est très mauvais~: sur-/sous-estime toujours à $\simeq 10^9$ près.
			\end{ex}

		\subsection{Estimateurs à dispersion minimale}
			\begin{déf} L'erreur quadratique moyenne d'un estimateur $\TnXn$ de $g(\theta)$ ($\in \R$) est la quantité~:
			\[\MSE_\theta[\TnXn] \coloneqq \E_\theta\left[\abs {\TnXn - g(\theta)}^2\right].\]
			\end{déf}

			\begin{déf} On appelle l'erreur absolue moyenne la quantité~:
			\[MAE_\theta[\TnXn] = \E_\theta\left[\abs {\TnXn - g(\theta)}\right]\]
			\end{déf}

			\begin{rmq}
			\begin{align*}
				\MSE_\theta[\TnXn] &= \E_\theta\left[\abs {\TnXn - g(\theta)}^2\right]
					= \Var_\theta\left[\TnXn - g(\theta)\right] - \E_\theta\left[\TnXn - g(\theta)\right]^2 \\
				&= \Var_\theta[\TnXn] - b_\theta^{(n)}(\TnXn)^2.
			\end{align*}

			Puisque $\Var_\theta(\TnXn) \geq 0$ et $b_\theta^{(n)}(\TnXn) \geq 0$, pour avoir $\MSE[\TnXn] \xrightarrow[n \to \pinfty]{} 0$, il faut~:
			\[\begin{cases}&\Var_\theta(\TnXn) \xrightarrow[n \to \pinfty]{} 0, \\&b_\theta^{(n)}(\TnXn) \xrightarrow[n \to \pinfty]{} 0.\end{cases}\]

			Cela dit que sous $\P_\theta^{(n)}$, on a $\TnXn \xrightarrow{L^2} g(\theta)$ (et également $\TnXn \cvgp g(\theta)$).

			On peut également dire que si $\MSE_\theta[\TnXn] \to 0$, alors $\TnXn$ est un estimateur faiblement convergent.
			\end{rmq}

			\begin{rmq} Le MSE sert d'«~arbitrage~» entre la variance et le biais. Cet arbitrage est bien souvent nécessaire car il arrive fréquemment que baisser
			le biais (respectivement la variance) augmente la variance (respectivement le biais).
			\end{rmq}

			\begin{ex} $X_1, \ldots, X_n$ iid $\Unif(0, \theta)$. Prenons $\theta \in \Theta = \Rp_0 \subset \R$. Prenons $T_1^{(n)}(\Xn) = X_{(n)}$. On sait que~:
			\[\E_\theta[T_1^{(n)}(\Xn)] = \frac n{n+1}\theta,\]
			ce qui nous permet de calculer la variance~:
			\[\Var_\theta(X_{(n)}) = \E_\theta(X_{(n)}^2) - \E_\theta(X_{(n)})^2,\]
			où~:
			\[\E_\theta(X_{(n)}^2) = \int_\R x^2f^{X_{(n)}}(x)\dif x = \int_0^\theta x^2\frac {nx^{n-1}}{\theta^2}\dif x = \frac n{\theta^n}\int_0^\theta x^{n+1}\dif x
				= \frac n{\theta^n}\left[\frac {x^{n+2}}{n+2}\right]_0^\theta = \frac {n\theta^2}{n+2}.\]
			On trouve finalement la variance donnée par~:
			\[\frac {n\theta^2}{n+2} - \frac {n^2\theta^2}{(n+1)^2} = n\theta^2\left(\frac {(n+1)^2 - n(n+2)}{(n+2)(n+1)^2}\right) = \frac {n\theta^2}{(n+2)(n+1)^2}.\]

			On trouve finalement une erreur quadratique moyenne de~:
			\[\MSE_\theta[T_1^{(n)}(\Xn)] = \frac {2\theta^2}{(n+1)(n+2)} \xrightarrow[n \to \pinfty]{} 0.\]

			En prenant $T_2^{(n)}(\Xn) = \frac {n+1}nX_{(n)}$, on annule le biais, mais on augmente la variance~:
			\[\Var_\theta[T_2^{(n)}(\Xn)] = \left(\frac {n+1}n\right)^2\Var_\theta[X_{(n)}] = \frac {\theta^2}{n(n+2)}.\]

			On trouve alors~:
			\[\forall \theta \in \Theta : \MSE_\theta[T_2^{(n)}(\Xn)] = \frac {\theta^2}{n(n+2)} \leq \MSE_\theta[T_1^{(n)}(\Xn)].\]

			Dans le cas présent, annuler le biais donne une erreur quadratique moyenne plus faible. Donc ici, la variance a «~moins augmenté que le biais n'a diminué~».
			\end{ex}

			\begin{ex} Prenons maintenant $X_1, \ldots, X_n$ iid $\Nms$ et $g(\theta) = \sigma^2$.

			Pour $T_1^{(n)}(\Xn) = \frac 1n\sum_{i=1}^n(X_i - \overline X)^2$, on trouve~:
			\[\MSE_\theta[T_1^{(n)}(\Xn)] = \frac {2(n-1)\sigma^4}{n^2}.\]

			À nouveau, il existe un estimateur sans-biais~:
			\[T_2^{(n)}(\Xn) = \frac 1{n-1}\sum_{i=1}^n(X_i - \overline X)^2 = \frac {n-1}nT_1^{(n)}(\Xn).\]
			Dans ce cas, on trouve~:
			\[\forall \theta \in \Theta : \MSE_\theta[T_2^{(n)}(\Xn)] = \frac {2\sigma^4}{n-1} \geq \MSE_\theta[T_1^{(n)}(\Xn)].\]
			\end{ex}

			\begin{rmq} Dans les deux exemples ci-dessus, l'inégalité est stricte pour $n \geq 2$.

			De plus, l'inégalité tient pour tout $\theta \in \Theta$. Cela permet de définir qu'un estimateur est meilleur que l'autre. Avoir deux estimateurs tels
			que l'erreur quadratique moyenne de l'un est meilleure que l'autre pour certaines valeurs de $g(\theta)$, et inversement pour d'autres ne permet pas de dire
			que l'un est meilleur que l'autre car la véritable valeur de $g(\theta)$ n'est pas connue. On ne peut donc pas savoir quel estimateur choisir.
			\end{rmq}

			\begin{déf} Soit $\mathcal C$, une classe d'estimateurs de $g(\theta)$. On dit que $T_*^{(n)}$ est à erreur quadratique moyenne minimale dans $\mathcal C$
			lorsque~:
			\begin{itemize}
				\item $T_*^{(n)} \in \mathcal C$~;
				\item $\forall \Tn \in \mathcal C : \forall \theta \in \Theta : \MSE_\theta[T_*^{(n)}] \leq \MSE_\theta[\Tn]$.
			\end{itemize}
			\end{déf}

			\begin{rmq} On peut prendre $\mathcal C = \{\Tn \tq \E_\theta[\Tn^2] < \pinfty\}$, mais on n'en prend qu'un sous-ensemble strict bien souvent car dans un tel
			$\mathcal C$, on peut prendre $T_{\theta_0}^{(n)}$, pour un certain $\theta_0 \in \Theta$ défini par $T_{\theta_0}^{(n)}(\Xn) = g(\theta_0)$.
			On trouve donc~:
			\[\MSE_\theta[T_{\theta_0}^{(n)}(\Xn)] = \E_\theta[(g(\theta_0) ) g(\theta))^2] = (g(\theta_0) - g(\theta))^2.\]
			La fonction d'erreur quadratique moyenne est donc une parabole qui s'annule en $\theta_0 = \theta$. Pour avoir un tel $T_*^{(n)}$, il faut que
			$\MSE_{\theta_0}[T_*^{(n)}] \leq \MSE_{\theta_0}[T_{\theta_0}^{(n)}] = 0$. Et ce, pour tout $\theta_0 \in \Theta$. Il faut donc avoir~:
			\[\forall \theta \in \Theta : T_*^{(n)} = g(\theta)\qquad\text{ sous }\P_\theta^{(n)}.\]

			Pour résoudre ce problème, il faut donc réduire $\mathcal C$ afin de ne plus avoir de tels estimateurs \textit{pathologiques}.
			\end{rmq}

			\begin{déf} Soit $Z = (Z_1, \ldots, Z_m)^T$, un vecteur aléatoire (vecteur de variables aléatoires). On définit~:
			\[\E_\theta(Z) \coloneqq (\E_\theta[Z_1], \ldots, \E_\theta[Z_m])^T.\]

			On définit également~:
			\[\Var_\theta[Z] = \E\left[(Z - \E_\theta[Z])(Z - \E_\theta[Z])^T\right] \in \R^{m \times m}.\]
			\end{déf}

			\begin{rmq} On observe que~:
			\[\Var_\theta[Z]_{ij} = \E_\theta\left[(Z - \E_\theta[Z])_i(Z - \E_\theta[Z])_j^T\right] = \E_\theta\left[(Z_i - \E_\theta[Z_i])(Z_j - \E_\theta[Z_j])\right]
				= \Cov[Z_i, Z_j].\]
			Cela veut dire~:
			\[\Var_\theta[Z] =
				\begin{pmatrix}
					\Var_\theta[Z_1]      & \Cov[Z_1, Z_2] & \ldots & \Cov[Z_1, Z_n] \\
					\Cov[Z_2, Z_1] & \Var_\theta[Z_2]      & \ddots & \Cov[Z_2, Z_n] \\
					\vdots         & \ddots         & \ddots & \vdots         \\
					\Cov[Z_n, Z_1] & \ldots         & \ldots & \Var_\theta[Z_n]
				\end{pmatrix}
			\]

			On appelle cette matrice la \textit{matrice de variance/covariance} (pour des raisons évidentes). On voit donc que cette matrice est symétrique car
			$\Cov[Z_i, Z_j] = \Cov[Z_j, Z_i]$.
			\end{rmq}

			\begin{déf} Soit $A$ une matrice. On dit que $A \geq 0$ si la forme bilinéaire associée est semi-définie positive.
			\end{déf}

			\begin{prp} Soit $Z$ un vecteur aléatoire, et soient $A \in \R^{k \times m}, b \in \R^k$. Alors~:
			\begin{itemize}
				\item $\E_\theta[AZ + b] = A\E_\theta[Z] + b$~;
				\item $\Var_\theta[AZ + b] = A\Var_\theta[Z]A^T$~;
				\item $\Var_\theta[Z] \geq 0$.
			\end{itemize}
			\end{prp}

			\begin{proof}~
			\begin{itemize}
				\item On observe que $\E_\theta[AZ + b] = \left(\E_\theta[(AZ)_i + b_i]\right)_i$,
				où~:
				\begin{align*}
					\E_\theta[(AZ)_i + b_i] &= \E_\theta[(AZ)_i] + b_i = \E_\theta\left[\sum_{j=1}^kA_{ij}Z_j\right] + b_i = \sum_{j=1}^k\E_\theta[A_{ij}Z_j] + b_i \\
					&= \sum_{j=1}^kA_{ij}\E_\theta[Z_j] + b_i = \sum_{j=1}^kA_{ij}(\E_\theta[Z])_j + b_i= (A\E_\theta[Z] + b)_i.
				\end{align*}
				\item On remarque premièrement que $\Var_\theta(AZ+b) = \E_\theta\left[(AZ+b-A\E_\theta(Z)-b)(AZ+b-A\E_\theta(Z)-b)^T\right]
					= \E_\theta\left[(AZ-A\E_\theta(Z))(AZ-A\E_\theta(Z))^T\right] = \Var_\theta[AZ]$.
				De là, on trouve~:
				\begin{align*}
					\Var_\theta[AZ]_{ij} &= \E_\theta\left[(AZ-A\E_\theta(Z))_i(AZ-A\E_\theta(Z))_j\right]
						= \E_\theta\left[((AZ)_i-\E_\theta((AZ)_i))((AZ)_j-\E_\theta((AZ)_j))\right] \\
					&= \E_\theta[(AZ)_i(AZ)_j] - \E_\theta[(AZ)_i]\E_\theta[(AZ)_j] \\
					&= \E_\theta\left[\left(\sum_{\delta=1}^mA_{i\delta}Z_\delta\right)\left(\sum_{\gamma=1}^mA_{j\gamma}Z_\gamma\right)\right]
						- \E_\theta\left[\sum_{\delta=1}^mA_{i\delta}Z_\delta\right]\E_\theta\left[\sum_{\gamma=1}^mA_{j\gamma}Z_\gamma\right] \\
					&= \sum_{\delta=1}^m\sum_{\gamma=1}^mA_{i\delta}A_{j\gamma}\E_\theta[Z_\delta Z_\gamma]
						- \sum_{\delta=1}^m\sum_{\gamma=1}^mA_{i\delta}A_{j\gamma}\E_\theta[Z_\delta]\E_\theta[Z_\gamma] \\
					&= \sum_{\delta=1}^m\sum_{\gamma=1}^mA_{i\delta}A_{j\gamma}\left(\E_\theta[Z_\delta Z_\gamma] - \E_\theta[Z_\delta]\E_\theta[Z_\gamma]\right) \\
					&= \sum_{\delta=1}^m\sum_{\gamma=1}^mA_{i\delta}A_{j\gamma}\Var_\theta[Z]_{\delta \gamma} \\
					&= \sum_{\delta=1}^m\sum_{\gamma=1}^mA_{i\delta}\Var_\theta[Z]_{\delta \gamma}(A^T)_{\gamma j} \\
					&= \left(A\Var_\theta[Z]A^T\right)_{ij}
				\end{align*}
				\item soit $v \in \R^m$. On remarque que $v^T\Var_\theta[Z]v = \Var_\theta[v^TZ] \geq 0$.
			\end{itemize}
			\end{proof}

			\begin{déf} Soient $Z = (Z_1, \ldots, Z_m)^T$, et $Y = (Y_1, \ldots, Y_\ell)^T$. On définit la covariance de $Y$ et $Z$ par~:
			\[\Cov[Y, Z] \coloneqq \E_\theta\left[(Y-\E_\theta[Y])(Z-\E_\theta[Z])\right].\]
			\end{déf}

			\begin{prp} Soient $Z = (Z_1, \ldots, Z_m)^T$, et $Y = (Y_1, \ldots, Y_\ell)^T$. Alors~:
			\begin{itemize}
				\item $\Cov[Y, Z]_{ij} = \Cov[Y_i, Z_j]$~;
				\item $\Cov[Z, Y] = \Cov[Y, Z]$~;
				\item $\Cov[Z, Z] = \Var_\theta[Z]$~;
				\item $\Cov[AY+b, CZ+d] = A\Cov[Y, Z]C^T$
			\end{itemize}
			\end{prp}

			\begin{déf} Soient $g(\theta) \in \R^m$ et $\TnXn$, un estimateur de $g(\theta)$. On définit~:
			\[\MSE_\theta[\TnXn] \coloneqq \E_\theta\left[(\TnXn - g(\theta))(\TnXn - g(\theta))^T\right] \in \Mat(m, m).\]
			\end{déf}

			\begin{rmq} $\MSE_\theta[\TnXn] = \E_\theta\left[\left(T_i^{(n)}(\Xn)-g(\theta)\right)\left(T_j^{(n)}(\Xn)-g(\theta)\right)^T\right]$. En particulier~:
			\[\MSE_\theta[\TnXn]_{ii} = \MSE_\theta[T_i^{(n)}].\]
			\end{rmq}

			\begin{déf} Soient $A, B \in \Mat(m, n)$. On dit que $A \geq B$ lorsque $A-B \geq 0$, c-à-d lorsque la forme bilinéaire définie par $(A-B)$ est semi définie
			positive.
			\end{déf}

			\begin{déf} Soit $\mathcal C$, une classe d'estimateurs de $g(\theta)$. Alors $T_*^{(n)}$ est à erreur quadratique moyenne minimale dans $\mathcal C$
			lorsque~:
			\begin{itemize}
				\item[$(i)$]  $T_*^{(n)} \in \mathcal C$~;
				\item[$(ii)$] $\forall \Tn \in \mathcal C : \forall \theta \in \Theta : \MSE_\theta[\Tn] \geq \MSE_\theta[T_*^{(n)}]$.
			\end{itemize}
			\end{déf}

			\begin{prp} Pour tout $\theta \in \Theta, \Tn \in \mathcal C$, on a~:
			\[\MSE_\theta[\Tn] \geq 0.\]
			\end{prp}

			\begin{proof} Soit $v \in \R^m$. On trouve~:
			\begin{align*}
				v^T\MSE_\theta[\Tn]v &=v^T\E_\theta\left[\left(\Tn-g(\theta)\right)\left(\Tn-g(\theta)\right)^T\right]v
					= \E_\theta\left[v^T\left(\Tn-g(\theta)\right)\left(\Tn-g(\theta)\right)^Tv\right] \\
				&= \E_\theta\left[\left(v^T(\Tn-g(\theta))\right)\left(v^T(\Tn-g(\theta))\right)^T\right] \\
				&= \E_\theta\left[\left(v^T(\Tn-g(\theta))\right)^2\right] \geq 0.
			\end{align*}
			\end{proof}

			\begin{prp} Soient $T_1^{(n)}, T_2^{(n)}$, deux estimateurs de $g(\theta)$ tels que~:
			\[\MSE_\theta[T_1^{(n)}] \geq \MSE_\theta[T_2^{(n)}].\]
			Alors~:
			\[\forall v \in \R^m : \MSE_\theta[v^TT_1^{(n)}] \geq \MSE_\theta[v^TT_2^{(n)}].\]
			\end{prp}
			\newpage
			\begin{proof}
			\begin{align*}
				\MSE_\theta[v^TT_1^{(n)}] - \MSE_\theta[v^TT_2^{(n)}]
					&= \E_\theta\left[\left(v^TT_1^{(n)} - v^Tg(\theta)\right)^2\right] - \E_\theta\left[\left(v^TT_2^{(n)}-v^Tg(\theta)\right)^2\right] \\
				&= \E_\theta\left[v^T\left(T_1^{(n)}-g(\theta)\right)\right] - \E_\theta\left[v^T\left(T_2^{(n)}-g(\theta)\right)\right] \\
				&= v^T\MSE_\theta[T_1^{(n)}]v - v^T\MSE_\theta[T_2^{(n)}]v = v^T\left(\MSE_\theta[T_1^{(n)}] - \MSE_\theta[T_2^{(n)}]\right)v \geq 0,
			\end{align*}
			par définition de semi-positivité.
			\end{proof}

		\subsection{Estimateurs efficaces}
			\begin{déf} On note l'ensemble des valeurs $x^{(n)}$ de $\R^n$ prises par $X^{(n)}$ avec probabilité non-nulle par l'ensemble~:
			\[\mathcal X^{(n)} \coloneqq \{x^{(n)} \in \R^n \tq L_\theta^{(n)}(x^{(n)}) > 0\}.\]
			\end{déf}

			\begin{déf} Soit un modèle statistique. On définit la \textit{matrice d'information de Fisher} par~:
			\[I^{(n)}(\theta) \coloneqq
				\int_{\mathcal X^{(n)}}\left[\nabla_\theta\ln L_\theta^{(n)}(x^{(n)})\right]\left[\nabla_\theta\ln L_\theta^{(n)}(x^{(n)})\right]^TL_\theta^{(n)}(x^{(n)})\dif x^{(n)}.\]
			\end{déf}

			\begin{déf} Soit $\statmod \R\Brl{\mathcal P}n$, un modèle statistique. On le dit \textit{régulier} lorsque~:
			\begin{itemize}
				\item[H1] $\Theta$ est ouvert~;
				\item[H2] $\mathcal X^{(n)}$ ne dépend pas de $\theta$~;
				\item[H3] $\forall x^{(n)} \in \mathcal X^{(n)} : \theta \mapsto L_\theta^{(n)}(x^{(n)})$ est différentiable sur $\theta$~;
				\item[H4] l'expression $\int_{\mathcal X^{(n)}}L_\theta^{(n)}(x^{(n)})\dif x^{(n)}$ est différentiable sous le signe~;
				\item[H5] $\forall \theta \in \Theta : I^{(n)}(\theta)$ existe, est finie, et est inversible.
			\end{itemize}
			\end{déf}

			\begin{rmq} Les intégrales doivent être comprises comme des sommes (voire des séries dans le cas infini) dans les cas discrets.
			\end{rmq}

			\begin{rmq} Un artifice fréquemment utilisé ici est le suivant~:
			\[\pd {}{\theta_i}L_\theta^{(n)}(x^{(n)}) = \pd {}{\theta_i}L_\theta^{(n)}(x^{(n)})\frac {L_\theta^{(n)}(x^{(n)})}{L_\theta^{(n)}(x^{(n)})}
				= \pd {}{\theta_i}\ln L_\theta^{(n)}(x^{(n)})L_\theta^{(n)}(x^{(n)}).\]
			\end{rmq}

			\begin{prp} Dans un modèle statistique régulier, on a~:
			\[\E_\theta\left[\nabla_\theta\ln L_\theta^{(n)}(x^{(n)})\right] = 0.\]
			\end{prp}

			\begin{proof} Par définition de l'espérance~:
			\[\E_\theta\left[\nabla_\theta\ln L_\theta^{(n)}(x^{(n)})\right]
				= \int_{\mathcal X^{(n)}}\left(\nabla_\theta\ln L_\theta^{(n)}(x^{(n)})\right)L_\theta^{(n)}(x^{(n)})\dif x^{(n)}
				= \int_{\mathcal X^{(n)}}\nabla_\theta L_\theta^{(n)}(x^{(n)})\dif x^{(n)}.\]

			Par hypothèse de régularité, on sait~:
			\[\int_{\mathcal X^{(n)}}\nabla_\theta L_\theta^{(n)}(x^{(n)})\dif x^{(n)} = \nabla_\theta\int_{\mathcal X^{(n)}}L_\theta^{(n)}(x^{(n)})\dif x^{(n)}.\]

			On trouve finalement~:
			\[\E_\theta\left[\nabla_\theta\ln L_\theta^{(n)}(x^{(n)})\right] = \nabla_\theta\int_{\mathcal X^{(n)}}L_\theta^{(n)}(x^{(n)})\dif x^{(n)}
				= \nabla_\theta 1 = 0.\]
			\end{proof}

			\begin{rmq} Par la proposition précédente, on peut dire~:
			\[I^{(n)}(\theta) = \E_\theta\left[\left[\nabla_\theta\ln L_\theta^{(n)}(x^{(n)})\right]\left[\nabla_\theta\ln L_\theta^{(n)}(x^{(n)})\right]^T\right]
				= \Var_\theta\left[\nabla_\theta\ln L_\theta^{(n)}(X^{(n)})\right].\]
			\end{rmq}

			\begin{prp} Dans un modèle d'échantillonage, on a~:
			\[I^{(n)}(\theta) = nI^{(1)}(\theta).\]
			\end{prp}

			\begin{proof} Les variables $X_1, \ldots, X_n$ étant iid (modèle d'échantillonage), on peut exprimer~:
			\begin{align*}
				I^{(n)}(\theta) &= \Var_\theta\left[\nabla_\theta \ln L_\theta^{(n)}(X^{(n)})\right]
					= \Var_\theta\left[\nabla_\theta \ln\left(\prod_{i=1}^n L_\theta^{(1)}(X_i)\right)\right] \\
				&= \Var_\theta\left[\sum_{i=1}^n\nabla_\theta \ln L_\theta^{(1)}(X_i)\right]
					= \sum_{i=1}^n\Var_\theta\left[\nabla_\theta \ln L_\theta^{(1)}(X_i)\right] \\
				&= n\Var_\theta\left[\nabla_\theta \ln L_\theta^{(1)}(X_1)\right] = nI^{(1)}(\theta).
			\end{align*}
			\end{proof}

			\begin{rmq} On en déduit que $n$ variables aléatoires iid contiennent $n$ fois plus d'information qu'une seule d'entre elles.
			\end{rmq}

			\begin{rmq} pour comprendre l'importance de l'hypothèse H5, supposons $k=1$ et $\forall \theta \in \Theta : I^{(n)}(\theta) = 0$. $0$ n'est pas inversible,
			ce qui viole H5. Alors par l'égalité d'une remarque précédente~:
			\[\Var_\theta\left[\od {}\theta \ln L_\theta^{(n)}(X^{(n)})\right] = 0.\]
			On en déduit que $\od {}\theta \ln L_\theta^{(n)}(X^{(n)}) = 0$, et donc que $\ln L_\theta^{(n)}(X^{(n)})$ est constante sur son espérance, c-à-d $0$.
			On en déduit~:
			\[\od{}\theta L_\theta^{(n)}(X^{(n)}) = 0 \qquad\forall \theta \in \Theta\quad \P_\theta-\ps.\]

			Et donc, pour tout $\theta \in \Theta$, on a~:
			\[\P[X^{(n)} \in B] = \int_B L_\theta^{(n)}\dif x^{(n)},\]
			et en dérivant les deux membres de l'égalité~:
			\[\od {}\theta\P[X^{(n)} \in B] = \int_B\od {}\theta L_\theta^{(n)}(x^{(n)})\dif x^{(n)}.\]

			Cela veut dire que la probabilité $\P[X^{(n)} \in B]$ est constante pour toutes valeurs de $\theta$. Les $\P_\theta$ ne sont donc plus différenciables.
			On a donc perdu toute information sur $\theta$.
			\end{rmq}

			\begin{déf} Dans un modèle statistique régulier, un estimateur $\TnXn$ de $g(\theta)$ est dit \textit{régulier} lorsque~:
			\begin{itemize}
				\item[$(i)$]  $\forall \theta \in \Theta : \E_\theta\left[\norm {\TnXn}^2\right] < \pinfty$~;
				\item[$(ii)$] $\psi_\theta \coloneqq \E_\theta[\TnXn] = \int_{\mathcal X^{(n)}}\Tn(x^{(n)})L_\theta^{(n)}(x^{(n)})\dif x^{(n)}$
				est différentiable sous le signe pour tout $\theta \in \Theta$.
			\end{itemize}
			\end{déf}

			\begin{thm}\label{thm:borneinfMSE} Soient un modèle statistique régulier, et $\TnXn$ un estimateur régulier de $g(\theta)$. Alors~:
			\[\forall \theta \in \Theta : MSE_\theta(\TnXn) = \E_\theta\left[\left[\TnXn - g(\theta)\right]\left[\TnXn - g(\theta)\right]^T\right]
				\geq \Delta_\theta{I^{(n)}(\theta)}^{-1}\Delta_\theta^T,\]
			où $\Delta_\theta = \Jac \psi = \left[\pd {\psi_i}{\theta_j}(\theta)\right] \in \Mat(m, k)$.
			\end{thm}

			\begin{rmq} Dans le cas où $g(\theta) = \theta = \psi(\theta)$, on a un estimateur non-biaisé, et donc~:
			\[\Jac \psi = \Jac \Id = \Id.\]
			Et donc, on a~:
			\[\forall \theta \in \Theta : \MSE_\theta(\TnXn) \geq {I^{(n)}(\theta)}^{-1}.\]
			\end{rmq}

			\begin{rmq} Dans le cas d'un modèle d'échantillonage, on a $(I^{(n)})^{-1} = n^{-1}{I^{(4)}(\theta)}^{-1}$, et donc~:
			\[\forall \theta \in \Theta : \MSE_\theta(\TnXn) \geq n^{-1}\Delta_\theta{I^{(1)}(\theta)}^{-1}\Delta_\theta^T \xrightarrow[n \to \pinfty]{} 0.\]
			\end{rmq}

			\begin{proof}[Preuve du Théorème~\ref{thm:borneinfMSE}] Notons d'abord~:
			\begin{align*} (\Delta_\theta)_{ij} &= \pd {\psi_i}{\theta_j}(\theta) = \pd {}{\theta_j}\E_\theta[T_i^{(n)}(\Xn)
				= \pd {}{\theta_j}\int_{\mathcal X^{(n)}}T_i^{(n)}(x^{(n)})L_\theta^{(n)}\dif x^{(n)} \\
			&= \int_{\mathcal X^{(n)}}T_i(x^{(n)})\pd {}{\theta_j}L_\theta^{(n)}(x^{(n)})\dif x^{(n)}
				= \int_{\mathcal X^{(n)}}T_i(x^{(n)})\pd {}{\theta_j}\ln L_\theta^{(n)}(x^{(n)})L_\theta^{(n)}(x^{(n)})\dif x^{(n)}.
			\end{align*}

			On a donc une espérance donnée par~:
			\[(\Delta_\theta)_{ij} = \E_\theta\left[T_i(X^{(n)})\pd {}{\theta_j}\ln L_\theta^{(n)}(X^{(n)})\right]
				= \E_\theta\left[\TnXn\nabla_\theta\ln L_\theta^{(n)}(X^{(n)})\right]_{ij}.\]

			On a alors l'égalité suivante~:
			\[\Delta_\theta = \E_\theta\left[\TnXn\nabla_\theta\ln L_\theta^{(n)}(X^{(n)})\right].\]

			On veut ensuite montrer~:
			\begin{align*}
				\MSE_\theta[\Tn] = \E_\theta\left[\left(\Tn - g(\theta)\right)\left(\Tn - g(\theta)\right)^T\right] &\overset {(1)}=
					Var[\Tn] + \left(\psi(\theta)-g(\theta)\right)\left(\psi(\theta)-g(\theta)\right)^T \\
				&\overset{(2)}\geq \Var_\theta[\Tn] \\
				&\overset{(3)}\geq \Delta_\theta{I^{(n)}(\theta)}^{-1}\Delta_\theta^T
			\end{align*}

			Montrons d'abord $(1)$. Pour cela~:
			\begin{align*}
				\MSE_\theta(\Tn) &= \E_\theta\left[\left((\Tn - \psi(\theta)) + (\psi(\theta) - g(\theta))\right)\left((\Tn - \psi(\theta)) + (\psi(\theta) - g(\theta))\right)^T\right] \\
				&=\E_\theta[(\Tn - \psi(\theta))(\Tn - \psi(\theta))^T] + \E_\theta[(\Tn - \psi(\theta))](\psi(\theta) - g(\theta))^T \\
				&\quad- (\psi(\theta) - g(\theta))\E_\theta[(\Tn - \psi(\theta))^T] + (\psi(\theta) - g(\theta))(\psi(\theta) - g(\theta))^T \\
				&= \Var_\theta[\Tn] + 0 + 0 + (\psi(\theta) - g(\theta))(\psi(\theta) - g(\theta))^T.
			\end{align*}

			Pour montrer $(2)$, on voit bien que $(\psi(\theta) - g(\theta))(\psi(\theta) - g(\theta))^T$ est semi-définie positive par construction ($uu^T$ est toujours
			semi-définie positive).

			Montrons alors $(3)$. Posons pour cela~:
			\[S_\theta \coloneqq \TnXn - \Delta_\theta{I^{(n)}(\theta)}^{-1}\nabla_\theta\ln L_\theta^{(n)}(X^{(n)}) \in \R^m.\]

			Calculons ensuite~:
			\[\E_\theta[S_\theta] = \E_\theta[\TnXn] - \Delta_\theta{I^{(n)}(\theta)}^{-1}\E_\theta[\nabla_\theta \ln L_\theta^{(n)}(X^{(n)})]
				= \E_\theta[\TnXn] - \Delta_\theta{I^{(n)}(\theta)}^{-1} \cdot 0.\]

			On sait également que~:
			\begin{align*}
				\Var_\theta[S_\theta] &= \E_\theta[S_\theta S_\theta^T] - \E_\theta[S_\theta]\E_\theta[S_\theta]^T \\
				&= \E_\theta\left[\left(\Tn - \Delta_\theta{I^{(n)}(\theta)}^{-1}\nabla_\theta \ln L_\theta^{(n)}(X^{(n)})\right)\left(\Tn - \Delta_\theta{I^{(n)}(\theta)}^{-1}\nabla_\theta \ln L_\theta^{(n)}(X^{(n)})\right)^T\right] - \psi(\theta)\psi(\theta)^T \\
				&= \Var_\theta[\Tn] - \E_\theta\left[\TnXn\left(\nabla_\theta\ln L_\theta^{(n)}(X^{(n)})\right)\right]\left({I^{(n)}(\theta)}^{-1}\right)^T\Delta_\theta^T \\
				&\quad-\Delta_\theta{I^{(n)}(\theta)}^{-1}\E_\theta\left[\left(\nabla_\theta\ln L_\theta^{(n)}(X^{(n)})\right)\TnXn\right] \\
				&\quad+\Delta_\theta{I^{(n)}(\theta)}^{-1}\E_\theta\left[\left(\nabla_\theta \ln L_\theta^{(n)}(X^{(n)})\right)\left(\nabla_\theta \ln L_\theta^{(n)}(X^{(n)})\right)^T\right]\left({I^{(n)}(\theta)}^{-1}\right)^T\Delta_\theta^T \\
				&= \Var_\theta[\TnXn] - \Delta_\theta\left({I^{(n)}(\theta)}^{-1}\right)^T\Delta_\theta^T - \Delta_\theta{I^{(n)}(\theta)}^{-1}\Delta_\theta^T
					+ \Delta_\theta{I^{(n)}(\theta)}^{-1}I^{(n)}(\theta)\left({I^{(n)}(\theta)}^{-1}\right)^T\Delta_\theta^T,
			\end{align*}
			par définition de $I^{(n)}(\theta)$. Puisque $I^{(n)}(\theta)$ est symétrique et inversible, on sait que ${I^{(n)}(\theta)}^{-1}$ est également
			symétrique. On trouve alors~:
			\[\Var_\theta[S_\theta] = \Var_\theta[\TnXn] - \Delta_\theta{I^{(n)}(\theta)}^{-1}\Delta_\theta^T - \Delta_\theta{I^{(n)}(\theta)}^{-1}\Delta_\theta^T
				+ \Delta_\theta{I^{(n)}(\theta)}^{-1}I^{(n)}(\theta){I^{(n)}(\theta)}^{-1}\Delta_\theta^T.\]
			En simplifiant les produits de matrice avec leur inverse, on obtient~:
			\begin{align*}
				\Var_\theta[S_\theta] &= \Var_\theta[\TnXn] - \Delta_\theta{I^{(n)}(\theta)}^{-1}\Delta_\theta^T - \Delta_\theta{I^{(n)}(\theta)}^{-1}\Delta_\theta^T
					+ \Delta_\theta{I^{(n)}(\theta)}^{-1}\Delta_\theta^T \\
				&= \Var_\theta[\TnXn] - \Delta_\theta{I^{(n)}(\theta)}^{-1}\Delta_\theta^T
			\end{align*}

			Or $\Var_\theta[S_\theta]$ est semi-définie positive par construction. Dès lors, on sait $\Var_\theta[\TnXn] - \Delta_\theta{I^{(n)}(\theta)}^{-1}\Delta_\theta^T~\geq~0$,
			ou encore $\Var_\theta[\TnXn] \geq \Delta_\theta{I^{(n)}(\theta)}^{-1}\Delta_\theta^T$.
			\end{proof}

			\begin{déf} Dans un modèle statistique régulier, l'estimateur régulier $\TnXn$ de $g(\theta)$ est dit \textit{efficace} lorsque~:
			\[\forall \theta \in \Theta : \MSE_\theta[\TnXn] = \Delta_\theta{I^{(n)}(\theta)}^{-1}\Delta_\theta^T.\]
			\end{déf}

			\begin{rmq} Pour avoir $\TnXn$ efficace, il faut $g(\theta) = \psi(\theta)$ et $\Var_\theta[\TnXn] = \Delta_\theta{I^{(n)}(\theta)}^{-1}\Delta^T$.
			L'estimateur doit donc être sans biais. De plus, la variance est définie par la borne de Cramer.
			\end{rmq}

			\begin{ex} Soient $X_1, \ldots, X_n$ iid $\mathcal N(\mu, \sigma_0^2)$, $g(\theta = \mu) = \mu$. On a donc $k=m=1$. On sait que $\TnXn = \overline X^{(n)}$
			est un estimateur sans biais. Comparons alors la variance de $\TnXn$ avec la borne de Cramer.
			\[\Delta_\mu = \od {}\mu \psi(\mu) = \od {}\mu\E_\theta[\overline X^{(n)}] = \od {}\mu\mu = 1.\]

			Calculons alors $I^{(n)}(\theta)$, qui est donné par~:
			\[L_\mu^{(n)}(x^{(n)}) = \left(\frac 1{\sqrt {2\pi}\sigma_0}\right)^n\exp\left(-\frac 1{2\sigma_0^2}\sum_{i=1}^n(X_i-\mu)^2\right),\]
			et donc~:
			\begin{align*}
				I^{(n)}(\theta) &= \Var_\mu\left[\od {}\mu \ln L_\mu^{(n)}(X^{(n)})\right]
					= \Var_\mu\left[\od {}\mu\left(n\ln \frac 1{\sqrt {2\pi}\sigma_0} - \frac 1{2\sigma_0^2}\sum_{i=2}^n(X_i-\mu)^2\right)\right] \\
				&=\frac 1{(2\sigma_0^2)^2}\Var_\mu\left[-2\sum_{i=1}^n(X_i-\mu)\right] = \frac 1{\sigma_0^4}\sum_{i=1}^n\Var_\mu(X_i)
					= \frac {\sigma_0^2}{\sigma_0^4} = \frac n{\sigma_0^2} = {\Var[\TnXn]}^{-1} = \MSE_\theta[\TnXn].
			\end{align*}

			On en déduit que l'estimateur $\overline X^{(n)}$ est efficace pour $g(\theta) = \mu$.
			\end{ex}

			\begin{rmq} On observe que $I^{(n)}(\theta) \propto {\Var_\theta[\TnXn]}^{-1}$, ce qui est cohérent avec l'intuition de la notion d'information : au plus
			la variance est faible, au plus les observations sont groupées, et au plus il y a d'information à déduire sur $\theta$, alors qu'au plus la variance est
			élevée, au plus les observations sont dispersées, et au moins il y a d'information contenue sur $\theta$.
			\end{rmq}

			\begin{ex} $X_1, \ldots, X_n$ iid $\Bern(p)$, avec $p \in (0, 1)$. On sait que $\E_p[X_i] = p$ et $\Var_p[X_i] = p(1-p)$. On pose $g(p) = p$.
			On calcule ensuite~:
			\begin{align*}
				I^{(n)}(\theta) &= \Var_p\left[\od {}p \ln L_p^{(n)}(X^{(n)})\right] = \Var_p\left[\od {}p\ln\left(p^{\sum_{i=1}^nX_i}(1-p)^{n-\sum_{i=1}^nX_i}\right)\right] \\
				&= \Var_p\left[\od {}p\left(\sum_{i=1}^nX_i \ln p + \left(n - \sum_{i=1}^nX_i\right)\ln(1-p)\right)\right] \\
				&= \Var_p\left[\frac 1p\sum_{i=1}^nX_i - \frac 1{1-p}\left(n-\sum_{i=1}^nX_i\right)\right]
					= \Var_p\left[\left(\frac 1p+\frac 1{1-p}\right)\sum_{i=1}^nX_i -\frac n{1-p}\right] \\
				&= \frac 1{(p(1-p)p^2}\sum_{i=1}^n\Var_p[X_i] = \frac 1{(p(1-p))^2}n(p(1-p)) = \frac n{p(1-p)}.
			\end{align*}

			En prenant $\TnXn = \overline X^{(n)}$, on a $\E_p[\TnXn] = p$, et $\Var_p[\TnXn] = \frac {\Var[X_1]}n = \frac {p(1-p)}n = {I^{(n)}(\theta)}^{-1}$.

			À nouveau, on en déduit que $\overline X^{(n)}$ est un estimateur efficace.
			\end{ex}

	\section{Méthodes d'estimation}
		\subsection{Méthode des moments}
			Plaçons-nous dans un modèle paramétrique $\statmod \R\Brl{\mathcal P}n$, avec $\theta \in \Theta \subseteq \R^k$. On suppose que pour tout $1 \leq r \leq k$,
			on a~: $\E_\theta[\abs {X_i}^r] < \pinfty$.

			\begin{déf} Soit $M : \Theta \to M(\Theta) : \theta \mapsto \left[\mu_i'(\theta)\right]_{1 \leq i \leq k}$. Supposons $M$ inversible. Soit $\Xn$ un vecteur
			d'observations de loi $\P_\theta^{(n)}$. On appelle \textit{estimateur de $\theta$ par la méthode des moments} la statistique~:
			\[\TnXn = M^{-1}\left([m_i']_{1 \leq i \leq k}\right),\]
			où $m_i'$ est le moment empirique d'ordre $i$ de $\theta$
			\end{déf}

			\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Nms$, avec $\theta \in \Theta \subset \R^2$. On connaît les moments d'ordre 1 et 2~:
			\begin{align*}
				\mu_1'(\theta) &= \E_\theta[X_1] = \mu \\
				\mu_2'(\theta) &= \E_\theta[X_1^2] = \Var_\theta[X_i] + {\E_\theta[X_1]}^2 = \mu^2 + \sigma^2
			\end{align*}

			On a alors la fonction $M$ donnée par~:
			\[M : \Theta \to M(\Theta) = \Theta : \theta = \begin{pmatrix}\mu \\ \sigma\end{pmatrix} \mapsto \begin{pmatrix}\mu \\ \mu^2 + \sigma^2\end{pmatrix},\]
			qui est bien inversible. Il faut alors résoudre le système~:
			\[\left\{\begin{aligned}
				\mu_1'(\theta) &= m_1' \\
				\mu_2'(\theta) &= m_2',
			\end{aligned}\right.\]

			ce qui donne~:
			\[\left\{\begin{aligned}
				\mu = \mu_1'(\theta) &= m_1' = \frac 1n\sum_{i=1}^nX_i = \overline X \\
				\sigma^2 + \mu^2 = \mu_2'(\theta) &= m_2' = \frac 1n\sum_{i=1}^nX_i^2.
			\end{aligned}\right.\]

			On prend alors pour estimateur~:
			\[\TnXn = \begin{pmatrix}\overline X \\\frac 1n\sum_{i=1}^nX_i^2 - {\overline X}^2\end{pmatrix} = \begin{pmatrix}\overline X \\ s^2\end{pmatrix}.\]

			Cet estimateur est convergent (car convergent en chaque composante), est exhaustif, est biaisé (car $s^2$ est un estimateur biaisé de $\sigma^2$), et est
			asymptotiquement efficace.
			\end{ex}

			\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Bern(p)$, $p \in [0, 1] \subset \R$. On obtient~:
			\[p = \mu = \mu_1'(\theta) = m_1'= \overline X,\]
			et donc on prend l'estimateur $\TnXn = \overline X$.
			\end{ex}

			\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Unif(0, \theta)$, $\theta \in \Rp_0 \subset \R$. Le système nous donne donc~:
			\[\frac \theta2 = \E_\theta[X_1] = \mu_1'(\theta) = m_1' = \overline X.\]

			L'estimateur de $\theta$ par la méthode des moments est alors donné par~:
			\[\TnXn = 2\overline X.\]
			\end{ex}

			\begin{rmq}
			\[\MSE_\theta[2\overline X] = 0^2 + \Var_\theta[2\overline X] = \frac 4n\Var[X_1] \propto \frac cn,\]
			alors qu'un estimateur efficace a un $\MSE_\theta \propto \frac c{n^2}$.
			\end{rmq}


		\subsection{Méthode du maximum de vraisemblance}
			\begin{rmq} Une notation usuelle pour noter un estimateur de $\theta$ est $\widehat \theta$. Cette notation remplace l'estimateur $\TnXn$ pour lequel il faut
			explicitement préciser la variable estimée. $\widehat \theta$ est une statistique, et ne dépend aucunement de $\theta$~!
			\end{rmq}

			L'idée d'un tel estimateur est de se baser sur la maximisation (selon $\theta$) de la vraisemblance. On est donc tenté d'écrire~:
			\[\widehat \theta = \argmax_{\theta \in \Theta}L_\theta^{(n)}(\Xn).\]

			Or il peut arriver qu'un tel $\theta$ de soit pas unique, et donc qu'il existe plusieurs estimateurs de $\theta$ par le maximum de vraisemblance.

			\begin{déf} L'estimateur $\widehat \theta$ est un estimateur de $\theta$ par la méthode du maximum de vraisemblance lorsque~:
			\[\forall \theta \in \Theta : L_{\widehat \theta}^{(n)}(\Xn) \geq L_\theta^{(n)}(\Xn).\]
			\end{déf}

			\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Unif\left(\left[\theta \pm \frac 12\right]\right)$, avec $\theta \in \R$. On peut écrire la vraisemblance~:
			\[L_\theta^\n(\Xn) = \prod_{i=1}^nf^{X_i}_\theta(X_i) = \prod_{i=1}^n\charfun {\theta-\frac 12 \leq X_i \leq \theta+\frac 12}
				= \charfun {\theta-\frac 12 \leq X_{(1)}}\charfun {X_\n \leq \theta+\frac 12} = \charfun {X_\n-\frac 12 \leq \theta \leq X_{(1)}+\frac 12}.\]

			La fonction de vraisemblance valant ici soit 1 soit 0, toute valeur de $\theta$ telle que $\theta \in \left[X_\n-\frac 12, X_{(1)}+\frac 12\right]$ maximise
			la vraisemblance.
			\end{ex}

			\begin{ex} Soient $X_1, \ldots X_n$ iid $\Unif(0, \theta)$, avec $\theta \in \Rp_0$. Ici, la fonction de vraisemblance ne vaut plus uniquement 0 ou 1~:
			\[\L_\theta^\n(\Xn) = \frac 1{\theta^n}\charfun {0 \leq X_{(1)}}\charfun {X_\n \leq \theta} = \frac 1{\theta^n}\charfun {0 \lneqq \theta \leq X_\n}.\]
			Pour $\theta \in [0, X_\n]$, la fonction de vraisemblance est nulle, et puis vaut $\theta^{-n}$ pour $\theta \geq X_\n$. Le maximum est donc en
			$\theta = X_\n$.

			De plus, la fonction $\theta \mapsto \theta^{-n}$ est strictement décroissante, donc le maximum de vraisemblance est unique.
			\end{ex}

			\begin{rmq} Par le critère de factorisation de Neymann-Fisher, si $S(\Xn)$ est une statistique exhaustive, on sait~:
			\[L_\theta^\n(\Xn) = (g_\theta \circ S)(\Xn) \cdot h(\Xn).\]

			Donc (en considérant $\argmax$ comme l'ensemble des arguments maximisant la valeur), on a~:
			\[\widehat \theta \in \argmax_{\theta \in \Theta}L_\theta^\n(\Xn) = \argmax_{\theta \in \Theta}(g_\theta \circ S)(\Xn) = (\alpha \circ S)(\Xn).\]

			Cela implique que dans le cas du maximum de vraisemblance, l'estimateur dépend toujours des observations à travers une statistique exhaustive.
			\end{rmq}

			\begin{prp} Si $\theta \mapsto L_\theta^\n(\Xn)$ est différentiable sur $\intr(\Theta$), alors tout estimateur du maximum de vraisemblance à valeurs dans
			$\intr(\Theta)$ vérifie~:
			\[\nabla_\theta L_\theta^\n(\Xn)\sVert[2]_{\theta=\widehat \theta} = 0.\]
			\end{prp}

			\begin{rmq} Par continuité et croissance stricte de $\ln(\cdot)$, on sait également que~:
			\[\nabla_\theta \ln\left(L_\theta^\n(\Xn)\right)\sVert[2]_{\theta = \widehat \theta} = 0.\]

			L'intérêt de passer par un logarithme est de transformer les produits en somme, ce qui est plus agréable à dériver.
			\end{rmq}

			\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Bern(p)$. On peut calculer la fonction de vraisemblance~:
			\[L_p^\n(\Xn) = \prod_{i=1}^np^{X_i}(1-p)^{1-X_i} = p^{\sum_{i=1}^nX_i}(1-p)^{n-\sum_{i=1}^nX_i} = p^{n\overline X}(1-p)^{n(1-\overline X)}.\]

			Annulons ensuite le gradient de $\ln \circ L_p^\n$~:
			\[0 = \pd {}p = n\overline X\pd {}p\ln p  + n(1-\overline X)\pd {}p\ln(1-p) = n\overline X\frac 1p + n(\overline X - 1)\frac 1{1-p}.\]
			En multipliant par $p(1-p)$ de part et d'autre ,on obtient~:
			\[0 = n\overline X(1-p) + n(\overline X - 1)p = n\overline X - n\overline Xp + n\overline Xp - np = n\overline X - np,\]
			ou encore $p = \overline X$. Dès lors, $\widehat p = \overline X$ est un estimateur de $p$ par le maximum de vraisemblance.
			\end{ex}

			\begin{prp} Soit $g : \Theta \to g(\Theta)$. Alors~:
			\[\forall \theta \in \Theta : \widehat {g(\theta)} = g(\widehat \theta).\]
			\end{prp}

			\begin{proof} Distinguons les deux cas où $g$ est bijective, et où $g$ n'est pas bijective.

			Si $g$ est bijective, alors c'est évident car $g(\theta)$ transforme juste le paramètre de manière univoque.

			Si $g$ n'est pas bijective, on observe que $g$ est surjective par construction. Et donc $g$ n'est pas injective. On pose alors~:
			\[G : \Theta \to g(\Theta) \times \Theta : \theta \mapsto \begin{pmatrix}g(\theta) \\ \theta\end{pmatrix}.\]

			$G$ est injective par construction, et donc est bijective (car surjective en chaque composante). On sait donc que~:
			\[\widehat {G(\theta)} = G(\widehat \theta),\]
			ce qui implique l'égalité composante par composante, et donc~:
			\[\widehat {g(\theta)} = g(\widehat \theta).\]
			\end{proof}

			\begin{déf} Soit $\widehat \theta$ un estimateur. On dit qu'il est \textit{asymptotiquement efficace} lorsque~:
			\[\Var_\theta[\widehat \theta] = \frac 1{nI^{(1)}(\theta)}.\]
			\end{déf}

			\begin{ex} Prenons $X_1, \ldots, X_n$ iid $\P_\theta^{(1)}$, avec $\theta \in \Theta \subseteq \R$. Par la loi des grands nombres, on a~:
			\[\forall \theta \in \Theta : \widehat \theta \cvgps \theta \quad \text{ sous } \P_\theta^{(1)}.\]

			On a également~:
			\[\sqrt n(\widehat \theta - \theta) \cvgd \mathcal N\left(0, \frac 1{I^{(1)}(\theta)}\right).\]

			Pour $n \gg$, on a~:
			\[\widehat \theta \approx \mathcal N\left(\theta, \frac 1{nI^{(1)}(\theta)}\right),\]
			et donc~:
			\begin{align*}
				\E_\theta[\widehat \theta] &\approx \theta \\
				\Var_\theta[\widehat \theta] &\approx \frac 1{nI^{(1)}(\theta)}.
			\end{align*}

			On sait dès lors que pour $n$ grand, on a~:
			\[\MSE_\theta[\widehat \theta] \approx b_\theta(\widehat \theta)^2 + \Var_\theta[\widehat \theta] = \Var_\theta[\widehat \theta]
				= \frac 1{nI^{(1)}(\theta)},\]
			qui est la borne de Cramer-Rao. $\widehat \theta$ est donc asymptotiquement efficace.
			\end{ex}

		\subsection{Comportement asymptotique des estimateurs du maximum de vraisemblance}
			On se place dans un modèle statistique $(\R^n, \Brl(\R^n), \mathcal P^\n = \{\P_\theta^\n \tq \theta \in \Theta \subseteq \R\})$ d'échantillonage.

			On considère les hypothèses suivantes~:
			\begin{itemize}
				\item[H1] $\Theta$ est ouvert~;
				\item[H2] $\mathcal X = \{x \in \R \tq L_\theta^{(1)}(x) > 0\}$ ne dépend pas de $\theta$~;
				\item[H3] $\forall x \in \mathcal X : \theta \mapsto L_\theta^{(1)}(x)$ est dérivable sur $\Theta$ et $\theta \mapsto \od {}\theta L_\theta^{(1)}(x)$ est
				dérivable sur $\Theta$~;
				\item[H4] l'expression $\int_{\mathcal X}L_\theta^{(1)}(x)\dif x$ peut être dérivée sous le signe deux fois~;
				\item[H5] $I^{(1)}(\theta) = \int_{\mathcal X}\left(\od {}\theta\ln L_\theta^{(1)}(x)\right)^2\dif x$ est fini et $\neq 0$~;
				\item[H6] Pour toute suite $(\theta_n)_n$ convergente en $\theta \in \Theta$, on a~:
				\[\sup_{x \in \mathcal X}\abs {B_{\theta_n}^{(1)}(x) - B_\theta^{(1)}(x)} \xrightarrow[n \to \pinfty]{} 0,\]
				avec $B_\theta^{(1)}(x) = \od[2]{}\theta \ln L_\theta^{(1)}(x)$.
			\end{itemize}

			\begin{rmq} À nouveau~:
			\[0 = \od {}\theta 1 = \od {}\theta\int_{\mathcal X}L_\theta^{(1)}(x)\dif x = \int_{\mathcal X}\od {}\theta L_\theta^{(1)}(x)\dif x
				= \int_{\mathcal X}\left(\od {}\theta \ln L_\theta^{(1)}(x)\right)L_\theta^{(1)}(x)\dif x = \E_\theta\left[\od {}\theta\ln L_\theta^{(1)}(X_1)\right],\]
			et donc~:
			\[I^{(1)}(\theta) = \E_\theta\left[\left(\od {}\theta \ln L_\theta^{(1)}(X_1)\right)^2\right] = \Var_\theta\left[\od {}\theta \ln L_\theta^{(1)}(X)\right].\]

			Également, on sait~:
			\[0 = \od {}\theta \int_{\mathcal X}\left(\ln L_\theta^{(1)}(x)\right)L_\theta^{(1)}(x)\dif x
				= \int_{\mathcal X}\left(\left(\od[2]{}\theta\ln L_\theta^{(1)}(x)\right)L_\theta^{(1)}(x)
					+ \left(\od {}\theta\ln L_\theta^{(1)}(x)\right)\od {}\theta L_\theta^{(1)}(x)\right)\dif x.\]

			On en déduit~:
			\[0 = \E_\theta\left[\od[2] {}\theta \ln L_\theta^{(1)}(X_1)\right] + \E_\theta\left[\left(\od {}\theta \ln L_\theta^{(1)}(X_1)\right)^2\right]
				= \E_\theta\left[\od[2] {}\theta \ln L_\theta^{(1)}(X_1)\right] + I^{(1)}(\theta),\]
			ou encore~:
			\[I^{(1)}(\theta) = -\E_\theta\left[\od[2] {}\theta \ln L_\theta^{(1)}(X_1)\right].\]
			\end{rmq}

			\begin{thm} Si le modèle satisfait (H1)-(H6), alors~:
			\begin{itemize}
				\item[$(i)$] il existe une suite $(\TnXn)_{n \in \N}$ de solutions de l'équation de vraisemblance~:
				\[\od {}\theta \ln L_\theta^{(n)}(\Xn) = 0\]
				qui est fortement convergente pour $\theta$, c-à-d~:
				\[\forall \theta \in \Theta : \TnXn \cvgps \theta \text{ sous } \P_\theta^\n\]
				\item[$(ii)$] pour toute telle suite $(\TnXn)_n$, on a~:
				\[\forall \theta \in \Theta : \sqrt n(\TnXn - \theta) \cvgd \mathcal N(0, I^{(1)}(\theta)^{-1}).\]
			\end{itemize}
			\end{thm}

			\begin{rmq} $(ii)$ implique que $\TnXn$ est asymptotiquement efficace.
			\end{rmq}

			\begin{proof} Supposons que $(i)$ est vrai et montrons $(ii)$. Posons~:
			\begin{align*}
				A^\n_\theta(x^\n) &\coloneqq \frac 1n \od {}\theta \ln L_\theta^\n(x^\n) \\
				B^\n_\theta(x^\n) &\coloneqq \frac 1n \od[2] {}\theta \ln L_\theta^\n(x^\n).
			\end{align*}

			On remarque~:
			\[A^\n_\theta(x^\n) = \frac 1n\sum_{i=1}^nA^{(1)}_\theta(x_i) \qquad\qquad\text{ et }\qquad\qquad B^\n_\theta(x^\n)
				= \frac 1n\sum_{i=1}^nB^{(1)}_\theta(x_i).\]

			Par définition de $\TnXn$, pour tout $n$, on a~:
			\[\od {}\theta \ln L_\theta(\Xn)\sVert[3]_{\theta = \TnXn} = 0,\]
			et donc~:
			\[A_{\TnXn}(\Xn) = \frac 1n\od {}\theta\ln L_\theta(\Xn) = 0.\]

			On en déduit~:
			\[\forall n : 0 = A_{\TnXn}^{(n)}(\Xn) = A_\theta^{(n)}(\Xn) + \od {}\theta A_\theta^{(n)}(\Xn)\sVert[3]_{T_*^{(n)}(\Xn)}(\Xn) \cdot (\TnXn - \theta),\]
			et donc (théorème des accroissements finis)~:
			\[T_*^{(n)} = \frac {-A_\theta^{(n)}(\Xn)}{\od {}\theta A_\theta^{(n)}(\Xn)\sVert[3]_{\theta=T_*^{(n)}(\Xn)}}
				= \frac {-A_\theta^{(n)}(\Xn)}{B_{T_*^{(n)}(\Xn)}^{(n)}(\Xn)},\]
			pour $T_*^{(n)}(\Xn) \in [0, \TnXn]$

			On a alors~:
			\[\sqrt n\left(\TnXn - \theta\right) = \frac {-\sqrt nA_\theta^{(n)}(\Xn)}{B_{T_*^{(n)}(\Xn)}(\Xn)}.\]

			Montrons indépendamment la convergence du numérateur et celle du dénominateur.

			\textbf{Convergence du numérateur~:}

			\[-\sqrt nA_\theta^{(n)}(\Xn) = \frac 1{\sqrt n}\sum_{i=1}^n\left(-A_\theta^{(n)}(\Xn)\right)
				\xrightarrow[n \to \pinfty]{\mathcal D\text{ sous } \P_\theta^{(n)}} \mathcal N(-\E_\theta[A_\theta^{(1)}(X_1)], I^{(1)}(\theta))
				= \mathcal N(0, \E_\theta[(A_\theta^{(1)}(\Xn))^2]).\]

			\textbf{Convergence du dénominateur~:}

			\begin{align*}
				B_{T_*^{(n)}(\Xn)}^{(1)}(\Xn) &= \frac 1n\sum_{i=1}^nB_{T_*^{(n)}}^{(1)}(X_i)
					= \frac 1n\sum_{i=1}^nB_\theta^{(1)}(X_i) + \frac 1n\sum_{i=1}^n\left(B_{T_*^{(n)}}^{(1)}(X_i) - B_\theta^{(1)}(X_i)\right) \\
				&= -I^{(1)}(\theta) + \frac 1n\sum_{i=1}^n\left(B_{T_*^{(n)}(X_i)}^{(1)}(\Xn) - B_\theta^{(1)}(X_i)\right).
			\end{align*}

			Par inégalité triangulaire, on peut dire~:
			\begin{align*}
				\abs {\frac 1n \sum_{i=1}^n\left(B_{T_*^{(n)}(\Xn)}^{(1)}(X_i) - B_\theta^{(1)}(X_i)\right)}
					&\leq \frac 1n\sum_{i=1}^n\abs {B_{T_*^{(n)}(\Xn)}^{(1)}(X_i) - B_\theta^{(1)}(X_i)} \\
				&\leq \frac 1n\sum_{i=1}^n\sup_{x \in \mathcal X}\abs {B_{T_*^{(n)}(\Xn)}^{(1)}(x) - B_\theta^{(1)}(x)} \\
				&= \sup_{x \in \mathcal X}\abs {B_{T_*^{(n)}(\Xn)}^{(1)}(x) - B_\theta^{(1)}(x)},
			\end{align*}
			ce qui, par (H6) converge presque sûrement vers 0.

			Le lemme de Slutzky nous affirme alors que~:
			\[\sqrt n\left(\TnXn - \theta\right) \xrightarrow[n \to \pinfty]{\mathcal D \text{ sous } \P_\theta{(n)}}
				\frac {\mathcal N(0, I^{(1)}(\theta))}{\left(I^{(1)}(\theta)\right)^2} = \mathcal N(0, {I^{(1)}(\theta)}^{-1}).\]
			\end{proof}

\chapter{Tests d'hypothèse}
	En se plaçant dans le modèle statistique $\statmod \R\Brl{\mathcal P}n$, avec $\mathcal P^{(n)} = \{\P_\theta^{(n)} \tq \theta \in \Theta\}$. On partitionne
	$\Theta$ en $\Theta_0 \subseteq \Theta$ et $\Theta = \Theta \setminus \Theta_0$.

	On parle de \textit{problème de test d'hypothèse} pour~:
	\[\begin{cases}
		&H_0 : \theta \in \Theta_0 \text{\; hypothèse nulle}, \\
		&H_1 : \theta \in \Theta_1 \text{\; contre-hypothèse}.
	\end{cases}\]
	Il y a deux choix possibles, à savoir RH0, le rejet de $H_0$, et \sout {RH0}, le non-rejet de $H_0$.

	\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Nms$, avec $\theta = [\mu, \sigma^2]^T \in \Theta \subsetneq \R^2$. Voici une partition de $\Theta$~:
	\[\Theta : \Theta_0 \sqcup \Theta_1 = \left\{[\mu, \sigma^2]^T \in \Theta \tq \mu > 15\right\}
		\sqcup \left\{[\mu, \sigma^2]^T \in \Theta \tq \mu \leq 15\right\}.\]

	\[\begin{cases}
		&\theta \in \Theta_0 \\
		&\theta \in \Theta_1
	\end{cases}\]
	\end{ex}

	\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Bern(p)$, pour $p \in [0, 1] \subset \R$. Soit $p_0 \in [0, 1]$. Voici un test d'hypothèse~:
	\[\begin{cases}
		&H_0 : p \in [0, p_0], \\
		&H_1 : p \in (p_0, 1].
	\end{cases}\]
	\end{ex}

	\begin{déf} On dit que le problème est \textit{unilatéral} s'il est sous la forme~:
	\[\begin{cases}
		&H_0 : \theta \leq \theta_0 \\
		&H_1 : \theta > \theta_0
	\end{cases}\]
	(une partition de $\Theta$ en deux espaces de mesure non-nulle).

	On dit que le problème est \textit{bilatéral} s'il est sous la forme~:
	\[\begin{cases}
		&H_0 : \theta = \theta_0 \\
		&H_1 : \theta \neq \theta_0
	\end{cases}\]
	(une partition de $\Theta$ en un singleton et $\Theta$ moins un point).

	Dans ce cas, on dit que $H_0 : \theta = \theta_0$ est l'\textit{hypothèse simple}, et $H_1 : \theta \neq \theta_0$ est l'\textit{hypothèse composée}.
	\end{déf}

	\begin{déf} Un test pur est une fonction mesurable $\phi^\n : \mathcal X^\n \to \{0, 1\}$, où $\mathcal X$ est l'ensemble des valeurs observables pour $X^\n$.
	\end{déf}

	\begin{déf} On appelle \textit{zone critique} l'ensemble~:
	\[\left(\phi^\n\right)^{-1}\left(\{1\}\right).\]
	\end{déf}

	\begin{déf} Un \textit{test randomisé} (ou \textit{test impur}) est une fonction mesurable $\phi^\n : \mathcal X^\n \to [0, 1]$.
	\end{déf}

	\begin{rmq} Soit $\gamma \coloneqq \phi^\n(x^\n)$. Si $\gamma = 0$, alors \sout {RH0}. Si $\gamma = 1$, alors RH0. Et si $\gamma \in (0, 1)$, alors
	RH0 avec probabilité $\gamma$.

	De plus~:
	\[\E_\theta[\phi^\n(X^\n)] = \int_{\mathcal X^\n}\phi^\n(x^\n)L_\theta^\n(x^\n)\dif x^\n
		= \int_{\mathcal X^\n}\P[\text{RH0} \, | \, X^\n = x^\n]L_\theta^\n(x^\n)\dif x^\n
		= \P[\text{RH0}].\]
	\end{rmq}

	\begin{déf} Le \textit{risque de première espèce} en $\theta \in \Theta_0$ est la valeur~:
	\[\P_\theta[\phi^\n(X^\n) \text{ rejette}] = \E_\theta[\phi^\n(X^\n)].\]

	Le \textit{risque de seconde espèce} en $\theta \in \Theta_1$ est la valeur~:
	\[\P_\theta[\phi^\n(X^\n) \text{ ne rejette pas}] = 1 - \E_\theta[\phi^\n(X^\n)].\]
	\end{déf}

	\begin{déf} La \textit{dimension} d'un test $\phi^\n$ en $\theta \in \Theta_0$ est $\E_\theta[\phi^\n(X^\n)]$.

	La \textit{puissance} d'un test $\phi^\n$ en $\theta \in \Theta_1$ est $\E_\theta[\phi^\n(X^\n)]$.
	\end{déf}

	\section{Principe de Neyman}
		\begin{déf} Soit $\phi^\n$ un test (pur ou randomisé). On dit que $\phi^\n$ est de niveau $\alpha \in (0, 1)$ lorsque~:
		\[\forall \theta \in \Theta : \E_\theta^\n[\phi^\n(X^\n)] \leq \alpha.\]
		\end{déf}

		\begin{déf} Le test $\phi_*$ est dit \textit{à puissance uniformément maximale} (PUM) dans la classe $C_\alpha$ lorsque~:
		\begin{itemize}
			\item $\phi_* \in C_\alpha$~;
			\item $\forall \phi \in C_\alpha : \forall \phi \in \Theta_1 : \E_\theta^\n[\phi_*(X^\n)] \geq \E_\theta^\n[\phi^\n(X^\n)]$.
		\end{itemize}
		\end{déf}

		Considérons ici le test le plus simple possible~:
		\begin{align}\label{eq:test_simple}
			\begin{cases}
				&H_0 : \theta = \theta_0 \\
				&H_1 : \theta = \theta_1
			\end{cases}
		\end{align}
		où $\Theta = \{\theta_0, \theta_1\}$. Pour $\alpha \in (0, 1)$, la classe $C_\alpha$ est donc~:
		\[C_\alpha = \left\{\phi : \mathcal X^\n \to [0, 1] \tq \E_{\theta_0}^\n[\phi(X^\n)] \leq \alpha\right\}.\]

		\begin{thm}[Lemme fondamental de Neyman]\label{thm:test_simple} Soit le problème d'hypothèse~\eqref{eq:test_simple}. Soit $\alpha \in (0, 1)$ et notons
		$L_\theta^\n$ la vraisemblance associée à $\P_\theta^\n$. Alors~:
		\begin{itemize}
			\item[$(i)$]  il existe $(k_\alpha, \gamma_\alpha) \in \R^+ \times [0, 1]$ tels que le test $\phi_*$ défini par~:
			\[\phi_*(x^\n) =
			\begin{cases}
				1 &\text{ si } L_{\theta_1}^\n(x^\n) > k_\alpha L_{\theta_0}^\n(x^\n) \\
				\gamma_\alpha &\text{ si } L_{\theta_1}^\n(x^\n) = k_\alpha L_{\theta_0}^\n(x^\n) \\
				0 &\text{ si } L_{\theta_1}^\n(x^\n) < k_\alpha L_{\theta_0}^\n(x^\n)
			\end{cases}\]
			vérifie $\E_{\theta_0}[\phi_*]$~;

			\item $\phi_*$ est à puissance uniformément maximale dans $C_\alpha$.
		\end{itemize}
		\end{thm}

		\begin{proof} Soit $F_{\theta_0}$ la fonction de répartition de la valeur aléatoire $Y^\n \coloneqq \frac {L_{\theta_1}^\n(\Xn)}{L_{\theta_0}^\n(\Xn)}$
		sous $\P_{\theta_0}^\n$, et posons $L_{\theta_0}^\n(\Xn) = 0 \Rightarrow Y^\n = \pinfty$.

		Pour $z \in \R$, on peut alors écrire~:
		\[\E_{\theta_0}^\n[\phi_*] = \P_{\theta_0}[Y^\n > k_\alpha] + \gamma_\alpha\P[Y=k^\alpha]
			= 1 - F_{\theta_0}(k_\alpha) + \lim_{\varepsilon \to 0}\gamma_\alpha\left(F_{\theta_0}(k_\alpha) - F_{\theta_0}(k_\alpha - \varepsilon)\right).\]

		Distinguons deux cas~:
		\begin{itemize}
			\item s'il existe $x \in \R$ tel que $F_{\theta_0}(z) = 1-\alpha$, alors on pose=
			\[(k_\alpha, \gamma_\alpha) = (z, 0).\]
			On a alors bien~:
			\[\E_{\theta_0}^\n[\phi_*(X^\n)] = 1 - F_{\theta_0}(z) + 0 = \alpha.\]

			\item Sinon, on pose~:
			\[k_\alpha \coloneqq \inf \{z \in \R \tq F_{\theta_0}(z) \geq 1-\alpha\}.\]

			Puisque $Y \geq 0$ (quotient de vraisemblances, qui sont positives), on sait $k_\alpha \geq 0$. Par continuité de $F_{\theta_0}$ (induite par la
			dérivabilité de $F_{\theta_0}$), on a~:
			\[F_{\theta_0}(k_\alpha-\varepsilon) < 1-\alpha < F_{\theta_0}(k_\alpha).\]
			On pose donc~:
			\[\gamma_\alpha \coloneqq \lim_{\varepsilon \to 0}\frac {F_{\theta_0}(k_\alpha) - (1-\alpha)}{F_{\theta_0}(k_\alpha) - F_{\theta_0}(k_\alpha - \varepsilon)} \in (0, 1).\]

			On a alors en effet~:
			\begin{align*}
				\E_{\theta_0}^\n[\phi_*(X^\n)] &= 1 - F_{\theta_0}(k_\alpha) +
					\lim_{\varepsilon \to 0}\frac {F_{\theta_0}(k_\alpha) - (1-\alpha)}{F_{\theta_0}(k_\alpha) - F_{\theta_0}(k_\alpha-\varepsilon)}
						\left(F_{\theta_0}(k_\alpha) - F_{\theta_0}(k_\alpha-\varepsilon)\right) \\
				&= 1 - F_{\theta_0}(k_\alpha) + (F_{\theta_0}(k_\alpha) - (1-\alpha)) = 1 - 1 + \alpha = \alpha.
			\end{align*}
		\end{itemize}

		Montrons alors le point 2. Prenons $\phi \in C_\alpha$, et considérons la fonction $g$ définie par~:
		\[g : \mathcal X^\n \to \R : x^\n \mapsto \left((\phi_*-\phi)(x^\n)\right)\left((L_{\theta_1}-k_\alpha L_{\theta_0})(x^\n)\right).\]

		Observons que $g$ est définie positive car~:
		\begin{align*}
			&\forall x^\n \in \mathcal X^\n : &\left(L_{\theta_1}^\n(x^\n) - k_\alpha L_{\theta_0}^\n(x^\n) \gneqq 0\right)
				\Rightarrow \left(\phi_*(x^\n) - \phi(x^\n) = 1 - \phi(x^\n) \geq 0\right), \\
			&\text{ et} &\left(L_{\theta_1}^\n(x^\n) - k_\alpha L_{\theta_0}^\n(x^\n) \lneqq 0\right) \Rightarrow
				\left(\phi_*(x^\n) - \phi(x^\n) = 0 - \phi(x^\n) \leq 0\right).
		\end{align*}

		On peut dès lors écrire, en distribuant $g$~:
		\begin{align*}
			0 &\leq \int_{\mathcal X^\n}g(x^\n)\dif x^\n = \int_{\mathcal X^\n}\left(\phi_*(x^\n)-\phi(x^\n)\right)L_{\theta_1}(x^\n)\dif x^\n
				- k_\alpha\left(\int_{\mathcal X^\n}\left(\phi_*(x^\n)-\phi(x^\n)\right)L_{\theta_0}\dif x^\n\right) \\
			&= \E_{\theta_1}^\n[\phi_*(X^\n) - \phi(X^\n)] + k_\alpha(\E_{\theta_0}^\n[\phi_*(X^\n)-\phi(X^\n)]) \\
			&= \E_{\theta_1}^\n[\phi_*(X^\n)] - \E_{\theta_1}[\phi(X^\n)] - k_\alpha(\alpha - \E_{\theta_0}[\phi(X^\n)]).
		\end{align*}

		Or, comme $\E_{\theta_0}[\phi(X^\n)] < \alpha$ (car $\phi \in C_\alpha$), on a~:
		\[\E_{\theta_1}^\n[\phi_*(X^\n)] \geq \E_{\theta_0}^\n[\phi(X^\n)] + k_\alpha(\alpha-\E_{\theta_0}[\phi(X^\n)] \geq \E_{\theta_1}[\phi(X^\n)].\]
		\end{proof}

		\begin{thm} Dans le cadre du lemme fondamental de Neyman, si $\phi$ est de niveau $\alpha \in (0, 1)$ et est de même puissance que $\phi_*$, i.e.
		ils ont la même espérance sous $\theta_1$, alors il existe une fonction $\gamma_\alpha : \mathcal X^\n \to [0, 1]$ telle que~:
		\begin{align*} \forall x^\n \in \mathcal X^\n : \phi(x^\n) =
			\begin{cases}
				1 &\text{ si } L_{\theta_1}(x^\n) < k_\alpha L_{\theta_0}(x^\n) \\
				\gamma_\alpha(x^\n) &\text{ si } L_{\theta_1}(x^\n) = k_\alpha L_{\theta_0}(x^\n) \\
				0 &\text{ sinon},
			\end{cases}
		\end{align*}
		où $k_\alpha$ est celui de $\phi_*$, et où le $\forall x^\n \in \mathcal X^\n$ signifie pour tout $x^\n$ dans $\mathcal X^\n$, sauf un ensemble
		de mesure nulle.

		De plus, $\E_{\theta_1}[\phi_*(X^\n)] > \alpha$.
		\end{thm}

		\begin{proof} On sait que~:
		\[-k_\alpha(\alpha-\E_{\theta_0}[\phi(X^\n)] = \E_{\theta_1}^\n[\phi_*(X^\n)] - \E_{\theta_1}^\n[\phi(X^\n)] - k_\alpha(\alpha - \E_{\theta_0}^\n[\phi(X^\n)]
			\geq 0.\]

		Or, on sait que $k_\alpha(\alpha-\E_{\theta_0}^\n[\phi(X^\n)]) \geq 0$. On a donc $\E_{\theta_0}^\n[\phi(X^\n)] = \alpha$, et donc~:
		\[\int_{\mathcal X^\n}g(x^\n)\dif x^\n = 0,\]
		c'est à dire, pour tout $x^\n \in \mathcal X^\n \setminus T$, tel que $T$ est de mesure nulle, on a~:
		\[\left(\phi_*(x^\n) - \phi(x^\n)\right) \cdot \left(L_{\theta_1}^\n(x^\n) - k_\alpha L_{\theta_0}^\n(x^\n)\right) = 0.\]

		Soit un tel $x^\n \in \mathcal X^\n \setminus T$. Si $L_{\theta_1}^\n(x^\n) \neq k_\alpha L_{\theta_0}^\n(x^\n)$, alors~:
		\[0 = \phi_*(x^\n) - \phi(x^\n).\]

		De plus, on pose $\gamma_\alpha = \phi$ sur $\{x^\n \in \mathcal X^\n \tq L_{\theta_1}^\n(x^\n) = k_\alpha L_{\theta_0}^\n(x^\n)\}$.

		Montrons que $\E_{\theta_1}[\phi_*(X^\n)] \gneqq \alpha$. Posons $\phi_\alpha : x^\n \mapsto \alpha$. On sait que $\phi_*$ est de puissance maximale,
		donc~:
		\[\E_{\theta_1}[\phi_*(X^\n)] \geq \E_{\theta_1}[\phi_\alpha(X^\n)] = \alpha.\]

		Supposons par l'absurde que $\E_{\theta_1}[\phi_*(X^\n)] = \alpha \in (0, 1)$. Par la première partie du théorème, on a~:
		\[\alpha = \phi_\alpha(x^\n) \not \in \{0, 1\}.\]
		Donc~:
		\[\forall x^\n \in \mathcal X^\n \setminus T : L_{\theta_1}^\n(x^\n) = k_\alpha L_{\theta_0}^\n(x^\n).\]
		En intégrant, on trouve~:
		\[1 = \int_{\mathcal X^\n}L_{\theta_1}^\n(x^\n)\dif x^\n = k_\alpha\int_{\mathcal X^\n}L_{\theta_0}^\n(x^\n)\dif x^\n = k_\alpha,\]
		et donc $L_{\theta_0}^\n = L_{\theta_1}^\n$ sur $\mathcal X^\n \setminus T$.

		Pour $B \in \Brl(\R^n)$, on a alors $\P_{\theta_0}[B] = \P_{\theta_1}[B]$, ce qui est une contradiction car $\theta_0 \neq \theta_1$.
		\end{proof}

	\section{Tests unilatéraux à puissance uniformément maximale}
		\begin{déf} Soit le modèle statistique $\statmod \R\Brl{\mathcal P}n$, avec $\mathcal P = \{\P_\theta \tq \theta \in \Theta \subseteq \R\}$.
		Ce modèle est dit \textit{à vraisemblance monotone en la statistique $T(x^\n)$} lorsque~:
		\[\forall \theta', \theta'' \in \Theta : \theta' < \theta'' \Rightarrow \exists h_{\theta', \theta''} \text{ strict. croissante } \tq
			h_{\theta', \theta''}(T(x^\n)) = \frac {L_{\theta''}^\n(x^\n)}{L_{\theta'}^\n(x^\n)}.\]
		\end{déf}

		\begin{thm} Soit un modèle statistique à vraisemblance monotone en une statistique $T(x^\n)$ et soit $\alpha \in (0, 1)$. Alors~:
		\begin{itemize}
			\item $\exists (t_\alpha^+, \gamma_\alpha) \in \R \times [0, 1] \tq \phi_{*T}^+$ défini par~:
			\[\phi_{*T}^+(x^\n) =
			\begin{cases}
				1 &\text{ si } T(x^\n) > t_\alpha^+ \\
				\gamma_\alpha &\text{ si } T(x^\n) = t_\alpha^+ \\
				0 &\text{ sinon}
			\end{cases}\]
			vérifie $\E_{\theta_0}^\n[\phi_{*T}^+(\Xn)] = \alpha$~;
			\item la fonction $\theta \mapsto \E_\theta[\phi_{*T}^+(\Xn)]$ est strictement croissante~;
			\item le test $\phi_{*T}^+$ est à PUM dans la classe des tests de niveau $\alpha$ pour le problème d'hypothèse~:
			\begin{align}\label{eq:test_hyp_leq_>}
				\begin{cases}
					&H_0 : \theta \leq \theta_0 \\
					&H_1 : \theta > \theta_0
				\end{cases}
			\end{align}
			avec $\theta_0 \in \intr \Theta$.
		\end{itemize}
		\end{thm}

		\begin{proof} Soit $F_{\theta_0}^T$ la fonction de répartition de $T(X^\n)$ sous $\P_{\theta_0}$. Comme pour le Théorème~\ref{thm:test_simple}, on a~:
		\[\E_{\theta_0}^\n[\phi_{*T}^+(X^\n)] =
			1 - F_{\theta_0}^T(t_\alpha^+) + \gamma_\alpha(F_{\theta_0}(t_\alpha^+)-\lim_{\varepsilon \to 0}F_{\theta_0}^T(t_\alpha^+ - \varepsilon),\]
		d'où on distingue deux cas~:
		\begin{itemize}
			\item s'il existe $z_\alpha \tq F_{\theta_0}^T(z_\alpha) = 1 - \alpha$, on prend~:
			\[(t_\alpha^+, \gamma_\alpha) \coloneqq (z_\alpha, 0)),\]
			\item et sinon, on pose~:
			\[t_\alpha^+ \coloneqq \inf\{z \in \R \tq F_{\theta_0}^T(z) \geq 1-\alpha\},\]
			et~:
			\[\gamma_\alpha \coloneqq \frac {F_{\theta_0}^T(t_\alpha^+)
				- (1-\alpha)}{\lim_{\varepsilon \to 0}F_{\theta_0}^T(t_\alpha^+) - F_{\theta_0}^T(t_\alpha^+ - \varepsilon)} \in (0, 1).\]
		\end{itemize}

		Pour le second point, on prend $\theta', \theta'' \in \Theta$ tels que $\theta' < \theta''$. On réécrit~:
		\begin{align*}
			\phi_{*T}^+(x^\n) &=
			\begin{cases}
				1 &\text{ si } h_{\theta',\theta''}(T(x^\n)) > h_{\theta',\theta''}(t_\alpha^+) \\
				\gamma_\alpha &\textit{ si } h_{\theta', \theta''}(T(x^\n)) = h_{\theta',\theta''}(t_\alpha^+) \\
				0 &\text{ sinon}
			\end{cases} \\
			&=
			\begin{cases}
				1 &\text{ si } L_{\theta''}(x^\n) > h_{\theta',\theta''}(t_\alpha^+)L_{\theta'}(x^\n) \\
				\gamma_\alpha & \text{ si } L_{\theta''}(x^\n) = h_{\theta',\theta''}(t_\alpha^+)L_{\theta'}(x^\n) \\
				0 &\text{ sinon}
			\end{cases}
		\end{align*}

		On trouve que $\phi_{*T}^+$ est le test de Neyman du problème d'hypothèse~:
		\[\begin{cases}
			&H_0 : \theta = \theta' \\
			&H_1 : \theta = \theta''
		\end{cases}\]
		au niveau $\E_{\theta'}[\phi_{*T}^+(\Xn)]$. Le Le théorème précédent donne donc~:
		\[\E_{\theta'}[\phi_{*T}^+(\Xn)] < \E_{\theta''}[\phi_{*T}^+(\Xn)].\]

		Pour le dernier point, soient $\theta_1 > \theta_0$ et $\phi$, un test de niveau $\alpha$ pour le problème d'hypothèse~\eqref{eq:test_hyp_leq_>}.
		$\theta_1$ étant choisi arbitrairement, on a que $\phi_{*T}^+$ est de niveau $\alpha$ pour le problème d'hypothèse~:
		\begin{align}\label{eq:test_hyp_=_>}
			\begin{cases}
				&H_0 : \theta = \theta_0 \\
				&H_1 : \theta > \theta_0
			\end{cases}
		\end{align}
		car $\phi_{*T}^+$ est à PUM de niveau $\alpha$ pour le problème d'hypothèse~\eqref{eq:test_simple}.

		Par choix de $\phi$, on sait en particulier que $\phi$ est de niveau $\alpha$ pour~\eqref{eq:test_hyp_=_>}. $\phi_{*T}^+$ domine donc $\phi$ uniformément
		en puissance. Puisque $\phi_{*T}^+$ est de niveau $\alpha$ pour~\eqref{eq:test_hyp_leq_>}, on a $\phi_{*T}^+$ à PUM pour~\eqref{eq:test_hyp_leq_>}.
		\end{proof}

		\begin{thm} Soit un modèle statistique à rapport de vraisemblance pour la statistique $T(x^\n)$ et soit $\alpha \in (0, 1)$. Alors~:
		\begin{itemize}
			\item $\exists (t_\alpha^-, \gamma_\alpha) \in \R \times [0, 1] \tq \phi_{*T}^-$ défini par~:
			\[\phi_{*T}^-(x^\n) =
			\begin{cases}
				1 &\text{ si } T(x^\n) < t_\alpha^- \\
				\gamma_\alpha &\text{ si } T(x^\n) = t_\alpha^- \\
				0 &\text{ sinon}
			\end{cases}\]
			vérifie $\E_{\theta_0}^\n[\phi_{*T}^-(\Xn)] = \alpha$~;
			\item la fonction $\theta \mapsto \E_\theta^\n[\phi_{*T}^-]$ est strictement décroissante~;
			\item le test $\phi_{*T}^-$ est à PUM dans la classe des tests de niveau $\alpha$ pour le problème~:
			\begin{align}\label{eq:test_hyp_geq_<}
				\begin{cases}
					&H_0 : \theta \geq \theta_0 \\
					&H_1 : \theta < \theta_0
				\end{cases}
			\end{align}
		\end{itemize}
		\end{thm}

	\section{Tests bilatéraux}
		\begin{thm} Pour tout $\alpha \in (0, 1)$, il n'existe pas de test à PUM au niveau $\alpha$ pour les problème~:
		\begin{align}\label{eq:test_hyp_=_neq}
			\begin{cases}
				&H_0 : \theta = \theta_0 \\
				&H_1 : \theta \neq \theta_0
			\end{cases}
		\end{align}
		\end{thm}

\end{document}
