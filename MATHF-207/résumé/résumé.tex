\documentclass{report}

\usepackage{commath}
\usepackage{hyperref}
\usepackage[francais]{babel}
\usepackage{mathtools}
\usepackage{eulervm, palatino}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[parfill]{parskip}
\usepackage{fullpage}
\usepackage{amsmath, amsthm,amssymb, amsfonts}
\usepackage{stmaryrd}

\title{Statistiques mathématiques}
\author{R. Petit}
\date{année académique 2016 - 2017}

\DeclareMathOperator{\tq}{\text{ t.q. }}
\DeclareMathOperator{\Unif}{Unif}  % uniform law
\DeclareMathOperator{\Bern}{Bern}  % Bernoulli law
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\MAE}{MAE}

\renewcommand{\P}{\mathbb P}
\newcommand{\E}{\mathbb E}

\newcommand{\minfty}{{-\infty}}
\newcommand{\pinfty}{{+\infty}}
\newcommand{\cvgp}{\xrightarrow[n \to \pinfty]{\P}}
\newcommand{\cvgLr}{\xrightarrow[n \to \pinfty]{L_r}}
\newcommand{\cvgd}{\xrightarrow[n \to \pinfty]{\mathcal D}}
\newcommand{\cvgps}{\xrightarrow[n \to \pinfty]{\text{p.s.}}}
\newcommand{\distreq}{\overset {\mathcal D}=}
\newcommand{\statmod}[4]{\left(#1^{#4}, #2\left(#1^{#4}\right), #3^{\left(#4\right)}\right)}
\newcommand{\probspace}[3]{\left(#1, #2, #3\right)}
\newcommand{\Nms}{\mathcal N(\mu, \sigma^2)}
\newcommand{\Nzo}{\mathcal N(0, 1)}
\newcommand{\vvp}[2]{\begin{pmatrix}#1 \\ #2\end{pmatrix}}

\newcommand{\N}{\mathbb N}
\newcommand{\Ns}{\N^{*}}
\newcommand{\R}{\mathbb R}
\newcommand{\Rp}{{\mathbb R^+}}
\newcommand{\Rm}{{\mathbb R^-}}

\newcommand{\Brl}{\mathcal B}  % \sigma-algebra of borelians
\newcommand{\charfun}[1]{I_{\left[#1\right]}}
\newcommand{\intint}[2]{\left\llbracket#1, #2\right\rrbracket}

\newcommand{\Xn}{{X^{(n)}}}
\newcommand{\Tn}{{T^{(n)}}}
\newcommand{\TnXn}{{\Tn(\Xn)}}

\newtheorem{thm}{Théorème}[chapter]
\newtheorem{lem}[thm]{Lemme}
\newtheorem{prp}[thm]{Proposition}
\theoremstyle{definition}
\newtheorem{déf}[thm]{Définition}
\theoremstyle{remark}
\newtheorem*{rmq}{Remarque}
\newtheorem{ex}{Exemple}[chapter]

\NoAutoSpaceBeforeFDP

\begin{document}
\pagenumbering{Roman}
\maketitle
\tableofcontents
\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

\chapter*{Introduction}
	En probabilités, une variable aléatoire $X$ donnée est entièrement définie par sa loi. On peut l'exprimer par la fonction de répartition $F^X$ ou par la
	fonction de densité $f^X = \od {}xF^X$. Ces fonctions permettent de déterminer~:
	\[\P[a \leq X \leq b] = \int_a^bf^X(x)\dif x = F^X(b) - F^X(a).\]
	Ou encore~:
	\[\E[X] = \int_\minfty^\pinfty xf^X(x)\dif x.\]

	Cependant, les fonctions $f^X$ et $F^X$ ne sont jamais connues précisément. Elles peuvent être approchées par des modélisations, mais les modèles ne sont
	jamais exacts. En probabilités, on cherche donc les observations sur base de la loi qui est connue, alors qu'en statistiques, on cherche à retrouver la loi
	sur base de $n$ observations $X_1, \ldots, X_n$.

	Nous allons nous intéresser à des \textit{modèles statistiques} sous la forme $\statmod \R\Brl{\mathcal P}n$ où~:
	\[\mathcal P^{(n)} = \left\{\P^{(n)}\right\} = \left\{\P^{(n)}_\theta \tq \theta \in \Theta \subset \R^k\right\},\]
	et donc les $P^{(i)}$ sont chacun une loi possible pour $(X_1, \ldots, X_n)$.

	Ces modèles sont dits \textit{paramétriques} car les différentes lois sont les mêmes au paramètre $\theta$ près. Nous n'étudierons que des modèles
	paramétriques où $\Theta$ est un espace de dimension $d \in \N$ finie.

	\begin{ex} Soient $X_1,\ldots, X_n$ des variables aléatoires iid (indépendantes et identiquement distribuées).
	\begin{itemize}
		\item Si les $X_i$ sont de loi normale $\Nms$, alors le paramètre $\theta$ est donné par ~:
		      \[\theta = \vvp \mu{\sigma^2} \in \Theta = \R \times \R^+ \subset \R^2~;\]
		\item si les $X_i$ sont de loi uniforme $\Unif(0, \theta)$, le paramètre $\theta$ est donné par $\theta \in \Theta = \R_0^+ \subset \R$~;
		\item si les $X_i$ sont de loi $\Bern(p)$, le paramètre $\theta$ est donné par $\theta = p \in \Theta = [0, 1] \subset \R$.
	\end{itemize}
	\end{ex}

	\begin{rmq} Une loi normale $\Nms$ est déraisonnable car les valeurs observables ne vont empiriquement pas vers les infinis alors que la distribution le
	permet théoriquement mais n'est pas \textbf{complètement} déraisonnable car ces probabilités sont négligeables grâce à l'exponentielle de $(-x^2)$ dans la
	formule de la densité.
	\end{rmq}

\chapter{Théorie de l'échantillonnage}
	\section{Terminologie et définitions}
		\begin{déf} On appelle \textit{modèle d'échantillonnage} un modèle d'observations iid.  \end{déf}

		\begin{déf} Soit un modèle statistique $\statmod E\Brl{\mathcal P}n$ où $\mathcal P^{(n)} =
		\left\{\P_\theta^{(n)} \tq \theta \in \Theta \subset \R^k\right\}$. On note ici $\P^{(n)}_\theta$ une loi possible pour $(X_1, \ldots, X_n)$ et $\P_\theta$
		une loi possible pour $X_i$ avec $i$ fixé. On dit alors que $\P^{(n)}_\theta$ est déterminé par $\P_\theta$.
		\end{déf}

		\begin{rmq} Ici, deux visions vont s'opposer et se compléter~: la vision \textit{population} qui est associée à $P_\theta$ et la version
		\textit{échantillonage} (ou \textit{empirique}), qui, elle, est associée à $P_\theta^{(n)}$.
		\end{rmq}

		\begin{déf} On définit la fonction indicatrice $I_{[\cdot]}$ qui vaut 1 quand l'expression entre crochets est vraie et 0 sinon. \end{déf}

		\begin{déf} Soit $X_1, \ldots, X_n$ une suite de $n$ observations. On définit la \textit{$i$eme statistique d'ordre} par
		$X_{(i)} = X_k \tq \abs {\{X_j \tq X_j < X_k, 1 \leq j \leq n\}} = i$. On définit également la \textit{statistique d'ordre} par $\left(X_{(i)}\right)_i$.
		\end{déf}

		\begin{déf} On définit les fonctions de répartitions comme suit~:
		\begin{itemize}
			\item la fonction de répartition population~:
			      \[F_\theta(x) = P_\theta[X_i \leq x]~;\]
			\item la fonction de répartition empirique~:
			      \[F_n(x) = \frac 1n\sum_{i=1}^nI_{[X_i \leq x]}.\]
		\end{itemize}
		\end{déf}

		\begin{rmq} La fonction $F_n$ empirique est une fonction en escaliers. Elle fait des sauts de hauteur $\frac 1n$, et est telle que~:
		\[\lim_{x \to \pinfty}F_n(x) = 1\qquad\qquad \text{ et } \qquad\qquad \lim_{x \to \minfty}F_n(x) = 0.\]

		On peut également remarquer que $F_n(X_{(i)}) = \frac in$. En effet, par définition de $X_{(i)}$, il y a exactement $i$ observations inférieures à
		$X_{(i)}$. Dès lors, la fonction indicatrice donnera $i$ fois la valeur 1 et $(n-i)$ fois la valeur 0. La somme donc donc $i$ et la fonction donne
		$\frac in$. \end{rmq}

		\begin{déf} On appelle \textit{statistique} toute fonction mesurable faisant intervenir \textbf{uniquement} des observations. \end{déf}

		\begin{ex} Par exemple $F_n$ est une statistique car seules les valeurs $X_i$ sont utilisée, mais $F_\theta$ n'est pas une statistique car la valeur
		du paramètre $\theta$ apparait et n'est pas une observation. \end{ex}

		\begin{rmq} Une statistique peut être à valeur scalaire ($X_{(i)}$ par exemple), à valeur vectorielle ($\left(X_{(i)}\right)_{1 \leq i \leq n}$ par
		exemple), à valeur ensembliste ($[X_i \pm \overline X]$ avec $i$ fixé par exemple), ou encore à valeur fonctionnelle ($F_n$ par exemple). \end{rmq}

		\begin{rmq} L'objectif est de pouvoir approximer la loi régissant les populations ($F_\theta$) à l'aide de la loi observée empiriquement. Par la loi
		des grands nombres, on a~:
		\[F_n(x) \xrightarrow[n \to \pinfty]{p.s. \text{ par } \P_\theta} F_\theta(x).\]
		\end{rmq}

		\begin{thm}[Théorème de Glivenko-Cantelli] Si $F_n$ et $F_\theta$ sont repsectivement une fonction de répartition empirique et de population, alors~:
		\[\sup_{x \in \R}\abs {F_n(x) - F_\theta(x)} \cvgps 0\]
		\end{thm}

	\section{Moments}
		\begin{déf}[Moments pour populations] On définit $\mu_r'(\theta)$ le \textit{moment \textbf{non}-centré} d'ordre $r$ avec $r \in \Ns$ par~:
		\[\mu_r'(\theta) \coloneqq E_\theta[X_1^r].\]

		On définit également $\mu_r(\theta)$, le \textit{moment \textbf{centré}} d'ordre $r$ avec $r \in \Ns$ par~:
		\[\mu_r(\theta) \coloneqq E_\theta\left[\left(X_1 - \mu_r'(\theta)\right)^r\right].\]
		\end{déf}

		\begin{déf}[Moments pour échantillon] On définit $m_r'$, le \textit{moment \textbf{non}-centré} d'ordre $r$ avec $r \in \Ns$ par~:
		\[m_r' \coloneqq \frac 1n\sum_{i=1}^n X_i^r.\]

		On définit également le \textit{moment \textbf{centré}} d'ordre $r$ avec $r \in \Ns$ par~:
		\[m_r \coloneqq \frac 1n\sum_{i=1}^n\left(X_i - m_r'\right)^r.\]
		\end{déf}

		\begin{rmq} La loi des grands nombres dit que~:
		\[m_r' \cvgps \mu_r'(\theta),\]
		mais on ne peut pas dire que~:
		\[m_r \cvgps \mu_r(\theta).\]

		Ce n'est donc pas possible car pour $m_r'$, il y a une somme de variables iid alors que pour $m_r$, les variables sommées ne sont pas iid (mais
		dépendent toutes de tous les $X_i$).

		En réalité, il y a convergence, mais on ne peut pas l'exprimer de manière triviale par la loi des grands nombres. \end{rmq}

		\subsection{Indicateurs}
			On peut observer que $\mu_1'(\theta) = \E_\theta[X_1]$. Pareil pour $m_1' = \overline X$. Le moment d'ordre 1 est donc un indice de position.
			On a alors $\mu \coloneqq \mu_1(\theta) = \E[(X - \E[X_1])] = \E[X_1] - \E[X_1] = 0$. Cette valeur n'est donc pas intéressante. Par contre~:
			\[\mu_2(\theta) = \E\left[(X_1 - \E[X_1])^2\right] \eqqcolon \Var(X) \qquad\qquad \text{ si }
				\qquad\qquad m_2 = \frac 1n\sum_{i=1}^n\left(X_i - \overline X\right)^2 \eqqcolon s^2.\]
			Le moment d'ordre 2 est donc un indice de dispersion.

			\begin{déf} On appelle le \textit{coefficient d'asymétrie de Fisher} la quantité~:
			\[\gamma_1 \coloneqq \mu_3(\theta) \cdot \left(\mu_2(\theta)\right)^{-\frac 32}.\]
			\end{déf}

			\begin{rmq} Le dénominateur $\mu_2(\theta)^{\frac 32}$ apparait afin de rendre invariant le coefficient d'asymétrie de Fisher aux transformations
			affines. \end{rmq}

			\begin{déf} Le coefficient d'asymétrie de Fisher \textit{empirique} est donné par~:
			\[m_3 \cdot m_2^{-\frac 32}.\]
			\end{déf}

			\begin{déf} On appelle \textit{coefficient d'applatissement de Fisher} la quantité~:
			\[\gamma_2 \coloneqq \mu_4(\theta) \cdot \left(\mu_2(\theta)\right)^{-2} - 3.\]
			\end{déf}

			\begin{déf} Le coefficient d'aplatissement de Fisher \textit{empirique} est donné par~:
			\[m_4 \cdot m_2^{-2} - 3.\]
			\end{déf}

			\begin{rmq} Si $\gamma_2 \gneqq 0$, c'est que les événements extrêmes sont de plus haute probabilité et si $\gamma_2 \lneqq 0$, c'est que les
			événements extrêmes sont de moins haute probabilité.

			À nouveau, le dénominateur y a été ajouté afin de rendre le coefficient invariant aux transformations affines. Et le terme $-3$ sert à annuler le
			coefficient d'aplatissement de Fisher pour une normale $\Nms$. \end{rmq}

	\section{Quantile}
		\begin{déf} Si $F_\theta$ est inversible, alors on définit $x_\alpha(\theta) \coloneqq F_\theta^{-1}(\alpha)$, et on appelle $x_\alpha(\theta)$ un
		\textit{quantile}. \end{déf}

		\begin{rmq} Il faut cependant faire attention car on peut avoir le cas de $F_\theta$ discontinue où on choisit $\alpha = F_\theta^{-1}$(point de
		discontinuité) ou alors le cas de $F_\theta$ admettant un plateau et où on choisit $\alpha$ sur le plateau. \end{rmq}

		\begin{déf} On définit alors~:
		\[x_\alpha(\theta) \coloneqq \inf\left\{x \in \R \tq F_\theta(x) \geq \alpha\right\}.\]
		\end{déf}

		\begin{rmq} On donne les noms de \textit{médiane}, \textit{quartile}, \textit{décile}, \textit{percentile} pour $\alpha$ valant, avec $k$ entier,
		respectivement $\frac 12$, $\frac k4$ avec $k < 4$, $\frac k{10}$ avec $k < 10$, et $\frac k{100}$ avec $k < 100$.
		\end{rmq}

		\begin{déf} Pour les échantillons, on définit le \textit{quantile empirique d'ordre $\alpha$} par~:
		\[x_\alpha^{(n)} \coloneqq \inf\{x \in \R \tq F_n(x) \geq \alpha\}.\]
		\end{déf}

		\begin{rmq} On peut également définir des indices de position, dispersion, asymétrie, aplatissement, etc. sur les quantiles plutôt que sur les moments.
		Ils auront des propriétés différentes et une robustesse différente aux valeurs aberrantes.
		\end{rmq}

		\begin{déf} La loi échantillonnée de $T(X^{(n)})$ est la loi déterminée par~:
		\[\P_\theta^{(n)}\left[T(X^{(n)}) \in B\right] = \P_\theta^{(n)}\left[\left\{x^{(n)} \in X^{(n)} \tq T(x^{(n)} \in B\right\}\right], B \in \Brl(\R^m).\]
		\end{déf}

		\begin{ex}[Bernoulli] $X^{(n)} = (X_1, \ldots, X_n)$ où les $X_i$ sont iid $\Bern(p)$. On a alors~: $T(X^{(n)}) = \sum_{i=1}^nX_i$, sous
		$\P_\theta^{(n)}$, est de loi $\Bin(n, p)$.
		\end{ex}

		\begin{ex}[Normale] $X^{(n)} = (X_1, \ldots, X_n)$ où les $X_i$ sont iid $\Nms$ et où
		$\theta = \begin{pmatrix}\mu \\ \sigma^2\end{pmatrix} \in \Theta = \R \times \R^+_0 \subset \R^2$.
		La statistique $T_1(X^{(n)}) = \sum_{i=1}^nX_i$, sous $P_\theta^{(n)}$, est de loi $\mathcal N(n\mu, n\sigma^2)$.

		La statistique $T_2(X^{(n)}) = \frac 1n\sum_{i=1}^nX_i$, sous $\P_\theta^{(n)}$, est de loi $\mathcal N(\mu, \frac {\sigma^2}n)$.
		\end{ex}

		\begin{ex}[Uniforme] $X^{(n)} = (X_1, \ldots, X_n)$ où les $X_i$ sont iid $\Unif(0, \theta)$, pour $\theta \in \Theta = \R^+_0 \subset \R$.
		On a donc $f_\theta^{X_i}(x) = \theta^{-1}\charfun {0 \leq x \leq \theta}$. Et donc~:
		\[F_\theta^{X_i}(x) = \begin{cases}0 &\text{ si } x < 0 \\\frac x\theta &\text{ si } 0 \leq x \leq \theta \\1 &\text{ sinon}\end{cases}.\]

		La statistique $T(X^{(n)}) = X_{(n)} = \max_{1 \leq k \leq n}\{X_k\}$ a pour fonction de répartition, sous $P_\theta^{(n)}$~:
		\[F_\theta^{(n)}(x) = \P[X_{(n)} \leq x] = \P[X_1 \leq x, X_2 \leq x, \ldots, X_n \leq x].\]

		La seconde forme est plus agréable car on a une intersection d'événements indépendants. Donc~:
		\[F_\theta^{(n)}(x) = \prod_{i=1}^n\P[X_i \leq x] = \prod_{i=1}^nF_\theta^{X_i}(x) =
			\begin{cases}0 &\text{ si } x < 0 \\\left(\frac x\theta\right)^n &\text{ si } 0 \leq x \leq \theta \\0 &\text{ sinon}\end{cases}.\]

		On a alors la fonction de densité~:
		\begin{align*}
			f_\theta^{X_{(n)}}(x) &= \od {}xF_\theta^{X_{(n)}}\sVert[3]_x =
			\begin{cases}0 &\text{ si } x < 0 \\\frac {nx^{n-1}}{\theta^n}\charfun{0 \leq x \leq \theta} &\text{ si } 0\leq x \leq \theta \\0 &\text{ sinon}\end{cases} \\
			&= \frac {nx^{n-1}}{\theta^n}\charfun{0 \leq x \leq \theta}.
		\end{align*}
		\end{ex}

		\begin{rmq} La loi échantillonnée n'est pas toujours possible à déterminer exactement analytiquement. Dans ce cas, on donne~:
		\begin{itemize}
			\item[$(i)$] les/des moments de la loi échantillonnée exacte~;
			\item[$(ii)$] la loi échantillonnée asymptotique.
		\end{itemize}

		Et pour de grandes valeurs de $n$, la loi asymptotique donne une assez bonne approximation de la loi exacte.
		\end{rmq}

		\begin{rmq} Ici, les termes \textit{exact} et \textit{asymptotique} s'opposent~: on parle d'objet \textit{exact} lorsque l'objet est connu pour $n$ fixé,
		et d'objet \textit{asymptotique} lorsque l'objet n'est connu que pour $n \to \pinfty$.
		\end{rmq}

		\begin{ex} Voici un cas où on ne peut exprimer de loi exacte mais où il est possible d'exprimer une loi asymptotique. Soit $X^{(n)} = (X_1, \ldots, X_n)$
		où les $X_i$ sont iid $F$ avec la fonction $F$ telle que $\Var_F(X_i) = \sigma^2 < \pinfty$ et donc $E_F(X_i) = \mu < \pinfty$. On peut dès lors
		appliquer le théorème central limite (TCL)~:
		\[\sqrt n(\overline X^{(n)} - \mu) \cvgd \mathcal N(0, \sigma^2).\]

		Pour $n \gg$, on peut alors dire~:
		\[\overline X^{(n)} \approx \mathcal N\left(\mu, \frac {\sigma^2}n\right),\]
		où le symbole $\approx$ se lit \textit{est à peu près de même loi}.

		On en conclut donc qu'avec $n$ suffisamment grand, on peut approximer $\overline X^{(n)}$, même sans connaitre sa loi exacte.
		\end{ex}

	\subsection{Lemme de Fisher}
		\begin{déf} La variable aléatoire $Q$ est de loi $\chi^2$ (chi-carrée) à $k (\in \N^*)$ degrés de liberté lorsque~:
		\[Q \distreq \sum_{i=1}^kZ_i^2,\]
		où les $Z_i$ sont iid $\Nzo$ et où «~$\distreq$~» veut dire \textit{a la même distribution que}. Cela se note~:
		\[Q \sim \chi^2_k\]
		\end{déf}
		\begin{rmq} Si $Q \sim \chi^2_k$, alors~:
		\[f^Q(x) = \frac 1{2^{\frac k2}\Gamma\left(\frac k2\right)}x^{\frac k2-1}\exp\left(-\frac x2\right)\charfun{x > 0},\]
		où $\Gamma$ est la fonction Gamma d'Euler définie par~:
		\[\Gamma(x) = \int_0^\pinfty t^{x-1}\exp(-t)\dif t.\]

		De plus, $\Var(Q) = 2k$, et $E(Q) = k$.

		On peut également noter que les $\chi^2$ sont stables par la somme~: si $Q_1 \sim \chi^2_{k_1}$ et $Q_2 \sim \chi^2_{k_2}$, alors~:
		\[Q_1 + Q_2 \sim \chi^2_{k_1+k_2}.\]
		\end{rmq}

		\begin{lem}\label{lem:preFisher} Soit $W = (W_1, \ldots, W_k)$ un vecteur de variables aléatoires, où $f^W$ : $\R^k \to \R^+$ est la fonction de
		densité du vecteur $W$. Alors~:
		\begin{enumerate}
			\item $\P[W \in B] = \int_Bf^W(x)\dif x$~;
			\item si $V = AW + b$ où $A$ est une matrice $k \times k$ inversible, alors~:
				\[f^V(v) = \abs {\det A^{-1}}f^W\left(A^{-1}(v-b)\right).\]
		\end{enumerate}
		\end{lem}

		\begin{thm}[Lemme de Fisher] Soient $X_1, \ldots, X_n$ iid $\Nms$ où $n \geq 2$. Alors~:
		\begin{enumerate}
			\item[$(i)$]   $\overline X \sim \mathcal N(\mu, \frac {\sigma^2}n)$~;
			\item[$(ii)$]  $\frac {ns^2}{\sigma^2} \sim \chi^2_{n-1}$~;
			\item[$(iii)$] $\overline X \sqcup s^2$.
		\end{enumerate}
		\end{thm}

		\begin{proof} Posons $Z_i \coloneqq \frac {X_i - \mu}\sigma$ pour $i \in \intint 1n$. Puisque les $X_i$ sont iid, les $Z_i$ le sont également (même
		transformation appliquée à tous les $X_i$ et chaque $Z_i$ ne fait intervenir que le $X_i$ correspondant). Notons que~:
		\[\overline X = \frac 1n\sum_{i=1}^nX_i = \frac 1n\sum_{i=1}^n\left(\sigma Z_i + \mu\right) = \sigma\overline Z + \mu,\]
		où $\overline Z$ est la moyenne empirique des $Z_i$. Notons également que~:
		\[ns^2 = \sum_{i=1}^n(X_i - \overline X)^2 = \sum_{i=1}^n\left(\left(\sigma Z_i + \mu\right) - \left(\sigma\overline Z + \mu\right)\right)^2
			= \sigma^2\sum_{i=1}^n\left(Z_i - \overline Z\right)^2 = n\sigma^2s_Z^2.\]

		Il nous faut alors montrer que $\overline Z \sim \Nzo$ et $ns_Z^2 \sim \chi^2_{n-1}$, avec $\overline Z \sqcup s_Z^2$.

		Pour cela, on sait que le vecteur $Z^{(n)} = (Z_1, \ldots, Z_n)$ a pour densité~:
		\[f^{Z^{(n)}}(z^{(n)}) = \prod_{i=1}^nf^{Z_i}(z_i) = \prod_{i=1}^n\left(\frac 1{\sqrt {2\pi}}\exp\left(\frac {z_i^2}2\right)\right)
			= \left(\frac 1{\sqrt {2\pi}}\right)^n\exp\left(-\sum_{i=1}^n\frac {z_i^2}2\right)
			= \left(\frac 1{\sqrt {2\pi}}\right)^n\exp\left(-\frac 12\norm {z^{(n)}}^2\right).\]

		Soit $O$ une matrice orthogonale de dimension $n \times n$ telle que $\forall j \in \intint 1n$ : $O_{1j} = \frac 1{\sqrt n}$. On pose alors~:
		\[(Y_1, \ldots, Y_n) = Y^{(n)} = OZ^{(n)}.\]

		Puisque la matrice $O$ est orthogonale, on sait que $O^{-1}$ existe et que $\abs {\det O} = \abs {\det O^{-1}} = 1$. Par le lemme~\ref{lem:preFisher},
		on peut dire~:
		\[f^{Y^{(n)}}(y^{(n)}) = \abs {\det O^{-1}}f^{Z^{(n)}}\left(O^{-1}y^{(n)}\right)
			= \left(\frac 1{\sqrt {2\pi}}\right)^n\exp\left(-\frac 12\norm {O^{-1}y^{(n)}}\right)
			= \left(\frac 1{\sqrt {2\pi}}\right)^n\exp\left(-\frac 12\norm {y^{(n)}}\right).\]

		On a donc $f^{Y^{(n)}} = f^{Z^{(n)}}$, ce qui implique que les $Y_i$ sont iid $\Nzo$.

		En particulier, $Y_1 = (Y^{(n)})_1 = (OZ^{(n})_1 = \sum_{i=1}^nO_{1i}Z_i = \sum_{i=1}^n\frac {Z_i}{\sqrt n} = \sqrt n\overline Z \sim \Nzo$.
		On peut alors en déduire que $\overline Z \sim \mathcal N(0, n^{-1})$.

		Montrons alors que $ns_Z^2 \sim \chi^2_{n-1}$~:
		\[ns_Z^2 = \sum_{i=1}^n(Z_i - \overline Z)^2 = \sum_{i=1}^nZ_i^2 - n(\overline Z)^2 = \norm {Z^{(n)}}^2 - (\sqrt n\overline Z)^2 = \norm{Y^{(n)}} - Y_1^2
			= \sum_{i=2}^nY_i^2.\]

		Or, les $Y_i$ sont $\Nzo$. On a alors bien $ns_Z^2 \sim \chi^2_{n-1}$ (car la somme sur $i$ commence à $2$, il y a donc $(n-1)$ variables sommées).

		De plus, puisque les $Y_i$ sont indépendantes deux à deux, que $\overline Z$ ne dépend que de $Y_1$ et que $ns_Z^2$ ne dépend pas de $Y_1$, on sait que
		$\overline Z \sqcup ns_Z^2$.
		\end{proof}

\chapter{Estimation ponctuelle}
	\section{Introduction}
		Considérons toujours un modèle statistique $\statmod \R\Brl{\mathcal P}n$ avec $\theta$ le paramètre vectoriel $\in \Theta \subset \R^k$.

		\begin{déf} Soit $g : \Theta \to \R^km$. Une statistique est appelée \textit{estimateur de $g(\theta)$} lorsqu'elle est à valeurs dans $g(\Theta)$.
		\end{déf}

		\begin{déf} Soit $\theta \in \Theta \subset \R^k$. Si $g : \Theta \to \R^m : \theta \mapsto (\theta_{\varphi(1), \ldots, \theta_{\varphi(m)}})$, on
		appelle les paramètres $\theta_{\varphi(i)}$ les paramètres \textit{d'intérêt}, et on appelle les autres paramètres les paramètres \textit{de nuisance}.
		\end{déf}

		\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Nms$. On sait $\theta = (\mu, \sigma^2) \in \Theta = \R \times \Rp_0 \subset \R^2$.
		Soit $g : \Theta \to \R : \theta \mapsto \mu$. $\mu$ est le paramètre d'intérêt et $\sigma^2$ est le paramètre de nuisance.
		\end{ex}

		\begin{rmq} Ne pas connaître le paramètre de nuisance induit une \textit{nuisance} pour déterminer le paramètre d'intérêt.
		\end{rmq}

	\section{Critères d'estimation}
		\begin{rmq} Afin de définir les estimateurs convergents, il faut définir la notion de convergence, or il n'existe pas une manière canonique de la définir.
		Il existe donc plusieurs définitions de convergences différentes.
		\end{rmq}

		\subsection{Définitions de convergence}
			Soient $Z^{(n)} = (Z_1, \ldots, Z_n)$ définis pour $n \geq 1$ et sur $\probspace \Omega{\mathcal F}\P$.

			\begin{déf} On dit que $Z^{(n)}$ converge \textit{presque sûrement} (ou \textit{stochastiquement}) vers $Z$ lorsque~:
			\[\P\left[\left\{\omega \in \Omega \tq Z^{(n)} \xrightarrow[n \to \pinfty]{} Z(\omega)\right\}\right] = 1.\]
			Cela se note~:
			\[Z^{(n)} \cvgps Z.\]
			\end{déf}

			\begin{déf} On dit que $Z^{(n)}$ converge \textit{en probabilités} vers $Z$ lorsque~:
			\[\forall \varepsilon > 0 : \P\left[\abs {Z^{(n)} - Z} > \varepsilon\right] \xrightarrow[n \to \pinfty]{} 0.\]
			Cela se note~:
			\[Z^{(n)} \cvgp Z.\]
			\end{déf}

			\begin{déf} On dit que $Z^{(n)}$ converge \textit{en $L_r$} vers $Z$ lorsque~:
			\[\E\left[\abs {Z^{(n)} - Z}\right] \xrightarrow[n \to \pinfty]{} 0.\]
			Cela se note~:
			\[Z^{(n)} \cvgLr Z.\]
			\end{déf}

			\begin{rmq} Lorsque $r=2$, on parle de convergence en moyenne quadratique.
			\end{rmq}

			\begin{déf} On dit que $Z^{(n)}$ converge \textit{en loi} (ou \textit{en distribution}) vers $Z$ lorsque~:
			\[\forall z \text{ point de continuité de }F^Z : F^{Z^{(n)}}(z) \xrightarrow[n \to \pinfty]{} F^Z(z).\]
			Cela se note~:
			\[Z^{(n)} \cvgd Z.\]
			\end{déf}

			\begin{rmq} Ces définitions sont faites pour des variables aléatoires réelles mais peuvent être étendues à $\R^n$ en appliquant la convergence composante
			par composante.
			\end{rmq}

		\subsection{Résultats élémentaires sur les convergences}
			\begin{prp} Les convergences sont induites mutuellement par les assertions suivantes~:
			\begin{enumerate}
				\item si $Z^{(n)} \cvgps Z$, alors $Z^{(n)} \cvgp Z$~;
				\item si $Z^{(n)} \cvgp Z$, alors $Z^{(n)} \cvgd Z$~;
				\item si $Z^{(n)} \cvgLr Z$, alors $Z^{(n)} \cvgp Z$.
			\end{enumerate}
			\end{prp}

			\begin{thm} Les convergences presque sûre, en probabilités, et en loi sont stables par transformations continues.
			\end{thm}

			\begin{thm}\label{thm:propconvg} Notons $\to$ une convergence soit presque sûre, soit en probabilités. Si $Z^{(n)} \to Z$, et $Y^{(n)} \to Y$, alors~:
			\begin{itemize}
				\item[$(i)$]   $Z^{(n)} + Y^{(n)} \to Z+Y$~;
				\item[$(ii)$]  $Z^{(n)} \cdot Y^{(n)} \to Z \cdot Y$~;
				\item[$(iii)$] si $\P[Y^{(n)} = 0] = 0$, alors $\frac {Z^{(n)}}{Y^{(n)}} \to \frac ZY$.
			\end{itemize}
			\end{thm}

			\begin{lem}[Lemme de Slutzky] Si $Z^{(n)} \cvgd Z$, et $Y^{(n)} \cvgd c \neq 0$, alors~:
			\begin{itemize}
				\item[$(i)$]   $Z^{(n)} + Y^{(n)} \cvgd Z+c$~;
				\item[$(ii)$]  $Z^{(n)} \cdot Y^{(n)} \cvgd Z \cdot c$~;
				\item[$(iii)$] $\frac {Z^{(n)}}{Y^{(n)}} \cvgd \frac Zc$.
			\end{itemize}
			\end{lem}

			\begin{thm}[Loi forte des grands nombres] Soient $Z_1, Z_2, \ldots$ iid avec $\E\left[\abs {Z_1}\right] < \pinfty$. Alors~:
			\[\overline Z^{(n)} = \frac 1n\sum_{k=1}^nZ_k \cvgps \mu = \E[Z_1].\]
			\end{thm}

			\begin{thm}[Loi faible des grands nombres] Soient $Z_1, Z_2, \ldots$ iid avec $\E\left[\abs {Z_1}\right] < \pinfty$. Alors~:
			\[\overline Z^{(n)} = \frac 1n\sum_{k=1}^nZ_k \cvgp \mu = \E[Z_1].\]
			\end{thm}

			\begin{thm}[Théorème central limite (TCL)] Soient $Z_1, Z_2, \ldots$ iid, avec $\E[Z_1^2] < \pinfty$. Alors~:
			\[\sqrt n\left(Z^{(n)} - \mu\right) \cvgd W,\]
			où~:
			\[W \sim \mathcal N(0, \sigma^2),\]
			avec $\sigma^2 = \Var(Z_1)$.
			\end{thm}

		\subsection{Estimateurs convergents}
			\begin{déf} Un estimateur $T^{(n)}(X^{(n)})$ de $g(\theta)$ est dit \textit{faiblement convergent} lorsque~:
			\[\forall \theta \in \Theta : T^{(n)}(X^{(n)}) \cvgp g(\theta) \qquad \text{ sur }\P_\theta^{(n)}.\]
			\end{déf}

			\begin{déf} Un estimateur $T^{(n)}(X^{(n)})$ de $g(\theta)$ est dit \textit{fortement convergent} lorsque~:
			\[\forall \theta \in \Theta : T^{(n)}(X^{(n)}) \cvgps g(\theta)\qquad \text{ sur }\P_\theta^{(n)}.\]
			\end{déf}

			\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Nms$. Prenons $g(\theta) = \mu$ et $T^{(n)}(X^{(n)}) = \overline X$. On a bien~:
			\[T^{(n)}(X^{(n)}) = \overline X^{(n)} \cvgps \mu = \E[X_1] \qquad \text{ sur }\P_{\mu, \sigma^2}^{(n)}.\]
			$T^{(n)}(X^{(n)})$ est donc un estimateur fortement convergent.

			Prenons maintenant $T_2^{(n)}(X^{(n)}) = \sigma^2$. On ne peut pas appliquer la loi des grands nombres car les variables aléatoires $(X_i - \overline X)^2$
			sommées ne sont pas indépendantes. On a alors~:
			\begin{align*}
				T_2^{(n)}(X^{(n)}) &= s^2 = \frac 1n\sum_{i=1}^n(X_i-\overline X)^2
					= \frac 1n\sum_{i=1}^n\left((X_i - \mu)^2 + (\mu - \overline X)^2 - 2(X_i - \overline X)(\overline X - X_i)\right) \\
				&= \frac 1n\sum_{i=1}^n(X_i - \mu)^2 - (\overline X - \mu)^2,
			\end{align*}
			où $\frac 1n\sum_{i=1}^n(X_i-\mu)^2 \cvgps \E\left[(X_i - \mu)^2\right] = \sigma^2$, par la loi forte des grands nombres,
			et $(\overline X - \mu) \cvgps 0$. Donc, par le théorème~\ref{thm:propconvg}, on a $T_2^{(n)}(X^{(n)}) \cvgps \sigma^2$
			\end{ex}

			\begin{rmq} On a également~:
			\[\frac 1{n-1}\sum_{i=1}^n(X_i-\overline X)^2 = \frac n{n-1}s^2 \cvgps 1 \cdot \sigma^2.\]
			\end{rmq}

			\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Unif(0, \theta)$, avec $\theta \in \Theta \subset \R$. On veut estimer $g(\theta) = \theta$. L'estimateur
			$T^{(n)}(X^{(n)}) = \overline X$ n'est pas un estimateur convergent car~:
			\[\overline X \cvgps \E_\theta(X_1) = \frac \theta2 \neq \theta \qquad \text{ sur }\P_\theta^{(n)}.\]
			Par contre, si on prend $T_2^{(n)}(X^{(n)}) = 2\overline X$, on a~:
			\[2\overline X \cvgps 2\E_\theta(X_1) = 2\frac \theta2 = \theta \qquad \text{ sur }\P_\theta^{(n)}.\]

			Si on prend $T_3^{(n)}(X^{(n)}) = X_{(n)}$, à savoir l'observation maximale, on a~:
			\[F_\theta^{X_{(n)}}(x) = \begin{cases}0 &\text{ si }x < 0 \\\frac {x^n}{\theta^n} &\text{ si } 0 \leq x \leq \theta \\1 &\text{ sinon}\end{cases}.\]
			Posons donc $\varepsilon > 0$. On calcule~:
			\begin{align*}
				\P_\theta^{(n)}\left[\abs {X_{(n)} - \theta} > \varepsilon\right] &= \P_\theta^{(n)}\left[X_{(n)} \leq \theta - \varepsilon\right]
					+ \P_\theta^{(n)}\left[X_{(n)} \geq \theta + \varepsilon\right] = \P_\theta^{(n)}\left[X_{(n)} \leq \theta - \varepsilon\right]
					= F_\theta^{X_{(n)}}(\theta - \varepsilon) \\
				&=\begin{cases}0 &\text{ si }\varepsilon \geq \theta \\\left(\frac {\theta-\varepsilon}{\theta}\right)^n &\text{ si } 0 < \varepsilon < \theta\end{cases}
					\longrightarrow 0
			\end{align*}
			on a alors convergence en probabilité de $X_{(n)}$ vers $\theta$. On en déduit que $T_3^{(n)}(X^{(n)})$ est un estimateur faiblement convergent.
			\end{ex}

			\begin{rmq} L'estimateur $\frac {n+1}nX_{(n)}$ est également faiblement convergent.
			\end{rmq}

			\begin{rmq} Il n'est pas toujours possible de s'en sortir en invoquant le TCL ou la loi des grands nombres pour déterminer la convergence d'un estimateur.
			Prenons par exemple $X_1, \ldots, X_n$ iid de densité~:
			\[f^X(x) = \frac 1{\pi(1 + (x-\theta)^2)}.\]
			On a effectivement $\E[X_1] = \pinfty$. En réalité~:
			\[\lnot\left(\overline X^{(n)} \cvgp \theta \qquad \text{ sur } P_\theta^{(n)}\right),\]
			mais bien~:
			\[\overline X^{(n)} \cvgd X_1\]
			\end{rmq}

	\section{Estimateur exhaustif}
		\begin{déf} Soit $T^{(n)}(X^{(n)})$, une statistique. On la dit \textit{exhaustive} lorsque~:
		\[\forall B \in \Brl(\R^n) : \forall t \in T^{(n)}(\R^n) : \P_\theta^{(n)}\left[X^{(n)} \in B | T^{(n)}(X^{(n)}) = t\right] \text{ ne dépend pas de } \theta.\]
		\end{déf}

		\begin{rmq} Puisque l'on travaille sur des modèles paramétriques $\statmod \R\Brl{\mathcal P}n$, il existe toujours un $B \in \Brl(\R^n)$ tel que
		$\P_\theta[X^{(n)} \in B]$ dépende de $\theta$.
		\end{rmq}

		\begin{rmq} On peut en comprendre qu'une statistique est exhaustive si la valeur prise par $T^{(n)}(X^{(n)})$ donne toutes les informations contenues par
		$X^{(n)}$ sur $\theta$.
		\end{rmq}

		\begin{ex} La statistique identité $x^{(n)} \mapsto x^{(n)}$ est une statistique exhaustive car~:
		\[\P\left[X^{(n)} \in B | X^{(n)} = x^{(n)}\right] =  \begin{cases}1 &\text{ si } x^{(n)} \in B \\0 &\text{ sinon}\end{cases}.\]
		\end{ex}

		\begin{ex} Prenons $X_1, \ldots, X_n$ iid $\Bern(p)$ avec la statistique $T^{(n)}(X^{(n)}) = \sum_{i=1}^nX_i$. Pour évaluer la probabilité~:
		\[\P_p^{(n)}\left[X^{(n)} \in \{x^{(n)}\} | \sum_{i=1}^nX_i = t\right],\]
		on est en présence d'une binomiale. Dès lors, si $\sum_{i=1}^nX_i \neq t$, alors la probabilité est nulle. Sinon, la probabilité est $\frac 1{\binom nt}$ car
		il y a $\binom nt$ moyens d'avoir $n$ observations dont $t$ valant 1 et $n-t$ valant 0. Ces probabilités ne dépendent donc pas de $\theta$, la statistique
		$T^{(n)}(X^{(n)}) = \sum_{i=1}^nX_i$ est donc une statistique exhaustive.
		\end{ex}

		\begin{rmq} Si $T^{(n)}(X^{(n)})$ est une statistique bijective, alors elle est exhaustive. Cependant, les estimateurs intéressants sont ceux qui «~réduisent~»
		l'information de manière à ce qu'elles soient plus facilement analysables.
		\end{rmq}

		\begin{déf} Soit $X^{(n)} = (X_1, \ldots, X_n)$. On appelle la \textit{fonction de vraissemblance de $X^{(n)}$} la fonction~:
		\[L_\theta^{(n)} : \R^n \to \R : x^{(n)} \mapsto \begin{cases}\P[X^{(n)}
			= x^{(n)}] &\text{ si $X^{(n)}$ est de loi discrète} \\f_\theta^{X^{(n)}}(x^{(n)} &\text{ sinon}\end{cases}.\]
		\end{déf}

		\begin{rmq} Dans le cas de variables $X_1, \ldots, X_n$ iid, la fonction de vraissemblance correspond toujours à un produit~:
		\[L_\theta^{(n)}(X^{(n)}) = \P[X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n] \overset \sqcup= \prod_{i=1}^n\P[X_i = x_i].\]
		\end{rmq}

		\begin{thm}[Critère de factorisation de Neymann-Fisher] Dans un modèle paramétrique $\statmod \R\Brl{\mathcal P}n$, une statistique $T^{(n)}(X^{(n)})$ est
		exhaustive \textbf{si et seulement si} pour tout $\theta \in \Theta$, la fonction de vraissemblance $L_\theta^{(n)}(X^{(n)})$ est factorisable sous la forme~:
		\[L_\theta^{(n)}(X^{(n)}) \left(= g_\theta \circ T^{(n)}\right) h(x^{(n)}),\]
		et ce $\P_\theta^{(n)}$-sûrement.
		\end{thm}

		\begin{rmq} Dans cette factorisation, la fonction $h$ ne peut dépendre de $\theta$, et seule la fonction $g_\theta$ peut en dépendre, mais uniquement par
		l'intermédiaire de $T^{(n)}$.
		\end{rmq}

		\begin{ex} En reprenant l'exemple d'au-dessus~: $X_1, \ldots, X_n$ iid $\Bern(p)$ et $T^{(n)}(X^{(n)}) = \sum_{i=1}^nX_i$, on a~:
		\[L_\theta^{(n)}(x^{(n)}) = \prod_{i=1}^np^{x_i}(1-p)^{1-x_i} = p^{\sum_{i=1}^nx_i}(1-p)^{n - \sum_{i=1}^nx_i}.\]
		Dès lors, en posant $g_\theta(x) = p^x(1-p)^{n-x}$ et $h(x^{(n)}) = 1$, on a bien une factorisation de Neymann-Fisher, ce qui implique que la statistique
		est exhaustive.
		\end{ex}

		\begin{rmq} Pour chaque statistique exhaustive, il en existe une infinité définies à bijection près. En effet, si $T(X^{(n)})$ est une statistique exhaustive
		et si $H$ est une fonction bijective quelconque, alors~:
		\[L_\theta^{(n)}(x^{(n)}) = g_\theta(T(x^{(n)})h(x^{(n)}) = (g_\theta \circ H^{-1} \circ H \circ T)(x^{(n)})h(x^{(n)}).\]
		La fonction $H \circ T$ est donc également une statistique exhaustive.
		\end{rmq}

		\begin{rmq} Le critère précédent peut également donner une manière de \textit{deviner} des statistiques exhaustives. Prenons par exemple
		$X^{(n)} = (X_1, \ldots, X_n)$ iid $\Nms$. Si $\theta = (\mu, \sigma^2) \in \Theta \subset \R^2$, on peut écrire~:
		\begin{align*}
			L_\theta^{(n)}(x^{(n)}) &= \prod_{i=1}^nf^{X_i}_\theta(x_i) = \prod_{i=1}^n\frac 1{\sqrt {2\pi}\sigma}\exp\left(-\frac {(x-\mu)^2}{2\sigma^2}\right)
				= \left(\frac 1{2\pi\sigma^2}\right)^{\frac n2}\exp\left(-\sum_{i=1}^n\frac {(x_i-\mu)^2}{2\sigma^2}\right) \\
			&= \left(\frac 1{2\pi\sigma^2}\right)^{\frac n2}\exp\left(-\frac 2{2\sigma^2}\sum_{i=1}^nx_i^2 - \frac {n\mu}{2\sigma^2} + \frac \mu{\sigma^2}\sum_{i=1}^nx_i\right).
		\end{align*}

		Dès lors, en prenant $T(X^{(n)}) = \left(\sum_{i=1}^nx_i, \sum_{i=1}^nx_i^2\right)$, on a bien une statistique exhaustive. Alors, de même, on peut dire que
		$\left(\frac 1n\sum_{i=1}^nx_i, \frac 1n\sum_{i=1}^nx_i^2\right)$ est un estimateur exhaustif (composition avec une bijection). On peut également dire que
		$\left(\frac 1n\sum_{i=1}^nx_i, \frac 1n\sum_{i=1}^nx_i^2 - \left(\frac 1n\sum_{i=1}^nx_i\right)^2\right)$ est un estimateur exhaustif (même argument). Or
		ce dernier vecteur correspond à $(\overline X, s^2)$.

		En prenant cette fois $X^{(n)} = (X_1, \ldots, X_n)$ iid $\Unif(0, \theta)$, on peut à nouveau construire des statistiques exhaustives~:
		\[L_\theta^{(n)}(x^{(n)}) = \prod_{i=1}^nf^{X_i}_\theta(x_i) = \prod_{i=1}^n\frac 1\theta\charfun{0 \leq x_i \leq \theta}
			= \frac 1{\theta^n}\charfun{0 \leq x_1, \ldots, x_n \leq \theta}.\]
		Cela garantit bien que $x^{(n)}$ est une statistique exhaustive. Mais de plus, par commutativité du produit~:
		\[L_\theta^{(n)}(x^{(n)}) = \prod_{i=1}^nf^{X_{(i)}}_\theta(x_i) = \frac 1{\theta^n}\charfun{0 \leq x_{(1)}, \ldots, x_{(n)} \leq \theta)}.\]
		On a donc que la statistique d'ordre est une statistique exhaustive. Or, la condition $0 \leq x_{(1)}, \ldots, x_{(n)} \leq \theta$ revient à la condition
		$0 \leq x_{(1)}, x_{(n)} \leq \theta$. La statistique $(x_{(1)}, x_{(n)})$ est donc également une statistique exhaustive. Pour aller plus loin, décomposant
		la fonction caractéristique $\charfun {0 \leq x_{(1)}, x_{(n)} \leq \theta)}$ en $\charfun {0 \leq x_{(1)}}\charfun{x_{(n)} \leq \theta}$, on peut poser
		$h(x^{(n)}) = \charfun{0 \leq x_{(1)}}$, ce qui amène à une nouvelle statistique exhaustive~: $x_{(n)}$.

		À chaque étape du raisonnement, la statistique exhaustive contient \textit{de moins en moins d'information} générale mais conserve l'information sur $\theta$
		qui est donc en quelque sorte contenue dans $x_{(n)}$.
		\end{rmq}

		\subsection{Estimateurs non biaisés}
			En se situant toujours dans un modèle statistique $\statmod \R\Brl{\mathcal P}n$, on veut estimer $g(\theta)$, avec $g : \Theta \to \R^m$.

			\begin{déf} Un estimateur $T^{(n)}(X^{(n)})$ de $g(\theta)$ est dit \textit{non biaisé} lorsque~:
			\[\forall \theta \in \Theta : \E_\theta[T^{(n)}(X^{(n)})] = g(\theta).\]
			\end{déf}

			\begin{déf} Un estimateur $T^{(n)}(X^{(n)})$ de $g(\theta)$ est dit \textit{asymptotiquement non biaisé} lorsque~:
			\[\forall \theta \in \Theta : \E_\theta[T^{(n)}(X^{(n)})] \xrightarrow[n \to \pinfty]{} g(\theta).\]
			\end{déf}

			\begin{déf} Le \textit{biais} d'un estimateur $T^{(n)}(X^ {(n)})$ est la quantité~:
			\[b^{(n)}_\theta = b^{(n)}(\theta) = \E_\theta[T^{(n)}(X^{(n)})] - g(\theta).\]
			\end{déf}

			\begin{rmq} On remarque donc qu'un estimateur non biaisé a un biais de 0 et qu'un estimateur asymptotiquement non biaisé a un biais qui tend vers 0 pour
			$n$ tendant vers $\pinfty$.
			\end{rmq}

			\begin{ex} $X_1, \ldots, X_n$ iid $\Unif(0, \theta)$, avec $\theta \in \Theta = \Rp_0 \subset \R$.
			\begin{itemize}
				\item Si $T^{(n)}(X^{(n)}) = 2\overline X$, on a~:
				\[\E_\theta^{(n)}\left[2\overline X\right] = \frac 2n\E_\theta^{(n)}\left[\sum_{i=1}^nX_i\right] = \frac 2n\sum_{i=2}^n\E_\theta[X_i]
					= \frac 2n\frac {n\theta}2 = \theta.\]
				\item Si $T^{(n)}(X^{(n)}) = X_{(n)}$, on a~:
				\[\E_\theta^{(n)}[X_{(n)}] = \int_\R xf_\theta^{X_{(n)}}(x)\dif x = \int_0^\theta x\frac {nx^{n-1}}{\theta^n}\dif x
					= \frac n{\theta^n}\int_0^\theta x^n\dif x = \frac n{\theta^n}\left[\frac {x^{n+1}}{n+1}\right]_0^\theta = \frac n{n+1}\theta.\]
				On en déduit que $T^{(n)}(X^{(n)})$ est biaisé car il \textit{vise en moyenne trop à gauche} et a un biais de $-\frac \theta{n+1}$. Il est cependant
				asymptotiquement non biaisé car $\frac n{n+1} \xrightarrow[n \to \pinfty]{} 1$.
			\end{itemize}
			\end{ex}

			\begin{rmq} Si le biais d'un estimateur est négatif, alors c'est que l'estimateur sous-estime en moyenne, alors que si le biais est positif, c'est que
			l'estimateur surestime.
			\end{rmq}

			\begin{rmq} On peut tout de même dire que l'estimateur $\frac {n+1}nX_{(n)}$ est non biaisé car~:
			\[\E_\theta^{(n)}\left[\frac {n+1}nX_{(n)}\right] = \frac {n+1}n\E_\theta^{(n)}[X_{(n)}] = \frac {n+1}n\frac n{n+1}\theta = \theta.\]
			\end{rmq}

			\begin{ex} Soient $X_1, \ldots, X_n$ iid $\Nms$, avec $\theta = (\mu, \sigma^2)$.
			\begin{itemize}
				\item pour tout $\theta \in \Theta$, on a~: $\E_\theta[\overline X] = \frac 1n\sum_{i=1}^n\E(X_i) = \frac {n\mu}n = \mu$. $\overline X$ est donc un
				estimateur sans biais de $\mu$~;
				\item $\E_\theta[s^2] = \E_\theta[X_1^2] - \E_\theta[\overline X^2]$. On remarque que pour toute variable aléatoire $Z$, on a~:
				\[\Var(Z) = \E[Z^2] - \E[Z]^2 \qquad \iff \qquad \E[Z^2] = \Var(Z) + \E[Z]^2.\]
				On peut donc remplacer dans la formule de l'espérance de $s^2$, et on obtient~:
				\[\E_\theta[s^2] = \Var_\theta(X_1) + \E(X_1)^2 - \Var_\theta(\overline X) - \E(\overline X)^2.\]
				Or on sait $\E_\theta(X_1) = \E_\theta(\overline X) = \mu$. On a donc~:
				\[\E_\theta[s^2] = \Var_\theta(X_1) - \Var_\theta(\overline X) = \sigma^2 - \frac 1{n^2}\Var_\theta\left(\sum_{i=1}^nX_i\right)
					= \sigma^2 - \frac 1{n^2}\sum_{i=1}^n\Var_\theta(X_i) = \sigma^2 - \frac {n\sigma^2}{n^2} = \frac {n-1}n\sigma^2.\]

				On en conclut que l'estimateur $s^2$ est biaisé pour $\sigma^2$ et de biais $\frac {-\sigma^2}n \xrightarrow[n \to \pinfty]{} 0$.

				Notons alors $S^2 \coloneqq \frac {n}{n-1}s^2$. On a que $\E_\theta[S^2] = \sigma^2$, et donc $S^2$ est un estimateur sans biais.
			\end{itemize}
			\end{ex}

			\begin{rmq} Le non biais est une propriété fragile. Soient $X_1, \ldots, X_n$ iid $\Nms$ où l'on veut estimer $g(\theta) = \sigma$ (et pas $\sigma^2$).

			Que vaut $\E_\theta[S]$~? On sait $\Var_\theta(S) = \E_\theta[S^2] - \E_\theta[S]^2 = \sigma^2 - \E_\theta[S]^2$. Or $\Var_\theta(S) \gneqq 0$, et donc
			$\sigma^2 \gneqq \E_\theta[S]^2$. On en déduit $\E_\theta[S] \lneqq \sigma$.
			\end{rmq}

			\begin{rmq} De plus, il n'existe pas toujours d'estimateur sans biais. Soit $X_1 \sim \Bern(p)$. On veut estimer $g(p) = p^2 \in [0, 1]$. L'estimateur
			$T(X^{(n)})$ est entièrement déterminé par $T(0)$ et $T(1)$. Imposons donc pour tout $p \in [0, 1] : \E_\theta[T(X_1)] = p^2$. On a donc~:
			\[p^2 = \E_\theta[T(X_1)] = T(0)\P[X_1 = 0] + T(1)\P[X_1 = 1] = T(0)(1-p) + T(1)p.\]
			En réarrangeant cette équation du second degré, on obtient~:
			\[\forall p \in [0, 1] : p^2 + (T(0) - T(1))p - T(0) = 0.\]
			Or une telle équation ne peut avoir que 2 racines tout au plus, et ici, une infinité non-dénombrable est requise. Il n'existe donc pas de telle fonction $T$,
			et donc par extension, il n'existe pas d'estimateur sans biais de $p^2$.
			\end{rmq}

			\begin{ex} Prenons $X_1, \ldots, X_n$ iid $\mathcal N(\mu, 1)$, $g(\theta) = g(\mu) = \mu$. Prenons $\TnXn = \overline \Xn  + Y$, où $Y = \pm 10^9$, avec
			probabilité $\frac 12$ pour chaque. On a alors~:
			\[\E_\mu[\TnXn] = \E_\mu[\overline \Xn] + \E_\mu[Y] = \mu + 0 = \mu.\]
			L'estimateur $\TnXn$ est donc sans biais, mais est très mauvais~: sur-/sous-estime toujours à $\simeq 10^9$ près.
			\end{ex}

		\subsection{Estimateurs à dispertion minimale}
			\begin{déf} L'erreur quadratique moyenne d'un estimateur $\TnXn$ de $g(\theta)$ ($\in \R$) est la quantité~:
			\[\MSE_\theta[\TnXn] \coloneqq \E_\theta\left[\abs {\TnXn - g(\theta)}^2\right].\]
			\end{déf}

			\begin{déf} On appelle l'erreur absolue moyenne la quantité~:
			\[MAE_\theta[\TnXn] = \E_\theta\left[\abs {\TnXn - g(\theta)}\right]\]
			\end{déf}

			\begin{rmq}
			\begin{align*}
				\MSE_\theta[\TnXn] &= \E_\theta\left[\abs {\TnXn - g(\theta)}^2\right]
					= \Var_\theta\left[\TnXn - g(\theta)\right] - \E_\theta\left[\TnXn - g(\theta)\right]^2 \\
				&= \Var_\theta[\TnXn] - b_\theta^{(n)}(\TnXn)^2.
			\end{align*}

			Puisque $\Var_\theta(\TnXn) \geq 0$ et $b_\theta^{(n)}(\TnXn) \geq 0$, pour avoir $\MSE[\TnXn] \xrightarrow[n \to \pinfty]{} 0$, il faut~:
			\[\begin{cases}&\Var_\theta(\TnXn) \xrightarrow[n \to \pinfty]{} 0, \\&b_\theta^{(n)}(\TnXn) \xrightarrow[n \to \pinfty]{} 0.\end{cases}\]

			Cela dit que sous $\P_\theta^{(n)}$, on a $\TnXn \xrightarrow{L^2} g(\theta)$ (et également $\TnXn \cvgp g(\theta)$).

			On peut également dire que si $\MSE_\theta[\TnXn] \to 0$, alors $\TnXn$ est un estimateur faiblement convergent.
			\end{rmq}

			\begin{rmq} Le MSE sert d'«~arbitrage~» entre la variance et le biais. Cet arbitrage est bien souvent nécessaire car il arrive fréquemment que baisser
			le biais (respectivement la variance) augmente la variance (respectivement le biais).
			\end{rmq}

			\begin{ex} $X_1, \ldots, X_n$ iid $\Unif(0, \theta)$. Prenons $\theta \in \Theta = \Rp_0 \subset \R$. Prenons $T_1^{(n)}(\Xn) = X_{(n)}$. On sait que~:
			\[\E_\theta[T_1^{(n)}(\Xn)] = \frac n{n+1}\theta,\]
			ce qui nous permet de calculer la variance~:
			\[\Var_\theta(X_{(n)}) = \E_\theta(X_{(n)}^2) - \E_\theta(X_{(n)})^2,\]
			où~:
			\[\E_\theta(X_{(n)}^2) = \int_\R x^2f^{X_{(n)}}(x)\dif x = \int_0^\theta x^2\frac {nx^{n-1}}{\theta^2}\dif x = \frac n{\theta^n}\int_0^\theta x^{n+1}\dif x
				= \frac n{\theta^n}\left[\frac {x^{n+2}}{n+2}\right]_0^\theta = \frac {n\theta^2}{n+2}.\]
			On trouve finalement la variance donnée par~:
			\[\frac {n\theta^2}{n+2} - \frac {n^2\theta^2}{(n+1)^2} = n\theta^2\left(\frac {(n+1)^2 - n(n+2)}{(n+2)(n+1)^2}\right) = \frac {n\theta^2}{(n+2)(n+1)^2}.\]

			On trouve finalement une erreur quadratique moyenne de~:
			\[\MSE_\theta[T_1^{(n)}(\Xn)] = \frac {2\theta^2}{(n+1)(n+2)} \xrightarrow[n \to \pinfty]{} 0.\]

			En prenant $T_2^{(n)}(\Xn) = \frac {n+1}nX_{(n)}$, on annule le biais, mais on augmente la variance~:
			\[\Var_\theta[T_2^{(n)}(\Xn)] = \left(\frac {n+1}n\right)^2\Var_\theta[X_{(n)}] = \frac {\theta^2}{n(n+2)}.\]

			On trouve alors~:
			\[\forall \theta \in \Theta : \MSE_\theta[T_2^{(n)}(\Xn)] = \frac {\theta^2}{n(n+2)} \leq \MSE_\theta[T_1^{(n)}(\Xn)].\]

			Dans le cas présent, annuler le biais donne une erreur quadratique moyenne plus faible. Donc ici, la variance a «~moins augmenté que le biais n'a diminué~».
			\end{ex}

			\begin{ex} Prenons maintenant $X_1, \ldots, X_n$ iid $\Nms$ et $g(\theta) = \sigma^2$.

			Pour $T_1^{(n)}(\Xn) = \frac 1n\sum_{i=1}^n(X_i - \overline X)^2$, on trouve~:
			\[\MSE_\theta[T_1^{(n)}(\Xn)] = \frac {2(n-1)\sigma^4}{n^2}.\]

			À nouveau, il existe un estimateur sans-biais~:
			\[T_2^{(n)}(\Xn) = \frac 1{n-1}\sum_{i=1}^n(X_i - \overline X)^2 = \frac {n-1}nT_1^{(n)}(\Xn).\]
			Dans ce cas, on trouve~:
			\[\forall \theta \in \Theta : \MSE_\theta[T_2^{(n)}(\Xn)] = \frac {2\sigma^4}{n-1} \geq \MSE_\theta[T_1^{(n)}(\Xn)].\]
			\end{ex}

			\begin{rmq} Dans les deux exemples ci-dessus, l'inégalité est stricte pour $n \geq 2$.

			De plus, l'inégalité tient pour tout $\theta \in \Theta$. Cela permet de définir qu'un estimateur est meilleur que l'autre. Avoir deux estimateurs tels
			que l'erreur quadratique moyenne de l'un est meilleure que l'autre pour certaines valeurs de $g(\theta)$, et inversement pour d'autres ne permet pas de dire
			que l'un est meilleur que l'autre car la véritable valeur de $g(\theta)$ n'est pas connue. On ne peut donc pas savoir quel estimateur choisir.
			\end{rmq}

			\begin{déf} Soit $\mathcal C$, une classe d'estimateurs de $g(\theta)$. On dit que $T_*^{(n)}$ est à erreur quadratique moyenne minimale dans $\mathcal C$
			lorsque~:
			\begin{itemize}
				\item $T_*^{(n)} \in \mathcal C$~;
				\item $\forall \Tn \in \mathcal C : \forall \theta \in \Theta : \MSE_\theta[T_*^{(n)}] \leq \MSE_\theta[\Tn]$.
			\end{itemize}
			\end{déf}

			\begin{rmq} On peut prendre $\mathcal C = \{\Tn \tq \E_\theta[\Tn^2] < \pinfty\}$, mais on n'en prend qu'un sous-ensemble strict bien souvent car dans un tel
			$\mathcal C$, on peut prendre $T_{\theta_0}^{(n)}$, pour un certain $\theta_0 \in \Theta$ défini par $T_{\theta_0}^{(n)}(\Xn) = g(\theta_0)$.
			On trouve donc~:
			\[\MSE_\theta[T_{\theta_0}^{(n)}(\Xn)] = \E_\theta[(g(\theta_0) ) g(\theta))^2] = (g(\theta_0) - g(\theta))^2.\]
			La fonction d'erreur quadratique moyenne est donc une parabole qui s'annule en $\theta_0 = \theta$. Pour avoir un tel $T_*^{(n)}$, il faut que
			$\MSE_{\theta_0}[T_*^{(n)}] \leq \MSE_{\theta_0}[T_{\theta_0}^{(n)}] = 0$. Et ce, pour tout $\theta_0 \in \Theta$. Il faut donc avoir~:
			\[\forall \theta \in \Theta : T_*^{(n)} = g(\theta)\qquad\text{ sous }\P_\theta^{(n)}.\]

			Pour résoudre ce problème, il faut donc réduire $\mathcal C$ afin de ne plus avoir de tels estimateurs \textit{pathologiques}.
			\end{rmq}

			\begin{déf} Soit $Z = (Z_1, \ldots, Z_m)^T$, un vecteur aléatoire (vecteur de variables aléatoires). On définit~:
			\[\E_\theta(Z) \coloneqq (\E_\theta[Z_1], \ldots, \E_\theta[Z_m])^T.\]

			On définit également~:
			\[\Var_\theta[Z] = \E\left[(Z - \E_\theta[Z])(Z - \E_\theta[Z])^T\right] \in \R^{m \times m}.\]
			\end{déf}

			\begin{rmq} On observe que~:
			\[\Var_\theta[Z]_{ij} = \E_\theta\left[(Z - \E_\theta[Z])_i(Z - \E_\theta[Z])_j^T\right] = \E_\theta\left[(Z_i - \E_\theta[Z_i])(Z_j - \E_\theta[Z_j])\right]
				= \Cov[Z_i, Z_j].\]
			Cela veut dire~:
			\[\Var[Z] =
				\begin{pmatrix}
					\Var[Z_1]      & \Cov[Z_1, Z_2] & \ldots & \Cov[Z_1, Z_n] \\
					\Cov[Z_2, Z_1] & \Var[Z_2]      & \ddots & \Cov[Z_2, Z_n] \\
					\vdots         & \ddots         & \ddots & \vdots         \\
					\Cov[Z_n, Z_1] & \ldots         & \ldots & \Var[Z_n]
				\end{pmatrix}
			\]

			On appelle cette matrice la \textit{matrice de variance/covariance} (pour des raisons évidentes). On voit donc que cette matrice est symétrique car
			$\Cov[Z_i, Z_j] = \Cov[Z_j, Z_i]$.
			\end{rmq}

			\begin{déf} Soit $A$ une matrice. On dit que $A \geq 0$ si la forme bilinéaire associée est semi-définie positive.
			\end{déf}

			\begin{prp} Soit $Z$ un vecteur aléatoire, et soient $A \in \R^{k \times m}, b \in \R^k$. Alors~:
			\begin{itemize}
				\item $\E_\theta[AZ + b] = A\E_\theta[Z] + b$~;
				\item $\Var_\theta[AZ + b] = A\Var_\theta[AZ + b]A^T$~;
				\item $\Var_\theta[Z] \geq 0$.
			\end{itemize}
			\end{prp}

			\begin{proof}~
			\begin{itemize}
				\item On observe que $\E_\theta[AZ + b] = \left(\E_\theta[(AZ)_i + b_i]\right)_i$,
				où~:
				\begin{align*}
					\E_\theta[(AZ)_i + b_i] &= \E_\theta[(AZ)_i] + b_i = \E_\theta\left[\sum_{j=1}^kA_{ij}Z_j\right] + b_i = \sum_{j=1}^k\E_\theta[A_{ij}Z_j] + b_i \\
					&= \sum_{j=1}^kA_{ij}\E_\theta[Z_j] + b_i = \sum_{j=1}^kA_{ij}(\E_\theta[Z])_j + b_i= (A\E_\theta[Z] + b)_i.
				\end{align*}
				\item On remarque premièrement que $\Var_\theta(AZ+b) = \E_\theta\left[(AZ+b-A\E_\theta(Z)-b)(AZ+b-A\E_\theta(Z)-b)^T\right]
					= \E_\theta\left[(AZ-A\E_\theta(Z))(AZ-A\E_\theta(Z))^T\right] = \Var_\theta[AZ]$.
				De là, on trouve~:
				\begin{align*}
					\Var_\theta[AZ]_{ij} &= \E_\theta\left[(AZ-A\E_\theta(Z))_i(AZ-A\E_\theta(Z))_j\right]
						= \E_\theta\left[((AZ)_i-\E_\theta((AZ)_i))((AZ)_j-\E_\theta((AZ)_j))\right] \\
					&= \E_\theta[(AZ)_i(AZ)_j] - \E_\theta[(AZ)_i]\E_\theta[(AZ)_j] \\
					&= \E_\theta\left[\left(\sum_{\delta=1}^mA_{i\delta}Z_\delta\right)\left(\sum_{\gamma=1}^mA_{j\gamma}Z_\gamma\right)\right]
						- \E_\theta\left[\sum_{\delta=1}^mA_{i\delta}Z_\delta\right]\E_\theta\left[\sum_{\gamma=1}^mA_{j\gamma}Z_\gamma\right] \\
					&= \sum_{\delta=1}^m\sum_{\gamma=1}^mA_{i\delta}A_{j\gamma}\E_\theta[Z_\delta Z_\gamma]
						- \sum_{\delta=1}^m\sum_{\gamma=1}^mA_{i\delta}A_{j\gamma}\E_\theta[Z_\delta]\E_\theta[Z_\gamma] \\
					&= \sum_{\delta=1}^m\sum_{\gamma=1}^mA_{i\delta}A_{j\gamma}\left(\E_\theta[Z_\delta Z_\gamma] - \E_\theta[Z_\delta]\E_\theta[Z_\gamma]\right) \\
					&= \sum_{\delta=1}^m\sum_{\gamma=1}^mA_{i\delta}A_{j\gamma}\Var[Z]_{\delta \gamma} \\
					&= \sum_{\delta=1}^m\sum_{\gamma=1}^mA_{i\delta}\Var[Z]_{\delta \gamma}(A^T)_{\gamma j} \\
					&= \left(A\Var[Z]A^T\right)_{ij}
				\end{align*}
				\item soit $v \in \R^m$. On remarque que $v^T\Var[Z]v = \Var[v^TZ] \geq 0$.
			\end{itemize}
			\end{proof}

			\begin{déf} Soient $Z = (Z_1, \ldots, Z_m)^T$, et $Y = (Y_1, \ldots, Y_\ell)^T$. On définit la covariance de $Y$ et $Z$ par~:
			\[\Cov[Y, Z] \coloneqq \E_\theta\left[(Y-\E_\theta[Y])(Z-\E_\theta[Z])\right].\]
			\end{déf}

			\begin{prp} Soient $Z = (Z_1, \ldots, Z_m)^T$, et $Y = (Y_1, \ldots, Y_\ell)^T$. Alors~:
			\begin{itemize}
				\item $\Cov[Y, Z]_{ij} = \Cov[Y_i, Z_j]$~;
				\item $\Cov[Z, Y] = \Cov[Y, Z]$~;
				\item $\Cov[Z, Z] = \Var[Z]$~;
				\item $\Cov[AY+b, CZ+d] = A\Cov[Y, Z]C^T$
			\end{itemize}
			\end{prp}

			\begin{déf} Soient $g(\theta) \in \R^m$ et $\TnXn$, un estimateur de $g(\theta)$. On définit~:
			\[\MSE_\theta[\TnXn] \coloneqq \E_\theta\left[(\TnXn - g(\theta))(\TnXn - g(\theta))^T\right] \in \Mat(m, m).\]
			\end{déf}

			\begin{rmq} $\MSE_\theta[\TnXn] = \E_\theta\left[\left(T_i^{(n)}(\Xn)-g(\theta)\right)\left(T_j^{(n)}(\Xn)-g(\theta)\right)^T\right]$. En particulier~:
			\[\MSE_\theta[\TnXn]_{ii} = \MSE_\theta[T_i^{(n)}].\]
			\end{rmq}

			\begin{déf} Soient $A, B \in \Mat(m, n)$. On dit que $A \geq B$ lorsque $A-B \geq 0$, c-à-d lorsque la forme bilinéaire définie par $(A-B)$ est semi définie
			positive.
			\end{déf}

			\begin{déf} Soit $\mathcal C$, une classe d'estimateurs de $g(\theta)$. Alors $T_*^{(n)}$ est à erreur quadratique moyenne minimale dans $\mathcal C$
			lorsque~:
			\begin{itemize}
				\item[$(i)$]  $T_*^{(n)} \in \mathcal C$~;
				\item[$(ii)$] $\forall \Tn \in \mathcal C : \forall \theta \in \Theta : \MSE_\theta[\Tn] \geq \MSE_\theta[T_*^{(n)}]$.
			\end{itemize}
			\end{déf}

			\begin{prp} Pour tout $\theta \in \Theta, \Tn \in \mathcal C$, on a~:
			\[\MSE_\theta[\Tn] \geq 0.\]
			\end{prp}

			\begin{proof} Soit $v \in \R^m$. On trouve~:
			\begin{align*}
				v^T\MSE_\theta[\Tn]v &=v^T\E_\theta\left[\left(\Tn-g(\theta)\right)\left(\Tn-g(\theta)\right)^T\right]v
					= \E_\theta\left[v^T\left(\Tn-g(\theta)\right)\left(\Tn-g(\theta)\right)^Tv\right] \\
				&= \E_\theta\left[\left(v^T(\Tn-g(\theta))\right)\left(v^T(\Tn-g(\theta))\right)^T\right] \\
				&= \E_\theta\left[\left(v^T(\Tn-g(\theta))\right)^2\right] \geq 0.
			\end{align*}
			\end{proof}

			\begin{prp} Soient $T_1^{(n)}, T_2^{(n)}$, deux estimateurs de $g(\theta)$ tels que~:
			\[\MSE_\theta[T_1^{(n)}] \geq \MSE_\theta[T_2^{(n)}].\]
			Alors~:
			\[\forall v \in \R^m : \MSE_\theta[v^TT_1^{(n)}] \geq \MSE_\theta[v^TT_2^{(n)}].\]
			\end{prp}
			\newpage
			\begin{proof}
			\begin{align*}
				\MSE_\theta[v^TT_1^{(n)}] - \MSE_\theta[v^TT_2^{(n)}]
					&= \E_\theta\left[\left(v^TT_1^{(n)} - v^Tg(\theta)\right)^2\right] - \E_\theta\left[\left(v^TT_2^{(n)}-v^Tg(\theta)\right)^2\right] \\
				&= \E_\theta\left[v^T\left(T_1^{(n)}-g(\theta)\right)\right] - \E_\theta\left[v^T\left(T_2^{(n)}-g(\theta)\right)\right] \\
				&= v^T\MSE_\theta[T_1^{(n)}]v - v^T\MSE_\theta[T_2^{(n)}]v = v^T\left(\MSE_\theta[T_1^{(n)}] - \MSE_\theta[T_2^{(n)}]\right)v \geq 0,
			\end{align*}
			par définition de semi-positivité.
			\end{proof}
\end{document}
